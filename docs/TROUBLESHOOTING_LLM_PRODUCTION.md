# üîç Diagn√≥stico: LLM n√£o est√° funcionando em Produ√ß√£o

## ‚ùå Problema Identificado

As respostas do chat est√£o usando **templates b√°sicos** em vez da **LLM inteligente**.

## üîé Causa Raiz

O sistema tem esta l√≥gica de fallback:

```python
# Em SimpleChatEngine.__init__
self.llm_client = get_default_llm_client()
self.use_llm = self.llm_client is not None  # ‚Üê Se None, usa templates

# Em get_default_llm_client()
try:
    return create_llm_client()
except (ValueError, ImportError) as e:
    print(f"‚ö†Ô∏è  Could not initialize LLM client: {e}")
    return None  # ‚Üê Retorna None se falhar
```

**Quando retorna None:**
1. ‚úÖ Vari√°veis de ambiente n√£o configuradas
2. ‚úÖ Pacote `google-generativeai` n√£o instalado
3. ‚úÖ API key inv√°lida/expirada
4. ‚úÖ ImportError ao importar o pacote

## üìã Como Verificar o Problema

### 1. Endpoint de Debug Melhorado

Acesse: `https://dataforgetest-backend.onrender.com/api/simple/debug`

Procure pela se√ß√£o `llm_status`:
```json
{
  "llm_status": {
    "configured": false,  // ‚Üê Se false, est√° usando fallback!
    "client_type": null,  // ‚Üê Deveria ser "GeminiClient"
    "provider": "not set",  // ‚Üê Deveria ser "gemini"
    "model": "not set",  // ‚Üê Deveria ser "gemini-1.5-flash"
    "gemini_key_set": false,  // ‚Üê Deveria ser true
    "anthropic_key_set": false
  }
}
```

### 2. Verificar Logs do Render

No painel do Render:
1. V√° em **Logs**
2. Procure por:
   ```
   ‚ö†Ô∏è  Could not initialize LLM client: ...
   ‚ö†Ô∏è  No LLM configured. Using simple template responses.
   ```

3. Se encontrar, veja o erro espec√≠fico:
   - `google-generativeai package not installed` ‚Üí Problema no build
   - `GEMINI_API_KEY is required` ‚Üí Vari√°vel n√£o configurada
   - `Invalid API key` ‚Üí Chave incorreta

### 3. Comparar Respostas

**Com LLM (esperado):**
```
"PySpark is a Python library for Apache Spark, which allows for 
distributed data processing and machine learning at scale. It 
provides an API that enables users to write applications in 
Python while leveraging the power of Spark's computational engine..."
```

**Sem LLM (templates - problema atual):**
```
"Based on the documentation:

PySpark is the Python API for Spark. It enables you to write Spark 
applications using Python APIs.

This information comes from the knowledge base and should provide 
guidance for your data quality needs."
```

## ‚úÖ Solu√ß√µes

### Solu√ß√£o 1: Verificar Vari√°veis no Render

**No Render Dashboard ‚Üí Environment:**

```bash
LLM_PROVIDER=gemini
GEMINI_API_KEY=AIzaSy...  # Sua chave real
GEMINI_MODEL=gemini-1.5-flash
```

**Checklist:**
- [ ] `LLM_PROVIDER` est√° definida?
- [ ] `GEMINI_API_KEY` est√° definida?
- [ ] `GEMINI_MODEL` est√° definida?
- [ ] Chave come√ßa com `AIza`?
- [ ] Salvou e fez redeploy?

### Solu√ß√£o 2: Testar API Key Localmente

```bash
# Configure localmente
set GEMINI_API_KEY=AIzaSy...
set LLM_PROVIDER=gemini
set GEMINI_MODEL=gemini-1.5-flash

# Instale o pacote
pip install google-generativeai

# Execute o teste
python test_gemini.py
```

Se funcionar localmente mas n√£o em produ√ß√£o ‚Üí problema nas vari√°veis do Render.

### Solu√ß√£o 3: Verificar Build do Render

No Render, v√° em **Logs** e procure durante o build:

```
Successfully installed google-generativeai-X.X.X
```

Se n√£o aparecer:
1. Verifique se `requirements.txt` tem `google-generativeai`
2. Force um redeploy: **Manual Deploy ‚Üí Deploy latest commit**

### Solu√ß√£o 4: Fallback para Ollama (n√£o recomendado)

Se Gemini n√£o funcionar, pode usar templates:
```bash
# Remova as vari√°veis de LLM
# O sistema usar√° fallback templates automaticamente
```

‚ö†Ô∏è Mas isso n√£o resolve o problema, apenas aceita o fallback.

## üß™ Script de Diagn√≥stico

Execute localmente:
```bash
python diagnose_llm_production.py
```

Cen√°rios testados:
1. ‚úÖ Sem vari√°veis (default ollama - funciona localmente)
2. ‚ùå Com Gemini mas sem pacote (simula problema)
3. ‚úÖ Com Ollama mas sem servidor (cria client, mas falha ao gerar)
4. ‚úÖ RAG integration completa

## üìä Status Atual

Execute este comando no terminal do Render para verificar:

```bash
python -c "import os; print('Provider:', os.getenv('LLM_PROVIDER')); print('Gemini Key:', 'SET' if os.getenv('GEMINI_API_KEY') else 'NOT SET')"
```

Ou adicione log tempor√°rio em `simple_chat.py`:
```python
print(f"üîç LLM DEBUG: provider={os.getenv('LLM_PROVIDER')}, key_set={bool(os.getenv('GEMINI_API_KEY'))}")
```

## ‚úÖ Confirma√ß√£o de Sucesso

Ap√≥s configurar corretamente, voc√™ ver√° nos logs:
```
‚úÖ LLM initialized with provider: gemini, model: gemini-1.5-flash
```

E o endpoint `/debug` mostrar√°:
```json
{
  "llm_status": {
    "configured": true,
    "client_type": "GeminiClient",
    "provider": "gemini",
    "gemini_key_set": true
  }
}
```

## üéØ A√ß√£o Imediata

**Passo a passo:**

1. **Verificar endpoint de debug:**
   - Acesse: `https://dataforgetest-backend.onrender.com/api/simple/debug`
   - Anote os valores de `llm_status`

2. **Se `configured: false`:**
   - V√° no Render Dashboard
   - Environment ‚Üí Adicione as 3 vari√°veis
   - Save Changes ‚Üí Aguarde redeploy

3. **Se `configured: true` mas respostas ruins:**
   - Problema pode ser na API key
   - Teste a chave localmente primeiro

4. **Ap√≥s configurar:**
   - Aguarde deploy (~5-10 min)
   - Teste novamente o chat
   - Verifique `/debug` novamente

---

**Precisa de ajuda?** Compartilhe:
1. Output do `/debug`
2. Logs do Render (√∫ltimas 50 linhas)
3. Exemplo de resposta do chat
