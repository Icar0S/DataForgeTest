{
  "documents": {
    "328a07f5-149e-46ee-b0de-0163aedfcde3": {
      "id": "328a07f5-149e-46ee-b0de-0163aedfcde3",
      "content": "Apache Spark Best Practices for Data Quality\n\nIntroduction:\nApache Spark is a powerful distributed computing framework that excels at processing large datasets. When implementing data quality checks, following best practices ensures optimal performance and reliable results.\n\nKey Principles:\n\n1. Partition Strategy\n   - Use appropriate partitioning to avoid skewed data\n   - Consider partition size (aim for 128MB-1GB per partition)\n   - Use repartition() vs coalesce() appropriately\n\n2. Caching Strategy\n   - Cache DataFrames that are accessed multiple times\n   - Use appropriate storage levels (MEMORY_AND_DISK_SER)\n   - Unpersist when no longer needed\n\n3. Data Quality Patterns\n   - Implement schema validation early in the pipeline\n   - Use built-in Spark functions for better performance\n   - Avoid collect() on large datasets\n\n4. Error Handling\n   - Implement graceful degradation for data quality issues\n   - Log quality metrics for monitoring\n   - Use checkpoints for long-running processes\n\n5. Resource Management\n   - Configure executor memory and cores appropriately\n   - Monitor garbage collection patterns\n   - Use dynamic allocation when possible\n\nExample Code Patterns:\n\ndef validate_data_quality(df):\n    # Count nulls efficiently\n    null_counts = df.select([\n        sum(col(c).isNull().cast(\"int\")).alias(f\"{c}_nulls\")\n        for c in df.columns\n    ]).collect()[0]\n    \n    # Check for duplicates\n    total_rows = df.count()\n    distinct_rows = df.distinct().count()\n    duplicate_rate = (total_rows - distinct_rows) / total_rows\n    \n    return {\n        'null_counts': null_counts.asDict(),\n        'duplicate_rate': duplicate_rate,\n        'total_rows': total_rows\n    }\n\nPerformance Tips:\n- Use broadcast joins for small lookup tables\n- Prefer DataFrames over RDDs for better optimization\n- Use columnar formats like Parquet for better I/O\n- Enable adaptive query execution (AQE) in Spark 3.0+",
      "metadata": {
        "filename": "spark_best_practices.txt",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\spark_best_practices.txt",
        "file_size": 1974,
        "file_type": ".txt",
        "imported_at": "2025-12-17T21:23:23.679465",
        "content_length": 1918
      }
    },
    "bcd1c4e9-600f-4286-8599-481901d79c71": {
      "id": "bcd1c4e9-600f-4286-8599-481901d79c71",
      "content": "# Estratégias de Validação de Dados\n\n## Validação em Tempo Real vs Batch\n\n### Validação Batch\n- Processa grandes volumes de dados históricos\n- Permite análises complexas e estatísticas\n- Ideal para relatórios e auditorias\n- Menos recursos em tempo real\n\n### Validação em Tempo Real\n- Verifica dados conforme chegam\n- Permite correção imediata\n- Requer mais recursos de infraestrutura\n- Crítica para sistemas transacionais\n\n## Implementação com Kafka + Spark Streaming\n\n```python\nfrom pyspark.streaming import StreamingContext\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\ndef process_streaming_data(df, epoch_id):\n    # Validações em tempo real\n    \n    # 1. Schema validation\n    expected_schema = [\"id\", \"timestamp\", \"value\", \"status\"]\n    if set(df.columns) != set(expected_schema):\n        raise ValueError(\"Schema mismatch detected\")\n    \n    # 2. Business rules validation\n    invalid_records = df.filter(\n        (col(\"value\") < 0) | \n        (col(\"value\") > 1000) |\n        (col(\"status\").isNull())\n    )\n    \n    invalid_count = invalid_records.count()\n    if invalid_count > 0:\n        print(f\"Warning: {invalid_count} invalid records found\")\n        # Log para sistema de monitoramento\n    \n    # 3. Store valid records\n    valid_records = df.subtract(invalid_records)\n    valid_records.write.mode(\"append\").saveAsTable(\"clean_data\")\n\n# Setup streaming\nspark = SparkSession.builder.appName(\"DataQualityStream\").getOrCreate()\ndf = spark.readStream.format(\"kafka\") \\\n    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n    .option(\"subscribe\", \"data-topic\") \\\n    .load()\n\nquery = df.writeStream \\\n    .foreachBatch(process_streaming_data) \\\n    .start()\n\nquery.awaitTermination()\n```\n\n## Padrões de Quarentena\n\n### Isolamento de Dados Problemáticos\n1. **Quarentena Automática**: Dados que falham validação básica\n2. **Quarentena Manual**: Dados suspeitos para revisão humana\n3. **Quarentena Temporária**: Dados aguardando informações adicionais\n\n### Estrutura de Quarentena\n```\nquarantine/\n├── schema_errors/\n├── business_rule_violations/\n├── data_quality_issues/\n└── pending_review/\n```\n\n## Métricas de Qualidade\n\n- **Completude**: % de campos preenchidos\n- **Validade**: % de dados em formato correto\n- **Consistência**: % de dados consistentes entre fontes\n- **Precisão**: % de dados corretos\n- **Atualidade**: % de dados recentes",
      "metadata": {
        "filename": "data_validation_strategies.md",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\data_validation_strategies.md",
        "file_size": 2518,
        "file_type": ".md",
        "imported_at": "2025-12-17T21:23:23.687972",
        "content_length": 2378,
        "type": "markdown",
        "title": "Estratégias de Validação de Dados"
      }
    },
    "1f4646b3-0d61-4f89-94b5-2c5e534ce81c": {
      "id": "1f4646b3-0d61-4f89-94b5-2c5e534ce81c",
      "content": "# Guia de Testes de Performance\n\n## Introdução\n\nTestes de performance são essenciais para garantir que aplicações de dados funcionem adequadamente sob carga. Este documento aborda estratégias e técnicas para testar sistemas de big data.\n\n## Tipos de Testes de Performance\n\n### 1. Testes de Carga\n- Verificar comportamento sob carga normal\n- Identificar limites de capacidade\n- Monitorar tempo de resposta e throughput\n\n### 2. Testes de Stress\n- Testar além dos limites normais\n- Identificar ponto de quebra do sistema\n- Verificar recuperação após sobrecarga\n\n### 3. Testes de Volume\n- Grandes volumes de dados\n- Avaliar escalabilidade\n- Testar limites de armazenamento\n\n## PySpark para Performance\n\n```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\n\n# Configuração otimizada\nspark = SparkSession.builder \\\n    .appName(\"PerformanceTest\") \\\n    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n    .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n    .getOrCreate()\n\n# Monitoramento de performance\ndef monitor_query_performance(df, query_name):\n    start_time = time.time()\n    result = df.count()  # ou qualquer operação\n    end_time = time.time()\n    \n    print(f\"Query: {query_name}\")\n    print(f\"Tempo: {end_time - start_time:.2f}s\")\n    print(f\"Registros: {result}\")\n    \n    return result\n```\n\n## Métricas Importantes\n\n- **Latência**: Tempo de resposta individual\n- **Throughput**: Operações por segundo\n- **Utilização de CPU**: Percentual de uso\n- **Memória**: Consumo e garbage collection\n- **I/O**: Leitura/escrita de dados\n\n## Ferramentas de Monitoramento\n\n- Spark UI para análise de jobs\n- Ganglia para métricas de cluster\n- Grafana para dashboards\n- JProfiler para análise de JVM",
      "metadata": {
        "filename": "performance_testing.md",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\performance_testing.md",
        "file_size": 1889,
        "file_type": ".md",
        "imported_at": "2025-12-17T21:23:23.700967",
        "content_length": 1801,
        "type": "markdown",
        "title": "Guia de Testes de Performance"
      }
    },
    "877f4762-0d7c-4c31-ab5f-f50d0487235e": {
      "id": "877f4762-0d7c-4c31-ab5f-f50d0487235e",
      "content": "[Página 1]\nAdvancing beyond technicism\nwhen managing big data in\ncompanies ’decision-making\nFrancesco Caputo, Barbara Keller, Michael Möhring, Luca Carrubbo andRainer Schmidt\nAbstract\nPurpose –In recognising the key role of business intelligence and big data analytics in influencing\ncompanies’ decision-making processes, this paper aims to codify the main phases through which\ncompanies can approach, develop and manage big data analytics.\nDesign/methodology/approach –By adopting a research strategy based on case studies, this paper\ndepicts the main phases and challenges that companies ‘‘live’’ through in approaching big data analytics\nas a way to support their decision-making processes. The analysis of case studies has been chosen as\nthe main research method because it offers the possibility for different data sources to describe aphenomenon and subsequently to develop and test theories.\nFindings –This paper provides a possible depiction of the main phases and challenges through which\nthe approach(es) to big data analytics can emerge and evolve over time with reference to companies’decision-making processes.\nResearch limitations/implications –This paper recalls the attention of researchers in defining clear\npatterns through which technology-based approaches should be developed. In its depiction of the main\nphases of the development of big data analytics in companies’ decision-making processes, this paper\nhighlights the possible domains in which to define and renovate approaches to value. The proposed\nconceptual model derives from the adoption of an inductive approach. Despite its validity, it is discussedand questioned through multiple case studies. In addition, its generalisability requires further discussion\nand analysis in the light of alternative interpretative perspectives.\nPractical implications –The reflections herein offer practitioners interested in company management\nthe possibility to develop performance measurement tools that can evaluate how each phase can\ncontribute to companies’ value creation processes.\nOriginality/value –This paper contributes to the ongoing debate about the role of digital technologies in\ninfluencing managerial and social models. This paper provides a conceptual model that is able to\nsupport both researchers and practitioners in understanding through which phases big data analytics\ncan be approached and managed to enhance value processes.\nKeywords Big data, Big data analytics, Companies’ decision-making, Smarter management\nPaper type Technical paper\n1. Preliminary reflections\nIn the past few decades, socio-economic configurations have profoundly changed because\nof the increasing use and accessibility of Information and Communication Technologies\n(ICT) in multiple domains of everyday life ( Forester, 1987 ;Turban et al. ,1 9 9 8 ;Drucker, 2011 ;\nCaputo et al. , 2019b ). Consolidated views based on the representation of technologies for\ndata management as “simple instruments” for supporting decision-making activities have\nprogressively shown that they are incapable of explaining ongoing dynamics and trends\n(Caputo et al. , 2019c ). Similarly, new interpretative approaches and managerial models are\nstrongly required by researchers and practitioners interested in effectively understandingFrancesco Caputo is based at\nthe Department of Economics,Management and Institutions,University of Naples Federico\nII, Naples, Italy.\nBarbara Keller is based at theDuale Hochschule Baden-Wu¨rttemberg Stuttgart,\nStuttgart, Germany.Michael Mo ¨hring is based at\nthe Department of Informatics –\nHHZ Reutlingen University,Reutlingen, Germany.Luca Carrubbo is based at theDepartment of Managementand Innovation Systems,University of Salerno, Salerno,Italy.\nRainer Schmidt is based at the\nDepartment of ComputerScience and Mathematics,University of Applied SciencesMunich, Munich, Germany.\nReceived 8 October 2022\nRevised 26 January 2023Accepted 25 February 2023\nCorrigendum : It has come to\nthe attention of the publisher\nthat the article: Caputo, F.,\nKeller, B., Mo ¨hring, M.,\nCarrubbo, L. and Schmidt, R.(2023), “Advancing beyondtechnicism when managing bigdata in companies’ decision-making”, Journal of Knowledge\nManagement , Vol. ahead-of-\nprint No. ahead-of-print.\nhttps://\ndoi.org/10.1108/JKM-10-2022-\n0794 did not accurately display\nMo¨hring, M.‘s affiliation.\nOur guidelines state that\naffiliations should be supplied\nin full when the article issubmitted.\nThe city corresponding to\nReutlingen University has been\namended from Munich toReutlingen.\nDOI10.1108/JKM-10-2022-0794 VOL. 27 NO. 10 2023, pp. 2797-2809, ©Emerald Publishing Limited, ISSN 1367-3270 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2797\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 2]\nwhat the main implications, consequences and effects of the increasing use of ICT in\nbusiness and social dynamics are ( Castells, 1999 ;Markus and Topi, 2015 ).\n[2015] Building upon this widely recognised need, in recent decades, a challenging debate\nhas emerged around the topic of big data analytics as “a way of extracting value from thesehuge volumes of information, and it drives new market opportunities and maximizes\ncustomer retention” (\nZakir et al. ,2 0 1 5 , p. 81). Several contributions have been provided\nwith reference to the multiple advantages that it is possible to obtain for companies from a“new” approach in the collection, coding and management of data related to the multiple\ndimensions of shopping expeditions and evaluations (\nGriffin et al. ,2 0 0 0 ;Mummalaneni,\n2005 ;Demangeot and Broderick, 2006 ;Amendola et al. , 2018 ;Ardito et al. ,2 0 1 8 ). Multiple\nstimuli for reflections have also been provided with reference to the ways in which people,processes and technologies can be combined to improve the quality of companies’ and\nmarkets’ approaches in data collection and use (\nAlter, 2006 ;Singh and Del Giudice, 2019 ).\nAs effectively summarised by Demchenko et al. (2012 , p. 614), “Data Science is becoming\na new technology driver and requires re-thinking a number of infrastructure, components,solutions and processes to address the following general challenges: Exponential growth of\ndata volume produced by different research instruments and/or collected from sensors;\nNeed to consolidate e-Infrastructure as [a] persistent research platform to ensure researchcontinuity and oration, deliver/offer persistent services, with [an] adequate governancemodel.” According to the authors’ reflections, the challenging domain about big data should\nmainly refer to the infrastructure and processes required for ensuring the effective collection\nand organisation of a huge volume of data.\nDespite the relevance of the aforementioned dimensions, it only represents a “small” part of\nthe multiple reflections that seem to require the ongoing transitions towards a knowledge\nera based on technology infrastructure. Several relevant elements related to human\napproaches to big data, the consequences of big data analytics in companies’ decision-making processes and the antecedents capable of addressing the ongoing digital transition(\nCaputo et al. , 2019a ;Chinnaswamy et al. ,2 0 1 8 ), among others, seem to be vastly\nunderestimated. Accordingly, the paper proposes extending current perspectives in the\nstudy of big data analytics by focusing attention on the intriguing domain of big dataanalytics, specifically “the extraction of hidden sight about consumer behaviour from bigdata and the exploitation of that insight through advantageous interpretation” (\nErevelles\net al. ,2 0 1 6 , p. 897). Thanks to the adoption of a research strategy based on case studies,\nthe paper aims to depict the main phases that companies face in the process of reshapingdecision-making processes through big data analytics. The analysis of case studies has\nbeen chosen as the main research method because it offers the possibility for different data\nsources to describe a phenomenon and subsequently to develop and test theories.\nThe paper is structured as follows. In Section 2, the theoretical background will be\npresented by focusing attention on smart management and on the role of big data analytics\nin companies’ decision-making processes as relevant domains with reference to which\nproposed reflections have been developed. In Section 3, the method and data collection ofthe proposed research will be reported, whilst in Section 4, the results of the proposedresearch will be summarised to enrich the current debate about the role of big data\nanalytics in reshaping companies’ decision-making processes. Finally, in Section 5, the\nstudy’s preliminary conclusions, main limitations, implications and possible future directionswill be presented.\n2. Theoretical background\nThe way in which organisations apply data analysis has changed over time ( Chen et al. ,\n2012 ). In recent years, different methods have been developed that depend on the different\ndata sources and related data structures.\nPAGE 2798jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 3]\nIn general, different data sources with structured and/or unstructured data can be part of\nbig data projects ( Gandomi and Haider, 2015 ). In the past, enterprises were only able to\nanalyse structured datasets like customer order data coming from, for example, CRM orERP systems (\nChen et al. , 2012 ). The data used for analyses mainly consisted of numbers\nor categorial variables, for example. The way of collecting, storing and analysing data was\nless complex in comparison to more recent data sources containing unstructured data(\nBuneman et al. , 1997 ;Blumberg and Atre, 2003 ;Baker and Thien, 2020 ;Del Giudice et al. ,\n2021 ). Today, however, up to 90% of the collected data is unstructured data like texts,\nimages, audio and video ( Harbart, 2021 ). The analysis of unstructured data is currently\nchallenging organisations because of its unsuitability for use in conventional data models(\nHarbart, 2021 ). The use of unstructured data together with structured data is manifold. For\ninstance, it can be used to improve the quality and the possibilities of prediction within big\ndata analytics ( Davenport et al. , 2021 ). Nevertheless, the more data types are included in\nanalytical projects, the more different methods must be used. Today, more and more IoT-related data sources like connected home appliances (\nBayer et al. , 2020 ) or services like\nGoogle Popular times ( Mo¨hring et al. ,2 0 2 0 ) can be used to predict and better understand\ncustomer behaviour. These new data sources must be integrated within the analytical\nlandscape to be used in related analysis. Another interesting use case that highlights thechallenges of the benefits of big data analytics is product returns in e-commerce. This fieldis even more important because it meets both customer behaviour and the sustainability\nconcept, as well as helping to easily understand the facets appearing in big data analysis.\nFor instance, if an organisation wants to use online customer reviews (unstructured textualdata) to predict the product returns probability (\nSchmidt and Mo ¨hring, 2013 ;Mo¨hring et al. ,\n2013 ), past customer order data from the CRM and ERP system (structured data) as well as\nimages (unstructured image data) from offered goods should also be integrated into the\nanalysis to enhance the quality of the prediction. Therefore, they must apply differentmethods like text mining for textual data, image pattern recognition for images and\ntraditional data mining techniques like regression or correlation analysis. In turn, this means\nthat different results, various key figures and quality criteria must be aggregated andharmonised within one comprehensive result (\nKaur et al. , 2019 ).\nFurthermore, the data must be stored in different locations like relational databases for the\norder data and/or within NoSql databases ( Stonebraker, 2010 ) like document-based\ndatabases for textual data. In sum, all these requirements will increase the complexity of big\ndata analytics projects and generate challenges for organisations running an analytical\nproject. In line with the identified methodological complexity and storing issues, thecomputational complexity also increases. The more variables are included in analyticalapproaches, the more steps for information processing and result calculation are\nnecessary. Therefore, organisations that are considering applying big data analytics must\nexplore the option of scalable public cloud computing services at major sites like AmazonAWS, Microsoft Azure and Google Cloud to capture the limitations of traditional non-scalable systems (\nSchmidt and Mo ¨hring, 2013 ).\n2.1 Challenges and dynamics of smart management\nNowadays, the dynamics in decision-making in all contexts are increasingly guided and\nconditioned by the reception, filtering, processing and use of data ( Raisinghani, 2000 ). The\nevolution of new technologies favours the development of virtuous processes [thanks to bigdata analytics techniques, data mining, machine learning, artificial intelligence (AI), etc.]\nthat support decision-making processes (\nNutt, 2008 ;Yang et al. , 2019 ). The growing\nuncertainty in all application areas accentuates the importance of the way in whichdecisions are made, especially if they involve significant consequences for the community.Decision making is a multidisciplinary topic that lends itself to different levels of analysis\nVOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2799\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 4]\nwhen we focus on the various elements (including technological ones) that condition or\nfacilitate it ( Papadakis et al. ,1 9 9 8 ).\nDecision-making processes are increasingly data-driven. Therefore, the decisions are more\n“informed” because the exchange of information is rapid (often in real-time); hence, it canbe precise, punctual, efficient and valid. From the electronic medical record (now fully\noperational) to the development of information systems to new communication protocols, it\nis possible to record a continuous flow of data, information (and the contained in it) and the\ninputs to be filtered, processed, used and managed in a timely manner (\nSharma et al. ,\n2014 ). The risk of “data-deluge” and the difficulty of having useful elements is very high,\nwhile the possibility of making quick, accurate, thoughtful decisions becomes more and\nmore necessary, indeed fundamental ( Sabherwal and King, 1995 ;Citroen, 2011 ). In this\nsense, the evolution of decision-support-systems (DSS) assumes increasing importance inmany critical “moments”, both for descriptive-analytics (e.g. diagnostics, evaluations and\nmonitoring), as well as in the follow-up analytics in the operational phases and even for\nforecasting possible choices in the future and related reasons through predictive analyticsand prescriptive analytics (\nBoonstra, 2003 ).\nIn general, information sharing with shared databases, data-storage, data extraction and data\nprocessing favours the design of a more functional, versatile, scalable, context-friendly service\nprovision, where the smart management can make a difference thus deserves to be furtherexplored. For this, it becomes important to study the main characteristics of data that can be\nacquired. Here, the so-called “10V[s] of big-data” (Volume, Velocity, Variety, Veracity, Value,\nValidity, Variability, Venue, Vocabulary and Vagueness) are often taken into consideration tounderstand how new knowledge is generated and, consequently, how much decision-making\nprocesses are affected; particularly with reference to the possible advantages of meta dating,\ndata modelling, architecture and data integration (\nManogaran et al. ,2 0 2 2 ).\nSimilarly, the most frequently used methods are studied to improve decision-making from\ndata management’s point of view. Typical topics of interest here are cloud computing for\ninformation sharing, artificial intelligence for the data interpretation available and the\ngeneration of new ones like data mining and machine learning. The aim here is to betterunderstand how the information flow works, what criticalities it presents, how it feeds the\nactivation and management of known protocols, how it integrates the various data-sources\nand how it supports the management of queries (\nHicks et al. ,2 0 0 6 ).\nAll this effectively integrates decision-making techniques (cost benefits, grid analysis, paired\ncomparison, compensatory strategies, etc.), with particular reference to conditions of uncertainty\nbecause of, for example, systematic errors, cognitive biases, risk situations, external distortions,\ninformation asymmetries, misalignments, internal friction, misunderstandings, technical oradministrative misunderstandings, legal aspects, technological crashes or even weak signals\nescaping, somatic markers and negative contingencies.\nThese issues are so fundamental and interesting that in the period between 2021 and 2027,\nEuropean investments will be geared towards building a smarter Europe throughinnovation, digitalisation, economic transformation and support for small- and medium-\nsized enterprises. EIT Digital has launched the 2022 call to promote entrepreneurship and\neducation for the construction of a strong digital Europe and contribute to the developmentof digital technology, digital industry, digital cities, digital wellbeing and digital finance.\nSince 2014, the European Commission has spoken out in favour of a thriving data-driven\neconomy (\nEuropean Commission, 2014 ); in 2015, it discussed a strategy for the digital\nsingle market in Europe ( European Commission, 2015 ). In 2018, the International Data\nCorporation estimated an increase of 16 trillion gigabytes of data, with an annual growth\nrate of 236% in terms of data generation to date; they linked this to the fact that decisions\nbased on knowledge generated by big data can lead to increased productivity andcompetitiveness and GDP (equal to 1.9% by 2020) (\nReinsel et al. , 2018 ).\nPAGE 2800jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 5]\nToday, the evolving trend of Big Data Analyses is an integral part of a new digital market.\nAccording to the European Commission, it guarantees the development of innovative and\ncompetitive business models. However, while having to comply with the EU data protectionframework, big data can involve significant risks and challenges, especially in fundamentalrights like privacy and data protection. More recently, the European Parliament discussedthe role of the data-based economy in the strategy for the digital union against the backgroundof all stakeholders and their daily life situations, such as consumers (ease of use, efficiency\nand savings), businesses (industry 4.0) and public administration (e-government), housing\n(smart cities), science, medicine (Mhealth), disaster response capacity and the fight againstcrime, etc.\n2.2 Big data in companies’ decision-making processes\nScientists and researchers have long since faced the challenges of data management,focusing their attention on possible ways to collect data both directly and indirectly(\nSapsford and Jupp, 1996 ;Hajian and Domingo-Ferrer, 2012 ). Several experiments have\nbeen conducted aiming to define the processes and protocols that enhance the\neffectiveness of data collection as a relevant way to extend consolidated knowledge aboutthe reasons, antecedents and motivations behind actors’ behaviours and decisions inmultiple domains (\nGrant and Mayer, 2009 ;Guiot and Roux, 2010 ;Daunt and Harris, 2012 ;\nRahrovani and Pinsonneault, 2020 ). Along this line, studies focusing on companies’\ndecision-making have also been developed and multiple approaches for collecting andanalysing data have been investigated (\nGoulding, 1999 ;Rokka and Uusitalo, 2008 ;Pac¸o\nand Lavrador, 2017 ).\nNowadays, all these approaches and contributions seem to be outmoded against the\nbackground of the disruptive role of big data analytics in the data and knowledge managementprocesses (\nPauleen and Wang, 2017 ). Today, big data infrastructure supports the handling of\ndata operations by facilitating the source’s integration and collaboration in real time with highstandards for control and data safety (\nSagiroglu and Sinanc, 2013 ).\nDemchenko et al. (2014 , p. 105) reports “the Big Data definition as having the following 5V\nproperties: Volume, Velocity, Variety that constitute native/original Big Data properties, andValue and Veracity as acquired as a result of data[’s] initial classification and processing in\nthe context of a specific process or model.” These properties effectively summarise the\nrelevant contributions that big data can provide the management of a high volume of data inreal time without “damaging” the granularity of information to ensure a realisticrepresentation of the phenomenon (\nPolyakova et al. ,2 0 1 9 ).\nAccording to Erevelles et al. (2016) , the properties of big data seem to provide a valuable\nsolution for organisations striving to find an answer to environmental and social changesthrough predictive approaches about market trends. More comprehensively, big data offersorganisations the opportunities to increase:\n/H17039their dynamic capabilities –their “ability to respond to change incorporates skills and\nknowledge embedded within the organization to alter existing resources and createnew value” ( Erevelles et al. , 2016 , pp. 898 –899); and\n/H17039adaptive capabilities –as capabilities that do not derive “from a specific change in\norganizational structure but from the overall ability to capture consumer activities andextract hidden in-sights” ( Erevelles et al. , 2016 , p. 899).\nRecognising the disruptive role of big data in reinventing firms’ market approaches, it is\npossible to underline its contribution in supporting enterprises in innovating theirrelationships with the market by focusing on the “implementation of creative ideas”(\nGumusluoglu and Ilsev, 2009 , p. 61). From this perspective, big data analytics can be seen\nas a valuable approach that supports firms to enforce their relationship by focusing on the\nVOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2801\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 6]\ndefinition of the innovation management path based on their “ability to effectively acquire\nand exploit new information” ( Chaston et al. ,2 0 0 1 , p. 147). Data acquisition and exploitation\nbecame the bridge with the capacity to link innovation management, information management\nand market analysis under the common umbrella of big data analytics; this offers the\nopportunity to understand current interest in developing an effective model for information\nmanagement, allowing firms to better understand (and predict) market trends and\nexpectations based on big data analytics ( Erevelles et al. ,2 0 1 6 ).\nIn a nutshell, big data can be considered a disruptive innovation ( Caputo et al. ,2 0 1 7 ) that is\npotentially able to reinvent firms’ approach to market analysis. Accordingly, Davenport et al.\n(2012 , p. 43) stated that big data supports firms “to understand their business environments\nat a more granular level, [ ...] creating new products and services, and [ ...] responding\nmore quickly to change as it occurs.” As a result, a new challenge emerges concerning how\nto decode the pattern for companies’ decision-making processes through big data\nanalytics.\n3. Method and data collection\nWith the aim to enrich current debate about the role of big data in companies’ decision-making, a case study approach was set as the research strategy (\nKohlbacher, 2016 ). The\nreasons why this approach was chosen are multi-faceted. On the one hand, the approach\nfollows the recommendations of Yin (2003) , who described the importance of case study\nresearch when a contemporary phenomenon is investigated in its real-world setting, and the\nboundaries between the phenomena itself and the related context are blurred. As a matter\nof fact, this method allows for a variety of research methods ( Yin, 2003 ;Kohlbacher, 2016 ).\nCase studies allow researchers to combine different data sources (such as interviews, texts\nand observations), as well as using qualitative and quantitative data analysis. Therefore,\nthey can be used to describe a phenomenon and Subsequently to develop and test\ntheories ( Darke et al. ,1 9 9 8 ).\nA widespread procedure is to use case studies in qualitative inquiries ( Stake, 2000 ;\nKohlbacher, 2016 ). This is especially relevant in contexts where the “why” and the “how” of\na phenomenon are the focus of an investigation. Consequently, a case study research\nstrategy with a qualitative inquiry thus seems to be an appropriate approach for an\ninvestigation and the provision of new insights. It is therefore unsurprising that case studies\nare an appropriate and popular way of investigating the implementation and use of\ninformation systems within organisations. This is particularly true in information systems\nresearch and related scientific areas, in which it is quite important to examine and\nunderstand the context of the phenomenon, because often researchers are unclear about\nhow a phenomenon arises or how individuals’ experiences and doings are critical to its\nactions and effects. Furthermore, numerous research approaches demand that with\nregards to the research question the number and topic of the cases must be determined at\nthe outset. Whilst a single case study is applied to gain deep and rich insights, multi-case\nstudies have the advantage of allowing replications (literal, theoretical) and comparisons\nbetween cases ( Darke et al. ,1 9 9 8 ).\nHere, a topic highly related to information systems research is investigated. Besides\nmanagerial and human factors, the research question also aims to understand the technical\nissues and their related problems. Following the recommendations given in the literature, as\ndescribed previously, a multiple case study research strategy was chosen as an appropriate\napproach in line with our research question. As the research focuses on different aspects, a\nsingle case study approach did not seem to be appropriate to best gain the desired insights\nabout the subject. Therefore, multiple cases were investigated by collecting different data from\ndifferent sources and conducting a qualitative analysis ( Yin, 1994 ,2012 ).\nPAGE 2802jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 7]\nConsequently, three different cases were examined. The investigated cases were a\nmanufacturing enterprise, an enterprise from the IT sector and a supplier for IT solutions. It\nis assumed that all the branches are equally affected by the challenges of implementing big\ndata analytics. In addition, the cases highlight and clarify that all sectors are affected by thechallenges of Big Data Analysis. The IT sector is no exception. The investigated enterpriseshave different sizes and turnovers. This circumstance is useful in terms of the generalisabilityof the findings. More details about the companies’ characteristics are reported in\nTable 1 .\nIn all cases, the process to implement the possibility of big data analytics was accompanied\nand supported by at least one of the researchers. As a result, a minimum of one person wasinvolved as an “action researcher” within the organisations (\nWalsham, 1995 ). Subsequently,\nboth the data and the contextual insights gathered are very rich and useful. Every case wascomprehensively investigated and hence a strong understanding of the phenomenon wasachieved (\nDarke et al. , 1998 ). Furthermore, the action researchers accompanied different big\ndata analytics projects within the companies chosen as cases. This allowed them to prove andcontrol the generalisability of the insights and findings in different settings (\nDarke et al. ,1 9 9 8 ).\nAs recommended in the literature, different data sources such as observations, interviews andquestionnaires were picked-up and combined (\nDarke et al. ,1 9 9 8 ). An overview about the data\nsources used in this investigation is provided in Table 1 .\nFor the data analysis, the Grounded Theory approach was conducted ( Strauss and Corbin,\n1994 ). This approach is very common and widespread in Information Systems research\n(Aarnikoivu et al. , 2019 ). In the first step, the open coding process was conducted. The data\nwas investigated, and the relevant aspects were tagged with abstract labels. This step isfollowed by the so-called axial coding process. As the second step of the procedure, the axialcoding process examines the relationships between the labels and tries to build networkscontaining relevant aspects. Hence, the identified labels were aggregated and networks werebuilt. In the third step, selective coding was applied, meaning that the networks were subsumedinto categories. In each step, all the team members did the coding process alone and theresults were discussed afterwards.\n4. Results\nThe data analysis revealed that in all cases along the project’s timeline specific patternsoccurred at special points in time. The findings are summarised in\nTable 2 and explained in\nmore detail subsequently.\nPhase (a) : Nearly all enterprises have recognised that the customer data they own is a\nhidden gem. Hence, it is not surprising that companies want to exploit this potential.\nConsequently, organisations have recognised the need for big data analytics to realise thebenefits provided by the data. Often, the top management takes the initiative to createplans for big data analytics projects. They set ambitious goals and objectives thatfrequently consist of a mix of dreams, wishes and reality. In many cases, the intended big\nTable 1 Overview of case studies and data gathering process\nEnterprise no. 1 (case 1) Enterprise no. 2 (case 2) Enterprise no. 3 (case 3)\nSector Manufacturing IT IT solution supplier\nCompany size Large Medium Small\nNo. employees >550 >200 63\nTurnover /C24200 Mio e /C24200 Mio e /C245 Mio e\nObservations by accompanying/supportive researcher x x x\nCross-divisional e-mail traffic x x x\nInterviews and expert talks x x x\nSurveys x\nSource: Authors’ elaboration\nVOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2803\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 8]\ndata analytics projects are not realisable for several reasons. Firstly, the company lacks\nconcrete processes, possibilities and outcomes along with the initial vague and imaginative\nassumptions. Hence, big data analytics projects begin similarly and specific requirements\nare often not respected because of the company’s inexperience with such projects.\nSubsequently, wrong estimations in terms of budget and staffing, as well as time and scope\noccur. In addition, some of the most prominent aspects in big data analytics projects are also\nneglected. Furthermore, the availability of data is a crucial factor that is often misjudged.\nOrganisations trust in their databases. However, it is not uncommon for data to be unusable\nbecause of poor data management and questionable data quality. There are also often\nassumptions about data sources that do not, in fact, exist in the reality of the company. In one\nof the cases in this study, an expert in case (1) stated that the management proclaimed that all\nthe needed data is stored and available in their proAlpha ERP system. However, it turned out\nthat this was a false estimation from the management. Even if the data is available, wrong\njudgement can be taken as case (3) revealed. The responsible persons in case (3) assumed\nthat they have high quality data about their customers and their behaviour. Although data\nabout the customers was available, it did not meet the requirements. Relevant aspects of\ncustomers’ behaviour were missing and, therefore, the potential for the analysis was quite\nrestricted.\nPhase (b) : Once a project is started, challenges because of human factors, as well as\ntechnical issues arise. On the human side, the challenges are two fold. On the one hand, it\nmight be that the assigned employees did not have the relevant knowledge for conducting\nthe project or cannot be identified. During the project, the management of case (1)\ndiscovered that their internal staff were not able to implement the AI models into their\nsystems. Therefore, they had to find an external service provider who was able to cope with\nthis challenge. On the other hand, missing openness and/or a restricted mindset are a\ncritical human factor too. This often results in staff hiding their knowledge to avoid changes\nthat could lead to more work or that has a negative impact on their job position.\nBesides challenges occurring because of human factors, we also observed technical\naspects that were crucial for the continuation of big data analytics projects. On the technicalTable 2 Main results about companies ’approach to big data\nPhase (a.): Before/at the\nbeginning of the project Phase (b.): During the projectPhase (c.): At the\nend/ finalization of\nthe project\nNeed for big data analytics Staff with adequate\nknowledge is missing or\ncannot be foundNot all requirements/\nautomation tasks\ncan be fulfilled\nMix-up of dreams, wishes\nand realityMissing openness/restricted\nmindsetPredictions by the\nalgorithms are not\nalways better than\nthe human ones\nBudget and available staff Data sources (e.g.\ndatabases) do not fitUsability issues\nImplementation/time horizon Identification of the best Big\nData algorithm(s)Time, costs and\neffort was\nunderestimated (run\nof time and budget)\nTrust in databases Must re-design the project\nand re-start\nIT infrastructure is old and notflexible\nData protection rules\nSource: Authors’ elaboration\nPAGE 2804jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 9]\nside, it might be the case that the database does not even contain the expected data or that\nthe data did not fit the requirements, as described previously. In many cases, the missingdata cannot be procured because the IT infrastructure is too old and inflexible. There aremissing interfaces, hence new analytical systems can connect to it and collect the dataneeded for analysis (in cases 1, 2 and 3). Modern standard application interfaces like\nREST-APIs (\nMasse, 2011 ) were not provided, which hindered the seamless collection of\ndata. Furthermore, the implementation of modern big data analytic and data visualisationtools into old systems might be difficult.\nBoth human and technical factors might stop or delay the project. In all cases (1) –(3), it was\nhard to find the correct experts with business and domain specific know-how. In cases (1)\nand (3), often the most suitable employees for the task were also not known by themanagement. Sometimes, a step back to the first phase was needed to re-define theresponsibilities and even the technical possibilities. In cases (1) and (3), the project had to\nbe restarted (a). In case (1), adjustments during the project were done. Hence, the aim of\nthe project must be reviewed and re-defined. Another aspect that sometimes occurs is thatthe best algorithm cannot be found. In all cases (1) –(3), there was no generally available\nalgorithm or approach fitting the project’s goal that would deliver a result within theexpected quality range from the very beginning. Furthermore, the available IT infrastructure\nresources (e.g. CPU, RAM, disk) for the analysis hindered the evaluation of different\nalgorithms. For example, (sample) data was split and patterns were reconstructed toevaluate the algorithms. Different algorithms were combined in all cases to accommodateissues such as linear and non-linear behaviour (e.g. linear regression and neuronal\nnetworks) and selected based on different rules (rule-based algorithm selection and\ncombination), as well as patterns that could only be identified during the actual dataanalysis. For instance, after starting the project the responsible persons in case (1) foundout that their systems could not be used to run analytical services. They did not anticipate in\nadvance that the necessary infrastructure capabilities (e.g. CPU/RAM) would be missing.\nPhase (c) : In the final project phase, further patterns were identified within the selected\ncases. Regarding the definition and targets of the big data projects at the beginning of theproject, not all requirements and automation tasks could be fulfilled. This is often a\nconsequence of the fact that the challenges from the two preceding project phases could\nnot be sufficiently taken into account. In cases (1) and (3), only a minor set of requirementscould be fulfilled because of the issues in the prior project phases. It was only in case (2)that important requirements during the project could be delivered. Sometimes, theprediction of human experts with years of experience is faster and more accurate\ncompared to the developed systems. This might mean that not all the necessary data is\navailable and the data behaviour patterns may not be recognised by the system. This wasparticularly true of case (1), where the system was not accurate compared to experiencedexperts. The developed big data systems are very complex. Therefore, their usability and\nuser friendliness are severely limited. Experts must configure the systems in advance by\nentering specific parameters. Consequently, the staff must be trained to use the system andto interpret the results with regards to the specific business demands. In all cases (1) –(3),\nthe effort needed to complete the project, in terms of, for instance, time, costs and budget,\nwas underestimated. In cases (1) and (3), the project ran out of time and budget and had to\nbe adjusted. Again, this might be a consequence of the identified patterns in the first twophases of the projects (a) and (b).\n5. Conclusions, implications, limitations and future research\nIn the past few years, big data and big data analytics tools have been presented as the new“miracle” for efficiency, survival and increased performance for any type of organisedentities (\nSchmarzo, 2013 ). These approaches attracted the interest of multiple researchers\nand the investment of multiple companies interested in the possibility of obtaining multiple\nVOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2805\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 10]\nadvantages by simply buying new instruments, software and digital devices. Despite the\nsummarised scenarios, the proposed research shows different scenarios in which collectedand analysed data demonstrate that the predictions made by the algorithms do not\nnaturally offer value in isolation. Sometimes, human predictions are even better because\nthey can involve more variable factors and are more intuitive.\nIn such a perspective, the research offers several practical implications because it\nunderlines how automation may not even be possible, and several manual steps are\nneeded as the usability of the tool decreases. Sometimes, users cannot work with the\nsystem because it is hard to handle or because they are not able to interpret the output of\nthe system and relate it to adequate strategical or operational measures. In addition,because of delays and re-definitions the project may run out of time and budget. Thus, the\nexpenses overcome the estimated benefit. Sometimes, projects must even be abandoned.\nFurthermore, issues related, e.g. to the technical foundation of the enterprises, used\nalgorithms and data quality hinder a good implementation and positive value of the system.\nIn the same perspective, the research also underlines several theoretical implications by\nascertaining that to run a big data analytics project successfully it is important to focus on the\nchallenges and anticipate consequences. Therefore, current interpretative paths and managerial\nmodels require radical rethinking to better catch and depict the interconnections that could be\npossible between humans and technology.\nDespite the conceptual and empirical advancements in the knowledge offered by the\nreflections herein, several limitations can be identified with reference to the proposed research\napproach because the results offered by the analyses of the case studies are subjective and\nrelated to the background in which they have been approached and analysed. In such a vein,the next steps for the research are required to test to what extent the proposed results and\nobservations can be generalised to different cognitive and geographical domains.\nReferences\nAarnikoivu, M., Nokkala, T., Siekkinen, T., Kuoppala, K. and Pekkola, E. (2019), “Working outside\nacademia? Perceptions of early-career, fixed-term researchers on changing careers”, European Journal\nof Higher Education , Vol. 9, pp. 172-189.\nAlter, S. (2006), The Work System Method: Connecting People, Processes, and IT for Business Results ,\nWork System Method.\nAmendola, C., Calabrese, M. and Caputo, F. (2018), “Fashion companies and customer satisfaction: a\nrelation mediated by information and communication technologies”, Journal of Retailing and Consumer\nServices , Vol. 43, pp. 251-257.\nArdito, L., Scuotto, V., Del Giudice, M. and Petruzzelli, A.M. (2018), “A bibliometric analysis of\nresearch on big data analytics for business and management”, Management Decision ,V o l .5 7\nNo. 8, pp. 1993-2009.\nBaker, O. and Thien, C.N. (2020), “A new approach to use big data tools to substitute unstructured data\nwarehouse”, 2020 IEEE Conference on Big Data and Analytics (ICBDA) , IEEE, pp. 26-31.\nBayer, S., Gimpel, H. and Rau, D. (2020), “IoT-commerce-opportunities for customers through an\naffordance lens”, Electronic Markets , Vol. 31 No. 1, pp. 27-50.\nBlumberg, R. and Atre, S. (2003), “The problem with unstructured data”, Dm Review , Vol. 13, pp. 42-49.\nBoonstra, A. (2003), “Structure and analysis of IS decision-making processes”, European Journal of\nInformation Systems , Vol. 12 No. 3, pp. 195-209.\nBuneman, P., Davidson, S., Fernandez, M. and Suciu, D. (1997), “Adding structure to unstructured data”,\nInternational Conference on Database Theory, Springer, pp. 336-350.\nCaputo, F., Cillo, V., Candelo, E. and Liu, Y. (2019a), “Innovating through digital revolution: the role\nof soft skills and big data in increasing firm performance”, Management Decision , Vol. 57 No. 8,\npp. 2032-2051.\nPAGE 2806jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 11]\nCaputo, F., Evangelista, F., Perko, I. and Russo, G. (2017), “The role of big data in value co-creation for\nthe knowledge economy”, in Vrontis, S., Weber, T., Tsoukatos, E. (Eds), Global and National Business\nTheories and Practice: bridging the past with the Future , EuroMed Press, pp. 269-280.\nCaputo, F., Garcia-Perez, A., Cillo, V. and Giacosa, E. (2019b), “A knowledge-based view of people and\ntechnology: directions for a value co-creation-based learning organisation”, Journal of Knowledge\nManagement , Vol. 3 No. 7, pp. 1314-1334.\nCaputo, F., Walletzky, L. and S ˇtep /C19anek, P. (2019c), “Towards a systems thinking based view for the\ngovernance of a smart city’s ecosystem”, Kybernetes , Vol. 48 No. 1, pp. 108-123.\nCastells, M. (1999), The Social Implications of Information and Communication Technologies ,UNESCO ’s\nWorld Social Science Report .\nChaston, I., Badger, B. and Sadler-Smith, E. (2001), “Organizational learning: an empirical assessment of\nprocess in small UK manufacturing firms”, Journal of Small Business Management ,V o l .3 9N o .2 ,\npp. 139-151.\nChen, H., Chiang, R.H. and Storey, V.C. (2012), “Business intelligence and analytics: from big data to big\nimpact”, MIS Quarterly , pp. 1165-1188.\nChinnaswamy, A., Papa, A., Dezi, L. and Mattiacci, A. (2018), “Big data visualisation, geographic\ninformation systems and decision making in healthcare management”, Management Decision , Vol. 57\nNo. 8, pp. 1937-1959.\nCitroen, C.L. (2011), “The role of information in strategic decision-making”, International Journal of\nInformation Management , Vol. 31 No. 6, pp. 493-501.\nDarke, P., Shanks, G. and Broadbent, M. (1998), “Successfully completing case study research:\ncombining rigour, relevance and pragmatism”, Information Systems Journal , Vol. 8 No. 4, pp. 273-289.\nDaunt, K.L. and Harris, L.C. (2012), “Motives of dysfunctional customer behavior: an empirical study”,\nJournal of Services Marketing , Vol. 26 No. 4, pp. 293-308.\nDavenport, T.H., Barth, P. and Bean, R. (2012), “How big data is different”, MIT Sloan Management\nReview , Vol. 54 No. 1, pp. 43-46.\nDavenport, T., Guszcza, J., Smith, T. and Stiller, B. (2021), Analytics and AI-Driven Enterprises Thrive in\nthe Age of With , Deloitte Insights.\nDel Giudice, M., Scuotto, V., Papa, A., Tarba, S.Y., Bresciani, S. and Warkentin, M. (2021), “A self-tuning\nmodel for smart manufacturing SMEs: effects on digital innovation”, Journal of Product Innovation\nManagement , Vol. 38 No. 1, pp. 68-89.\nDemangeot, C. and Broderick, A.J. (2006), “Exploring the experiential intensity of online shopping\nenvironments”, Qualitative Market Research: An International Journal , Vol. 9 No. 4, pp. 325-351.\nDemchenko, Y., De Laat, C. and Membrey, P. (2014), “Defining architecture components of the big data\necosystem”, 2014 International Conference on Collaboration Technologies and Systems (CTS) ,IEEE ,\npp. 104-112.\nDemchenko, Y., Zhao, Z., Grosso, P., Wibisono, A. and De Laat, C. (2012), “Addressing big data\nchallenges for scientific data infrastructure”, 4th IEEE International Conference on Cloud Computing\nTechnology and Science Proceedings ,IEEE , pp. 614-617.\nDrucker, P.F. (2011), Technology, Management, and Society , Harvard Business Press.\nErevelles, S., Fukawa, N. and Swayne, L. (2016), “Big data consumer analytics and the transformation of\nmarketing”, Journal of Business Research , Vol. 69 No. 2, pp. 897-904.\nEuropean Commission (2014), “Communication from the commission to the European parliament, the\ncouncil, the european economic and social committee and the committee of the regions”, available at:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52014DC0442andfrom=EN\nEuropean Commission (2015), “Communication from the commission to the European parliament, thecouncil, the European economic and social committee and the committee of the regions”, A Digital SingleMarket Strategy for Europe, available at:\nhttps://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=\nCELEX:52015DC0192andfrom=EN\nForester, T. (1987), High-Tech Society: The Story of the Information Technology Revolution , MIT Press.\nGandomi, A. and Haider, M. (2015), “Beyond the hype: big data concepts, methods, and analytics”,\nInternational Journal of Information Management , Vol. 35 No. 2, pp. 137-144.\nVOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2807\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 12]\nGoulding, C. (1999), “Consumer research, interpretive paradigms and methodological ambiguities”,\nEuropean Journal of Marketing , Vol. 33 Nos 9/10, pp. 859-873.\nGrant, A.M. and Mayer, D.M. (2009), “Good soldiers and good actors: prosocial and impression\nmanagement motives as interactive predictors of affiliative citizenship behaviors”, Journal of Applied\nPsychology , Vol. 94 No. 4, pp. 900-920.\nGriffin, M., Babin, B.J. and Modianos, D. (2000), “Shopping values of Russian consumers: the impact of\nhabituation in a developing economy”, Journal of Retailing , Vol. 76 No. 1, pp. 33-52.\nGuiot, D. and Roux, D. (2010), “A second-hand shoppers’ motivation scale: antecedents, consequences,\nand implications for retailers”, Journal of Retailing , Vol. 86 No. 4, pp. 355-371.\nGumusluoglu, L. and Ilsev, A. (2009), “Transformational leadership, creativity, and organizational\ninnovation”, Journal of Business Research , Vol. 62 No. 4, pp. 461-473.\nHajian, S. and Domingo-Ferrer, J. (2012), “A methodology for direct and indirect discrimination\nprevention in data mining”, IEEE Transactions on Knowledge and Data Engineering ,V o l .2 5N o .7 ,\npp. 1445-1459.\nHarbart, T. (2021), “Tapping the power of unstructured data”, MIT Sloan Management School,\navailable at: https://mitsloan.mit.edu/ideas-made-to-matter/tapping-power-unstructured-data\nHicks, B.J., Culley, S.J. and McMahon, C.A. (2006), “A study of issues relating to information managementacross engineering SMEs”, International Journal of Information Management , Vol. 26 No. 4, pp. 267-289.\nKaur, S., Gupta, S., Singh, S.K. and Perano, M. (2019), “Organizational ambidexterity through global\nstrategic partnerships: a cognitive computing perspective”, Technological Forecasting and Social\nChange , Vol. 145, pp. 43-54.\nKohlbacher, F. (2016), “The use of qualitative content analysis in case study research”, Forum Qualitative\nSozialforschung/Forum: Qualitative Social Research , Vol. 7 No. 1, pp. 1-30.\nManogaran, G., Thota, C. and Lopez, D. (2022), “Human-computer interaction with big data analytics”,\nResearch Anthology on Big Data Analytics, Architectures, and Applications, IGI Global, pp. 1578-1596.\nMarkus, M.L. and Topi, H. (2015), Big Data, Big Decisions for Science, Society, and Business , National\nScience Foundation.\nMasse, M. (2011), REST API Design Rulebook: designing Consistent RESTful Web Service Interfaces ,\nO’Reilly Media.\nMo¨hring, M., Keller, B., Schmidt, R. and Dacko, S. (2020), “Google popular times: towards a better\nunderstanding of tourist customer patronage behavior”, Tourism Review , Vol. 76 No. 3, pp. 553-593.\nMo¨hring, M., Walsh, G., Schmidt, R., Koot, C. and Ha ¨rting, R.C. (2013), “Returns management in\neCommerce”, HMD , Vol. 50 No. 5, pp. 66-75.\nMummalaneni, V. (2005), “An empirical investigation of web site characteristics, consumer emotional\nstates and on-line shopping behaviors”, Journal of Business Research , Vol. 58 No. 4, pp. 526-532.\nNutt, P.C. (2008), “Investigating the success of decision making processes”, Journal of Management\nStudies , Vol. 45 No. 2, pp. 425-455.\nPac¸o, A. and Lavrador, T. (2017), “Environmental knowledge and attitudes and behaviours towards\nenergy consumption”, Journal of Environmental Management , Vol. 197, pp. 384-392.\nPapadakis, V.M., Lioukas, S. and Chambers, D. (1998), “Strategic decision-making processes: the role of\nmanagement and context”, Strategic Management Journal , Vol. 19 No. 2, pp. 115-147.\nPauleen, D.J. and Wang, W.Y. (2017), “Does big data mean big knowledge? KM perspectives on big\ndata and analytics”, Journal of Knowledge Management , Vol. 21 No. 1, pp. 1-6.\nPolyakova, A., Loginov, M., Serebrennikova, A. and Thalassinos, E. (2019), “Design of a socio-economic\nprocesses monitoring system based on network analysis and big data”, International Journal of\nEconomics and Business Administration , Vol. 7 No. 1, pp. 30-139.\nRahrovani, Y. and Pinsonneault, A. (2020), “Innovative IT use and innovating with IT: a study of the\nmotivational antecedents of two different types of innovative behaviors”, Journal of the Association for\nInformation Systems , Vol. 21 No. 4, pp. 5-14.\nRaisinghani, M.S. (2000), “Knowledge management: a cognitive perspective on business and\neducation”, American Business Review , Vol. 18 No. 2, pp. 105-131.\nPAGE 2808jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025\n\n[Página 13]\nReinsel, D., Gantz, J. and Rydning, J. (2018), The Digitization of the World. From Edge to Core ,A nI D C\nWhite Paper, Seagate.\nRokka, J. and Uusitalo, L. (2008), “Preference for green packaging in consumer product choices –do\nconsumer’s care?”, International Journal of Consumer Studies , Vol. 32 No. 5, pp. 516-525.\nSabherwal, R. and King, W.R. (1995), “An empirical taxonomy of the decision-making processes\nconcerning strategic applications of information systems”, Journal of Management Information Systems ,\nVol. 11 No. 4, pp. 177-214.\nSagiroglu, S. and Sinanc, D. (2013), “Big data: a review”, 2013 International Conference on Collaboration\nTechnologies and Systems (CTS) ,IEEE , pp. 42-47.\nSapsford, R. and Jupp, V. (Eds) (1996), Data Collection and Analysis , Sage.\nSchmarzo, B. (2013), Big Data: Understanding How Data Powers Big Business , John Wiley and Sons.\nSchmidt, R. and Mo ¨hring, M. (2013), “Strategic alignment of cloud-based architectures for big data”,\n2013 17th IEEE International Enterprise Distributed Object Computing Conference ,IEEE , pp. 136-143.\nSharma, R., Mithas, S. and Kankanhalli, A. (2014), “Transforming decision-making processes: a research\nagenda for understanding the impact of business analytics on organisations”, European Journal of\nInformation Systems , Vol. 23 No. 4, pp. 433-441.\nSingh, S.K. and Del Giudice, M. (2019), “Big data analytics, dynamic capabilities and firm performance”,\nManagement Decision , Vol. 57 No. 8, pp. 1729-1733.\nStake, R.E. (2000), “Case studies”, in Denzin, N.K and Lincoln, Y.S (Eds), Handbook of Qualitative\nResearch , Sage, pp. 435-453.\nStonebraker, M. (2010), “SQL databases v. NoSQL databases”, Communications of the ACM ,V o l .5 3\nNo. 4, pp. 10-11.\nStrauss, A. and Corbin, J. (1994), “Grounded theory methodology: an overview”, in Denzin, N.K. and\nLincoln, Y.S. (Eds), Handbook of Qualitative Research , Sage, pp. 273-285.\nTurban, E., McLean, E. and Wetherbe, J. (1998), Information Technology for Management Making\nConnections for Strategic Advantage , John Wiley and Sons, Inc.\nWalsham, G. (1995), “Interpretive case studies in IS research: nature and method”, European Journal of\nInformation Systems , Vol. 4 No. 2, pp. 74-81.\nYang, Q., Steinfeld, A. and Zimmerman, J. (2019), “Unremarkable AI: fitting intelligent decision support\ninto critical, clinical decision-making processes”, Proceedings of the 2019 CHI Conference on Human\nFactors in Computing Systems , pp. 1-11.\nYin, R.K. (1994), “Designing single-and multiple-case. Improving educational management: through\nresearch and consultancy”, in Bennett, N., Glatter, R. and Levacic, R. (Eds), Improving Educational\nManagement: Through Research and Consultancy , Sage, pp. 135-155.\nYin, R.K. (2003), Case Study Research, Design and Methods , 3rd ed., Sage, Vol. 5.\nYin, R.K. (2012), “Case study methods”, in Cooper, H., Camic, P.M., Long, D.L., Panter, A.T., Rindskopf,\nD. and Sher, K.J. (Eds), APA Handbook of Research Methods in Psychology , Vol. 2.Research Designs:\nQuantitative, Qualitative, Neuropsychological, and Biological , American Psychological Association,\npp. 141-155.\nZakir, J., Seymour, T. and Berg, K. (2015), “Big data analytics”, Issues in Information Systems ,V o l .1 6\nNo. 2, pp. 81-90.\nCorresponding author\nFrancesco Caputo can be contacted at: francesco.caputo2@unina.it\nFor instructions on how to order reprints of this article, please visit our website:\nwww.emeraldgrouppublishing.com/licensing/reprints.htm\nOr contact us for further details: permissions@emeraldinsight.com\nVOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2809\nDownloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025",
      "metadata": {
        "filename": "Advancing beyond technicism-2022.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\Advancing beyond technicism-2022.pdf",
        "file_size": 191470,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:33.619110",
        "content_length": 55999
      }
    },
    "38fd80e4-1db5-4f15-a691-9d1e300929d7": {
      "id": "38fd80e4-1db5-4f15-a691-9d1e300929d7",
      "content": "[Página 1]\nIssues in Big Data Testing and Benchmarking\nAlexander Alexandrov  \nTechnische Universität Berlin  \nEinsteinufer 17  \n10587 Berlin, Germany  \n+49 30 314 23555  \nalexander.alexandrov@tu-\nberlin.de Christoph Brücke  \nTechnische Universität Berlin  \nEinsteinufer 17  \n10587 Berlin, Germany  \n+49 30 314 23555  \nchristoph.bruecke@campus.tu-\nberlin.de Volker Markl  \nTechnische Universität Berlin  \nEinsteinufer 17  \n10587 Berlin, Germany  \n+49 30 314 23555  \nvolker.markl@tu- berlin.de \n \n \nABSTRACT   \nThe academic community and industry are currently researching \nand building next generation data management systems. These \nsyste ms are designed to analyze data sets of high volume  with \nhigh data ingest  rates and short response time s executing complex \ndata analysis algorithms on data that does not adher e to relational \ndata model s. As these big data systems differ from standard \nrelational database systems with respect to data and workloads, \nthe traditional benchmarks used by the database community are insufficient. In this paper , we describe initial solutions and \nchallenges wit h respect to big data generation, methods for \ncreating realistic, privacy -aware, and arbitrarily scalable data  sets, \nworkloads, and benchmarks from real world data. We will in \nparticular discuss why we feel that workloads currentl y discussed \nin the testing and benchmarking community do not capture the real complexity of big data  and highlight several research \nchallenges with respect to massively -parallel data generation and \ndata characterization.  \nCategories and Subject Descriptors   \nD.2.5  [Testing and Debugging ]: testing tools, data generators  \nGeneral Terms  \nMeasurement, Performance, Experimentation   \nKeywords  \nBig Data, Data Generation, Data Profiling, Workloads, Benchmarking  \n1. INTRODUCTION \nThe database systems building community is currently at a peak \nof new activity, creating novel systems for managing and \nanalyzing what is commonly called “big data.” Big data  is usually \ncharacterized by the requirement to conduct advanced analytics on \nlarge volumes of data of variable format, wh ich is ingested into \nthe system with high -velocity  with the need for fast response \ntimes. Novel big data  analytics systems differ from traditional \ndata analysis systems  for varying reasons, they : (a) c an process \nterabytes or even p etabytes of data due to their scale-o ut abilities, \nemployin g massively parallel processing , (b) support complex \ndata types in addition to relational sets of tuples (i.e., data of complex structure, such as text documents, hierarchies, graphs, or even images, audio, or video files) , (c) allow for defining and \nprocessing complex analytics tasks that go beyond the traditional \noperations of the relational algebra ( e.g., user -defined functions, \ndata mining or machine learning algorithms, graph algorithms) , \n(d) provide fault -tolerance in order to ensure termination even for \nlong-running computations , and  (e) compute answers with low -\nlatency, producing r esults in a pipelined fashion.  \nSome examples of systems that showcase several of these features \nare Google MapReduce [ DG04 ], its open source implementation \nHadoop [ Had13] , its ecosystem of languages (e.g ., Hive \n[TSJ+09 ], JAQL [ BEG+11 ], Pig [ ORS+08]) and libraries such as \nMahout [ Mah13],  and other big data systems such as  Asterix \n[ABG+12 ], GraphLab [ LBG+12 ], Spark [Spa13] and our own \nStratosphere system [ ABE+10 , Str13 ]. At the same time, there is a \ntrend to make more traditional relational data analysis sy stems \nmore scalable. Examples of these efforts are SAP Hana \n[FML +12], Impala [ Imp13 ], Oracle Exadata [ GSA+11 ], or the \ncolumnar storage extensions to Microsoft’s and IBM’s database products , to name a few.  \nWhile all these systems have advanced the capabilit ies of data \nanalysis with respect to the five dimensions above, database testing and benchmarking have not moved forward to provide data \ngenerators, data sets, and workloads. In particular, we see the need \nto generate large, realistic data sets at scale, as well as the need for \nwell-defined workloads that capture the nature of novel, modern \nanalysis tasks.  \n2. BIG DATA GENERATION \nData generation tools and practices can be principally assigned to \none of two classes : (a) reusing existing, well-known data \ngeneration tools, or (b ) implementing custom, use -case tailored \ndata generator s. We first review the benefits of each  one of these \nclasses and then discuss some  implications for the evaluation of \nbig data  analytics systems.  \nSince the establishment of standardized benchmarks as a “gold \nstandard” for performance evaluation of database systems  in the \nearly 90’s , experimental results reported  in research papers often \nreuse data sets and queries from well-known benchmarks , like \nTPC-H, TPC-C [TPC13 ], and XMLGen [ XML13 ]. This practice \nis justified  by two main factors. First, the synthetic data used by \nstandardized  or public  benchmarks typically adheres to a short \ntextual specification  that is well-known in the database \ncommunity . Reusing data sets from such benchmark s therefore \nmakes the data properties and their impact on the evaluated tasks  \nmore comprehensible  and increases the trust in the reported \nexperiment  result s. Second , well-known benchmarks typically \nprovide  open-source tools for data and workload generation , \nwhich can be adapted and used by third parties relatively easy . \nThis reduces the overall effort required to prepare and execute  \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee prov ided that copies are \nnot made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. To copy \notherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior specif ic permission and/or a fee.  \nDBTEST ’13, June 24, 201 3, New York City,  NY, USA \nCopyright 2 013 ACM 1 -58113 -000-0/00/0010  …$15 .00.\n\n[Página 2]\n“proof -of-concept” experiments  and allows researchers  to spend \nmore time working on the actual prototype s rather than the tooling \nto evaluate them . \nAn alternative approach that sometimes is preferred for \nspecialized experimental studies  is to define an d implement a \ncustom data generator  tailored towards the requirements of the \nconcrete experiment s at hand . If the experiments are recognized as \nrelevant by the database community, the data and tasks described \nin the original research are often reused by other  authors in \nfollow -up work. For example, Pavlo et al. followed this approach \nin their comparison of approaches for large -scale data analytics \n[PPR+ 09] and implemented a synthetic generator  that generates a \ncollection of linked HTML documents and associated data (e.g., \nuser traffic , PageRank ). The data generator and the tasks have \nsince then been used in several other papers dealing with large -\nscale data analytics systems [ DQJ+10 , JOS+10 ]. For graph data, \nthe Kronecker multiplication approach suggested by Leskovec et \nal. [LC K+05]  offers a simple algorithm  for synthetic generation of \nunlabeled graphs with real world characteristics ( e.g., shrinking \ndiameter, skewed degree distribution) . Due to the lack of \npublic ally available  real-world graph s in the terabyte range , \nKronecker graphs are often featured in the evaluation sections of \nseveral graph -mining  papers over the past few years [ KTF 09, \nKTA +11]. \nPrincipally , the main issue with both classes  is the inherent \nsimplicity in the statistical structure of the generated data.  In the \nfirst case , this simplicity is driven by the need for concise and \nunderstandable specification  for standardized benchmarks. In the \nsecond case,  the main hindering factor is the complexity \nintroduced in the data generation programs by the need for \ncorrelated data and the amount of resources that researchers are \nwilling to invest in their development . \n  \nFigure 1: Simplified Retail Database  Schema  \nIn reference to the characteristic s of new big data  analysis  systems \npresented in Section 1 , the use of oversimplified synthetic data \ncreates a subtle pitfall that may impact the relevance of research \nresults  for real -world applications . The reason for this is that per \ndefinition such systems must work in a distributed execution \nenvironment  (cluster or cloud), and also must use some form of \ndata-parallelism in order to ensure scale-out . These design \ndecisions are highly sensitive to data skew, which often is present \nin many target application domains “a priori”  and potentially \nchanges over time. To illustrate the problem, consider the retail \ndatabase schema depicted on Figure 1 and a use -case, where the \nbenchmarks or experimental setup models an application that \nwants to compute the top -k most purchased items per product \ncategory.  Since some product categories are n aturally more in demand  than others, introducing a skew over the product category \ndistribution in the joined LINEITEM -PRODUCT view is critical \nto the relevance of the generated data.  As most systems will \nprocess each product category g roup in parallel, skew  will \nobviously influence system performance for this particular task. \nMoreover, for an online computation of the same counts in a streaming setting, the degree of skew  will depend on the time of \nthe current window ( e.g., in the U.S. shopping peaks between \nThanksgiving Day & Christmas  and attains a maximum on “Black \nFriday ”). In this case, assuming  an evenly distributed load across \ntime is an oversimplif ication  that can influence the relevance of \nthe experimental results for real -world applications.  \nWith the advent of big data  co\n mes the requirement to quickly \ngenerate huge data sets. This is particularly a challenge when \ngenerating data sets with key/foreign -key relationships or other \ncomplex correlations across tables. Using specialized random number generators with seed skipping allows for doing so in parallel without having to communicate data generated on one node of a  shared -nothing cluster to another [RFS+10 , FPR12, \nASP+1 1, ATM12], resulting in toolkits such as PDGF [PDG13] \nor Myriad [Myr13] . Both toolkits provide a set of domain-specific \nprimitives for data generation  that facilitate the transparent use of \nseed-skip PRNGs and complementary technique s for scalable \ngeneration of complex data . \n3. GENERATING REALISTIC DATA SETS  \nThe advances in new methods for scalable generation of realistic \ndata highlight an important practical question: “If the data \ngenerator program can be expressed in terms of a small set of \nspecial primitives, then to which extent and in which scenarios \ncan the specification  process itself be executed automatically ?” A \nnaïve  general approach  is based on the analysis of empirical \nobservations in the modeled  domain and the subsequent synthesis \nof a data generator specification  from  these observations . In \nbusiness scenarios , however, the analysis is often done  in the \ncontext of a reference dataset that represents a ground truth for the \nderived data generator. This section  sketches our vision for an \nintegrated framework  for such usage scenarios . We propose an \nextensible architecture with  clean separation between the data \ngeneration  primitives and the methods and techniques used to \nextract relevant features from the ground truth  data set.  \nA large problem for benchmarking and testing of big data system s \nis the lack of realistic data sets. Many  synthetic  data sets follow \nsimplistic assumptions ( e.g., few correlations, most ly uniform \ndistributions, over simplified schema ) that are not re presentative \nfor real-world data . A promising, generalizable, and more \neffective way is to automatically extract the domain information \nfrom a ground truth data set , which is often available  in practice. \nFigure 2 illustrates our envisioned pipeline. The domain \ninformation is first extracted from the reference database in the form of domain constraints , which can be either  hard (e.g., foreign \nkeys, unique keys, and other functional dependencies ) or soft \n(e.g., local statistical models) .  The obtained structural, semantic, \nand statistical information is then unified into a n intermediate \nmodel representing the schema information with ann otated \nconstraints . A final synthesis pass transforms the intermediate \nrepresentation into a data generator specification for a specific \ntarget environment  like the Myriad . This specification is then used  \nto create a concrete data generator instance that is able to mimic \nthe original data set.\n\n[Página 3]\nFigure 2: A Pipeline for the A nalysis & Synthesis of D ata Generators \nWe note that in the first step of this process, the circumstances in \nwhich the analysis is performed will influence its depth and \nconsequently the quality of the collected domain information.   \nIf the reference database cannot be accessed  directly and the \ndomain information is available only in a derived form , such as in \na database catalog, the analysis must be performed indirectly  and \ncan only extract the available catalog information . This \ninformation commonly consists of attribute value statistics ( e.g., \nfrequen cy values, histograms, number of distinct va lues, and \nnumber of NULLs) , schema information , and integrity constraints \n(e.g., referential integrity , primary key s, and unique  constraints as \nwell as  other constraints represent ing domain invariants ).  \nAlternativ ely, if the reference database is available directly , \nadvanced profiling methods could be leveraged to obtain information beyond the catalog in order to capture a more accurate domain model . This approach will require us to \ndetermine additional characterizations of the dataset to be generated (e.g., advanced  multivariate statistics [SHM+06] and \nsoft constraints [IMH+04, BH03, SBH+06]) on the data with \nscalable methods (see [HIL+09] for an overview of statistical methods, and [Nau13] for an overview of  data profiling).  Using \nthese techniques will allow for determining the essential characteristics of real -world data set s and correspondingly will \nenable one to scale up or down synthetic clones . \nThe integration  of data profiling  and data generation workflows is \nrelevant in the era of big data for a number of reasons . First, many \ninstitutions publish their data sets in order to let others perform \ntheir experiments on them. However, database sizes are becoming larger and larger . Conseque ntly, it is becom ing increasingly  \ndifficult to transfer these huge data sets to the person wishing to \nuse them  due to network and bandwidth constraints . Therefore, it \nis desirable to have a compact specification of the data sets, i.e. , a \nsynopsis or profile  from which one can automatically generate a \ndata generator specification  and thus the dataset . Second, data \nprofiling will increase the relevance tests or benchmarks. Huppler \n[Hup09] describes  five key aspects for a good benchmark , namely  \na good b enchmark has to be relevant, repeatable, fair, verifiable, \nand economical. Section 2 mainly addressed the latter one, while \ndata profiling will help to improv e the relevance.  \nCurrently, w e are develo ping a prototype called Oligos [Oli13]  \nthat adheres to our aforementioned vision. The initial version of Oligos can generate data generator specifications for the Myriad Toolkit [Myr13] from the system catalog of a database system. Our long -term vision is provide a modular API that will allow \nlearning advanced statistics and correlation information, in order \nto ge nerate even more realistic data sets.  \n4. AN APPLICATION: REGRESSION \nTESTING OF BIG DATA SYSTEMS  \nAn important part of the maintenance lifecycle of co mmercial big \ndata system s as well as general data management system s is \ndevoted to the diagnosis of performance regressions observed by customers in a production setting. When trying to reproduce the \nproblematic behavior in a test environment, database system \ndevelopers often face the problem of missing data – even though \nthe database schema and the problematic queries can be provided by the customer as part of the regression report, the actual \ndatabase instance typically cannot be obtained ( e.g., due to \nprivacy restrictions). T ypically , what  is available  is the database \ncatalog, which contains a statistical approximation of the \nreference database in the form of value distributions, cardinalities , \nand histograms on columns or column groups. As a fallback solution, developers currently trick  the optimizer of a test database \nby feeding customer catalog data in order to obtain the query \naccess paths of the actual production system. As the underlying \ndata is missing and  the database catalog is usually lacking crucial \ninformation  (e.g., on multivariate distributions ) synthetic data  sets \ngenerated in the lab are not representative. Thus, information on \nhow the query access paths perform requires further assistance \nand feedback from the client. The lack of a complete and \nrepresentative regression database therefore slows down the maintenance process and causes additional costs . The methods for \ndata generation based on data and workload characterization as \nenvisioned in Oligos and Myriad would offer a remedy to this \nproblem.  \n5. OPEN ISSUES AND CONCLUSIONS   \nWe have given an overview of issues in big data  benchmarking \nand testing, with a strong focus on data generation. We believe that efficiently generating a huge, realistic data set is an important \nprerequisite for the advancement, evaluation, and fair comparison \nof big data systems. Myriad [Myr13], PDGF [DPG13] , and Oligos \n[Oli13 ] are a first step in this  direction.  However, in the context of \nbig data generation and benchmarking, a large number of \nchallenges remain open.  \nHowever, in the context of big data generation and benchmarking, \na l\narge number of challenges remain open. For realistic data \ngeneration from  a given reference dataset the challenges exist \nboth in the analysis and the synthesis phase.  \nDuring the analysis phase, a combination of data characterization \nand profiling methods can be identified and applied in order to increase the quality of the dom ain information that can be inferred \ndirectly from the reference database. Such methods will allow to \nefficiently determine multi-key dependencies, in particular \nreferential integrity, as well as to profile data with complex structure (e.g., text, graphs, NF² and hierarchical data). In order to preserve privacy when conducting data profiling, data obfuscation \nmethods may as well be required. [Nau13] lists further challenges \nin the area of data profiling.  \nInferred schema information and constraints must be then unified \ninto an intermediate representation (IR) in the synthesis phase. \nTwo problems exist in this context. First, in order to facilitate the subsequent translation of the IR into a data generator specification, the IR should lend itself to the features and \nprimitives common to the underlying data -generation engines. \nSecond, the unification process should determine and handle\n\n[Página 4]\ninconsistencies in the domain information collected in the analysis \nphase. Recently, Arasu [AKL11] and Torlak [Tor12] suggested \ntwo different constraint -based languages for data generator \nspecification that can serve as a starting point for the development of a suitable IR and synthesis algorithm. For both languages, the authors give sufficiency conditions for the existence of a data set \nfulfilling the input constraints and provide approximate \nalgorithms to find such an instance. The approach presented in \n[Tor12] uses a mix of hard (dimension or integrity) and soft \n(statistical) c onstraints and is restricted to dimension models, \nwhereas [AKL11] works on general relational models and relies \nsolely on soft (cardinality) constraints (hard constraints are \nrepresented implicitly as a special form of soft constraints). As the target lang uage in our setting is likely to include primitives that \ndirectly enforce certain types of hard constraints (e.g. unique keys, \nforeign keys), we believe that a distinction between soft and hard \nconstraints in the IR is a more promising approach.  \nAnother big open area is the provisioning of workloads. \nTraditional benchmarks focus on simple workloads that \nessentially follow the relational algebra  or an NF² algebra/ \nXQuery. For evaluating and testing big data analytics systems, we \nwill require m ore complex workloads that involve machine \nlearning algorithms, information extraction, and graph analysis/mining. The lack of a standardized data analysis language currently is a big obstacle for arriving at realistic, comparable, and universally useful w orkload specifications. Ideally, u ntil a  \nstandardized  declarative language is available use-case \nrepositories may be a first step in  this direction.      \n6. ACKNOWLEDGMENTS  \nWe thank Berni Schiefer from IBM and Tillmann Rabl from the \nUniversity of Toronto for interesting discussions. Our \ninvestigations were funded by a CAS grant from IBM, the ICT  \nLabs of the European Institute of Technology as well as the DFG \n(German National Science Foundation) via the Stratosphere Collabor ative Research Unit. \n7. REFERENCES  \n[ABE+10] A. Alexandrov, D. Battré, S. Ewen, M. Heimel, F. \nHueske, O. Kao, V. Markl, E. Nijkamp, D. Warneke: Massively Parallel Data Analysis with PACTs on Nephele. PVLDB Vol. 3, No. 2, pp. 1625– 1628 \n(2010)   \n[ABG+12]  S. Als ubaiee, A. Behm, R. Grover, R. Vernica, V. \nBorkar, M. J. Carey, C. Li: ASTERIX: Scalable \nWarehouse -Style Web Data Integration. In \nProceedings of the Ninth International Workshop on \nInformation Integration on the Web, Article 2, ACM, (2012)   \n[AKL11] A. Arasu, R. Kaushik, J. Li: Data Generation using \nDeclarative Constraints. Proceeding of the SIGMOD Conference, pp. 685-696 (2011)  \n [ASP+1 1] A. Alexandrov, B. Schiefer, J. Poelman,  S. Ewen, T. \nBodner, V. Markl: Myriad - Parallel Data Generation \non Shared-Not hing Architectures , In Proc. ASBD, pp. \n30-33 (2011) \n[ATM12]  A. Ale xandrov, K. Tzoumas, V. Markl: Myriad: \nScalable and Expressive Data Generation , In Proc. \nVLDB(5) pp. 1890- 1893 ( 2012) \n[BH03]  P. Brown, P. Haas: BHUNT: Automatic Discovery of \nFuzzy Algebraic Constraints in Relational Data. \nVLDB 2003: 668-679 [BEG+11] K. S. Beyer, V. Ercegovac, R. Gemulla, A. Balmin, M. Eltabakh, C.-C. Kanne, E. J. Shekita: Jaql: A scripting language for large scale semistructured data \nanalysis.  In Proc. of VLDB Conference. (20 11) \n[DG04]  J. Dean, S. Ghemawat: MapReduce: simplified data processing on large clusters , In OSDI, pp. 137 -150 \n(2004) \n[FML+ 12] F. Färber, N . May, W . Lehner, P . Große, I . Müller, H . \nRauhe, J . Dees: The SAP HANA Database -- An \nArchitecture Overview. IEEE Data Eng. Bull. 35(1): \n28-33 (2012)  \n[FPR12]  M. Frank, M. Poess, T. Rabl: Efficient update data generation for DBMS benchmarks. ICPE 2012: 169-180 \n[GSA+11 ] R. Greenwald, R. Stackowiak,  M. Alam,  M. Bhuller.. \nAchieving extreme performance with Oracle Exadata. McGraw -Hill Osborne Media  (2011)  \n[Had13]  \nhttp://hadoop.apache.org/ , last accessed 05 -10-2013 \n[HIL+09] P. J. Haas, I. Ilyas, G. Lohman, V. Markl: Disco vering \nand Exploiting Statistical Properties for Query Optimization in Relational Databases: A Survey. Statistical Analysis and Data Mining 1(4): 223 -250 \n(2009)  \n[Hup93 ] K. Huppler: The Art of Building a Good Benchmark. \nTPCTC 2009: 18- 30 (2009)  \n[IMH+04]  I. Ilyas, V . Markl, P . Haas, P . Brown, A . Aboulnaga: \nCORDS: Automatic Discovery of Cor relations and \nSoft Functional Dependencies. SIGMOD Conference 2004: 647-658 \n[Imp13]  \nhttps://github.com/cloudera/impala , last accessed 05 -\n10-2013  \n[LBG+12] Y. Low, D. Bickson , J. Gonzalez, C. Guestrin , A. \nKyrola , J. M.  Hellerstein: DistributedGraphLab: A \nframework for machine learning and data mining in \nthe cloud. Proceedings of the VLDB Endowment, \n5(8), pp. 716-727 (2012)  \n[Mah13]  Mahout: http://mahout.apache.org/ , last accessed 04 -\n21-2013  \n[Myr13]  https://github.com/TU -Berlin-DIMA/myriad -\ntoolkit/wiki , last accessed 05 -10-2013  \n[Nau13]  http://www.hpi.uni-\npotsdam.de/naumann/publications/publications_by_ty\npe/year/2013/2276/Nau13.html , SIGMOD Record \n(2013)  \n[Oli13]  https://github.com/TU -Berlin-DIMA/myriad -\ntoolkit/wiki/Using -Oligos -Guide , last accessed 05 -10-\n2013  \n[ORS+08]  C. Olston, B. Reed, U. Srivastava, R. Kumar, A. \nTomkins: Pig Latin: A Not-So -Foreign Language for \nData Processing. Proceedings of the SIGMOD \nConference (SIGMOD), pp. 1099 -1110, (2008)  \n[PDG13]  http://www.paralleldatageneration.org/drupal6/ , last \naccessed 05 -10-2013 \n[RFS+10]  T. Rabl, M . Frank, H . Sergieh, H . Kosch: A Data \nGenerator for Cloud-Scale Benchmarking. TPCTC 2010: 41- 56\n\n[Página 5]\n[SBH+06]  Y. Sismanis, P. Brown, P. Haas, B. Reinwald: \nGORDIAN: Efficient and Scalable Discovery of Composite Keys. VLDB 2006: 691-702 \n[SHM+06]  U. Srivastava, P. Haas, V . Markl, M . Kutsch, T .Tran: \nISOMER: Consistent Histogram Construction Using \nQuery Feedback. ICDE (2006) \n[Spa 13]  \nhttp://spark -project.org/ , last accessed 05 -10-2013  \n[Str13] http://www.stratosphere.eu/ , last accessed 05 -10-2013 \n[Tor12] E. Torlak: Scalable test data generation from multidimensional models . Proceedings of the ACM \nSIGSOFT 20th International Symposium on the \nFoundations of S oftware Engineering  (2012) \n[TPC13] \nhttp://www.tpc.org , last accessed 05 -10-2013  \n[TSJ+09]  A. Thusoo, J. S.Sarma, N. Jain, Z. Shao, P.Chakka, S. Anthony, H. Liu, P. Wyckoff, R. Murthy: Hive - A \nWarehousing Solution Over a M ap-Reduce \nFramework. PVLDB 2(2), pp. 1626- 1629 (2009)  \n[XML13] \nhttp://www.xml-benchmark.org/ , last accessed 05 -10-\n2013  [PPR+09]    A. Pavlo, E . Paulson, A . Rasin, D. Abadi, D . DeWitt, \nS. Madden, M . Stonebraker: A comparison of \napproaches to large-scale data analysis. SIGMOD \nConference 2009: 165 -178 \n[DQJ+10]   J. Dittrich, J . Quiané -Ruiz, A . Jindal, Y . Kargin, V . \nSetty, J . Schad: Hadoop++: Making a Yellow \nElephant Run Like a Cheetah (Without It Even \nNoticing). PVLDB 3(1): 518- 529 (2010)  \n[JOS+10]    D. Jiang, B . C. Ooi, L.  Shi, S . Wu: The Performance \nof MapReduce: An In-depth Study. PVLDB 3(1):472-483 (2010)  \n[LCK+05]  J. Leskovec, D . Chakrabarti, J . Kleinberg, C . \nFaloutsos: Realistic, Mathematically Tractable Graph Gene ration and Evolution, Using Kronecker \nMultiplication. PKDD 2005: 133 -145 \n[KTF09]     U. Kang, C .E. Tsourakakis, C . Faloutsos: PEGASUS: \nA Peta -Scale Graph Mining System. ICDM 2009: \n229-238 \n[KTA+11] U. Kang, C.E. Tsourakakis, A.P.  Appel, C . Faloutsos, \nJ. Leskovec: HADI: Mining Radii of Large Graphs. \nTKDD 5(2): 8 (2011)",
      "metadata": {
        "filename": "alexandrov2013.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\alexandrov2013.pdf",
        "file_size": 123038,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:33.788781",
        "content_length": 27035
      }
    },
    "c1e3eb5c-c9e9-4b54-91de-8f4b3dfab991": {
      "id": "c1e3eb5c-c9e9-4b54-91de-8f4b3dfab991",
      "content": "[Página 1]\nComputers in Biology and Medicine 163 (2023) 107166\nAvailable online 9 June 2023\n0010-4825/© 2023 Elsevier Ltd. All rights reserved.An enhanced grey wolf optimizer boosted machine learning prediction \nmodel for patient-flow prediction \nXiang Zhanga, Bin Lub, Lyuzheng Zhangc, Zhifang Pand, Minjie Liaoa, Huihui Shena, \nLi Zhange, Lei Liuf, Zuxiang Lig,*, YiPao Huh,**, Zhihong Gaoi,*** \naWenzhou Data Management and Development Group Co.,Ltd, Wenzhou, Zhejiang, 325000, China \nbWenzhou City Bureau of Justice, Wenzhou, Zhejiang, 325000, China \ncB-soft Co.,Ltd., B-soft Wisdom Building, No.92 Yueda Lane, Binjiang District, Hangzhou, 310052, China \ndThe First Affiliated Hospital of Wenzhou Medical University, Wenzhou, 325000, China \neWenzhou Hongsheng Intellectual Property Agency (General Partnership), Wenzhou, Zhejiang, 325000, China \nfCollege of Computer Science, Sichuan University, Chengdu, Sichuan, 610065, China \ngOrganization Department of the Party Committee, Wenzhou University, Wenzhou, 325000, China \nhWenzhou Health Commission, Wenzhou, Zhejiang, 325000, China \niZhejiang Engineering Research Center of Intelligent Medicine, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou, 325000, China   \nARTICLE INFO  \nKeywords: \nPatient-flow prediction \nSupport vector regression \nMachine learning \nMeta-heuristic \nSwarm-intelligence ABSTRACT  \nLarge and medium-sized general hospitals have adopted artificial intelligence big data systems to optimize the \nmanagement of medical resources to improve the quality of hospital outpatient services and decrease patient \nwait times in recent years as a result of the development of medical information technology and the rise of big \nmedical data. However, owing to the impact of several elements, including the physical environment, patient, \nand physician behaviours, the real optimum treatment effect does not meet expectations. In order to promote \norderly patient access, this work provides a patient-flow prediction model that takes into account shifting dy-\nnamics and objective rules of patient-flow to handle this issue and forecast patients ’ medical requirements. First, \nwe propose a high-performance optimization method (SRXGWO) and integrate the Sobol sequence, Cauchy \nrandom replacement strategy, and directional mutation mechanism into the grey wolf optimization (GWO) al-\ngorithm. The patient-flow prediction model (SRXGWO-SVR) is then proposed using SRXGWO to optimize the \nparameters of support vector regression (SVR). Twelve high-performance algorithms are examined in the \nbenchmark function experiments ’ ablation and peer algorithm comparison tests, which are intended to validate \nSRXGWO ’s optimization performance. In order to forecast independently in the patient-flow prediction trials, the \ndata set is split into training and test sets. The findings demonstrated that SRXGWO-SVR outperformed the other \nseven peer models in terms of prediction accuracy and error. As a result, SRXGWO-SVR is anticipated to be a \nreliable and efficient patient-flow forecast system that may help hospitals manage medical resources as effec-\ntively as possible.   \n1.Introduction \nPrimary medical care is the guarantee of people ’s survival and \ndevelopment. With the continuous development of economic, cultural, \nand social construction, people ’s demand for medical resources is much higher. Their awareness of medical and health care also increases re-\nquirements for the current medical industry. Since the medical service \nsystem is complex, it is not only influenced by factors such as local de-\nmographic characteristics, socio-economic conditions, natural environ -\nmental conditions, medical hardware, software facilities, and patient \n*Corresponding author. \n**Corresponding author. \n***Corresponding author. \nE-mail addresses: zhxan@126.com (X. Zhang), wzlubin@139.com (B. Lu), 66199293@qq.com (L. Zhang), panzhifang@wmu.edu.cn (Z. Pan), 1829820@qq.com \n(M. Liao), ylvias7@126.com (H. Shen), 101744491@qq.com (L. Zhang), liulei.cx@gmail.com (L. Liu), lizuxiang@wzu.edu.cn (Z. Li), huyipao@outlook.com (Y. Hu), \ngzh@wzhospital.cn (Z. Gao).  \nContents lists available at ScienceDirect \nComputers in Biology and Medicine \nu{�~zkw! s{yo|kro>! ÐÐÐ1ow�o �to~1m{y2w{m k�o2m{y|lt{ yon!\nhttps://doi.org/10.1016/j.compbiomed.2023.107166 \nReceived 10 March 2023; Received in revised form 25 May 2023; Accepted 8 June 2023\n\n[Página 2]\nComputers in Biology and Medicine 163 (2023) 107166\n2and doctor behaviors [1]. But there are also various interactions and \npositive and negative feedback between these influencing factors, which \nmay result in the longer the waiting time in the hospital, the more \nattractive the patients are, or the regular changes in the hospital waiting \nqueue, etc. Self-organized regularities and Emergent behavior make it \ndifficult for hospitals to implement optimal outpatient management \nmeasures and cause the actual use of available resources not to match \nthe expected results [2]. Therefore, to improve the efficiency of existing \nmedical resources, improve the quality of hospital outpatient services, \nshorten patient waiting queues and waiting times, it is crucial to un-\nderstand the changing dynamics and objective patterns of patient-flow \nto provide a basis for dynamic adjustment of physician consultation \nplans and to achieve orderly and effective patient control. \nIn recent years, the advancement of medical informatization and the \nrise of big medical data has allowed studying patient-flow prediction \nbased on big data mining. Researchers have conducted some research in \nthe analysis of patient-flow change patterns, analysis of patient-flow \ninfluencing factors, and patient-flow prediction. Li et al. [3] proposed \na time series patient-flow prediction method based on XGBoost, a sup-\nport vector machine (SVM), to solve the problem of planning and allo-\ncation of healthcare resources by government and hospital management. \nNikakhtar et al. [4] proposed a patient visit prediction model based on \neigendistance and mesocentricity that can help healthcare managers and \ndecision-makers predict the trend of infectious patient-flow. Sharafat \net al. [5] proposed an emergency room patient-flow prediction model \n(PatientFlowNet) based on a deep learning framework, including pre-\ndicting arrival, treatment, and discharge rates. The results show that \nPatientFlowNet has higher accuracy and lower average absolute error \nthan the benchmark algorithm. Tavakoli et al. [6] proposed a seasonal \nautoregressive integrated moving average (SARIMA) model for \npatient-flow prediction of the current epidemic of neocrown pneumonia \ndisease, effectively predicting the number of patients’ visits to Thai \nhospitals in the next 30. According to the current research status, it is \neasy to find that more and more researchers are using machine learning \ntechniques to predict the number of patient visits in hospitals. However, \nsince most of the prediction models use a monadic time-series feature \nprediction method and the changes of patient-flow are affected by a \nvariety of complex factors and do not have obvious linear characteris -\ntics, resulting in the accuracy of the models is not high. On the other \nhand, it is limited by the defects of the classification predictor itself, \nwhich leads to large prediction bias of prediction models based on SVM \nand other prediction models. Therefore, how to improve the accuracy \nand reduce the error of patient-flow prediction models is a major chal-\nlenge in current medical resource scheduling research. \nAs a novel optimization method with strong robustness and flexi-\nbility, the swarm intelligence optimization algorithm is widely used in \npredictive optimization problems. The swarm intelligence optimization \nalgorithm is a stochastic optimization algorithm abstracted by simu-\nlating the collaborative behavior of animals, insects, and other organ -\nisms. The current well-known algorithms are, grey wolf optimization \n(GWO) [7], bat-inspired algorithm (BA) [8], different evolution (DE) \n[9], sine cosine algorithm (SCA) [10], salp swarm algorithm (SSA) [11], \nwhale optimizer (WOA) [12], moth-flame optimization (MFO) [13], \nparticle swarm optimization (PSO) [14], hunger games search (HGS) \n[15], Harris hawks optimization (HHO) [16], rime optimization algo-\nrithm (RIME) [17], colony predation algorithm (CPA) [18], Runge Kutta \noptimizer (RUN) [19], weighted mean of vectors (INFO) [20], slime \nmould algorithm (SMA) [21,22], opposition-based SCA (OBSCA) [23], \nmodified SCA (m_SCA) [24], boosted GWO (OBLGWO) [25], A-C para-\nmetric WOA (ACWOA) [26], fruit fly optimizer (FOA) with \nmulti-population outpost mechanism (MOFOA) [27], SCA with differ -\nential evolution (SCADE) [28], and so on. They also have been applied to \nsolve many problems such as bankruptcy prediction [29], feature se-\nlection [30–34], economic emission dispatch [35], multi-objective \noptimization [36], global optimization [37,38], dynamic \nmulti-objective optimization [39], numerical optimization [40–42], scheduling optimization [43,44], feed-forward neural networks [45], \nmedical image segmentation [46–48], feature selection [49,50], per-\nformance optimization [51,52], identification of pulmonary hyperten -\nsion animal [53], constrained multi-objective optimization [54], and \nlarge-scale complex optimization [55]. \nMore and more researchers are considering optimizing models using \nswarm intelligence optimization methods to improve the accuracy of \nprediction methods. Chou et al. [56] proposed a swarm intelligence \nalgorithm-based support vector machine prediction model (SFALSSVM) \nusing the smart firefly algorithm (SFA) to optimize the parameters of the \nleast squares support vector regression (SVR) and successfully applied it \nto several geotechnical engineering problems. Kaushik et al. [57] pro-\nposed a binary swarm intelligence algorithm by combining the firefly \nalgorithm and bat algorithm with a wavelet neural network (WNN) and \noffered a prediction model for software development effort (SDEE), \nwhich has high prediction accuracy. Mehraein et al. [58] proposed a \nCatBoost (CB) prediction model based on a swarm intelligence algorithm \nfor predicting the monthly flow of satellite precipitation data and \ndemonstrated a significant reduction in the root mean square error \n(RMSE) of the proposed CB compared with an artificial neural network \n(ANN). Zhu et al. [59] combined the WOA and the simulated annealing \nalgorithm (SA) to optimize the kernel extreme learning machine \n(KELM). They proposed an enhanced search-based prediction algorithm \n(EMWS) that effectively addresses defect prediction in software \nmodules. \nZhou et al. [60] improved the Firefly algorithm (FA) by incorpo -\nrating chaotic mapping, adaptive inertia weights, and Levy flight for \naccurate prediction of reinforcement tensile loads for assessing the in-\nternal stability of geosynthetic reinforced soil (GRS) structures. They \nused the improved FA to optimize the hyperparameters of the \nleast-squares SVR model. The improved SVR model had excellent ac-\ncuracy with an average absolute percentage error of less than 10%. Ma \net al. [61] proposed an SVR prediction model integrated with k-fold \ncross-validation (CV) and used an artificial bee colony (ABC) algorithm \nand genetic algorithm (GA) to optimize the hyperparameters of the \nmodel. The results showed that the hybrid approach can be used to \ndetermine the optimal hyperparameters and present statistical signifi -\ncance. Huang et al. [62] proposed a swarm intelligence algorithm (DFP) \nintegrating floral pollination algorithm (FPA) and differential evolution \n(DE) and an algorithmic model for predicting the groutability of cement \npaste in combination with SVR. Luo et al. proposed a hybrid prediction \nmodel (LS-SVMR) using a coupled simulated annealing (CSA) algorithm \nto optimize the hyperparameter selection of SVR, which effectively \nimplemented the lateral strength prediction of reinforced concrete (RC) \ncolumns. \nBased on the above improvement methods for prediction models, it \ncan be found that swarm intelligence optimization algorithms can \neffectively help prediction models find optimal hyperparameters, and \nSVR is applied very frequently in many models. However, due to the \nvariety of swarm intelligence algorithms, each algorithm has defects, \nsuch as low convergence accuracy, slow search speed, and easy falling \ninto local optimality. Therefore, in this paper, to accurately predict the \nnumber of patients and reasonably schedule medical resources, an SVR \nprediction model based on improved GWO is proposed using the GWO \nalgorithm with high exploitation capability combined with SVR pre-\ndiction methods. First, to give full play to the exploitation advantages of \nGWO and overcome the shortcomings of GWO in the search process as \nmuch as possible, the following three methods are used for improve -\nment: (1) To address the problem of narrow coverage of the initialized \nsearch agent of GWO, the original random initialization method is used \ninstead of Sobol sequence to expand the distribution of the initial so-\nlution. (2) To address the problem of too little information exchange \namong GWO search agents, a directional mutation mechanism is used to \nincrease the interactivity of solutions, improving the algorithm’s search \nefficiency. (3) To address the problem of imbalance between GWO \nsearch and exploitation, a Cauchy random replacement strategy is added X. Zhang et al.\n\n[Página 3]\nComputers in Biology and Medicine 163 (2023) 107166\n3to the core update formula to adjust the weights of search and exploi -\ntation of the algorithm in the iterative process. Based on the above ideas, \nSobol sequence-based population initialization, Cauchy random \nreplacement strategy, and directional mutation mechanism are intro-\nduced into GWO to propose a high-performance GWO variant \n(SRXGWO). Then, to verify the optimization performance of SRXGWO, \nthis paper designs comparative simulation experiments based on the \nclassical IEEE CEC2014 test set and compares SRXGWO with other X \nmethods. The experiments show that the proposed SRXGWO method \nsignificantly improves initialization, search efficiency, and defects of \niterative balance. This paper also analyzes the comparative results using \nthe Wilcoxon signed-rank test [63] and the Friedman test [64]. \nSRXGWO has a higher convergence speed compared with peer algo-\nrithms and accuracy. \nFurther, this paper proposes a multivariate SRXGWO-SVR prediction \nmodel for predicting patient flow by optimizing two hyperparameters of \nSVR using high-performance SRXGWO. To validate the real prediction \nability of the SRXGWO-SVR model, the prediction results of the model \nare presented in detail using real clinical data sets and divided into \ntraining and test sets. Further, the SRXGWO-SVR model based on \nSRXGWO, the GWO-SVR model based on GWO, and the original SVR \nmodel are compared in this paper, and the experimental results also \ndemonstrate that the SRXGWO-SVR can effectively outperform the two \noriginal models without improvement. Finally, this paper also compares \nthe SRXGWO-SVR model with well-known prediction models such as \nRadial basis function networks, convolutional neural networks, etc. R- \nsquared (R2), root mean squared error (RMSE), and mean absolute error \n(MAE) are used for validation and confirm that SRXGWO-SVR is more \nadvantageous in predicting hospital patient-flow. The data set used in \nthis paper is the attendance statistics of Wenzhou Medical University \nHospital in China, which serves a radius of nearly 30 million people and \nhas an annual outpatient volume of 5.3 million. Due to the large volume \nof data, the latest data from January 2022 to September 2022 is selected, \nwith a sample size of 240 items. The main contributions of this paper are \nas follows.  \n1. Sobol sequence-based population initialization, Cauchy random \nreplacement strategy, and directional mutation mechanism are \nintroduced into GWO to propose a high-performance algorithm \nSRXGWO. The strategies and mechanisms employed in this paper can \nprovide a valid reference for the field of evolutionary computation. \n2.We designed experiments comparing SRXGWO with 12 similar al-\ngorithms to verify the algorithm ’s improvement ideas and optimi -\nzation performance. Experiments can effectively demonstrate the \nperformance of SRXGWO ’s benchmark functions and provide illus-\ntrations for their specific applications.  \n3. SRXGWO is used to optimize the hyperparameters of SVR, and the \nSRXGWO-SVR multivariate prediction model is proposed and suc-\ncessfully applied to predict patient flow. The proposed model can \neffectively predict patient flow and provide useful suggestions for \nhospital management.  \n4. We designed a comparison experiment between SRXGWO-SVR and \neight similar prediction models to verify the effectiveness of the \nimprovement and the accuracy of the prediction. The experiments \nillustrate that the proposed model has great potential for predicting \nother time series problems. \nThe rest of this paper is organized as follows. Section 2 describes the \nprediction dataset, the original GWO, and SVR. In Section 3, SRXGWO is \nproposed based on three improvement strategies, and the SRXGWO-SVR \nmodel is proposed in conjunction with SVR. In Section 4, benchmark \nfunction comparison experiments and simulation prediction comparison \nexperiments are designed. Finally, Section 5 summarizes the work of this \npaper and illustrates further research directions. 2.Materials and methods \nThis section introduces the swarm intelligence optimization algo-\nrithm GWO and the regression prediction model SVR used in this study. \n2.1. Description of GWO algorithm \nIn the GWO algorithm, grey wolf individuals are divided into four \nclasses: α、β、δ and ω. α is mainly responsible for participating in the \ndecision-making and management of the pack; ω is for other grey wolf \nindividuals; β and δ are for grey wolf individuals with the second highest \nadaptation level to α. The GWO algorithm focuses on three behaviors: \nencirclement behavior, hunting behavior, and attack behavior.  \n1. Encirclement behavior \nThe first stage of prey predation by grey wolves is to encircle the \nprey, and the mathematical model can be described by Eq. (1) and Eq. \n(2). \nD↗⃦⃦⃦⃦C↗⋅X↗\npt\u0000X↗t⃦⃦⃦⃦(1)  \nX↗t1X↗\nvt\u0000A↗⋅D↗(2)  \nwhere D↗is the distance between the prey and the wolves; A↗2a⋅r2\u0000\na, C↗2⋅r↗\n2; X↗is the current location of the wolves; t is the number of \ncurrent iterations; X↗\np is the location of the prey; r1 , r2 are random \nnumbers, between 0C1; a∃2C0.  \n2. Hunting behavior \nAfter a wolf pack surrounds a prey, it will hunt the surrounding prey. \nIf α is the global optimal solution, β is the global second solution, and δ is \nthe global third solution, then the mathematical model of α, β, and δ \nrepositioning can be described by Eqs. (3)–(5). \nD↗\nα⃦⃦⃦⃦C↗\n1⋅X↗\nα\u0000X↗⃦⃦⃦⃦(3)  \nD↗\nβ⃦⃦⃦⃦C↗\n2⋅X↗\nβ\u0000X↗⃦⃦⃦⃦(4)  \nD↗\nδ⃦⃦⃦⃦C↗\n2⋅X↗\nδ\u0000X↗⃦⃦⃦⃦(5)  \nwhere D↗\nα, D↗\nβ and D↗\nδ denote the approximate distances of α, βCand δ \nfrom X↗, respectively; X↗\nα, X↗\nβ, X↗\nδ denote the position information of α, \nβ, and δ, respectively; C↗\n1, C↗\n2 and C↗\n3 denote the random vectors, \nrespectively. The current solution X↗and the updated solution X↗t1\ncan be described by Eq. (6)-Eq. (9). \nX↗\n1X↗\nα\u0000A↗\n1⋅(\nD↗\nα)\n(6)  \nX↗\n2X↗\nβ\u0000A↗\n2⋅[\nD↗\nβ]\n(7)  \nX↗\n3X↗\nδ\u0000A↗\n3⋅(\nD↗\nδ)\n(8)  \nX↗′\nt1[\nX↗\n1X↗\n2X↗\n3][\n3 (9)  \nwhere A↗\n1 , A↗\n2 , and A↗\n3 denote random vectors, respectively. X. Zhang et al.\n\n[Página 4]\nComputers in Biology and Medicine 163 (2023) 107166\n43. Attack behavior \nThe final stage of the GWO algorithm is the prey attack phase, which \ncan be achieved by adjusting the parameter A. If †A†≼1, the whole wolf \npack approaches the prey X∗CY∗and focuses on the prey; if †A†F1, the \nwhole wolf pack moves away from the prey and looks for new prey \nagain. \n2.2. Description of support vector regression \nSupport vector machine (SVM) models are used to classify data by \nmapping the input metric data to a higher dimensional space, then \nconstructing an optimal hyperplane in this higher dimensional space so \nthat the constructed hyperplane has the largest edges to classify the \ninput data. The learning strategy used by the support vector machine is \ninterval maximization, which can be formalized as solving a convex \nquadratic programming problem. \nInstead of the traditional statistical induction followed by deduction, \nthe SVR model constructs a regression function to infer a prediction \nmodel on the training data and then uses the model to make predictions. \nThe objective of SVR modeling is to build a classification surface that \nseparates the two types of samples as well as possible. SVR modeling \naims to minimize the distance between all the sample data and the \nclassification surface. The accuracy of the SVR model is highly depen -\ndent on the kernel function ’s quality and the penalty factor ’s accuracy, \nand the appropriate choice of parameters dramatically improves the \naccuracy of the regression model. When the parameters of the regression \nmodel are not selected appropriately, the regression model will not be \napplicable to solve the actual problem. For the training data, regression \naims to solve the following regression function, as in Eq. (10). \nfy〈W0y〉b (10) \nThe above equation is 〈w0y〉 is the inner product of w and y. The \nfollowing equation is the constraint to solve the constrained optimiza -\ntion problem: \nMin 1⎡\n2Dw0wFĈm\ni1\u0000\nξiξ∗\ni)\n(11)  \nZi\u0000Dw0yiFb≼εξi (12)  \nDw0yiF\u0000zib≼εyj\u0000yk (13)  \nwhere C represents the penalty factor of the model, the value of C is \npositively related to the complexity of the model, the complexity of the \nmodel increases with the value of C, and the value of C is negatively \nrelated to the computational error of the model, the error of the model \nbecomes smaller as the value of C increases. \nThe solution of the optimization problem is first transformed into the \ncorresponding pairwise problem and, secondly transformed into the \nsolution of the maximum constraint value by introducing the kernel \nfunction. Finally, the regression equation of the model is shown in Eq. \n(14). \nfy̂m\ni1\u0000\naj\u0000aj)0k\u0000\nyCyj)\nb (14)  \n3.The proposed method \nIn this section, three improvement ideas are described, namely, \nSobol sequence-based population initialization, Cauchy random \nreplacement strategy, and directional mutation mechanism. Finally, the \nproposed SRXGWO is used to optimize the hyperparameters of the SVR \nmodel, and the patient-flow prediction model SRXGWO-SVR is \nproposed. 3.1. Proposed GWO variant \n3.1.1. Sobol sequence-based population initialization \nThe population initialization of the original GWO algorithm is \nrandomly generated, which primarily affects the algorithm ’s perfor -\nmance. In contrast, the Sobol sequence can make the spatial points \nuniformly distributed and generate unlimited samples without pre- \ndetermining the number of samples and storing them. Therefore, this \npaper introduces the Sobol sequence to filter the initialization position \nof the grey wolf population, improve the uniformity and diversity of the \ngrey wolf population, and improve the performance of the original GWO \nalgorithm. \nEach dimension of the Sobol sequence is a Radical inversion with \nbase 2, and each dimension has a different generating matrix C. When C \nis taken as a unit vector, the corresponding Sobol sequence is repre -\nsented as \nNi̂M\nk12\u0000kaki (15)  \nwhere i is denoted as a binary number, akion each bit of the number is \narranged as a vector, which is mirrored to the right of the decimal point \nand converted to decimal, resulting in a one-dimensional Sobol \nsequence Xi⊔N1CN2…CNiCi∃N⊓, and a multi-dimensional Sobol \nsequence is obtained by multiplying the generating matrix C of each \ndimension. The Sobol sequence is used to uniformly distribute n points \nwithin the threshold of the target parameter search as the initialized \npopulation space location. The first three solutions are defined as α, β, \nand δ wolves, respectively. To confirm the effectiveness of Sobol \nsequence-based population initialization, Ablation experiments of \nSRXGWO are designed in Section 4.1.2 , where SGWO is the improved \nGWO using this strategy alone. \n3.1.2. Cauchy random replacement strategy \nIn the iterative process, the position update of GWO is conservative. \nOn the one hand, such an update is beneficial to the exploitation of the \nalgorithm. Still, on the other hand, it may cause the algorithm to have a \npoor quality of the search solution and fall into local optimum when \ndealing with multi-peaked problems. Therefore, in this paper, to solve \nthis problem, the Cauchy replacement search strategy is used to \nappropriately perturb the dimensionality of the search agent and \nimprove the interaction between individuals. \nSpecifically, firstly, the grey wolf population with the number of \nindividuals N is traversed by the parameter l, and the selected one is the \nXl individual. Then, according to the ratio of the remaining runs of the \nalgorithm to the total number of runs compared with the Cauchy \nrandom number, if the Cauchy random number is less than the ratio, the \nh-th dimensional value of Xl is replaced with the hth dimensional value \nof the optimal solution α wolves. Finally, the fitness value of the updated \nXl The evaluation function calculates the optimal solution, and the \noptimal fitness value are replaced if the fitness value is better than the \noptimal solution. Otherwise, it remains unchanged. To confirm the \neffectiveness of the Cauchy replacement search strategy, RGWO in \nAblation experiments of SRXGWO is the GWO improved using this \nstrategy alone. \n3.1.3. Directional mutation strategy \nSince the original GWO relies too much on the searchability of the \ntop three ranked wolves to find the optimal solution, it is easy to fall into \nthe local optimal trap and reduce the accuracy of the optimal solution. \nTherefore, this paper proposes a directional mutation strategy based on \ngenetic algorithms ’ mutation and crossover strategies. The directional \nmutation strategy consists of two important operations: directional \ncrossover and directional variation.  \n1. Directional crossover (DM) X. Zhang et al.\n\n[Página 5]\nComputers in Biology and Medicine 163 (2023) 107166\n5The when-directed crossover mechanism uses the position informa -\ntion of the current iteration ’s optimal individual to guide the in-\ndividual ’s next change trend. There are four main parameters, which are \ncrossover rate (pc), variable crossover probability (pcv), directional \nprobability (pd) and multiplication factor (α). First, the execution of the \ndirected crossover mechanism requires different parent individuals in \nthe current population. The parent individuals are generated by random \nselection from the population, pj\n1 and pj\n2, j∃1Cd]. pj\nmean and pj\nbest are the \nmean value of the parent individuals in the jth dimension and the value \nof the best individual in the jth dimension, respectively. In the first case, \nwhen pj\nbest≽pj\nmean (c1 and c2 does the directed hybridization mechanism \ngenerate the individuals). \nval1\u00000B5e⌈\n†pj\n1\u0000pj\n2†\nyj\nu\u0000yj\nl⌉\n(16)  \nβr3\nα2(17)  \nc1val∗\u0000\npj\n1\u0000pj\n2)\nαr3∗e1\u0000β∗1\u0000val∗⃦⃦pj\n1\u0000pj\n2⃦⃦Cifr4≼pd (18)  \nc21\u0000val∗\u0000\npj\n1\u0000pj\n2)\n\u0000α1\u0000r3∗e\u0000β∗val∗⃦⃦pj\n1\u0000pj\n2⃦⃦Cifr4≼pd (19)  \nc1val∗\u0000\npj\n1pj\n2)\n\u0000αr3∗e1\u0000β∗1\u0000val∗⃦⃦pj\n1\u0000pj\n2⃦⃦Cifr4Fpd (20)  \nc21\u0000val∗\u0000\npj\n1pj\n2)\nα1\u0000r3∗e\u0000β∗val∗⃦⃦pj\n1\u0000pj\n2⃦⃦Cifr4Fpd (21) \nWhen pj\nbestDpj\nmean. \nc1val∗\u0000\npj\n1pj\n2)\n\u0000αr3∗e1\u0000β∗1\u0000val∗⃦⃦pj\n1\u0000pj\n2⃦⃦Cifr4≼pd (22)  \nc21\u0000val∗\u0000\npj\n1pj\n2)\nα1\u0000r3∗e\u0000β∗val∗⃦⃦pj\n1\u0000pj\n2⃦⃦Cifr4≼pd (23)  \nc1val∗\u0000\npj\n1pj\n2)\nαr3∗e1\u0000β∗1\u0000val∗⃦⃦pj\n1\u0000pj\n2⃦⃦Cifr4Fpd (24)  \nc21\u0000val∗\u0000\npj\n1pj\n2)\n\u0000α1\u0000r3∗e\u0000β∗val∗⃦⃦pj\n1\u0000pj\n2⃦⃦Cifr4Fpd (25)  \nIf the parent individuals have the same value, but pj\nbestℑpj\nmean. \nval1\u00000B5e⌈\n†pj\nbest\u0000pj\nmean†\nyj\nu\u0000yj\nl⌉\n(26)  \nβr3\nα2(27)  \nc1val∗\u0000\npj\nbestpj\nmean)\nαr3∗e1\u0000β∗1\u0000val∗\u0000\npj\nbest\u0000pj\nmean)\nCifr4≼pd\n(28)  \nc21\u0000val∗\u0000\npj\nbestpj\nmean)\n\u0000α1\u0000r3∗e\u0000β∗val∗\u0000\npj\nbest\u0000pj\nmean)\nCifr4\n≼pd\n(29)  \nc1val∗\u0000\npj\nbestpj\nmean)\n\u0000αr3∗e1\u0000β∗1\u0000val∗\u0000\npj\nbest\u0000pj\nmean)\nCifr4Fpd\n(30)  \nc21\u0000val∗\u0000\npj\nbestpj\nmean)\nα1\u0000r3∗e\u0000β∗val∗\u0000\npj\nbest\u0000pj\nmean)\nCifr4Fpd\n(31)  \nwhere r3 and r4 are two different random numbers, r3∃0C1and r4∃\n0C1. val and β are two parameters computed in each iteration. yj\nu and yj\nl \nare the upper and lower bounds of the individual in the jth dimension, \nrespectively. А is the multiplicative factor.  \n2. Directional variation First, assume that the dimensions of population size and objective \nfunction are D and d, respectively. Assume that the current iteration \nindividual is y. The guided variation mechanism guides the variation of \nthe current iteration individual y based on the position information of \nthe current optimal individual ybest. When individual y is selected for \nguided mutation operation, the DM mechanism will compare the size of \nyj\ni and yj\nbest, if yj\nbest≽yj\ni. \nβ1e[\n2r\u00002\nr]\n(32)  \nβ2e[\nr\u00002\nr]\n(33)  \nym|\n〈\n⎜yj\niβ1∗\u0000\nyj\nu\u0000yj\ni)\nCifr2≼pd\nyj\ni\u0000β2∗\u0000\nyj\ni\u0000yj\nl)\nCotherwise(34)  \nwhere β1 and β2 are two parameters, which can also be called the \nweights that determine the change steps of the formula. r and r2 are two \nrandom numbers, r∃0C1and r2∃0C1, rℑ0. yj\nu and yj\nl are the upper \nand lower bounds of the individual in the jth dimension, respectively. pd \nrepresents the orientation probability, pd∃0B5C1. If yj\nbestDyj\ni. \nym|\n〈\n⎜yj\ni\u0000β1∗\u0000\nyj\ni\u0000yj\nl)\nCifr2≼pd\nyj\niβ2∗\u0000\nyj\nu\u0000yj\ni)\nCotherwise(35) \nTo illustrate the effectiveness of the Directional mutation strategy, \nthe XGWO in ablation experiments of SRXGWO is the GWO improved \nusing this strategy alone. \n3.1.4. Proposed SRXGWO \nThe analysis shows that GWO is an excellent algorithm with solid \nexploitation capability, but several aspects still need improvement. First, \nGWO is randomly generated with strong uncertainty in the initialization \nof the grey wolf population, which will lead to the initial solution of the \nwhole population cannot effectively cover the solution space of the \nproblem, thus causing problems such as low efficiency in the search \nphase. Secondly, the lack of information exchange among individuals in \nthe iterative process of GWO tends to make the algorithm suffer from \npoor-quality of search solutions and fall into local optimum when \ndealing with multi-peaked problems. In addition, GWO relies too much \non the exploitation ability of the top three ranked wolves to find the \noptimal solution, which cannot effectively search the whole solution \nspace, leading to the inability to find the optimal solution and reducing \nthe quality of understanding. \nTherefore, this paper addresses the above three problems and makes \ncorresponding improvements to GWO. First, Sobol sequence-based \npopulation initialization is used instead of the original random initiali -\nzation method to generate a low-sequence population of grey wolves, \nwhich covers the whole solution space uniformly. Second, the dimen -\nsional values between search agents are effectively exchanged by Cau-\nchy’s random replacement strategy to enhance the information \nexchange between individuals and improve the exploitation capability \nof the algorithm. Third, the directional mutation mechanism is intro-\nduced to perform crossover and mutation at the level of the search so-\nlution, and the crossover or mutation operation is performed for the \nnature of the current individuals, which effectively improves the search \nability of the algorithm and the ability to jump out of the local optimum. \nThe algorithm flowchart of SRXGWO as shown in Fig. 1. \n3.2. The proposed SRXGWO-SVR model \nTo accurately predict the number of patients and reasonably \nschedule medical resources, this section combines the high-performance X. Zhang et al.\n\n[Página 6]\nComputers in Biology and Medicine 163 (2023) 107166\n6SRXGWO algorithm with the SVR prediction method and proposes the \nSRXGWO-SVR, an SVR prediction model based on the improved GWO. \nAccording to Section 2.2, SVR is a supervised machine learning \nmethod with two key parameters: the penalty parameter C and the \nkernel function parameter g. The penalty parameter C affects the \ncomplexity and stability of the model, the kernel function parameter \nreflects the distribution of samples in the feature space, and the \nparameter selection directly impacts the prediction accuracy and \ngeneralization ability of the model. Therefore, to address the above is-\nsues, SRXGWO is introduced to optimize the radial basis kernel function \nparameters and penalty factors in the SVR patient-flow prediction model \nto form the best combination of parameters to improve the prediction \naccuracy and reduce the error size. The specific steps for building the \nSRXGWO-SVR model are as follows.  \n(1) Data pre-processing. Routine data pre-processing is performed on \nthe collected patient-flow data, including data cleaning, missing \nvalue processing, outlier processing, etc.  \n(2) Establish the objective function. The sample data are substituted \ninto the mean square error minimization function as shown in Eq. \n(26), and then the optimal radial basis kernel function parameters \nC and penalty factor γ are obtained. \nQmCCσ1\nn̂n\nk1yk\u0000}yksBtBC∃CminCCmaxCγ∃γminCγmax (36)  \nwhere yk denotes the actual size of the patient flow, and √yk denotes the \ncorresponding size value of the patient-flow prediction.  \n(3) Search for hyperparameters using SRXGWO. First, the parameters \ninvolved in the SRXGWO algorithm are set initially. The fitness \nfunction RMSE is applied to calculate the fitness values of the \npopulation individuals, where m is the number of samples. RMSE⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪\n1\nm̂m\nk1yk\u0000}yk2̅\n(37)    \n(4) Determine whether the maximum number of iterations is \nreached. The iteration is continued if the maximum number of \niterations is not reached. Suppose the maximum number of iter-\nations is reached. In that case, the C and γ corresponding to the \noptimal individual location information is output. The best \ncombination of the two parameters is applied to build the \nSRXGWO-SVR prediction model. Then the patient-flow dataset is \npredicted. \nThe flow chart of the SRXGWO-SVR prediction model based on \nhospital patient-flow proposed in this section is shown in Fig. 2. \n4.Experimental results and discussions \nIn this section, ablation and benchmark function experiments are \ndesigned to validate the global optimization performance of SRXGWO. \nThen, the proposed SRXGWO-SVR is used in patient-flow prediction \nexperiments to demonstrate the accuracy and validity of SRXGWO-SVR. \n4.1. Benchmark functions comparison experiment \n4.1.1. Benchmark test experiment setup \nFirst, the running environment of the benchmark function test \nexperiment needs to be described. the software of the experiment is \nMatlab2017b and the core hardware is Intel(R) Xeon(R) CPUE5-2660v3 \n(2.60 GHz). The benchmark function test set used in this section is the \ncurrently familiar IEEE CEC2014, described in detail in Table 1. The \ncomparison experiments include SRXGWO and GWO and well-known \nFig. 1.Algorithm flow chart of SRXGWO \nThe algorithmic complexity of SRXGWO comes \nmainly from Sobol sequences, core formula updates, \nCauchy random replacement strategy, and directional \nmutation mechanism. The complexity level of Sobol \nsequence initialization is ON; the computational \ncomplexity level of the core formula is ON2\nN∗logN; the computational complexity level of \nCauchy random replacement strategy is ON∗logN; \nand the complexity level of directional mutation \nmechanism is ON2. By comprehensive calculation, \nthe overall complexity level of SRXGWO is \nOSRXGWOON2N∗logN.   X. Zhang et al.\n\n[Página 7]\nComputers in Biology and Medicine 163 (2023) 107166\n7algorithms such as PSO, SCA, etc. Therefore, to ensure the validity and \nfairness of the experiments, all swarm intelligence algorithms are \nsearched in dimension 30, the population size is 30, the number of \nevaluations is also uniformly 300,000, and the internal parameters of \nthe algorithms are all default values. Finally, to ensure the correctness \nand validity of the experimental results, all the algorithms were run \nindependently 30 times, and the results of the experiments were further \nverified using Wilcoxon signed-rank test and the Friedman test. \n4.1.2. Ablation experiments \nIn this section, ablation experiments of SRXGWO were designed to \ndiscuss the effects of Sobol sequence-based population initialization, \nCauchy random replacement strategy, and directional mutation mech -\nanism on the effect of GWO. First, the experiments combined the three \nimproved strategies with GWO by permutation, including GWO itself, \nwith a total of eight algorithms, as shown in Table 2. In the table, S \nstands for Sobol sequence-based population, R stands for Cauchy \nrandom replacement strategy, and X stands for directional mutation \nmechanism. in addition, “1″ indicates that the current strategy is used, \nand “0″ indicates that no strategy is used. For example, SGWO uses the \nSobol sequence but not the other two strategies. \nTable 3 shows the experimental results of SRXGWO with the other \nseven algorithms, including the Wilcoxon signed-rank test results and P- \nvalue. The number of algorithms that are “better than/equal to/worse \nthan ” other algorithms. “Mean ” indicates the average ranking of the 30 \nfunctions tested, and “rank ” indicates the final overall ranking. In the \nresults of the Wilcoxon test, SRXGWO is 23 better than the unimproved \nGWO, which indicates that the improvement of GWO by the three \nimprovement strategies is very significant. In addition, SRXGWO has a \nsignificant advantage over SGWO, RGWO, and XGWO using a single \nmechanism, with at least 14 stronger than them. Finally, SRXGWO has \nan advantage over the two-two combination of SRGWO, SXGWO, and \nRXGWO, indicating that the three SRXGWO improvement strategies are \neffective. The table also shows the empirical p-values, and the bolded data indicate that SRXGWO is significantly different from other algo-\nrithms, and it can be said that the advantage of SRXGWO is more \nprominent compared to other algorithms. In summary, the mechanism \nemployed in SRXGWO is reasonable and effective, and can significantly \nimprove the performance of GWO. \n4.1.3. Comparison of SRXGWO with well-known peer algorithms \nIn this subsection, similar algorithm comparison experiments are \ndesigned based on 30 benchmark functions to compare SRXGWO with \n12 other peer algorithms to demonstrate that the proposed algorithm \nhas more robust optimization performance among the same type of al-\ngorithms. Among the compared algorithms, six original algorithms are \nPSO, SCA, MFO, WOA, BA, and FA, all highly cited algorithms. The other \nsix algorithms are new variants proposed recently, including OBSCA, \nm_SCA, OBLGWO, ACWOA, MOFOA, and SCADE. \nTable 4 shows the experimental results of the comparison. Where \nAVG denotes the average optimal fitness value of 30 independent ex-\nperiments, STD denotes the variance of the experiments, and the bolded \ndata are the optimal values of the current function of the algorithm. In \nthe experimental results, SRXGWO finds the optimal solution relative to \nits peer algorithms in most of the function evaluations, especially in the \nclass of complex functions F23–F30, which indicates that SRXGWO is \nmore advantageous in dealing with complex problems. In addition, the \nSTD fluctuation of SRXGWO is small, which suggests that the algorithm \nhas strong stability. \nSimilarly, to further validate the SRXGWO experimental results, we \nused the Wilcoxon signed-rank test to compare and validate SRXGWO, \nand the results are shown in Table 5; the Friedman test was used to verify \nthe average ranking of SRXGWO, and the results are shown in Fig. 3, \nwhich can be more intuitive to observe the comparison results. The \nWilcoxon signed-rank test results show that SRXGWO ranks first overall \nwhen comparing other algorithms and is at least 19 better than other \nhigh citation algorithms and 20 better than other variants. The Friedman \ntest shows that the average ranking of SRXGWO is slightly different, but \nFig. 2.SRXGWO-SVR prediction model based on hospital patient-flow.  X. Zhang et al.\n\n[Página 8]\nComputers in Biology and Medicine 163 (2023) 107166\n8it is still better than PSO and MFO algorithms, and the overall perfor -\nmance is also the first. In summary, the results of the comparison \nexperiment are valid and reasonable, and SRXGWO does outperform \nother peer algorithms. \nTo further demonstrate the advantages of SRXGWO over other al-\ngorithms, this experiment recorded the optimization search process of \neach algorithm and plotted it as an iterative curve, as shown in Fig. 4. \nThe horizontal coordinate indicates the number of evaluations, and the \nvertical coordinate indicates the fitness value. Firstly, it can be seen that \nSRXGWO has good convergence accuracy on F6, F8, F9, F10, F11 and \nF13 in unimodal and simple multimodal function classification and \nfaster search speed than other similar algorithms. In addition, it can be \nobserved in the hybrid and combinatorial functions F16, F23, F30 that \nSRXGWO also has excellent results in solving complex optimization \nproblems. Further in the figure, SRXGWO has a clear advantage in the \nF6, F8, F9, F10, and F16 test functions. Both in the search period of the \nsearch process and the exploitation period of the iteration, SRXGWO can \nquickly find the current optimal solution. At the same time, the other algorithms cannot outperform SRXGWO from the beginning to the end. \nIn addition, SRXGWO has a clear decreasing inflection point in the \nmiddle of the algorithm iteration in the function tests of F11 and F16. \nFew other algorithms can continue the development, which indicates \nthat SRXGWO has a strong ability to jump out of the local optimum. \nFinally, the nine function tests in the figure demonstrate that SRXGWO \nhas stronger search and exploitation capabilities than other algorithms \nand is a high-performance optimization algorithm. In future work, it also \nbe applied to more cases, such as optimization of machine learning \nmodels [65], MRI reconstruction [66], service ecosystem [67], compu -\ntational experiments [68,69], power distribution network [70], and \nmedical signals [71,72]. \n4.2. Patient-flow prediction \nThe patient-flow dataset is presented in this section, and SRXGWO- \nSVR training and test experiments are designed. First, the patient flow \ndataset used is presented. Immediately after, the experimental setup \nincluding comparison methods, parameter settings, and evaluation \ncriteria are described. Finally, SRXGWO-SVR is proposed and applied to \nthe prediction of patient flow. \n4.2.1. Patient-flow dataset \nThe data set used in this section is the attendance statistics of \nWenzhou Medical University Hospital in China, which serves a radius of \nnearly 30 million people and has an annual outpatient volume of 5.3 \nmillion. Due to the large volume of data, the latest data from January \n2022 to September 2022 is selected, with a sample size of 240 items. The \ndata ’s main characteristic attribute is “number of appointments, ” and \nthe label attribute is “number of actuals ”. In addition, to reduce the \ndependence of the model on a single time series and the error of the \nprediction results, this paper also selects three independent attribute \nseries, namely, “number of people without pre-deposit system ”, “num-\nber of people without ID”, and “number of late arrivals ”. “Three inde-\npendent attribute series are selected to describe the trend changes of \npatient-flow with the influence of multiple factors. Finally, when col-\nlecting data, there are inevitably null values and outliers, and this paper \nalso preprocesses the data by removing abnormal samples and linear \ninterpolation. Fig. 5 shows a 240-day line graph of actual hospital visits. \nFirst of all, according to Fig. 5, we can see that the number of hospital \nvisits as a whole fluctuates a lot, and there is a local repetition, mostly \nbetween 14,000 and 4,200 visits. The main reason for this phenomenon \nis that the 14,000 visits are during the weekdays, i.e., Monday through \nFriday, when the hospital doctors are in regular attendance and the \nequipment is functioning normally, and the number of visits is relatively \nhigher. The 4,200 visits are due to the fact that most of the departments \nand facilities are closed during the weekends, and the number of visits is \nrelatively low. In addition, it can be seen that the average number of \nhospital visits between 180 and 220 days was very high, reaching \n18,000 at one point, and the number of weekend visits did not drop too \nmuch. This is because this period corresponds to July and August, which \nis the free time of summer vacation, and most people will concentrate on \ntheir visits during this period. In general, this data set shows a cyclical \ndistribution, and the difficulty in building the model is to reduce the \nerror while avoiding the problem of overfitting. \n4.2.2. Experimental setup \nFirst, the numerical settings of the SRXGWO and GWO algorithms \nused for hyperparameter optimization are presented. The number of \npopulations is set to 20, the dimension is defined as 2, the maximum \nnumber of iterations is 50, the upper and lower bounds for the value of C \nare 100 and 0.1, and the upper and lower bounds for the value of R are \nalso 100 and 0.1. Then, to prove the effectiveness of the prediction \nmodel SRXGWO-SVR improvement, the SRXGWO-SVR was compared \nwith GWO-SVR and the original SVR in the experiments. Also, to prove \nthe effectiveness of SRXGWO-SVR model, backpropagation (BP), Table 1 \nDescription of the 30 benchmark functions.  \nClass No. Functions F∗\ni\nFix∗\nUnimodal Functions 1 Rotated High Conditioned Elliptic \nFunction 100 \n2 Rotated Bent Cigar Function 200 \n3 Rotated Discus Function 300 \nSimple Multimodal \nFunctions 4 Shifted and Rotated Rosenbrock ’s \nFunction 400 \n5 Shifted and Rotated Ackley ’s Function 500 \n6 Shifted and Rotated Weierstrass \nFunction 600 \n7 Shifted and Rotated Griewank ’s \nFunction 700 \n8 Shifted Rastrigin ’s Function 800 \n9 Shifted and Rotated Rastrigin ’s Function 900 \n10 Shifted Schwefel ’s Function 1000 \n11 Shifted and Rotated Schwefel ’s Function 1100 \n12 Shifted and Rotated Katsuura Function 1200 \n13 Shifted and Rotated HappyCat Function 1300 \n14 Shifted and Rotated HGBat Function 1400 \n15 Shifted and Rotated Expanded \nGriewank ’s plus Rosenbrock ’s Function 1500 \n16 Shifted and Rotated Expanded Scaffer ’s \nF6 Function 1600 \nHybrid Functions 17 Hybrid Function 1 (N 3) 1700 \n18 Hybrid Function 2 (N 3) 1800 \n19 Hybrid Function 3 (N 4) 1900 \n20 Hybrid Function 4 (N 4) 2000 \n21 Hybrid Function 5 (N 5) 2100 \n22 Hybrid Function 6 (N 5) 2200 \nComposition \nFunctions 23 Composition Function 1 (N 5) 2300 \n24 Composition Function 2 (N 3) 2400 \n25 Composition Function 3 (N 3) 2500 \n26 Composition Function 4 (N 5) 2600 \n27 Composition Function 5 (N 5) 2700 \n28 Composition Function 6 (N 5) 2800 \n29 Composition Function 7 (N 3) 2900 \n30 Composition Function 8 (N 3) 3000  \nTable 2 \nGWO variants based on three strategies.  \nAlgorithms S R X \nSRXGWO 1 1 1 \nGWO 0 0 0 \nSGWO 1 0 0 \nRGWO 0 1 0 \nXGWO 0 0 1 \nSRGWO 1 1 0 \nSXGWO 1 0 1 \nRXGWO 0 1 0  X. Zhang et al.\n\n[Página 9]\nComputers in Biology and Medicine 163 (2023) 107166\n9random forest (RF), KELM, radial basis function network (RBF), con-\nvolutional neural networks (CNN), and other well-known predictive \nclassifiers are added to the comparison experiments. To verify the pre-\ndiction effectiveness of the proposed patient-flow prediction models, \nthree evaluation metrics are applied to evaluate the performance of \nvarious prediction models in this paper. The three-evaluation metrics \nare the spearman correlation coefficient (R2) of Eq. (38), the mean ab-\nsolute error (MAE) of Eq. (39), and the root mean square error (RMSE) of \nEq. (40) for the evaluation analysis. \nR21\u0000⋃m\nk1yk\u0000}yk2\n⋃m\nk1yk\u0000}yk2(38)  \nMAE1\nm̂m\u00001\ni0†yi\u0000}yi† (39)  \nRMSE⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪\n1\nn̂m\nk1yk\u0000}yk2̅\n(40)  \nwhere m is the number of samples, yk is defined as the actual value size \nof the test sample, yk is the mean size of the test sample, and √yk is the \npredicted value of the test sample. \n4.2.3. Prediction results and analysis \nTo perform regression calculations on the decomposed subsequences \nusing the SVR model, the patient-flow data set needs to meet the input \nformat of the SVR model. For this purpose, the original data samples are \nprocessed as follows. \nFirst, for the time series y1Cy2C…yn, define the input matrix. X⎫\n⎭y1⋯ yd\n⋮ ⋱ ⋮\nyn\u0000d⋯ yn\u00001⎩\n⎨ (41)  \nwhere d is the step size parameter and is the number of sample attri-\nbutes, which in this paper is 4. \nThen, define the output labels. \ny⎫\n⎭yd1\n⋮\nyn⎩\n⎨ (42) \nFinally, use X and y defined above as the input and label of the SVR \nmodel, respectively. In practice, X and y are divided into a training set \nand a test set in the ratio of 1:1. The training set is used to train the \nmodel and determine the optimal parameters of the model. Then, the \ntrained model is simulated and tested on the test set to demonstrate the \ntraining effect of the prediction model. Finally, the accuracy perfor -\nmance of the model is verified by evaluating the metrics R2, RMSE, and \nMAE. The following are the experimental results and training and test \nsets analysis.  \n1. Prediction experiments on the training set \nThe patient–flow dataset is divided into 120 sample sets by 1:1 \ncrossover as the training set for training seven prediction models, \nincluding SRXGWO-SVR, GWO-SVR, SVR, BP, RF, KELM, RBF, and CNN. \nFig. 6 shows the prediction result plot of SRXGWO-SVR. The original \nfold represents the training set’s original data distribution and the Pre-\ndicted fold represents the prediction results given by the SRXGWO-SVR \nmodel. The line graph shows that the overall prediction effect of the \nSRXGWO-SVR model is excellent, especially in the interval of 70–120 \ndays. The Original and Predicted lines nearly overlap, which indicates \nthat the prediction is very accurate. The large deviations between the Table 3 \nResults of Wilcoxon signed-rank test for ablation experiments and P-value.  \nItem SRXGWO GWO SGWO RGWO XGWO SRGWO SXGWO RXGWO \n/\u0000/  ~ 23/1/6 15/1/14 14/3/13 18/0/12 6/5/19 9/0/21 9/2/19 \nMean 2.57 6.90 5.40 4.47 4.67 2.93 4.13 3.53 \nRank 1 8 7 5 6 2 4 3 \nF1 N/A 1.9209E-06 1.0246E-05 4.0483E-01 4.7162E-02 2.8948E-01 9.7772E-02 1.6503E-01 \nF2 N/A 1.9209E-06 1.9209E-06 8.3071E-04 1.6394E-05 2.4118E-04 3.7243E-05 3.3269E-02 \nF3 N/A 1.7344E-06 1.7344E-06 6.0350E-03 8.9364E-01 6.8359E-03 6.2683E-02 3.1849E-01 \nF4 N/A 2.3704E-05 3.8822E-06 6.2884E-01 3.6094E-03 4.4052E-01 7.8647E-02 5.9994E-01 \nF5 N/A 1.7344E-06 2.6033E-06 6.8923E-05 1.7344E-06 4.1955E-04 1.7344E-06 8.1302E-01 \nF6 N/A 4.7162E-02 3.1618E-03 7.0356E-01 4.4052E-01 9.0993E-01 9.0993E-01 9.0993E-01 \nF7 N/A 1.7344E-06 1.7344E-06 1.1499E-04 1.2453E-02 4.0715E-05 3.1618E-03 6.5833E-01 \nF8 N/A 1.7344E-06 1.9209E-06 9.3676E-02 1.7344E-06 1.9861E-01 2.3534E-06 7.1889E-01 \nF9 N/A 3.6004E-01 2.9894E-01 2.4308E-02 8.6121E-01 5.5774E-01 2.2888E-01 7.0356E-01 \nF10 N/A 1.7344E-06 1.7344E-06 4.7162E-02 2.1266E-06 4.7162E-02 1.9209E-06 6.2884E-01 \nF11 N/A 7.3433E-01 3.0861E-01 5.0383E-01 4.1653E-01 1.3591E-01 9.2626E-01 5.5774E-01 \nF12 N/A 8.2206E-02 5.4401E-01 5.9836E-02 3.3173E-04 7.7309E-03 1.1079E-02 3.6004E-01 \nF13 N/A 2.2102E-01 3.9333E-01 1.8462E-01 5.5774E-01 2.9894E-01 3.1849E-01 4.1653E-01 \nF14 N/A 1.3975E-02 1.8326E-03 2.6230E-01 8.5896E-02 1.2544E-01 1.7791E-01 2.3694E-01 \nF15 N/A 1.4773E-04 6.3391E-06 3.6826E-02 4.9080E-01 2.7653E-03 1.8462E-01 1.0201E-01 \nF16 N/A 5.3197E-03 2.9575E-03 1.1138E-03 7.5213E-02 2.5637E-02 1.7138E-01 6.5641E-02 \nF17 N/A 9.8421E-03 3.0861E-01 3.1849E-01 8.7297E-03 3.8723E-02 7.1889E-01 6.5833E-01 \nF18 N/A 6.8359E-03 9.3157E-06 8.5896E-02 8.9187E-05 6.5641E-02 1.4936E-05 1.4773E-04 \nF19 N/A 1.4839E-03 8.9443E-04 1.9861E-01 6.4352E-01 1.3591E-01 2.0589E-01 2.1827E-02 \nF20 N/A 1.9209E-06 1.7344E-06 5.3070E-05 5.3044E-01 5.2165E-06 1.5886E-01 7.3433E-01 \nF21 N/A 9.0993E-01 4.7795E-01 7.5213E-02 1.0639E-01 2.1827E-02 8.2901E-01 5.0383E-01 \nF22 N/A 1.6503E-01 6.5641E-02 7.1903E-02 1.6503E-01 3.8203E-01 1.8519E-02 2.4519E-01 \nF23 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 \nF24 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 \nF25 N/A 1.2290E-05 1.0000E00 1.7344E-06 5.6061E-06 1.0000E00 1.0000E00 1.7344E-06 \nF26 N/A 1.9729E-05 1.6566E-02 1.0357E-03 1.3820E-03 1.5286E-01 3.1603E-02 3.1618E-03 \nF27 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 \nF28 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 \nF29 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 \nF30 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06  X. Zhang et al.\n\n[Página 10]\nComputers in Biology and Medicine 163 (2023) 107166\n10Table 4 \nComparison results of SRXGWO with other algorithms.  \nFun F1  F2  F3  \nItem AVG STD AVG STD AVG STD \nSRXGWO 1.5817E07 8.9158E06 1.7773E08 1.4681E08 4.5005E03 3.3271E03 \nPSO 9.0808E06 1.6903E06 1.4837E08 1.5123E07 9.9378E02 1.2790E�02 \nSCA 2.2839E08 6.9799E07 1.6889E10 2.3915E09 3.7046E04 6.6934E03 \nMFO 8.7549E07 1.0414E08 1.0114E10 5.9855E09 1.0275E05 5.8223E04 \nWOA 2.7540E07 1.1331E07 5.0637E06 8.0209E06 3.2575E04 2.0632E04 \nBA 7.7059E�05 3.5272E�05 5.2698E�05 2.7431E�05 4.2251E�02 1.6464E02 \nFA 2.5269E08 5.1675E07 1.5002E10 1.8122E09 6.4325E04 1.0623E04 \nOBSCA 4.0160E08 1.2958E08 2.4801E10 4.7138E09 5.0550E04 9.2351E03 \nm_SCA 6.3874E07 4.1104E07 6.3318E09 3.7149E09 2.6908E04 6.6947E03 \nOBLGWO 2.2042E07 1.2605E07 1.6887E07 1.2778E07 9.1358E03 3.3451E03 \nACWOA 1.3860E08 6.2461E07 7.4290E09 3.9581E09 5.0191E04 9.0562E03 \nMOFOA 1.2354E09 7.4867E07 7.7038E10 2.4594E09 7.8687E04 3.7238E03 \nSCADE 4.5429E08 1.1842E08 3.0003E10 4.0210E09 5.6160E04 7.2834E03 \nFun F4  F5  F6  \nItem AVG STD AVG STD AVG STD \nSRXGWO 5.4006E02 3.2112E01 5.2075E02 7.2959E-02 6.1118E�02 2.5044E00 \nPSO 4.6707E02 3.2003E�01 5.2095E02 4.0216E-02 6.2317E02 3.2594E00 \nSCA 1.4150E03 2.7588E02 5.2093E02 6.2064E-02 6.3356E02 2.3449E00 \nMFO 1.5209E03 1.0125E03 5.2030E�02 1.6938E-01 6.2361E02 3.5309E00 \nWOA 5.9251E02 6.0017E01 5.2034E02 1.6112E-01 6.3494E02 3.5778E00 \nBA 4.2155E�02 3.2061E01 5.2095E02 6.4791E-02 6.3398E02 3.6948E00 \nFA 1.5337E03 1.5192E02 5.2096E02 4.5044E-02 6.3359E02 9.2350E-01 \nOBSCA 2.3121E03 7.5405E02 5.2095E02 5.7443E-02 6.3205E02 1.4049E00 \nm_SCA 8.0286E02 1.1489E02 5.2056E02 1.4351E-01 6.2212E02 2.8889E00 \nOBLGWO 5.4647E02 4.7860E01 5.2096E02 5.9910E-02 6.1916E02 3.3318E00 \nACWOA 1.1803E03 2.6266E02 5.2085E02 1.7768E-01 6.3363E02 2.7978E00 \nMOFOA 1.0092E04 6.9816E02 5.2106E02 3.7558E-02 6.4079E02 6.7902E-01 \nSCADE 2.2480E03 4.6553E02 5.2097E02 4.3335E-02 6.3428E02 2.4021E00 \nFun F7  F8  F9  \nItem AVG STD AVG STD AVG STD \nSRXGWO 7.0144E02 4.4844E-01 8.3494E�02 6.4659E�00 9.9741E�02 2.4837E01 \nPSO 7.0229E02 1.4348E-01 9.7268E02 2.6092E01 1.1067E03 2.4938E01 \nSCA 8.4528E02 2.6369E01 1.0362E03 1.9353E01 1.1756E03 2.4065E01 \nMFO 7.9627E02 6.3419E01 9.4824E02 3.3320E01 1.1205E03 4.4316E01 \nWOA 7.0099E02 7.2969E-02 9.9955E02 4.1935E01 1.1246E03 5.0520E01 \nBA 7.0066E�02 1.6102E-01 1.0275E03 5.2626E01 1.1641E03 5.6092E01 \nFA 8.4000E02 1.0997E01 1.0240E03 1.2118E01 1.1595E03 1.3038E01 \nOBSCA 9.1758E02 4.4244E01 1.0576E03 1.8074E01 1.1960E03 1.9095E01 \nm_SCA 7.4867E02 2.2125E01 9.3470E02 2.3339E01 1.0491E03 1.9402E01 \nOBLGWO 7.0119E02 9.2779E-02 9.2058E02 3.4783E01 1.0637E03 2.9684E01 \nACWOA 7.3883E02 2.1566E01 9.8681E02 1.5413E01 1.1270E03 1.7226E01 \nMOFOA 1.4082E03 4.6569E01 1.1760E03 1.1881E01 1.2583E03 9.4200E�00 \nSCADE 9.1691E02 4.4469E01 1.0684E03 1.0564E01 1.2058E03 1.8217E01 \nFun F10  F11  F12  \nItem AVG STD AVG STD AVG STD \nSRXGWO 1.7815E�03 2.3016E�02 4.1565E�03 1.0677E03 1.2012E03 3.7913E-01 \nPSO 5.0248E03 5.6761E02 5.8289E03 4.4923E02 1.2023E03 3.0765E-01 \nSCA 7.0064E03 5.2529E02 8.0775E03 3.0696E02 1.2025E03 2.1633E-01 \nMFO 4.6021E03 8.7516E02 5.2295E03 7.7681E02 1.2004E�03 1.9653E-01 \nWOA 4.9691E03 7.4150E02 5.8744E03 9.0861E02 1.2017E03 4.7579E-01 \nBA 5.5034E03 5.6881E02 6.0313E03 6.9746E02 1.2011E03 3.5842E-01 \nFA 7.5532E03 3.1957E02 7.9058E03 2.9315E02 1.2026E03 2.3995E-01 \nOBSCA 6.3076E03 4.9831E02 7.3709E03 3.6056E02 1.2022E03 4.1510E-01 \nm_SCA 4.0584E03 7.1133E02 4.7823E03 6.5478E02 1.2008E03 3.3864E-01 \nOBLGWO 3.8703E03 8.9566E02 5.4446E03 1.0838E03 1.2023E03 5.7151E-01 \nACWOA 4.7309E03 7.3276E02 6.1655E03 9.3475E02 1.2018E03 4.7511E-01 \nMOFOA 9.2300E03 3.9968E02 9.0883E03 2.8283E�02 1.2029E03 2.7367E-01 \nSCADE 7.3914E03 2.4356E02 8.2418E03 2.8346E02 1.2026E03 2.4238E-01 \nFun F13  F14  F15  \nItem AVG STD AVG STD AVG STD \nSRXGWO 1.3004E�03 7.4709E-02 1.4005E03 2.8662E-01 1.5163E03 6.0201E00 \nPSO 1.3004E03 7.7571E-02 1.4003E03 1.2817E-01 1.5166E03 1.1804E�00 \nSCA 1.3030E03 2.6429E-01 1.4439E03 7.6871E00 5.5707E03 5.0710E03 \nMFO 1.3020E03 1.3201E00 1.4347E03 2.4514E01 2.1529E05 5.9281E05 \nWOA 1.3006E03 1.4348E-01 1.4003E�03 4.2398E-02 1.5738E03 2.6213E01 \nBA 1.3005E03 1.5518E-01 1.4003E03 1.3344E-01 1.5296E03 6.4355E00 \nFA 1.3028E03 1.9987E-01 1.4404E03 4.2258E00 1.4383E04 5.6495E03 \n(continued on next page) X. Zhang et al.\n\n[Página 11]\nComputers in Biology and Medicine 163 (2023) 107166\n11Table 4 (continued ) \nFun F1  F2  F3  \nOBSCA 1.3037E03 3.6249E-01 1.4731E03 1.1450E01 1.7595E04 1.0828E04 \nm_SCA 1.3009E03 7.5448E-01 1.4172E03 7.0904E00 2.1370E03 8.9061E02 \nOBLGWO 1.3005E03 1.1306E-01 1.4004E03 1.7893E-01 1.5162E�03 4.9642E00 \nACWOA 1.3015E03 1.0565E00 1.4197E03 1.4944E01 2.0795E03 6.3700E02 \nMOFOA 1.3081E03 3.0417E-01 1.6411E03 9.7254E00 2.2096E05 3.2757E04 \nSCADE 1.3040E03 3.7540E-01 1.4874E03 8.7317E00 1.9117E04 6.0793E03 \nFun F16  F17  F18  \nItem AVG STD AVG STD AVG STD \nSRXGWO 1.6110E�03 4.7850E-01 6.0312E05 7.0192E05 1.0517E04 9.4630E03 \nPSO 1.6120E03 5.3328E-01 2.9096E05 1.3413E05 1.9795E06 5.9660E05 \nSCA 1.6127E03 2.3363E-01 6.2791E06 3.3409E06 1.4952E08 6.6811E07 \nMFO 1.6128E03 4.8942E-01 3.9449E06 7.4952E06 1.2984E08 4.9905E08 \nWOA 1.6124E03 4.0816E-01 4.2933E06 3.4224E06 7.9323E�03 5.7540E�03 \nBA 1.6133E03 3.0344E-01 1.0170E�05 9.2079E�04 9.5662E04 4.7410E04 \nFA 1.6129E03 2.1659E-01 6.7984E06 1.7537E06 3.0346E08 8.6628E07 \nOBSCA 1.6130E03 2.5196E-01 9.4571E06 3.4434E06 1.8689E08 1.1951E08 \nm_SCA 1.6114E03 7.5109E-01 1.7735E06 1.3758E06 2.3094E07 3.3825E07 \nOBLGWO 1.6120E03 4.3556E-01 1.2085E06 9.3079E05 3.4535E04 3.1262E04 \nACWOA 1.6122E03 4.8843E-01 1.5272E07 1.2808E07 5.6959E07 4.4496E07 \nMOFOA 1.6134E03 2.3187E-01 8.7256E07 2.6488E07 5.7808E09 1.0374E09 \nSCADE 1.6127E03 2.0380E-01 1.5384E07 5.7531E06 1.6460E08 8.4537E07 \nFun F19  F20  F21  \nItem AVG STD AVG STD AVG STD \nSRXGWO 1.9173E03 1.3658E01 2.9079E03 1.1572E03 3.7294E05 3.3804E05 \nPSO 1.9172E03 1.9835E�00 2.2959E�03 6.5024E�01 1.1324E05 6.7643E04 \nSCA 1.9893E03 2.5079E01 1.5103E04 3.7270E03 1.4348E06 6.6889E05 \nMFO 1.9738E03 5.5538E01 5.2933E04 4.0442E04 1.0824E06 2.5030E06 \nWOA 1.9384E03 2.7102E01 3.2328E04 2.0050E04 1.1294E06 1.7119E06 \nBA 1.9335E03 3.4019E01 2.4023E03 1.1992E02 6.4514E�04 3.2131E�04 \nFA 2.0050E03 1.2211E01 1.8924E04 7.2016E03 1.6271E06 7.2788E05 \nOBSCA 2.0080E03 9.9922E00 2.9021E04 1.1924E04 1.8445E06 8.5563E05 \nm_SCA 1.9502E03 2.9621E01 1.0791E04 4.6890E03 3.7774E05 5.2345E05 \nOBLGWO 1.9170E�03 1.6997E01 5.6962E03 2.3328E03 5.2217E05 3.6592E05 \nACWOA 2.0080E03 2.5161E01 3.9788E04 1.9571E04 6.9036E06 5.4050E06 \nMOFOA 2.2412E03 1.8281E01 1.4788E05 5.3365E04 3.9619E07 1.4979E07 \nSCADE 2.0087E03 1.1766E01 2.8049E04 9.6164E03 2.3498E06 1.0171E06 \nFun F22  F23  F24  \nItem AVG STD AVG STD AVG STD \nSRXGWO 2.6550E03 1.8361E02 2.5000E�03 0.0000E�00 2.6000E�03 0.0000E�00 \nPSO 2.9439E03 1.8435E02 2.6161E03 5.8346E-01 2.6261E03 5.6750E00 \nSCA 2.9644E03 1.3112E02 2.6653E03 1.3746E01 2.6001E03 6.9049E-02 \nMFO 3.0695E03 2.1885E02 2.6708E03 3.4130E01 2.6722E03 2.7522E01 \nWOA 3.0538E03 2.9728E02 2.6334E03 1.0652E01 2.6118E03 3.7279E01 \nBA 3.3420E03 4.1760E02 2.6152E03 3.0962E-03 2.6654E03 2.6008E01 \nFA 3.0002E03 1.1217E�02 2.7329E03 1.7512E01 2.7050E03 4.5757E00 \nOBSCA 3.1226E03 1.6474E02 2.6858E03 1.7839E01 2.6000E03 3.0468E-04 \nm_SCA 2.6046E�03 2.1219E02 2.6370E03 6.7666E00 2.6000E03 6.8563E-04 \nOBLGWO 2.7106E03 1.7350E02 2.6181E03 1.4048E00 2.6009E03 5.0249E00 \nACWOA 3.1046E03 2.2793E02 2.5122E03 4.6578E01 2.6000E03 5.0998E-06 \nMOFOA 1.8112E04 1.1960E04 2.5000E03 0.0000E00 2.6000E03 0.0000E00 \nSCADE 3.1435E03 1.3870E02 2.5000E03 0.0000E00 2.6000E03 1.9769E-07 \nFun F25  F26  F27  \nItem AVG STD AVG STD AVG STD \nSRXGWO 2.7000E�03 0.0000E�00 2.7004E�03 8.2706E-02 2.9000E�03 0.0000E�00 \nPSO 2.7118E03 7.4419E00 2.7871E03 3.4604E01 3.4367E03 2.8726E02 \nSCA 2.7269E03 8.2372E00 2.7023E03 6.7894E-01 3.4443E03 3.2075E02 \nMFO 2.7194E03 1.1345E01 2.7024E03 1.2575E00 3.6640E03 1.4731E02 \nWOA 2.7153E03 1.6594E01 2.7005E03 1.3903E-01 3.8579E03 2.9527E02 \nBA 2.7314E03 1.2072E01 2.7005E03 1.5158E-01 3.8975E03 3.7586E02 \nFA 2.7336E03 3.7833E00 2.7024E03 3.2727E-01 3.7997E03 2.1775E01 \nOBSCA 2.7000E03 1.4243E-08 2.7040E03 4.1439E-01 3.2568E03 4.0280E01 \nm_SCA 2.7124E03 4.1923E00 2.7008E03 2.1587E-01 3.1851E03 1.2821E02 \nOBLGWO 2.7000E03 0.0000E00 2.7006E03 1.2740E-01 3.1171E03 3.1805E02 \nACWOA 2.7000E03 0.0000E00 2.7636E03 4.8645E01 3.7129E03 3.5075E02 \nMOFOA 2.7000E03 0.0000E00 2.7925E03 2.3425E01 2.9000E03 0.0000E00 \nSCADE 2.7000E03 0.0000E00 2.7070E03 1.7566E01 3.2989E03 1.9042E02 \nFun F28  F29  F30  \nItem AVG STD AVG STD AVG STD \n(continued on next page) X. Zhang et al.\n\n[Página 12]\nComputers in Biology and Medicine 163 (2023) 107166\n12real and predicted series appear on the 3rd day, around the 32nd day, \netc., due to the large fluctuations of the real series, which are difficult to \npredict and lead to the deviation of the model. \nTo illustrate the improvement of SRXGWO-SVR compared to GWO- \nSVR, the iteration curves when SRXGWO and GWO optimized SVR are \nrecorded in this paper, as shown in Fig. 7. The vertical axis represents the fitness value of the swarm intelligence algorithm, i.e., the deviation \nin the model, and the horizontal axis represents the number of iterations. \nThe blue curve represents the iteration curve of SRXGWO-SVR, and the \nbrown curve represents the iteration curve of GWO-SVR. The iterations \nalso confirm that the two hyperparameters of the SRXGWO-SVR pre-\ndiction model are C 76.2569 and R 0.0101. The hyperparameters of \nthe GWO-SVR are C 2.3654 and R 0.0309. Since the overall de-\nviations of both SRXGWO-SVR and GWO-SVR are small, and the process \nof iteration spans an extensive numerical range, we have enlarged the \nkey parts were enlarged. First, in terms of initialization, SRXGWO-SVR \nhas a smaller fitness value than GWO-SVR, which indicates that the \nSobol sequence initialization method enhances the pre-search capability \nof SRXGWO. Then, it can be seen by the magnified image that both \nSRXGWO and GWO find the near-optimal solution at the iteration \nnumber of 2, but it is evident that SRXGWO has a better fitness value for \nthe near-optimal solution. Finally, during the iterations, SRXGWO also \nkeeps searching for the optimal solution, and the fitness value of \nSRXGWO is optimized from 0.0003285 at the beginning to 0.0003271. \nThe fitness value of GWO does not change significantly, and the algo-\nrithm falls into a local optimum. Therefore, it can be said that SRXGWO \ncan improve SVR’s prediction performance more effectively than GWO. \nThis work compares SRXGWO-SVR with well-known classification \nprediction models including GWO-SVR, SVR, BP, RF, KELM, RBF, and Table 4 (continued ) \nFun F1  F2  F3  \nSRXGWO 3.0000E�03 0.0000E�00 3.1000E�03 0.0000E�00 3.2000E�03 0.0000E�00 \nPSO 6.8849E03 8.7157E02 7.4382E04 1.3763E05 1.1678E04 6.2526E03 \nSCA 4.7736E03 2.6752E02 1.2836E07 7.6163E06 2.3980E05 7.9328E04 \nMFO 3.9703E03 2.4525E02 3.6610E06 3.9023E06 6.3694E04 5.2942E04 \nWOA 5.0223E03 6.7902E02 6.3246E06 4.5803E06 7.5080E04 4.8586E04 \nBA 5.1296E03 5.6070E02 3.6448E07 2.6098E07 1.3731E04 1.2024E04 \nFA 4.2282E03 1.4435E02 3.1490E06 8.4923E05 1.7420E05 3.9597E04 \nOBSCA 5.3567E03 2.9466E02 2.0712E07 9.7835E06 3.7443E05 1.9299E05 \nm_SCA 3.8890E03 1.2875E02 1.9729E06 4.4218E06 5.5540E04 2.8810E04 \nOBLGWO 3.4266E03 5.0458E02 4.9452E06 4.3781E06 1.9074E04 1.4566E04 \nACWOA 4.3232E03 1.2224E03 1.8950E07 1.5200E07 3.7383E05 2.2958E05 \nMOFOA 3.0000E03 0.0000E00 3.1000E03 0.0000E00 3.2000E03 0.0000E00 \nSCADE 4.9933E03 8.5262E02 1.5512E07 9.5368E06 4.8922E05 1.6393E05  \nTable 5 \nWilcoxon signed-rank test results of SRXGWO versus other peers.  \nAlgorithm /\u0000/  Mean Rank \nSRXGWO ~ 2.13 1 \nPSO 19/8/3 4.80 4 \nSCA 30/0/0 8.57 9 \nMFO 26/2/2 7.33 7 \nWOA 25/4/1 6.13 6 \nBA 20/7/3 5.93 5 \nFA 30/0/0 9.47 10 \nOBSCA 29/0/1 9.70 11 \nm_SCA 26/2/2 4.73 3 \nOBLGWO 20/2/8 4.00 2 \nACWOA 28/0/2 7.57 8 \nMOFOA 23/0/7 10.17 13 \nSCADE 27/0/3 9.87 12  \nFig. 3.Friedman test results of SRXGWO versus other peers.  X. Zhang et al.\n\n[Página 13]\nComputers in Biology and Medicine 163 (2023) 107166\n13\nFig. 4.Convergence curves of SRXGWO and peer algorithms.  \nFig. 5.240-day folding graph of the number of actual hospital visits.  X. Zhang et al.\n\n[Página 14]\nComputers in Biology and Medicine 163 (2023) 107166\n14CNN to further highlight the benefits of SRXGWO-SVR. It uses R2, RMSE, \nand MAE to assess the accuracy of the predictions. In order to guarantee \nthe stability of the prediction results and prevent chance mistakes, the \n10-fold cross-validation is also utilised in the model training process. \nTable 6 displays the evaluation findings for each model, and it is clear \nthat SRXGWO-SVR performs the best in terms of R2, RMSE, and MAE \nassessment indices. The correlation coefficient, R2, is 0.99879, which \nshows that there is a strong connection between the prediction results of \nthe SRXGWO-SVR model and the actual value. It is clear that SRXGWO- \nSVR performs best in R2, RMSE, and MAE evaluation indices. RMSE and \nMAE are used to evaluate errors. The two forms of SVR errors are the \nleast, with corresponding values of 159.5753 and 100.0009. Following \nline graph analysis, iterative graph analysis, and evaluation result \nanalysis, it can be shown that the SRXGWO-SVR model has a very high \nprediction accuracy and also has more advantages than other \nFig. 6.Prediction results of SRXGWO-SVR.  \nFig. 7.Iteration curves of SRXGWO and GWO when optimizing SVR.  \nTable 6 \nEvaluation results of each prediction model.  \nModel R2 RMSE MAE \nSRXGWO-SVR 0.99879 159.5753 100.0009 \nGWO-SVR 0.99869 159.5886 100.0069 \nSVR 0.99861 166.1568 105.0999 \nBP 0.99820 584.2596 119.5581 \nRF 0.98379 176.6171 335.1838 \nKELM 0.99819 195.6333 144.1484 \nRBF 0.99865 168.8734 110.3226 \nCNN 0.99744 228.9898 110.3226  X. Zhang et al.\n\n[Página 15]\nComputers in Biology and Medicine 163 (2023) 107166\n15algorithms.  \n2. Prediction experiments on the test set \nThe model trained by the real sequence must be closer to the training \nset itself, and there may be problems of false accuracy of the prediction \nresults and overfitting of the prediction model. Moreover, the prediction \nproblem, in reality, will not be the same as the real sequence of the \ntraining set, so it is necessary to simulate and test the completed trained \nmodel by the test set. \nFig. 8 shows the prediction fold of SRXGWO-SVR for the test set. \nAgain, the Original fold represents the data distribution of the test set, \nand the Predicted fold represents the prediction results given by the \nSRXGWO-SVR model. It can be seen that SRXGWO-SVR also predicts \nvery well in the test set prediction with high correlation. However, the \ndeviation of SRXGWO-SVR in predicting the test set is more significant \nthan the training set, e.g., the deviation of the dashboard on days \n7,10,13,36 is larger. Therefore, overall, SRXGWO-SVR still has a highly \naccurate prediction performance and does not fall into the overfitting \nproblem when faced with brand-new patient-flow data. However, it \ncannot achieve the results in training. \nTo further explore the performance of SRXGWO-SVR in the face of \nnew sample sequences and to show the advantages of SRXGWO-SVR \nover other algorithms, the test set experiments also compare \nSRXGWO-SVR with well-known classification prediction models such as \nGWO-SVR, SVR, and BP, and evaluate the prediction results using R2, \nRMSE, and MAE. The evaluation results of each model are shown in \nTable 7. It can be seen that SRXGWO-SVR has higher Spearman corre -\nlation and lower error in RMSE, MAE for prediction results compared \nwith GWO-SVR, SVR, which indicates that SRXGWO-SVR still has an \nadvantage over the unimproved GWO-SVR and SVR in the face of new \ndata sets. In addition, it can be seen that SRXGWO-SVR still has a greater \nadvantage over BP, RF, KELM, RBF, and CNN classical models, and \nperforms better in terms of R2, RMSE, and MAE. \nFinally, this paper combines the prediction results of the training set \nand the test set for statistical comparisons in order to further highlight \nthe significance of the training set experiments and the test set experi -\nments, as well as to demonstrate the prediction effectiveness of \nSRXGWO-SVR for various data sets and the advantages of SRXGWO-SVR \nover other algorithms. The comparison findings are shown in Figs. 9–11, \nwhere the horizontal axis represents each comparison model and the \nvertical axis the assessment standards. Fig. 9 shows that when SRXGWO- SVR is moved from the training set to the test set, the prediction rele-\nvance of the model diminishes and that KELM fluctuates the least. \nHowever, SRXGWO-SVR still outperforms KELM in terms of accuracy, \nsuggesting that it may continue to hold the top spot in future patient- \nflow prediction. The assessment findings were normalized in this \nresearch and then shown once more since RMSE and MAE are prediction \nerrors and the difference between the data is too great. Figs. 10 and 11 \nshow intuitively how much more accurate SRXGWO-SVR is than other \nmodels like BP, RF, CNN, and others. Additionally, even after switching \ndatasets, there is little error variation in the SRXGWO-SVR prediction \nresults, demonstrating the model ’s great stability. It can be shown that \nSRXGWO-SVR is a very accurate, highly generalizable, and highly stable \nprediction model based on the experimental findings of the training and \ntest sets. \n5.Conclusions and future works \nThis paper proposes a high-performance optimization algorithm \nSRXGWO and an effective patient-flow prediction model SRXGWO-SVR, \naiming to predict patients ’ medical needs and achieve orderly patient \naccess by analyzing the changing dynamics and objective laws of \nPatient-flow. First, this paper introduces the current research status of \nartificial intelligence technology for predicting patient-flow and finds \nthat the existing prediction models are not strong in prediction accuracy \nand generalization. Therefore, to improve the accuracy and general -\nization of the prediction model, SRXGWO is proposed based on three \nimprovement strategies and GWO, in which the Sobol sequence im-\nproves the solution space coverage of population initialization, Cauchy \nrandom replacement strategy enhances the information exchange be-\ntween individuals, directional mutation mechanism improves the search \nFig. 8.SRXGWO-SVR predictions for the test set.  Table 7 \nEvaluation results of each model based on the test set.  \nModel R2 RMSE MAE \nSRXGWO-SVR 0.99835 199.0553 125.6847 \nGWO-SVR 0.99802 199.0954 125.7070 \nSVM 0.99783 218.1971 136.1934 \nBP 0.99738 232.2147 150.2261 \nRF 0.97952 701.2146 427.7865 \nKELM 0.99819 291.1310 185.8860 \nRBF 0.99831 201.5883 129.3960 \nCNN 0.98132 628.8679 363.9654  X. Zhang et al.\n\n[Página 16]\nComputers in Biology and Medicine 163 (2023) 107166\n16ability of the algorithm and the ability to jump out of the local optimum. \nThen, the SRXGWO-SVR prediction model is proposed by combining the \nhigh-performance SRXGWO algorithm with the SVR prediction method \nto accurately predict the number of patients and reasonably schedule \nmedical resources. In the experimental part, ablation experiments are \nfirst conducted to compare SRXGWO with GWO combined with different \nmechanisms. It is verified that SRXGWO, with three improved strategies, \nsimultaneously is the strongest performance. Then, SRXGWO is \ncompared with 12 highly cited algorithms, such as PSO, SCA, etc., by 30 \nbenchmark functions to demonstrate that SRXGWO is also superior in \nthe search ability and exploitation ability of peer algorithms. Finally, a \nreal patient-flow dataset is used to validate the prediction ability of the SRXGWO-SVR model. Comparing with the other seven prediction \nmodels, such as BP, CNN, etc., and evaluating R2, RMSE, and MAE, it is \nproved that the prediction results of SRXGWO-SVR are more accurate, \neffective and stronger than other models. \nOf course, the research in this paper also has some limitations. For \nexample, three improvement mechanisms were added to GWO, which \nincreased the algorithm ’s complexity. In the future, we will try to solve \nthis problem using parallel techniques and high-performance com-\nputers. In addition, in future work, we will further enhance SRXGWO \nand SRXGWO-SVR and apply them to more fields. \nFig. 9.R2 comparison results based on two dataset models.  \nFig. 10.Comparison results of RMSE based on two dataset models.  \nFig. 11.Comparison results of MAE based on two dataset models.  X. Zhang et al.\n\n[Página 17]\nComputers in Biology and Medicine 163 (2023) 107166\n17Declaration of competing interest \nThe authors declare that there is no conflict of interests regarding the \npublication of article. \nReferences \n[1]L. Zhang, L. Li, Study on the Equilibrium of Spatial Allocation of Medical Resources \nat Different Levels in Shanghai, Urban Studies, 2019, p. 26. \n[2]D.Y. Zhou, L.Y. Gao, Q.H. Pan, M.F. He, The Impacts of Medical Resources on \nEmerging Self-Limiting Infectious Diseases, vol. 12, Applied Sciences-Basel, 2022 . \n[3]H. Li, D.M. Mu, P. Wang, Y. Li, D.X. Wang, Prediction of obstetric patient flow and \nhorizontal allocation of medical resources based on time series analysis, Front. \nPublic Health 9 (2021) . \n[4]A. Nikakhtar, S.A. Abbasian-Hosseini, H. Gazula, S.M. Hsiang, Social Network \nbased sensitivity analysis for patient flow using computer simulation, Comput. Ind. \nEng. 88 (2015) 264–272. \n[5]A.R. Sharafat, M. Bayati, PatientFlowNet: a deep learning approach to patient flow \nprediction in emergency departments, IEEE Access 9 (2021) 45552 –45561 . \n[6]M. Tavakoli, R. Tavakkoli-Moghaddam, R. Mesbahi, M. Ghanavati-Nejad, \nA. Tajally, Simulation of the COVID-19 patient flow and investigation of the future \npatient arrival using a time-series prediction model: a real-case study, Med. Biol. \nEng. Comput. 60 (2022) 969–990. \n[7]S. Mirjalili, S.M. Mirjalili, A. Lewis, Grey wolf optimizer, Adv. Eng. Software 69 \n(2014) 46–61. \n[8]X.-S. Yang, A new metaheuristic bat-inspired algorithm, in: J.R. Gonz ˘alez, D. \nA. Pelta, C. Cruz, G. Terrazas, N. Krasnogor (Eds.), Nature Inspired Cooperative \nStrategies for Optimization (NICSO 2010), Springer Berlin Heidelberg, Berlin, \nHeidelberg, 2010, pp. 65–74. \n[9]R. Storn, K.J.J.o.G.O. Price, Differential evolution – a simple and efficient heuristic \nfor global, Optimization over Continuous Spaces 11 (1997) 341–359. \n[10] S. Mirjalili, SCA, A Sine Cosine Algorithm for solving optimization problems, \nKnowl. Base Syst. 96 (2016) 120–133. \n[11] S. Mirjalili, A.H. Gandomi, S.Z. Mirjalili, S. Saremi, H. Faris, S.M. Mirjalili, Salp \nSwarm Algorithm: a bio-inspired optimizer for engineering design problems, Adv. \nEng. Software 114 (2017) 163–191. \n[12] S. Mirjalili, A. Lewis, The whale optimization algorithm, Adv. Eng. Software 95 \n(2016) 51–67. \n[13] S. Mirjalili, Moth-flame optimization algorithm: a novel nature-inspired heuristic \nparadigm, Knowl. Base Syst. 89 (2015) 228–249. \n[14] J. Kennedy, R. Eberhart, Particle swarm optimization, in: Proceedings of ICNN ’95 \nvol. 1944, International Conference on Neural Networks, 1995, pp. 1942 –1948 . \n[15] Y. Yang, H. Chen, A.A. Heidari, A.H. Gandomi, Hunger games search: visions, \nconception, implementation, deep analysis, perspectives, and towards performance \nshifts, Expert Syst. Appl. 177 (2021), 114864 . \n[16] A.A. Heidari, S. Mirjalili, H. Faris, I. Aljarah, M. Mafarja, H. Chen, Harris hawks \noptimization: algorithm and applications, Future Generation Computer Systems- \nthe International Journal of Escience 97 (2019) 849–872. \n[17] H. Su, D. Zhao, A. Asghar Heidari, L. Liu, X. Zhang, M. Mafarja, H. Chen, RIME: A \nPhysics-Based Optimization, Neurocomputing, 2023 . \n[18] J. Tu, H. Chen, M. Wang, A.H. Gandomi, The colony predation algorithm, Journal \nof Bionic Engineering 18 (2021) 674–710. \n[19] I. Ahmadianfar, A. Asghar Heidari, A.H. Gandomi, X. Chu, H. Chen, RUN beyond \nthe metaphor: an efficient optimization algorithm based on Runge Kutta method, \nExpert Syst. Appl. (2021), 115079 . \n[20] I. Ahmadianfar, A. Asghar Heidari, S. Noshadian, H. Chen, A.H. Gandomi, INFO: an \nefficient optimization algorithm based on weighted mean of vectors, Expert Syst. \nAppl. (2022), 116516 . \n[21] H. Chen, C. Li, M. Mafarja, A.A. Heidari, Y. Chen, Z. Cai, Slime mould algorithm: a \ncomprehensive review of recent variants and applications, Int. J. Syst. Sci. (2022) \n1–32. \n[22] S. Li, H. Chen, M. Wang, A.A. Heidari, S. Mirjalili, Slime mould algorithm: a new \nmethod for stochastic optimization, Future Generat. Comput. Syst. 111 (2020) \n300–323. \n[23] M. Abd Elaziz, D. Oliva, S. Xiong, An improved opposition-based sine cosine \nalgorithm for global optimization, Expert Syst. Appl. 90 (2017) 484–500. \n[24] C. Qu, Z. Zeng, J. Dai, Z. Yi, W. He, A modified sine-cosine algorithm based on \nneighborhood search and greedy Levy mutation, Comput. Intell. Neurosci. (2018), \n2018) 4231647-4231647 . \n[25] A.A. Heidari, R. Ali Abbaspour, H. Chen, Efficient boosted grey wolf optimizers for \nglobal search and kernel extreme learning machine training, Appl. Soft Comput. 81 \n(2019), 105521 . \n[26] M.A. Elhosseini, A.Y. Haikal, M. Badawy, N. Khashan, Biped robot stability based \non an A–C parametric Whale Optimization Algorithm, Journal of Computational \nScience 31 (2019) 17–32. \n[27] H. Chen, S. Li, A.A. Heidari, P. Wang, J. Li, Y. Yang, M. Wang, C. Huang, Efficient \nmulti-population outpost fruit fly-driven optimizers: framework and advances in \nsupport vector machines, Expert Syst. Appl. (2020) 142. \n[28] H. Nenavath, R.K. Jatoth, Hybridizing sine cosine algorithm with differential \nevolution for global optimization and object tracking, Appl. Soft Comput. 62 \n(2018) 1019 –1043 . \n[29] Y. Zhang, R. Liu, A.A. Heidari, X. Wang, Y. Chen, M. Wang, H. Chen, Towards \naugmented kernel extreme learning models for bankruptcy prediction: algorithmic \nbehavior and comprehensive analysis, Neurocomputing 430 (2021) 185–212. [30] Y. Liu, A.A. Heidari, Z. Cai, G. Liang, H. Chen, Z. Pan, A. Alsufyani, S. Bourouis, \nSimulated annealing-based dynamic step shuffled frog leaping algorithm: optimal \nperformance design and feature selection, Neurocomputing 503 (2022) 325–362. \n[31] Y. Xue, B. Xue, M. Zhang, Self-adaptive particle swarm optimization for large-scale \nfeature selection in classification, ACM Trans. Knowl. Discov. Data 13 (2019) 1–27. \n[32] Y. Xue, X. Cai, F. Neri, A multi-objective evolutionary algorithm with interval \nbased initialization and self-adaptive crossover operator for large-scale feature \nselection in classification, Appl. Soft Comput. 127 (2022), 109420 . \n[33] X. Wang, X. Dong, Y. Zhang, H. Chen, Crisscross Harris hawks optimizer for global \ntasks and feature selection, Journal of Bionic Engineering (2022) 1–22. \n[34] W. Shan, H. Hu, Z. Cai, H. Chen, H. Liu, M. Wang, Y. Teng, Multi-strategies boosted \nmutative crow search algorithm for global tasks: cases of continuous and discrete \noptimization, Journal of Bionic Engineering 19 (2022) 1830 –1849 . \n[35] R. Dong, H. Chen, A.A. Heidari, H. Turabieh, M. Mafarja, S. Wang, Boosted kernel \nsearch: framework, analysis and case studies on the economic emission dispatch \nproblem, Knowl. Base Syst. 233 (2021), 107529 . \n[36] C. Zhao, Y. Zhou, X. Lai, An integrated framework with evolutionary algorithm for \nmulti-scenario multi-objective optimization problems, Inf. Sci. 600 (2022) \n342–361. \n[37] W. Deng, J. Xu, X.Z. Gao, H. Zhao, An enhanced MSIQDE algorithm with novel \nmultiple strategies for global optimization problems, IEEE Transactions on \nSystems, Man, and Cybernetics: Systems 52 (2022) 1578 –1587 . \n[38] G. Sun, R. Han, L. Deng, C. Li, G. Yang, Hierarchical Structure-Based Joint \nOperations Algorithm for Global Optimization, Swarm and Evolutionary \nComputation, 2023, 101311 . \n[39] K. Yu, D. Zhang, J. Liang, K. Chen, C. Yue, K. Qiao, L. Wang, A correlation-guided \nlayered prediction approach for evolutionary dynamic multiobjective \noptimization, IEEE Trans. Evol. Comput. (2022), 1-1. \n[40] G. Sun, G. Yang, G. Zhang, Two-level parameter cooperation-based population \nregeneration framework for differential evolution, Swarm Evol. Comput. 75 \n(2022), 101122 . \n[41] C. Li, G. Sun, L. Deng, L. Qiao, G. Yang, A population state evaluation-based \nimprovement framework for differential evolution, Inf. Sci. 629 (2023) 15–38. \n[42] G. Sun, C. Li, L. Deng, An adaptive regeneration framework based on search space \nadjustment for differential evolution, Neural Comput. Appl. 33 (2021) 9503 –9519 . \n[43] X. Wen, K. Wang, H. Li, H. Sun, H. Wang, L. Jin, A two-stage solution method based \non NSGA-II for Green Multi-Objective integrated process planning and scheduling \nin a battery packaging machinery workshop, Swarm Evol. Comput. 61 (2021), \n100820 . \n[44] G. Wang, E. Fan, G. Zheng, K. Li, H. Huang, Research on Vessel Speed Heading and \nCollision Detection Method Based on AIS Data, Mobile Information Systems, 2022 . \n[45] Y. Xue, Y. Tong, F. Neri, An ensemble of differential evolution and Adam for \ntraining feed-forward neural networks, Inf. Sci. 608 (2022) 453–471. \n[46] J. Chen, Z. Cai, H. Chen, X. Chen, J. Escorcia-Gutierrez, R.F. Mansour, M. Ragab, \nRenal pathology images segmentation based on improved cuckoo search with \ndiffusion mechanism and adaptive beta-hill climbing, Journal of Bionic \nEngineering (2023) . \n[47] Y. Han, W. Chen, A.A. Heidari, H. Chen, Multi-verse optimizer with rosenbrock and \ndiffusion mechanisms for multilevel threshold image segmentation from COVID-19 \nchest X-ray images, Journal of Bionic Engineering 20 (2023) 1198 –1262 . \n[48] J. Xing, H. Zhao, H. Chen, R. Deng, L. Xiao, Boosting whale optimizer with quasi- \noppositional learning and Gaussian barebone for feature selection and COVID-19 \nimage segmentation, Journal of Bionic Engineering 20 (2023) 797–818. \n[49] H. Hu, W. Shan, J. Chen, L. Xing, A.A. Heidari, H. Chen, X. He, M. Wang, Dynamic \nindividual selection and crossover boosted forensic-based investigation algorithm \nfor global optimization and feature selection, Journal of Bionic Engineering \n(2023) . \n[50] X. Wang, X. Dong, Y. Zhang, H. Chen, Crisscross Harris hawks optimizer for global \ntasks and feature selection, Journal of Bionic Engineering 20 (2023) 1153 –1174 . \n[51] C. Lin, P. Wang, A.A. Heidari, X. Zhao, H. Chen, A boosted communicational salp \nswarm algorithm: performance optimization and comprehensive analysis, Journal \nof Bionic Engineering 20 (2023) 1296 –1332 . \n[52] C. Lin, P. Wang, X. Zhao, H. Chen, Double mutational salp swarm algorithm: from \noptimal performance design to analysis, Journal of Bionic Engineering 20 (2023) \n184–211. \n[53] J. Hu, S. Lv, T. Zhou, H. Chen, L. Xiao, X. Huang, L. Wang, P. Wu, Identification of \npulmonary hypertension animal models using a new evolutionary machine \nlearning framework based on blood routine indicators, Journal of Bionic \nEngineering 20 (2023) 762–781. \n[54] J. Liang, K. Qiao, K. Yu, B. Qu, C. Yue, W. Guo, L. Wang, Utilizing the relationship \nbetween unconstrained and constrained pareto fronts for constrained \nmultiobjective optimization, IEEE Trans. Cybern. (2022) 1–14. \n[55] C. Huang, X. Zhou, X. Ran, Y. Liu, W. Deng, W. Deng, Co-evolutionary competitive \nswarm optimizer with three-phase for large-scale complex optimization problem, \nInf. Sci. 619 (2023) 2–18. \n[56] J.S. Chou, J.P.P. Thedja, Metaheuristic optimization within machine learning- \nbased classification system for early warnings related to geotechnical problems, \nAutom. ConStruct. 68 (2016) 65–80. \n[57] A. Kaushik, N. Singal, A hybrid model of wavelet neural network and metaheuristic \nalgorithm for software development effort estimation, Int. J. Inf. Technol. 14 \n(2022) 1689 –1698 . \n[58] M. Mehraein, A. Mohanavelu, S.R. Naganna, C. Kulls, O. Kisi, Monthly Streamflow \nPrediction by Metaheuristic Regression Approaches Considering Satellite \nPrecipitation Data, vol. 14, Water, 2022 . X. Zhang et al.\n\n[Página 18]\nComputers in Biology and Medicine 163 (2023) 107166\n18[59] K. Zhu, S. Ying, N.N. Zhang, D.D. Zhu, Software defect prediction based on \nenhanced metaheuristic feature selection optimization and a hybrid deep neural \nnetwork, J. Syst. Software 180 (2021) . \n[60] J.S. Chou, K.H. Yang, J.P. Pampang, P. Anh-Duc, Evolutionary metaheuristic \nintelligence to simulate tensile loads in reinforcement for geosynthetic-reinforced \nsoil structures, Comput. Geotech. 66 (2015) 1–15. \n[61] J.W. Ma, D. Xia, H.X. Guo, Y.K. Wang, X.X. Niu, Z.Y. Liu, S. Jiang, Metaheuristic- \nbased support vector regression for landslide displacement prediction: a \ncomparative study, Landslides 19 (2022) 2489 –2511 . \n[62] N.D. Hoang, D.T. Bui, L. Kuo-Wei, Groutability estimation of grouting processes \nwith cement grouts using differential flower pollination optimized support vector \nmachine, Appl. Soft Comput. 45 (2016) 173–186. \n[63] S. García, A. Fern˘andez, J. Luengo, F. Herrera, Advanced nonparametric tests for \nmultiple comparisons in the design of experiments in computational intelligence \nand data mining: experimental analysis of power, Inf. Sci. 180 (2010) 2044 –2064 . \n[64] J. Derrac, S. García, D. Molina, F. Herrera, A practical tutorial on the use of \nnonparametric statistical tests as a methodology for comparing evolutionary and \nswarm intelligence algorithms, Swarm Evol. Comput. 1 (2011) 3–18. \n[65] C. Zhao, H. Wang, H. Chen, W. Shi, Y. Feng, JAMSNet: a remote pulse extraction \nnetwork based on joint attention and multi-scale fusion, IEEE Trans. Circ. Syst. \nVideo Technol. (2022), 1-1. [66] J. Lv, G. Li, X. Tong, W. Chen, J. Huang, C. Wang, G. Yang, Transfer learning \nenhanced generative adversarial networks for multi-channel MRI reconstruction, \nComput. Biol. Med. 134 (2021), 104504 . \n[67] X. Xue, G. Li, D. Zhou, Y. Zhang, L. Zhang, Y. Zhao, Z. Feng, L. Cui, Z. Zhou, X. Sun, \nResearch roadmap of service ecosystems: a crowd intelligence perspective, \nInternational Journal of Crowd Science 6 (2022) 195–222. \n[68] X. Xue, X.-N. Yu, D.-Y. Zhou, X. Wang, Z.-B. Zhou, F.-Y. Wang, Computational \nExperiments: Past, Present and Future, 2022 arXiv preprint arXiv:2202.13690 . \n[69] X. Xue, X. Yu, D. Zhou, C. Peng, X. Wang, D. Liu, F.-Y. Wang, Computational \nexperiments for complex social systems —Part III: the docking of domain models, \nIEEE Transactions on Computational Social Systems (2023) . \n[70] X. Cao, T. Cao, Z. Xu, B. Zeng, F. Gao, X. Guan, Resilience constrained scheduling of \nmobile emergency resources in electricity-hydrogen distribution network, IEEE \nTrans. Sustain. Energy (2022) 1–15. \n[71] Y. Dai, J. Wu, Y. Fan, J. Wang, J. Niu, F. Gu, S. Shen, MSEva: a musculoskeletal \nrehabilitation evaluation system based on EMG signals, ACM Trans. Sens. Netw. 19 \n(2022) 1–23. \n[72] J. Zhou, X. Zhang, Z. Jiang, Recognition of imbalanced epileptic EEG signals by a \ngraph-based extreme learning machine, Wireless Commun. Mobile Comput. 2021 \n(2021), 5871684 . X. Zhang et al.",
      "metadata": {
        "filename": "An enhanced grey wolf optimizer boosted.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\An enhanced grey wolf optimizer boosted.pdf",
        "file_size": 8733829,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:34.531945",
        "content_length": 89059
      }
    },
    "d27acfd1-c16a-4c63-8631-72a61182680e": {
      "id": "d27acfd1-c16a-4c63-8631-72a61182680e",
      "content": "[Página 1]\nActa Astronautica 192 (2022) 276–290\nAvailable online 28 December 2021\n0094-5765/© 2021 IAA. Published by Elsevier Ltd. All rights reserved.An industry 4.0 approach to large scale production of satellite \nconstellations. The case study of composite sandwich panel manufacturing \nM. Eugenia,*, T. Querciaa, M. Bernabeia, A. Boschettoa, F. Costantinoa, L. Lampania, \nA. Marchetti Spaccamelab, A. Lombardob, M. Mecellab, L. Querzonib, R. Usingerc, \nM. Aliprandic, A. Stancuc, M.M. Ivagnesd, G. Morabitod, A. Simonid, A. Brand ~aoe, P. Gaudenzia \naDepartment of Mechanical and Aerospace Engineering, University of Rome “La Sapienza ”, Via Eudossiana 18, Rome, 00184, Italy \nbDepartment of Computer, Control, and Management Engineering Antonio Ruberti, University of Rome “La Sapienza ”, Via Ariosto 25, Rome, 00185, Italy \ncRUAG Schweiz AG, RUAG Space, Schaffhauserstrasse 580, 8052, Zürich, Switzerland \ndThales Alenia Space Italy, Via Saccomuro, 24, Rome, 00131, Italy \neEuropean Space Agency, ESTEC: European Space Research and Technology Centre Keplerlaan 1, 2201, AZ Noordwijk, Netherlands   \nARTICLE INFO  \nKeywords: \nIndustry 4.0 \nSpace 4.0 \nSmart manufacturing \nCyber-physical systems \nInternet of things \nDigital twin \nArtificial intelligence \nSpace Systems MAIT \nMega constellations ABSTRACT  \nIn recent years the so-called New Space Economy or Space 4.0 paradigm has seen a number of new commercial \nplayers entering the satellite industry and creating completely new business models, most of which based on very \nlarge constellations consisting of several hundreds or even thousands of satellites. The production of the high \nnumber of satellites involved in modern mega-constellations is bringing in the space industry the necessity of \nimproved and optimized manufacturing approaches suitable for serial production , i.e., standard environment/ \nhigh number of platforms. In this framework, the adoption of Industry 4.0 methodologies within the space in-\ndustry will lead to a significant improvement and optimization of the whole Manufacturing Assembly Integration \nand Testing (MAIT) cycle. The main aim of Industry 4.0 is the creation of intelligent factories where \nmanufacturing technologies are upgraded and transformed by Cyber-Physical Systems (CPSs), the Internet of \nThings (IoT), Cloud Computing and Big Data Analytics with predictive monitoring features. Main element of the \nIndustry 4.0 approach is the synergic use of embedded sensing technologies in the frame of intelligent production \nprocesses, fostering a radical evolution of the industrial values chains, production value chains, and business \nmodels. In the present work, a possible application of the Industry 4.0 concepts to space industry is presented and \ndiscussed in terms of applicability and obtainable advantages. As a case study, the composite sandwich panel \nmanufacturing line of RUAG Space is considered. Particular focus will be given to the development of a CPS, by \nestablishing a control network of sensors (e.g. temperature, location, load) over a targeted MAIT process.   \n1.Introduction \nNowadays, the terms “Industry 4.0” and “Smart Manufacturing ” \nhave become extremely popular to address the so-called Fourth Indus -\ntrial Revolution (4IR) [1] where the evolution of connectivity and \ncomputational calculus permit to create a bridge between physical and \nvirtual worlds. This connection is represented by Cyber-Physical Sys-\ntems, which will be the core of the present study. The same revolution \napplies indeed to the space sector. In 2016, while the European indus -\ntrial context was rushing into the innovation of factories to take \nadvantage of this new concept, the space industry – namely, the Euro-\npean Space Agency as its main promoter - followed through and launched the so-called “New Space ” or “Space 4.0” era [2]. The main \nproblem these ambitious initiatives aim to solve can be synthesized as \nmanufacturing inefficiency in a globalized competitive environment, i.e. \nthe slow operational response to customers ’ complex demand driven by \nincreasing availability of open information. \nThe background scenario to take into consideration to understand \nthis revolution is the worldwide rise of ICTs as a disruptive force of \nchange in any context, even society itself [3]. The faster and easier \navailability of data, as much as the greater reachability of people and \nplaces all over the world both physically and remotely, kicked-off an \nunstoppable globalization driver, increasing competitiveness and \nunlocking new opportunities of sharing knowledge to advance research \nor make a profit. For this reason, the rising demand for greater \n*Corresponding author. \nE-mail address: marco.eugeni@uniroma1.it (M. Eugeni).  \nContents lists available at ScienceDirect \nActa Astronautica \nu{�~zkw! s{yo|kr o>!ÐÐÐ1ow�o �to~1m{y2w{m k�o2km�kk��~{!\nhttps://doi.org/10.1016/j.actaastro.2021.12.039 \nReceived 25 November 2021; Accepted 23 December 2021\n\n[Página 2]\nActa Astronautica 192 (2022) 276–290\n277connectivity pushed new commercial players to risk large investments in \nthe space industry with completely new business models [4]. American \nventures and start-ups led the way and invented a new concept of \nexploitation of already in vogue small satellites, putting them in large \nconstellations and in LEO to give high-bandwidth, low latency internet \naccess to remote areas or to gather new data from more frequent or \nhigher quality observations. The market segment grew exponentially in \nthe last decade, with constellations of even thousands of satellites being \nalready in the launch phase. Nowadays 2500 satellites are actively \norbiting around Earth and are expected to be 50 k in ten years [4]. Fig. 1 \nshows the present status of the largest constellations: Space X’s Starlink \nis the most ambitious one, followed by Amazon ’s Project Kuiper. Both \nprojects aim at operating more than 1000 satellites at a time. In the top \nten also Airbus One Web can be found, whose bankruptcy had worried \ninvestors and shareholders, up to its recent rescue by UK government to \nconvert it into a navigation system after the loss of participation in \nGalileo project because of Brexit [5]. Among the well-known English \ncompanies, SatRevolution is an example of a company mainly based in \nEurope. However, all these companies have made international coop-\neration with ventures, billionaires, or big space players to allow the \nrealization of their projects. A significant reduction of costs is therefore \nnecessary for the industry to take advantage of such a promising new \nmarket segment and open it to smaller businesses or more traditional \nmanufacturing players. Producing thousands of satellites of high quality \nand with tighter deadlines then becomes the top priority, thus requiring \nan innovative approach to manufacturing processes, which made Smart \nManufacturing and its related technologies the best available solutions. \nThe biggest challenges in trying to reduce costs while leveraging capa-\nbility are the following:  \n≡the increased diversification of requirements asked by customers or \nusers.  \n≡the short lead-time to market from product development to product \ndelivery, reduced by global competitiveness. ≡the higher quality assurance needed by more complex new tech-\nnologies [3].  \n≡products reliability, stability, and longevity [6]. \nThe main challenge related to cost reduction is linked with the \nlimited-in-time capital investments, especially concerning the cost of \nlaunch. Developments are being made to make smaller, more flexible \nlaunchers at better prices. Analytics, computing power and AI (Artificial \nIntelligence) algorithms can improve the operations management of \nlarge constellations, reducing response times and operating costs. The \ngoal is to reach the autonomous or semiautonomous spacecraft control \nand management [4]. However, in the space industry the scale of vol-\nume product does not allow for the introduction of automation [7] as \nmuch as it does in mass-market sectors, thus generating the need for \nalternative concepts of product and process optimization, relying more \non IT (Information Technologies) than OT (Operational Technologies) \nor, better, on the integration of both. Even when more easily applicable, \na new wave of automation would require the conversion of blue-collar \njobs to white-collar jobs, with a fast reskilling and new training of \nhuman resources toward greater horizontal connectivity and interop -\nerability [8]. In any case, the space industry needs to become “smarter ” \nand its smartness level will be measured by the degree of reflection of its \nproducts and processes in the new digital world, also called the “cy-\nberspace ”. Competition is not between products or processes anymore, \nbut rather between the information services and analytics algorithms \nbehind them. The solution proposed to convey “smartness ” to the real-\nization of large constellations of small satellites can be borrowed by the \nnewest frontiers of Smart Manufacturing, especially in the framework of \n“Industry 4.0” initiatives spread all over the world [9]. Thanks to the \nprinciples of Smart Manufacturing, it is possible to translate a conven -\ntional in-line dedicated manufacturing process into a fully integrated \ndigitalized process using the latest information technologies. The space \nindustry has not a long experience in serial process optimization, \ntherefore it must take advantage of the state of the art in other industries \nto win the challenges previously mentioned and meet the need for a Acronyms/abbreviations \n4IR Fourth Industrial Revolution \nAGV Automated Guidance Vehicle \nALM Application Lifecycle Management \nAM Additive Manufacturing \nAPM Automated Insert Potting Machine \nASIC Application Specific Integrated Circuit \nCPS Cyber-Physical Systems \nCPPS Cyber-Physical Production Systems \nDT Digital Twin \nERP Enterprise Resource Planning ICT Information and Communication Technology \nIoT Internet of Things \nKET Key Enabling Technology \nKPI Key Performance Indicator \nLEO Low Earth Orbit \nMAIT Manufacturing Assembly Integration and Testing \nMEMS Micro Electro-Mechanical Systems \nMES Manufacturing Executive System \nMOM Manufacturing Operations Management \nNDI Non-destructive Inspection technique \nOT Operational Technology \nUT Ultrasonic Testing  \nFig. 1.Pareto chart of planned and launched small satellite constellations per number of satellites as of March 2020. [© newspace.im ].  M. Eugeni et al.\n\n[Página 3]\nActa Astronautica 192 (2022) 276–290\n278better capability over cost ratio. It will need to master the latest de-\nvelopments in other industries in the field of Smart Manufacturing and \ntake them to the next level for the first time. Opportunities for stan-\ndardization, modularization and serialization are evident, especially \nbenchmarking with the Do-It-Yourself philosophy that the Chinese \nAerospace industry is trying to pursue with micro and nanosatellites. For \nexample, from a strategic point of view, with respect to German-born \n“Industry 4.0”, Chinese plan (called “Made in China 2025”) has star-\nted with a pilot and then will be extended step by step [10,11]. Their \ngoal is to introduce a comprehensive innovation system on a small scale \nto increase the manufacturing capability index and thus the convenience \nof the product at macro level both for producers and customers [6]. \nBesides the fundamental trend of lowering costs, satellite \nmanufacturing moves toward the concept of universality, that is the \npossibility to leverage international competitiveness for supplies and \nraw materials to set factories for enhanced rapidity, easier maintenance, \nand better upgradability [6]. Following this reasoning, the UK space \nsector in 2019 explored the interesting concept of a “Global Production \nNetwork”, focused on dynamics and thus on the importance of heritage \nto manage associated risks. In particular, the UK sector is trying to \nmitigate risks by counting on well-proven technology and structuring \nsolid relationships with national and international regulators [12]. The \nBrexit and covid-19 pandemic, however, will put the success of this \nviewpoint in doubt for two reasons: first of all, after Brexit the UK will be \nless and less protected by the European Community, being more of a \ncompetitor in the European market, and thus incurring in higher taxa-\ntion for import/export, thus losing the advantage of lowering process \nand product costs [5]; secondly, having disrupted logistics, travel and \ntransportation, the covid-19 and its safety regulations will force Euro-\npean space companies to rapidly invest in new home-made technologies \nin order to keep their workforce and avoid tensions coming from the risk \nof increased unemployment and difficulty to expatriate. \nThe paper is organized as follows: Section 2 explains the theoretical \nbackground necessary to understand the use of cyber-physical systems in \na space factory; Section 3 introduces the approach to its implementation, \nreviewing hardware technology, software technology, sensors systems \nand Non Destructive Inspection (NDI) techniques; in Section 4 the \napproach is applied to the case study on the real process of RUAG’s \nsandwich composite panel manufacturing; eventually, Section 5 pre-\nsents the conclusions of the study. \n2.Theoretical background: cyber-physical systems \nIn this Section, an overview of Smart Manufacturing concepts, tools \nand strategies is presented together with the most adopted SM frame -\nwork, RAMI 4.0, are illustrated. Among SM concepts, the theory of \nCyber-Physical-Systems is highlighted as the foundation for the inte-\ngration of IT and OT enabling the improvement of an MAIT process in \nthe space industry. Concepts, characteristics, and contextualization in a \nproduction environment are given. \n2.1. Smart Manufacturing concepts, tools and strategies \nSmart Manufacturing focuses on establishing intelligent and \ncommunicative systems based on interoperability, i.e. machine-to- \nmachine and human-to-machine interconnections, dealing with a digi-\ntalized data flow from intelligent and distributed system interaction \n[13]. Products, machines, and company processes acquire a higher level \nof knowledge by data acquisition of parameters, e.g. product charac -\nteristics, localization, process parameters (temperature, pressure, speed, \netc.), and also information from the other stakeholders (e.g. customers, \nsuppliers). This data collection is transferred through internal or \nexternal communication networks, to be shared and to enable \nself-control capacities of products, machines, processes. Thus, these el-\nements become “smart”: capable to measure, recognize, communicate, \ncarry out decision-making processes (mostly without man intervention), to activate actions and operations in production [14]. Smart \nmanufacturing in short is “a data intensive application of information \ntechnology at the shop floor level and above to enable intelligent, effi-\ncient, and responsive operations” [15]. To consider a process “smart”, it \nis necessary to satisfy the following characteristics [16]: (I) computeri -\nzation, or the ability to control or monitor operations through pro-\ngrammable logics such as PLC, microcontroller, or microcomputer; (II) \nconnectivity, achieved through communication networks such as 4G, \n5G, Wi-Fi or specialized protocols; (III) visibility; (IV) transparency, \nbuilding an operating history and allowing problem solving based on \nreal data; (V) predictive capacity, adopting models based on algorithms \nthat correlate past operations with the measured real-time parameters; \n(VI) adaptability, allowing the system to adapt its operations. \nSmart Manufacturing strategic action lines are focused to reach im-\nprovements on autonomous interoperability, agility, flexibility, \ndecision-making, efficiency or cost reductions, mass customization, \nservitization [3,17–19]. It enables companies to cope with the chal-\nlenges of producing individualised products as expected by customers \nwith a short lead-time to market and at the cost of mass production [20]. \nSmart Manufacturing relies on the interdisciplinary and complex \nimplementation of several different technologies, such as \nCyber-Physical-Systems, Artificial Intelligence [21], Cloud Computing \n[22], Big Data analytics [23], Machine Learning [24], Internet of Things \n[25], Augmented Reality and Virtual Reality [26], etc. This paper will \nfocus on those selected to the implementation of a CPS architecture in a \ncomplex MAIT process in the space industry. However, a common \nstandard infrastructure is shared among all these technologies, helping \nto contextualize them in the overall product life-cycle value chain: the \nso-called RAMI 4.0 [27]. RAMI 4.0 ensures intercommunication and \nunderstanding across all business units and functions with a \nservice-oriented architecture, starting from physical things and arriving \nto the most digital business processes through a bi-dimensional hori-\nzontal and vertical expansion, following respectively the increase of \nvalue and the increase of authority, see Fig. 2. RAMI 4.0 well represents \nIndustry 4.0 concepts of holistic integration as well as easy interopera -\nbility, modularity and reconfigurability, bringing them directly in the \nstructure of the business, sometimes called enterprise, for its compre -\nhensive service-oriented goals. Being RAMI 4.0 such a complex archi -\ntecture, a hybrid model with the upper layers substituted with \ntraditional MES and/or ERP is under study to fasten its implementation \n[28]. Among all SM tools, the CPS has the best potential to reproduce \nthis framework, being the only one able to also integrate all other \ntechnologies. \n2.2. Cyber-physical systems applied to a manufacturing environment \nRecently, there has been an explosive growth in the development and \nimplementation of various Cyber-Physical Systems (CPS) [29]. CPS \n(cyber-physical systems) are physical systems that incorporate in-\ntegrations of computation, networking-communication, and physical \nprocesses control, see Fig. 3. They are made of heterogeneous cooper -\nating components interacting through a complex, coupled physical \nenvironment operating over many spatial and temporal scales [30]. \nEmbedded computers and networks monitor and control the physical \nprocesses, with feedback loops where physical processes affect compu -\ntations and vice-versa. CPS are defined as transformative technologies \nfor managing interconnected systems between their physical assets and \ncomputational capabilities [31]. CPS are systems of integrated compu -\ntational entities which are in intensive connection with the surrounding \nphysical world and its on-going processes, providing and using, at the \nsame time, data-accessing and data-processing services available on the \nInternet [32]. In other words, CPS can be generally characterized as \n‘‘physical and engineered systems whose operations are monitored, \ncontrolled, coordinated, and integrated by a computing and communi -\ncating core’’ [33]. To this end, CPSs are able to Ref. [34]: M. Eugeni et al.\n\n[Página 4]\nActa Astronautica 192 (2022) 276–290\n279(i) collect data referred to themselves and their environment  \n(ii) process and evaluate these data  \n(iii) connect and communicate with other systems  \n(iv) initiate actions. \nA CPS is defined as a system in which physical objects are required to \nbe accompanied by their representation in the digital world, to be in-\ntegrated with elements with computing, storage, and communication \ncapabilities, and to be networked between them. They are considered \none of the key technological innovations (Key Enabling Technology - \nKET) of the Fourth Industrial Revolution, a transformative technology \nthat can be placed in the foreground for the potential promised for the \ncreation of value along with the three dimensions of the digitalization of \nmanufacturing: the smart product, Smart Manufacturing, and changes in \nthe business models of companies [35]. Smart manufacturing systems \nuse CPS predominantly as a tool to monitor the physical world and make \ndecentralized decisions in the virtual world, often referring to Cyber-Physical Production Systems (CPPS). The growing availability, \naffordability and adaptability of sensors and connection systems are \nincreasing the widespread adoption of CPS and CPPS. Production data \nare easier to be collected and transferred to cloud platforms, where \nanalytics and AI tools permit to analyse and predict the production be-\nhaviours, and consequently act (manually or automatically) to increase \nperformance. A complete CPS should be able to get information from the \nphysical world and act on it, usually after data computations suggested \nthe action to be implemented. CPS should not be confused with IoT, \nbecause IoT is part of a CPS system, that for example could also include \nAI technology. Some insights on these technologies can be found in \nRef. [36]. Fig. 4 shows how CPPS connect a system in the physical world \nand its Digital Twin (in the cyber world), with an important remark \nabout the human-centred vision of these systems. Indeed, the oper-\nator/manager is always needed to check the process reliability and often \nto validate the analysis and the actuating decisions. In the design of a \nCPS it is recommended by Ref. [30] to pay attention to issues of \nFig. 2.RAMI 4.0 architecture is the most common standard framework for the application of Smart Manufacturing to a whole enterprise value chain. The archi -\ntecture is structured on a bi-directional and multi-layer way, with developments going both horizontally, following product life cycle value (procurement to sales) \nand hierarchical levels of complexity (product to connected world) and vertically, expanding from the simple asset (e.g. shop floor equipment) to the entire busi-\nness [27]. \nFig. 3.The figure shows a layout of the Cyber- \nPhysical System of a sensorized MAIT process plant. \nIt illustrates the cycle from physical to cyber domains, \npassing by control, communication and computation \nfunctions. In the computational layer, data records \nand analysis are performed. The Digital Twin re-\nproduces the process plant in the Cyber Domain, \nwhile the Internet of Things allows its communication \nwith the physical domain through the interconnection \nof sensors in an online platform. Eventually, intelli -\ngent analytics can be performed by AI algorithms \nintroduced in the computation phase and aimed at \nimproving the data reports, allowing faster decision- \nmaking, possibly made autonomously or semi- \nautonomously by the process machines themselves.   M. Eugeni et al.\n\n[Página 5]\nActa Astronautica 192 (2022) 276–290\n280reliability and security, level of abstraction and architecture styles for \nmodular design and development, new frameworks and algorithms, \nconcepts of dependability, reconfigurability, certifiability and trust-\nworthiness. More research on this topic can be found in Refs. [37,38]. \n3.Approach: implementing a CPS architecture in space \nmanufacturing \nThe main problem a space factory nowadays faces is related to the \nhigh costs of keeping the pace of a competitive technological market, \nleading companies worldwide with the help of new business models to \nlower entry barriers to the segment. Technological innovation inte-\ngrating the newest IT solutions is requested to traditional manufacturing \nshop floors to leverage space long-term heritage while keeping the \nbusiness sustainable. The CPS was chosen among all SM tools, according \nto the features described in the previous chapter, as the best candidate to \ngive a measurable and reliable improvement to a space manufacturing \nprocess. Introducing a CPS into a space manufacturing facility requires a \ntwo-level approach:  \n1. Monitoring the product to be manufactured.  \n2. Monitoring the production, integration and test means necessary to \ndeliver the product. \nTo fully realize this approach, three main areas of technical \ncompetence have been considered:  \n● Hardware technology, to identify the critical operations of a complex \nMAIT process;  \n● Software technology, to identify the most performant solutions to \ndigitalize the process;  \n● Sensor systems and Non-Destructive Inspection (NDI) techniques, to \nidentify types of sensors and related techniques to enhance product \nand process control and monitoring. \n3.1. Hardware technology \nIn this paragraph an overview of the applications of SM tools from \nthe point of view of Hardware Technology is given. First, the illustration \nof typical production systems will explain the convergence toward the \ncellular system. Then, traditional production characteristics in the space \nindustry will be mentioned and their evolution following SM principles \nfrom the point of view of HW technology will be presented. \nConcerning production systems, the aerospace industry is mainly \ncharacterized by intermittent production and the management of the production is typically based on job-shop criteria [39]. This system type \nis characterized by low volume and high variety with relatively low \nproduction rate and high flexibility. It is also noteworthy that the \nplanning, routing, and scheduling function is typically done for each \npart independently. The efficiency of the machines is low and, to reduce \ncost, they are general purpose machines. The machines and the move -\nments are reduced, and few setup operations are required. On the con-\ntrary, in large-scale productions machines are dedicated, and processing \nparameters are optimized for few types of parts. A continuous flow must \nbe maintained. In this case, high costs and highly specialized machines \nare affordable thanks to the large production volume. This type of \nproduction system is referred to as ‘process-based ’. The addressing of \nresources is completely dedicated to the optimization of specific pro-\ncesses and the routing of the single part reflects the sequence of the \noperations over the selected machine. As a result, the movements are \nmany and the mean lead time is affected. Between these two extremes, a \n‘combination layout ’ is usually proposed in industrial manufacturing. It \nis the so-called ‘cellular production ’ that requires a systematic approach \nin the design methodology that incorporates all the previous benefits \nand can easily move between the extremes, see Fig. 5 [39]. The benefits \nof the cellular production system are widely accepted in industrial \nproduction for the so-called mass customization, but many items must \nbe considered in the space industry. It is particularly important to \nmaintain the quality assurance of the fabricated components and it is \ndifficult to allow the automation of labour-intensive operations and \ncombinations between process options. \nTraditionally, space production systems, besides being of “job-shop ” \ntype, were mainly designed for single units. In Ref. [24] the example of \nBoeing is presented: the focus was on single unit delivery models and \nunique parts were supplied by customized contracts with suppliers \ncoming only from the space industry, with prototypes being qualified on \ndemand. Other traditional features included: (I) most of the documen -\ntation produced and archived in paper; (II) a low presence of automation \nor robotization; (III) single shift/5 days schedule; (IV) long life-cycle \nproducts of typically 10 years; (V) siloed structures for the different \ndepartments; (VI) “push ” approach with large stock of finite product \n[24,40–42]. Most of these characteristics evolved in the framework of \nIndustry 4.0 and Space 4.0 initiatives. The following interesting SM \nconcepts have been applied to HW technology, specifically in the \ncontext of small satellites ’ constellations [43]:  \n● Automated Guidance Vehicles (AGVs) \nFig. 4.An example of the implementation of a cyber-physical system in the \nproduction department. The job flows from production orders to machines, \nwhile the decisions rise from machines back up to customer ’s orders. At every \nstage of data gathering and processing, human intervention is always necessary \nto provide advanced monitoring functions and interpreting results [91]. \nFig. 5.Types of production systems in terms of volume & variety and flexibility \n& efficiency. At the extremes, job-shop system qualifies as high variety and high \nflexibility and process-based system as high efficiency & high volume. The \nhybrid type cellular system lies in between. M. Eugeni et al.\n\n[Página 6]\nActa Astronautica 192 (2022) 276–290\n281Equipped with cameras and navigation software, these vehicles \nallow the transportation of heavy components or the final assembly \nthrough the factory. Well known in the automotive industry, this level of \nautomation was used by OneWeb facility in Florida.  \n● Spring-based loading machines \nSpecific machines equipped with springs are used to load satellites to \navoid human non ergonomic operations. In general, flexibility of \norientation and vertical movement is required by satellite platforms to \nallow the last operations, when most subassemblies are completed and \nreaching parts is more difficult.  \n● Additive Manufacturing (AM) \nAM is based on a layer-by-layer addition of material instead of \ntraditional machining ’s material-removing approach, thus allowing the \nrapid prototyping of even complex geometries thanks to advanced 3D \nsoftware design (for this reason, the technique is also called 3D printing \n[44]). A 3D printing machine was used by Telesat ’s facility in Ottawa, \nCanada, to realize the apertures of the phased-array antennas. This \nallowed the reduction of multiple part numbers into a single standard \none, besides a significant acceleration of times and reduction of costs. \nThe main limitation of the AM manufactured part is of comparable low \nstrength and associated quality, coupled with a high cost of the printing \nmachine system [45].  \n● Robots & cobots \nMultiple robotic solutions were applied for example by Telesat to \nmake repetitive and heavy operations easier, from manipulation of parts \nto cutting. However, these were used only to make prototypes, as the \nmass production is yet to come. The new frontier of robotization con-\ncerns “cobots ”: interconnected and easily programmable; autonomous, \nflexible, and collaborative; able to avoid collisions based on pre-set up \n360•visualizations of the environment; easily programmable [46]. An \nexample of learning cobots for painting can be found in Bombardier \n[47]. \n3.2. Software technology \nStarting from the traditional manufacturing data management sys-\ntems and passing by the concepts of interoperability, given by the In-\ndustrial Internet of Things (IIoT), and digitalization, given by the Digital \nTwin (DT), in this paragraph the most used CPS architecture will be \npresented. \nMany industries adopt the HMI-SCADA System, a comprehensive \nreal-time data control hardware and software architecture for \nManufacturing plants [48]. The Supervisory Control And Data Acquisi -\ntion system (SCADA) represents the overall control system, gathering \nand analysing data in real-time, while the Human Machine Interface \n(HMI) is the software showing data in a digestible format for humans \nthrough computing systems, allowing the interoperability of workers \nand machines. Interacting with equipment through user-friendly SW \ninterfaces, humans can reduce repetitive, unsafe, and heavy work or \nfacilitate their day-to-day process monitoring activities. The \nHMI-SCADA System architecture is based on executive functions and \ncommunicating functions. The executive functions are represented by \nthe field instrumentation (in-house instruments monitoring and con-\ntrolling automation processes) and Remote Terminal Units (RTU) or \nProgrammable Logic Controllers (PLC), whose concepts are mostly \noverlapped and represent the interface between plant equipment and \ntheir computing control units. The communication functions, on the \nother hand, are represented by a data communication layer, transferring \ndata from the plant to the server; a telemetry layer, transmitting and \nreceiving data from external sources (e.g. Earth telecommunication stations or satellite ground stations); the SCADA host or supervisory \nsystem, including the HMI software, representing the data receiving \nserver. In Fig. 6 the system is vertically contextualized as a level of the \noverall complex enterprise system standardized by the ISA95 model \n[49], including the device level at the bottom and the management \n(MOM or MES) and enterprise (ERP) interfaces at the top, the last two \nrepresenting the data analytics and integration platforms. Enterprise \nsystems (ES) or enterprise information systems (EIS) concepts have been \nresearched and utilized for decades, with applications in the aerospace \nindustry being studied at [50]. In some industries, like the pharmaceu -\ntical one, the application of this standard from day one allowed the \ntransition to paperless processes [51]. \nOver the next ten years, the number of connected devices will exceed \nthe number of inhabitants of the world [33]. The IIoT represents a \npossible evolution or integration of the HMI-SCADA System in the new \nindustrial landscape. The IIoT is defined as a network of physical sys-\ntems that can interact with each other thanks to standard communica -\ntion protocols, to achieve a common goal. Physical systems, and \ntherefore ’things ’, are represented by sensors, actuators, communication \nmodules and devices that can collaborate with each other, through \nintelligent components and applied software, and therefore achieve \nobjectives that strongly depend on their ability to transmit and process \ninformation. It is a multi-directional communication between processes, \nincluding the machinery used, the components and the products. The \nmain form of communication allowed by IIoT technology with respect to \nSCADA/HMI is machine-to-machine communication: the devices \ncommunicate directly using programmable electronic devices and \nwireless technologies. This form of interoperability among machines \ncould extensively contribute to the implementation of a CPS architec -\nture. Other recommendable IIoT characteristics are self-optimization, \nself-healing, self-configuration, and self-protection [52]. A use-case of \nIIoT-based architecture applied in aerospace manufacturing can be \nfound in Ref. [53]. \nTo implement a CPS, process physical entities must also have a \nfaithful representation in the digital world. This representation is \ndefined as ‘digital twin ’ (DT). DTs are commonly known as a key enabler \nfor the digital transformation in manufacturing. Different definitions \nagree on features such as (i) connectivity, i.e., the ability to communi -\ncate with other entities, (ii) autonomy, i.e., the possibility to live inde-\npendently from other entities, (iii) homogeneity, i.e., the capability, \nstrictly connected to autonomy, that allows using the same DT regard -\nless of the specific production environment, (iv) easiness of custom -\nization, i.e., the possibility to modify the behaviour of a physical entity \nby using the functionalities exposed by its DT, and (v) traceability, i.e., \nthe ability to trace the activity of the corresponding physical entity. To \nallow traceability, systems based on barcodes, QR codes or RFIDs [54] \nare applied or incorporated in the product. Finally, DTs monitor and \ncontrol the physical entities, where physical entities send data to update \nwhat are commonly referred to as the virtual models [55,56]. Many are \nthe advantages of this concept. First, it is easily useable for small series \nof customized products. Secondly, the DT allows modular simulation: \nbeing able to reproduce the operating system, it allows to modify \nproducts in a flexible way and to speed up innovation processes. The \npossibility of minimizing the time between design and product delivery \nthrough a DT is a good alternative not to change the process itself, which \nis often more complicated and more expensive. What a DT facilitates \nthat other technologies are not able to is the real time reproduction of \nthe system. Real time is a key concept in process monitoring, as the \nevolution of industrial trends follows speed, with dynamic systems \nhandling high volumes of data [57], also thanks to the introduction of \nnew semiconductor materials which can fasten electronical connections \nof process equipment and information systems. A challenge to consider, \nespecially when scaling the concept to a whole process, is the risk to \ndesign closed cycles, with monitoring functions heavy dependant on the \ndigital reproduction itself. Simulation models made of DTs are able to \nembrace the entire value chain and the entire life cycle of the products, M. Eugeni et al.\n\n[Página 7]\nActa Astronautica 192 (2022) 276–290\n282thus providing the necessary parameters not only to make fast and \nshort-term decisions, but also to allow more sustainable decisions in the \nlong-term, using the permanent collection of data through historical \nseries, which become rich material for statistical models to build more \naccurate correlation coefficients and to show more complete predictive \ngraphical instruments for trends ’ interpretation. \nThe integration of the SCADA/HMI level with the machine-to- \nmachine communication characteristic of the IIoT, linked to a 3D real- time throughout the process virtual representation of all sensors and \nmachinery using DTs, allows the implementation of a fully compre -\nhensive CPS architecture. The so-called CPS 5C level architecture [58] \nclearly defines, through sequential activity flows, the architecture of a \nCPS starting from the initial data acquisition, up to the creation of final \nvalue. The architecture is characterized by five levels: 1. Smart \nConnection Level: guarantees the timely and reliable acquisition of data \nfrom sensors, controllers or company production systems (e.g. ERP, \nFig. 6.Pyramidal architecture of an overall enterprise SM standards ’ system, showing the incorporation of HMI/SCADA level [53].  \nFig. 7.CPS 5-levels architecture is the most used. Levels of connection, conversion, cyber, cognition and configuration are shown. Related assets, users, and functions \nare displayed. [58]. M. Eugeni et al.\n\n[Página 8]\nActa Astronautica 192 (2022) 276–290\n283MOM, MES). It is central, considering the heterogeneity of the data, to \nselect appropriate data acquisition methods and sensors (in terms of \ntypes and specifications). 2. Data-to-Information Conversion Level: \nconverts the data collected into significant information through specific \nalgorithms and analysis. 3. Cyber-Level: acts as a central hub, where all \nthe information deriving from the various machines and components, \narrives and creates an intelligent network. They are then analysed to \nunderstand specific or collective information about the state of the \nsystem and evaluated to predict future events. 4. Cognition-Level: the \nimplementation of the CPS at this level generates an in-depth knowledge \nof the monitored system, a valuable support in the decision-making \nprocess. This knowledge allows operators to manage the system opti-\nmally. To ensure visibility, clarity, and immediacy in the understanding \nof the system by the operators, it is often necessary to implement graphic \nanalysis and representations. 5. Configuration-Level: the configuration \nlevel constitutes the feedback of the cyberspace in the physical space and \nacts as a supervisory control to make the machines self-configuring and \nself-adaptable. It acts as a resilience control system (RCS) and allows to \nmonitor, prevent, and correct the systems. This 5-level framework is by \nfar the main reference for CPS. Fig. 7 represents its levels and functions. \nFuture developments of CPS use the 5G protocol network, aiming to low \nlatency (ms) and high data rates (Gbps) [59]. With 6G, already started to \nbe studied, even Tbps could be reached [60]. However, it is estimated \nthat 6G will not be implemented until 2030 [61]. \n3.3. Sensor systems and non-destructive inspection techniques \nSensor systems and Non-Destructive Inspection techniques are \nreviewed as suitable to be integrated in the framework of a CPS to \nimprove the process by faster and more qualitative structural health \nmonitoring. Specific references to advantages in the space industry are \nmade. \n3.3.1. Fiber optics sensors \nThe principle of fiber optics sensors is that of an input light reflected \non a fiber and showing an interference pattern passing by a light de-\ntector. Fiber optics sensors can measure all traditional sensed parame -\nters in structural health monitoring (e.g. strain, temperature, crack \npropagation, leakage, corrosion). Fiber optic sensors have numerous \nadvantages for application in aerospace. In fact, they are lightweight, \ncan be easily embedded into composite structures and are immune to \nelectromagnetic interference. Furthermore, considering that a huge \nnumber of sensors will be necessary to completely cover the structural \nelements of a space structure, the multiplexing capability of optical fi-\nbers, that is the possibility of writing several sensors into one single \nfiber, results in a notable advantage both in terms of low complexity and \nlow weight. The fact that optical fibers do not involve any electric signal \nis a clear advantage from a safety point of view. Other advantages are \nthe long-term stability, low signal losses, the ability to operate in a wide \nrange of temperatures. Drawbacks using these sensors are the difficulty \nin replacing or repairing the fiber if it fails and some technological dif-\nficulties at cryogenic conditions such as low response time for hydrogen \nsensing and low sensitivity for temperature measurements. One of the \nmost interesting applications for small satellite manufacturing is that it \nis possible to monitor the degree of cure by simply measuring the \nrefractive index changes in isothermal conditions [62]. Optical fibers \ncan also be employed for chemical sensing during the cure of composite \nmaterials. At NASA-Langley chemical spectra were obtained using single \nmode optical fibers [63]. \n3.3.2. Acoustic emission sensors \nAcoustic emission (AE) sensors resort to the analysis of emissions \nfrom active defects and are sensitive to defect activity when a structure \nis loaded either during service or a proof test. AE analysis is a useful \nmethod for the investigation of local damage in materials. It is also \npossible to observe damage processes during the entire load history without any disturbance to the specimen. Acoustic emission sensors are \nused for monitoring a wide number of defects in materials such as dy-\nnamic strain, crack growth, leakage, corrosion, delamination, fiber \nbreakage. They are particularly suitable for monitoring the material \nfatigue behaviour since dynamic strain is measured. Conventional \ntechnologies used for AE monitoring are piezoelectric transducers, but \nfiber optic-based AE sensing technology is gaining more and more \nconsideration for the already mentioned advantages related to the use of \noptical fibers [64]. In-flight AE sensors have been successfully demon -\nstrated on the DC-XA in-flight experimentation vehicle. The AE moni -\ntoring system was conceived to have information on temperature limits, \nvibrations, noise characterization and to provide in-flight data from the \nLH2 cryogenic tank. The control unit AEFIS [65] (Acoustic Emission \nFlight Instrumentation System) was able to monitor and send informa -\ntion to the on-board computer for real-time monitoring. The system was \nalso conceived for active monitoring through excitation of the acoustic \nemission sensors. A health monitoring system with 48 sensors for strain \nand hydrogen monitoring was used on the composite hydrogen tank for \nthe X-33 experimental vehicle during on-ground tests. In addition, AE \nsensors for high temperatures have been developed for the structural \nmonitoring of the nose TPS on the X-38, now cancelled. Acoustic \nemission sensors have also been successfully applied during static tests \nof the X34 composite wing. \n3.3.3. Piezoelectric materials \nPiezoelectric materials [79] are composite materials with incorpo -\nrated electrical connections. Under the application of stress, their elec-\ntrodes are excited and the material is charged. Moreover, it manifests a \nlinear change in shape. Charge and linear change represent the char-\nacteristics of such materials in terms of their dual use as actuators \n(transforming electrical energy into mechanical energy) and sensors \n(detecting possible defects measuring structural variations). The most \ncommon family of piezoelectric materials is the so-called PZT (zirconate \ntitanate family). Used as actuators, PZTs sensors can actively monitor \nthe structure. Functioning as both transmitters and receivers, they can \nbe part of a flexible structural health monitoring system capable of \nperforming several evaluation functions. Presently, they can be used for \nactive damage detection with high-frequency electro-mechanical \nimpedance method, or active damage detection with the pulse-echo and \npitch-catch techniques using Lamb-waves, or as passive sensors for \nlow-impact damage detection and acoustic emission detection [66]. \nFurthermore, they can be used in a phased array of sensors that allows, \nthrough the superposition of the generated waves, to focus or steer the \nbeam in a specific direction. Several studies have demonstrated the \ncapability of piezoelectric sensors for damage detection in composite \nmaterials. Studies at ONERA have demonstrated that Lamb waves are \nsensitive to debonding caused by low impact in a sandwich structure \n[67]. \n3.3.4. Micro-Electromechanical Systems (MEMS) \nMicro-Electromechanical Systems (MEMS) are thin-film devices \nproduced through photolithography and chemical etching. Sensors for \ntemperature and pressure measurements are already available as com-\nmercial off-the-shelf products, but other MEMS sensors exist such as \naccelerometers, gyros, acoustic emission, and chemical sensors. The \nadvantage of using MEMS sensors is their small size and potentially low \ncost. They can be easily embedded or surface bonded. Furthermore, with \nan ASIC (Application Specific Integrated Circuit) technology, it is \npossible to create a microsystem of different sensors in one single chip \n[68]. Some issues for structural health monitoring of aerospace struc-\ntures are the temperature range that goes to the best from \u000050 •C to \n175 •C. Furthermore, the temperature dependency of some sensors \nmay affect the measurements, thus limiting the performance [69]. \nDevelopment is required to attain space qualification, and most of all, \nthese devices should be tested in real environment conditions. Another \nissue is the packaging of MEMS sensors. As an example, a smart layer M. Eugeni et al.\n\n[Página 9]\nActa Astronautica 192 (2022) 276–290\n284composed by PZT sensors developed by Acellent Tech. Inc. has been \nembedded into a composite laminate that was also equipped with an \nelectromagnetic layer to measure electrical resistance properties. \n3.3.5. Self-monitoring materials \nSome structural materials can be used as self-monitoring materials, \nwhich means they can sense their own strain and damage by measuring \ntheir electrical resistance [70]. Carbon fiber-reinforced polymers are \nvery suitable as self-monitoring materials since the fibers are electrically \nconducting and the electrical properties of the material are sensitive to \ndamage. Self-monitoring materials are intrinsically smart, which means \nthey don’t need embedded or attached sensors, so they have some ad-\nvantages like low cost, simple design, great durability, large sensing \nvolume and absence of mechanical property degradation due to \nembedding of sensors. A problem of concern for electrical measurements \nis the electrostatic disturbance due to the electrical charging of the \nstructure when flying through charged atmosphere at high speeds or in \norbit due to encounter with ionized molecules. Another issue to be \naddressed is the ability to locate the damage in large composite struc-\ntures. As an alternative, CFRP self-healing materials are under study \n[71], having the advantage of using a new ISOX (iso-\ncyanurate-oxazolidone) thermosetting matrix able to restructure itself in \ncase of delamination or debonding (fiber breakages are not detectable \nthough). \n3.3.6. Thermocouples, strain gauges and accelerometers \nTogether with new sensing technologies such as optical fibers, pie-\nzoelectrics and so on, conventional sensors are also used for structural \nhealth monitoring in the space industry. The major issues for sensors \nsuch as thermocouples, strain gauges and accelerometers are the weight \npenalty from the sensor itself, but also from the wires required to pro-\nvide power and data communication. Wireless transceivers can be used \nto overcome this penalty. These have been flight tested at NASA in the \nframe of ARIES experiment as part of an Integrated Vehicle Health \nMonitoring architecture [72]. The transceivers radio frequency emis-\nsions have been demonstrated to not have interference with communi -\ncation and navigation antennas. \n3.3.7. Thermography \nThermographic methods are non-destructive inspection methods in \nwhich the presence of flaws is determined by monitoring the flow of heat \nover the surface of a structure after some external introduction of a \ntemperature gradient [73]. The presence of flaws disrupts the normal \npattern of heat flow that would be expected in a sound structure. The \nmethod is more sensitive to flaws near the surface. Modern thermo -\ngraphic systems commonly use infrared (IR) cameras to detect radiated \nheat and are controlled by TV video electronics which sample the field of \nview at a typical rate of 50 Hz, allowing temperature variations on a 20 \nms timescale to be resolved [74]. The camera is sensitive to temperature \nchanges of about 0.005 •C and covers a chosen range of temperature, \n4 •C and 8 •C being commonly suitable, although operation is feasible \nbetween \u000050 •C and 100 •C. Liquid crystal coatings and pyroelectric \ndetectors have also been used to detect IR radiation. Thermographic \nmethods fall broadly into two groups: active methods, and passive \nmethods. Active methods are those in which the thermal gradient is \nproduced and continuously maintained by the application of cyclic \nstress. An interesting application of IR thermographic technique is the \ninstallation of a thermo-camera to an unmanned aerial vehicle for the \nmonitoring of defects at the distance of 2 m and 6 m [75]. Passive \nmethods are those in which the thermal gradient results from a transient \nchange. Passive methods are the most widely applied NDI techniques in \ncomposites inspection. Also, non-IR conductive thermography has been \napplied to aerospace applications, such as in the field of Maintenance, \nRepair and Overhaul (MRO), being able to identify defects in a laminate \ncomposite at low temperature [76]. 3.3.8. Ultrasonic testing \nUltrasonic testing (UT) is the most widely used non-destructive in-\nspection method for the examination of composites [77]. On micro -\nscopically homogenous materials (i.e. non-composite) it is commonly \nused in the frequency range 20 kHz to 20 MHz. With composite mate-\nrials the testing range is significantly reduced because of the increased \nattenuation, so the operating frequency limit is usually 5 MHz or less. \nHowever, the ability to resolve small flaws will also be reduced. In most \ntechniques, short pulses of ultrasound (typically a few microseconds) are \npassed into the composite material and detected after having interro -\ngated the structure. The techniques include pulse-echo, through- -\ntransmission, back-scattering, acoustic-ultrasonics, and ultrasonic \nspectroscopy. In these methods, it is important to avoid frequencies at \nwhich resonance occurs between ply interfaces. For unidirectional plies \nspaced at 8 plies/mm this frequency is usually about 12 Mhz. There may \nbe an additional resonance for woven fabrics at approximately 6 Mhz for \n0.25 mm plies, although resonance at other frequencies has been seen in \npractice. Different approaches can be used: manual, immersion, and \nlaser testing. Moreover, an example of combination of UT and conven -\ntional IR thermography techniques is presented in Ref. [78], using car-\nbon/epoxy patches bonded on an aluminium plate and producing fusion \nalgorithms correlating both inspection results. \nSensors and NDI techniques above mentioned can be object of trade- \noff analyses to improve space manufacturing processes according to \ncustomers ’ requests, mainly to increase factories ’ KPIs like Quality of \nService (QoS) and defects rate. In the following chapter the case study of \na real space manufacturing process includes the assessment on the use of \nsome of these technologies. \n4.Case study: RUAG ’s composite sandwich panel manufacturing \nAs a case study, RUAG ’s composite sandwich panel manufacturing \nprocess was taken in consideration. Panel manufacturing today is still a \nlargely manual process. This is especially valid for large, non-serial \nspacecrafts for scientific missions. With the establishment of constella -\ntions during the last years, considerable effort was made to industrialise \nthe overall manufacturing process. Still, the state-of-the-art \nmanufacturing process is distant from an Industry 4.0 philosophy. \nHere follow the main process areas (according to the job-shop pro-\nduction system), each of which is made of stations, operations and \nphases:  \n- Parts preparation: procured and stored parts (cut-to-shape \naluminium face sheets, already expanded aluminium honeycomb \ncore, adhesives, foams, inserts and heat pipes) are machined and pre- \nassembled. Parts whose surface is destined to external exposure are \ntreated under galvanic bath to prevent corrosion.  \n- Panel assembly: the pre-assembly is bonded under hot press.  \n- Panel inspection and testing: Non-Destructive Inspections (NDI) \ntechniques (e.g., Ultrasonic Inspection - UT) and testing (e.g., flat-\nwise tensile strength) are performed.  \n- Panel equipment: hot-bonded inserts are automatically potted, and \ncold-bonded inserts are machined; thermal equipment (e.g., paint \nand heat pipes) is integrated. \nFor a summary of existing sensors or automated equipment deliv-\nering process data, with related measurement properties and units the \nreader can refer to Table 1 \nThe general approach to industrial panel manufacturing varies at \ndifferent points compared to the more traditional solution. With \nindustrial-based manufacturing, materials and processes are tailored to \nthe product itself. In the case of the sandwich panels, this means that \nface sheets are already procured cut to shape. Furthermore, time- \nconsuming processes are being automized, as for instance the bonding \nof inserts. A two-level approach has been considered to improve the \nprocess as shown in Fig. 9. M. Eugeni et al.\n\n[Página 10]\nActa Astronautica 192 (2022) 276–290\n285First, existing data must be collected, categorized, and interpreted, so \nthat bottlenecks or shortcomings can be more easily identified. Different \ntypes of sensors and non-destructive sensing techniques were identified, \nsee Fig. 10(a) and Fig. 10(b) showing how a sensing network is deployed \nover the observed process. The collection and categorization of data will \nbe possible thanks to the improvement of the built-in traceability sys-\ntem, extending it to the whole process and introducing an IoT infra-\nstructure based on a sensors network and a data processing and analytics \nplatform. Process parameters like pressure, humidity and temperature \nare tracked, as well as product part numbers; optical imagery aids \nquality control, sound alarms on thresholds helps day-by-day opera -\ntions; all these functions (and more programmable ones according to \nproduction needs) are so interconnected and easily monitorable by a \ndashboard by means the software architecture shown in Fig. 10 (c)). \nObservations are used to perform an AS-IS analysis about data \ncollection within the case process. The Acatech maturity model is chosen \nas a foundation for the development of a new assessment model to \nrepresent the current smartness level of the process. The new assessment \nmodel is based both on the evaluation of single activities, which is \ncrucial to thoroughly verify every operating step of the process, and on \nthe assessment of the whole process, which allows to identify transversal \nintegration elements, which would be otherwise scarcely visible. The \nfirst assessment focuses on the single activities. This process is based \nupon a customization of the Acatech model, which ensures a digital \nmaturity level assessment comprising six maturity stages: from a not- \ndigitalized company to a company with all the features of Industry \n4.0. This model was adjusted to the objective of the assessment, i.e. to \nmeasure the smartness of the process in terms of data collection, and to \nassess single activities. In particular, a qualitative assessment was per-\nformed, and the achievement of a smart level was evaluated according to \nthe maturity model ’s features of computerization, connectivity, visibil -\nity, transparency, predictive capacity and adaptability, see Sec. 2.1 and \nFig. 8. \nAn analysis of possible gatherings of new useful information by new \ntechnologies or new stations can be conducted once the already avail-\nable are collected and analysed by means a suitable software and \ncomputing infrastructure. In case the interpretation of data executed at \nstep 1 needed a deeper insight or critical points in the process were \nidentified, some new technologies should be added accordingly. One of Table 1 \nAs-is process: sensors and automated equipment with related measurement pa-\nrameters and units. © RUAG Space.  \nPROCESS \nSTATION/ \nOPERATION/ \nPHASE SENSOR/ \nEQUIPMENT MEASUREMENT \nPROPERTY MEASUREMENT \nUNIT \nParts preparation/ \nPanel milling Laser External dimensions \n(lenght, width, pocket \npositions) Mm \nParts preparation/ \nFacesheet \nbonding surface \npreparation/ \nGalvanic bath Timer Time of bath S \nParts preparation/ \nFacesheet \nbonding surface \npreparation/ \nGalvanic bath Sensor Chemical \ncomposition pH \nParts preparation/ \nFacesheet \nbonding surface \npreparation/ \nGalvanic bath Sensor Chemical \ncomposition Concentration \nParts preparation/ \nInsert bonding \nsurface \npreparation/ \nGalvanic bath Timer Time of bath S \nParts preparation/ \nInsert bonding \nsurface \npreparation/ \nGalvanic bath Sensor Chemical \ncomposition pH \nParts preparation/ \nInsert bonding \nsurface \npreparation/ \nGalvanic bath Sensor Chemical \ncomposition Concentration \nParts preparation/ \nAdhesives/ \nIncoming \ninspection Tensile \ntesting \nmachine Lap shear strenght Mpa \nParts preparation/ \nAdhesives/ \nStoring Timer Storage time S \nSandwich \nassembly/ \nSandwich layup Laser Alignment Mm \nSandwich \nassembly/Panel \nbonding Hot press Pressure Bar \nSandwich \nassembly/Panel \nbonding Hot press Temperature •C \nSandwich \nassembly/Panel \nbonding Hot press Time S \nPanel inspection \nand testing/ \nUltrasonic \ninspection Sensor Panel defects \n(delamination, \ninhomogeneity, \nbonding defects, etc.) dB \nPanel inspection \nand testing/ \nFlatwise tensile \ntest Tensile \ntesting \nmachine Tensile strenght Mpa \nPanel inspection \nand testing/3- \npoint and 4- \npoint bending \ntest Tensile \ntesting \nmachine Bending strenght Mpa \nPanel inspection \nand testing/ \nThermal cycling Thermal \nchamber Outgassing % \nPanel equipment/ \nInsert potting APM Insert-injected \nadhesive mass G  Table 1 (continued ) \nPROCESS \nSTATION/ \nOPERATION/ \nPHASE SENSOR/ \nEQUIPMENT MEASUREMENT \nPROPERTY MEASUREMENT \nUNIT \nPanel equipment/ \nInsert potting APM Adhesive mixing ratio % \nPanel equipment/ \nInsert potting APM Insert height w.r.t. \nfacesheet Mm \nPanel equipment/ \nInsert potting APM Insert angle w.r.t. \nfacesheet Rad \nPanel equipment/ \nInsert potting APM Insert position Mm \nPanel equipment/ \nAdhesive curing Sensor oven Curing temperature •C \nPanel equipment/ \nAdhesive curing Sensor oven Curing time S \nPanel equipment/ \nInsert proof-load \ntest Sensor Load-displacement \ndiagram N/mm \nPanel equipment/ \nInsert pull-out \ntest Sensor Pull-out load N \nPanel equipment/ \nHeater bonding Laser Position Mm \nPanel equipment/ \nMLI bonding Testing \nmachine Bonding strenght Mpa \nPanel equipment/ \nTie-base bonding Testing \nmachine Bonding strenght Mpa  M. Eugeni et al.\n\n[Página 11]\nActa Astronautica 192 (2022) 276–290\n286the main process phases to concentrate on to add information is quality \ntesting. Quality testing usually requires long times and heavily impacts \nboth the technical and economic aspects of the process. Making it more \nagile and automating it would fasten the process and make the testing \nitself more accurate thanks to incorporated statistical models. In our \ncase study, testing stations J4 and J5 right after panel bonding (process \nstep 11) and J2 just after panel machining and inserts installation by \nmeans of RUAG ’s fully automated APM technology (process step 13) is \nof particular interest for future developments. \nThe method proposed to assess the AS-IS process status will be \napplied to understand the steps required to reach the desired “smart ” \nlevel, in terms of individual activities, and to understand how to generate a greater level of interconnection and be able to monitor a \ngreater number of performances. If, for instance, the intention is to \nguarantee that the available data generates an “Enterprise ” level of \ninterconnection throughout the entire process, see Fig. 6, it would be \nnecessary to guarantee a circulation of data that goes beyond the com-\npany ’s internal borders, in an extensive and transversal manner between \nthe various constituent areas. The aim of this study ’s CPS is to reach the \nManufacturing Operations Management (MOM) or Manufacturing \nExecution System (MES) level. However, its inherent feature of scal-\nability allows the extension from the single process to the overall factory \nto the overall plant. \nAn example of process improvement through the application of the \nFig. 8.An example of process performance assessment using the AS-IS model.  \nFig. 9.The study ’s approach has two levels: data collection and interpretation, aimed to gather data from the process, and CPS architecture and implementation, to \ndigitalize the existing data and possibly add new information. Measurement of KPIs is then applied to both industrial and digital aspects of the study to verify \nimprovements. M. Eugeni et al.\n\n[Página 12]\nActa Astronautica 192 (2022) 276–290\n287CPS model was realized through a preliminary simulation of raw data \ncoming from the Automated Potting Machine and connected to the \nsoftware architecture described in Fig. 11. First of all, the APM data \n(represented by a list of measurements and their timestamp) is included \nin a database. Once the database is collected, data is normalized and aggregated online according to the different timeframes and stored in \nthe data lake. In batches, such data is clustered and displayed in a \ndashboard. The data collection and visualization allow the monitoring, \ncontrol, and use of data analysis to detect process deviations for example \nto stop the line or alert operators. A possible dashboard and an example \nFig. 10.RUAG ’s sandwich composite panel manufacturing process is shown before (a) and after (b) the integration of existing sensors with an IoT network com-\nmanded by a computing infrastructure. Sensors measure temperature, pressure and humidity and scan panel ’s surface through optical and laser systems. Traceability \nis also performed through barcodes. The whole process is included in a tree-shaped system. The computing infrastructure is then represented in detail (c). Online \nprocessing of sensors ’ data inputs is performed through actions including preprocessing, normalization, thresholds ’ check, and monitoring. Processed data is then \nstored in a data lake, where users are able to have continuous open access, while data are interpreted by a statistical model-based closed-loop of KPIs ’ prediction and \nforecast and are displayed through a user-friendly visual dashboard. Some of the many SW platforms available in the market to realize such concept are mentioned \n[81–86,88–90,92]. \nFig. 11.The CPS∕layers as a flux of data from input \nto output. In the first layer data from interconnected \nsensors (IoT) is simulated or collected from historical \narchives, so that the process is reconstructed (DT). In \nthe cyber layer, i.e. the core of the CPS, data collec -\ntion, storage and analytics is done with the help of \nstatistical predictive models, allowing data correla -\ntion (AI). In the final layer, data can be visualized \nthrough reports and insights and interpreted with \nhuman touch, allowing to understand causation \neffects.   M. Eugeni et al.\n\n[Página 13]\nActa Astronautica 192 (2022) 276–290\n288of graphs displayable as output are shown respectively in Fig. 12 and \nFig. 13. \nThe approach was extended to the whole process thanks to its layout \nreconstruction in the cyber space, see Fig. 10. In Fig. 14 a representation \nof the Sandwich Panel Manufacturing process using BPMN and a simu-\nlation through Bizagi Modeler allows the performance of a «what if » \nanalysis. This tool is useful to investigate costs and times needed to \nexecute the entire process. A top-down approach was applied: starting \nfrom a model of the macro-tasks, and then defining each task following \nthe most left representation to define each block as an independent \nprocess. This allows a detailed analysis, gaining a more realistic repre -\nsentation on the timing of the macro-block. Finally, the model reaches \nautomation level and is upgraded with a Markov-chain-based AI algo-\nrithm able to show probabilities of failure for sample properties of in-\nterest. The system upgrade can be categorized in three levels:  \n1. Level 1 – “Process monitoring ” \nThis level is characterized by the ability of the CPS to process the \ncollected data automatically generating reports and sending alarms, \nbased on inputs pregiven manually. In case of failures being signalized, \nthe information provided allows the operators and/or process engineers \nto intervene and adjust the process parameters to address the issue. \nReports can assist in the identification of trends by displaying data over \na longer period.  \n2. Level 2 – “Small-scale process control ” \nAt this level, further analysis and interpretation is performed auto-\nmatically by the AI algorithms to predict the outcome of the process. For \ninstance, the CPS can stop and restart the potting process with a new \ninsert if the probability of negative process outcome is high. Based on \nidentified trends, the CPS can signal potential failures before they occur. \nHowever, the system is incapable of adjusting any of the process pa-\nrameters to keep the process running and avoid the identified threats.  \n3. Level 3 – “Large-scale process control ” \nAt level 3, the AI-assisted CPS can optimize the process parameters to \nachieve optimal process result – delivering the right product quality in \nthe shortest production time. It can perform continuous predictive analysis on all production system components using the data fed in real- \ntime by the sensor network. Based on the performance forecast, the CPS \ncan predict the completion time for each panel, tool exchange rates, and \nequipment maintenance intervals, thereby being able plan the entire \nmaterial flow through the station. At this stage, multiple production \nstations can be interconnected using the same CPS. \nTo reach these levels, capital investment in upgrading the production \nsystem is necessary. Table 2 shows estimated investment figures needed \nto support the CPS implementation. \n5.Conclusions \nThe paper contextualized Smart Manufacturing technologies in the \nfast-evolving market of large constellations of small satellites and \nrelated new production paradigms. A review of fundamental theoretical \nconcepts behind Industry 4.0 disruptive change was presented, focused \non Cyber-Physical Systems and their 5C-level standard architecture. \nPossible Smart Manufacturing solutions, in terms of hardware and \nsoftware technologies, were reviewed to contribute to a future signifi -\ncant improvement and optimization of a whole MAIT cycle. CPS, DT and \nIoT were selected as the most promising technologies to be adopted and \nRUAG ’s composite sandwich panel manufacturing process was taken as \ncase study. The process was reconstructed so that each sensor could be \nsimulated in the cyber space as a flux of data. In parallel, an assessment \nof the SM level of the process according to the Acatech maturity model \nwas carried on unlocking the process improvement potential. The flux of \ndata flowing from the sensing layer into the cyber layer of the CPS \nthrough an interconnected IoT network is represented by unit blocks \nrelated to each process step. The use of AI upgrades the model, giving it \nthe ability to also reach some level of process control and optimization. \nThree different levels of process improvement are identified each of \nwhich is linked to its economic estimation of the necessary computing \ninfrastructure. By this model equipment data can be interpreted through \nits pre-processing, normalization, storage and distribution to a user- \nfriendly visual dashboard, according to a new logical analysis of the \nindustrial process, delivering the final improvement, represented by the \nopportunity of reconfiguring the production line to reach the goals \nmeasured by traditional Key Performance Indicators (KPIs), among \nwhich panel production rate and Overall Equipment Efficiency (OEE), \nand optimize specific parameters related to SM, such as process agility \nand flexibility and the CPS scalability. \nFig. 12.An example of the CPS dashboard.  M. Eugeni et al.\n\n[Página 14]\nActa Astronautica 192 (2022) 276–290\n289Declaration of competing interest \nThe authors declare that they have no known competing financial \ninterests or personal relationships that could have appeared to influence \nthe work reported in this paper. \nAcknowledgment \nThe present paper results from the project “Smart Manufacturing for \nfuture constellations ” funded by the European Space Agency (ESA ITT \nAO/1 –10002/19/NL/AR for technology development) and developed in \ncollaboration by Sapienza University of Rome, Thales Alenia Space Italy \nand RUAG Space. \nReferences \n[1]M. Blanchet, THINK ACT. INDUSTRY 4.0. The New Industrial Revolution. How \nEurope Will Succeed, Roland Berger, March 2014 . [2]E. S. Agency, What Is Space 4.0? [Online]. Available, November 2021. November \n2021, https://www.esa.int/About_Us/Ministerial_Council_2016/What_is_space_4. \n0. \n[3]R.Y. Zhong, X. Xua, E. Klotz, S.T. Newmanc, Intelligent manufacturing in the \ncontext of industry 4.0: a review, Engineering 3 (2017) 616–630. \n[4]C. Daehnick, I. Klinghoffer, B. Maritz, B. Wiseman, “Large LEO Satellite \nConstellations: Will it Be Different This Time?, ” McKinsey &Co, Aerospace and \nDefence Practice, May 2020 . \n[5]UK saves OneWeb, Spaceflight 62 (September) (2020) . \n[6]J. Hou, Y. Zhao, Y. Zhou, X. Du and Z. Li, “The creative application of DIY \nmanufacturing technology in remote sensing satellite, ” Aero. China. Vol. 17. N.2, \nSummer 2016. \n[7]K. Jackson, K. Efthymioua, J. Borton, “Digital Manufacturing and Flexible \nAssembly Technologies for Reconfigurable Aerospace Production Systems, ” \nChangeable, Agile, Reconfigurable & Virtual Production Conference, 2016 . \n[8]A. Kusiak, Smart manufacturing, Int. J. Prod. Res. 56 (2018) 508–517. \n[9]S. Marigonda, “Smart Manufacturing: sfide e opportunit ˇa.,” Digital Tools 4.0. \n[10] L. Li, China ’s manufacturing locus in 2025: with a comparison of “Made-in-China \n2025 ” and “Industry 4.0, Technol. Forecast. Soc. Change 135 (2018) 66–74. \n[11] L.D. Xu, Industry 4.0: state of the art and future trends, Int. J. Prod. Res. 56 (8) \n(2018) . \n[12] C. Bryson, Heritage and Satellite Manufacturing: Firm-Level Competitiveness and \nthe Management of Risk in Global Production Networks, Economic Geography, \n2019, pp. 423–441. \n[13] C. Salkin, M. Oner, A. Ustundag, E. Cevikcan, A Conceptual Framework for \nIndustry 4.0, 2018 . \n[14] K. Nakamoto, K. Shirase, Simulation technologies for the development of an \nautonomous and intelligent machine tool, Int. J. Autom. Technol. (2013), https:// \ndoi.org/10.20965/ijat.2013.p0006 . \n[15] K.D. Thoben, S. Wiesner, T. Wuest, Industrie 4.0’ and smart manufacturing – a \nreview of research issues and application examples, Int. J. Autom. Technol. 11 (1) \n(January 2017) 4–16. \n[16] G.G. Schuh, Industrie 4.0 Maturity Index. Managing the Digital Transformation of \nCompanies [Online]. Available:, 2017. February 2021, https://hal.archives-ouver \ntes.fr/hal-02455705 . \n[17] V. Cruz-Machado, Scanning the industry 4.0: a literature review on technologies \nfor manufacturing systems, Engineering Science and Technology, an International \nJournal 22 (3) (June 2019) 899–919. \n[18] D.P. Perales, F.A. Valero, A.B. García, Industry 4.0, A Classification Scheme, 2018 . \n[19] O. Cardin, Classification of cyber-physical production systems applications: \nproposition of an analysis framework, Comput. Ind. 104 (January 2019) 11–21, \nhttps://doi.org/10.1016/j.compind.2018.10.002 . \n[20] A. Rojko, Industry 4.0 concept: background and overview, International Journal of \nInteractive Mobile Technologies 11 (5) (2017) . \nFig. 13.Examples of graphs showable by the dashboard: the first graph represents the single operation ’s timing vs time, the second one the production efficiency vs \ntime and the last one a map of discarded APM inserts for adhesive quantity. Scales are not shown for confidential reasons. \nFig. 14.The process layout represented in the cyber space and its focus at APM.  \nTable 2 \nProduction volume requirements - rough order of magnitude estimates.  \nCPS \nUpgrade \nLevel Level \nDescription Estimated \nMachine \nProcurement \nCost Increase \n[%] Estimated CPS \nImplementation \nand Operation \nCost [EUR] Minimum \nProduction \nVolume \n[inserts] \nLevel 1 Process \nmonitoring 3–5 42∕000 €/5 years 20.000 \nLevel 2 Small-scale \nprocess \ncontrol 10–15 55∕500 €/5 years 200.000 \nLevel 3 Large-scale \nprocess \ncontrol 40–60 82∕500 €/5 years 1.000.000  M. Eugeni et al.\n\n[Página 15]\nActa Astronautica 192 (2022) 276–290\n290[21] B.-h. Li, H. Bao-cun, L. Xiao-bing, Y. Chun-wei, Y. Wen-tao, Applications of \nartificial intelligence in intelligent manufacturing: a review, Frontiers of \nInformation Technology & Electronic Engineering 18 (1) (2017) 86–96. \n[22] J. Jadaan, K.S. Siderska, Cloud manufacturing: a service-oriented manufacturing, \nEngineering Management in Production and Services 10 (1) (2018) 22–31. \n[23] N. Khan, I. Yaqoob, I. Abaker, T. Hashem, Z. Inayat, W. Kamaleldin, A. Mahmoud, \nM. Alam, M. Shiraz, A. Gani, Big Data: Survey, Technologies, Opportunities, and \nChallenges, ” The Scientific World Journal, July 2014 . \n[24] C. Duke, G. Sadlier, D. Herr, Industry 4.0 and the Future of UK Space, ” London \nEconomics, 2019 . \n[25] E. Sisinni, A. Saifullah, S. Han, U. Jennehag, M. Gidlung, Industrial internet of \nthings: challenges, opportunities, and directions, IEEE Trans. Ind. Inf. 10 (10) \n(2018) . \n[26] H. Li, Application research of virtual reality and augmented reality, Advances in \nIntelligent Systems and Computing 1233 (2021) 494–499. \n[27] Federal Ministry for Economic Affairs and Energy, Plattform Industrie 4.0 - \nRAMI4.0 – a reference framework for digitalisation, Plattf. Ind. 4 (2019), 0. \n[28] M. Yli-Ojanper aa, S. Sierla, N. Papakonstantinou, V. Vyatkin, Adapting an agile \nmanufacturing concept to the reference architecture model industry 4.0: a survey \nand case study, Journal of Industrial Information Integration 15 (2019) 147–160. \n[29] J.H. Kim, A review of cyber-physical system research relevant to the emerging IT \ntrends: industry 4.0, IoT, big data, and cloud computing, Journal of Industrial \nIntegration and Management 2 (3) (2017) . \n[30] H. Gill, R. Baheti, Cyber-physical systems, in: T. Samad, A.M. Annaswamy (Eds.), \nThe Impact of Control Technology, 2011 . \n[31] H. Gill, R. Baheti, Cyber-physical Systems: from Theory to Practice, 2011 . \n[32] L. Monostori, Cyber-physical systems in manufacturing, CIRP Ann 65 (2) (2016) \n621–641. \n[33] R. Rajkumar, I. Lee, L. Sha, J. Stankovic, Cyber-physical systems: the next \ncomputing revolution, Des. Autom. Conf. (2010) 731–736. \n[34] A. Napoleone, M. Macchi, A. Pozzetti, A review on the characteristics of cyber- \nphysical systems for the future smart factories, J. Manuf. Syst. 54 (December) \n(2019) . \n[35] S. Thiede, M. Juraschek, C. Herrmann, Implementing cyber-physical production \nsystems in learning factories, Procedia CIRP 54 (2016) 7–12. \n[36] C. Zhan, Y. Chen, A review of research relevant to the emerging industry trends: \nindustry 4.0, IoT, blockchain, and business analytics, Journal of Industrial \nIntegration and Management 5 (1) (2020) 165–180. \n[37] H. Chen, Theoretical foundations for cyber-physical systems: a literature review, \nJournal of Industrial Integration and Management 2 (3) (2017) . \n[38] Y. Lu, Cyber physical system (CPS)-based industry 4.0: a survey. Journal of \nIndustrial Integration and Management, Journal of Industrial Integration and \nManagement 2 (3) (2017) . \n[39] G.K. Rand, N. Singh, D. Rajamani, Cellular manufacturing systems design, planning \nand control, J. Oper. Res. Soc. (1997) . \n[40] T. Pultarova, “Satellite Manufacturing in a State of Transition, ” [Online]. \nAvailable: http://interactive.satellitetoday.com/via/march-2019/satellite-manu \nfacturing-in-a-state-of-transition/_fragment.html . [Accessed October 2020]. \n[41] P.M. Laurent Jaffarta, Constellations: The satellite serial production challenge, in: \n71st International Astronautical Congress (IAC) – the CyberSpace Edition, October \n2020, pp. 12–14. \n[42] e. directory, “WorldView legion constellation, ” European Space Agency. [Online]. \n[Accessed February 2021]. \n[43] C. Hofacker, How to Make a Megaconstellation, March 2020 [Online]. Available: \nhttps://aerospaceamerica.aiaa.org . \n[44] T. Gornet, T. Wohlers, History of Additive Manufacturing, ” Wohlers, 2014 . \n[45] A. Javaid, M. Haleem, Additive manufacturing applications in industry 4.0: a \nreview, Journal of Industrial Integration and Management 4 (4) (2019) . \n[46] K. Schwab, The Fourth Industrial Revolution, Portfolio Penguin, 2017 . \n[47] A. B˘ecue, CyberFactory#1 – securing the Industry 4.0 with cyber-ranges and digital \ntwins, in: IEEE, 2018 . \n[48] HMI/SCADA software in the age of Industrial IoT and evolving human machine \ninterfaces, ” I-Scoop, [Online]. Available: https://www.i-scoop.eu/industry-4-0/h \nmi-scada-software/ . [Accessed February 2021]. \n[49] Y. Lu, Current Standards Landscape for Smart Manufacturing Systems, ” National \nInstitute of Standards and Technology - US Department of Commerce, February \n2016 . \n[50] H. Wang, Enterprise system and its application in aerospace industry, Journal of \nIndustrial Integration and Management 2 (2) (2017) . \n[51] I.C. Reinhardt, Current perspectives on the development of industry 4.0 in the \npharmaceutical sector, Journal of Industrial Information Integration 18 (3) (2020) . \n[52] H. Wu, S. Li, L.D. Xu, Internet of things in industries: a survey, IEEE Trans. Ind. Inf. \n10 (4) (2014) 2233 –2243 . \n[53] A. B˘ecue, A new concept of digital twin supporting optimization and resilience of \nfactories of the future, Appl. Sci. 10 (2020) 4482 . \n[54] T. Fei, Z. Meng, Digital twin shop-floor: a new shop-floor paradigm towards smart \nmanufacturing, IEEE Access 5 (2017) . \n[55] H. Gill, R. Baheti, Cyber-physical systems. The impact of control technology, IEEE \nControl Systems Society 1 (2011) . \n[56] E.A. Lee, Cyber physical systems: design challenges, in: 11th IEEE. International \nSymposium on Object and Component-Oriented Real-Time Distributed Computing, \nISORC)., 2008, pp. 363–369. [57] M. Abdirad, A two-stage metaheuristic algorithm for the dynamic vehicle routing \nproblem in industry 4.0 approach, J. Manag. Anal. 1 (15) (2020) . \n[58] J. Lee, B. Bagheri, H.A. Kao, A Cyber-Physical Systems architecture for Industry \n4.0-based manufacturing systems, Manufacturing Letters 3 (2015) 18–23. \n[59] G. Aceto, V. Persico, A. Pescap ˘e, Industry 4.0 and health: internet of things, big \ndata, and cloud computing for healthcare 4.0, Journal of Industrial Information \nIntegration 18 (2020) . \n[60] X. You, Towards 6G Wireless Communication Networks: Vision, Enabling \nTechnologies, and New Paradigm Shifts, vol. 64, Science China - Information \nSciences, 2021 . \n[61] Y. Lu, Security in 6G: the prospects and the relevant technologies, Journal of \nIndustrial Integration and Management 5 (3) (2020) 271–289. \n[62] A. Cusano, P. Salvarezza, G. Breglio, A. Cutolo, A. Calabr ˇo, M. Giordano, S. De \nNicola, An integrated fiber optic sensing system for in situ characterization of the \ncuring, Proc. SPIE 4328 (2001) 275–284. \n[63] K.H. Wood, T.L. Brown, M.C. Wu, C.B. Gause, Fiber Optic Sensors for Cure-Health, \n” Proceeding 3rd Intern. Workshop on Structural Health, 2001, pp. 1149 –1157 . \n[64] K. Saddik, M. Alam, A. El, C2ps: a digital twin architecture reference model for the \ncloud-based cyber-physical systems, IEEE Access 5 (2017) 2050 –2062 . \n[65] R.D. Finlayson, M. Friesel, M. Carlos, P. Cole, Health monitoring of aerospace \nstructures with acoustic emission and acousto-ultrasonics, in: 15th World \nConference on Non-destructive Testing, 2000 . \n[66] V. Giurgiutiu, A. Zagrai, J.J. Bao, Piezoelectric wafer embedded active sensors for \naging aircraft structural health monitoring, Int. J. Struct. Health Monitor. \nNovember (2001) . \n[67] D. Devillers, F. Taillade, D. Osmont, D. Balageas, D. Royer, Interaction of Lamb \nwaves with defects in composite sandwich structures, in: European COST F3 \nConference on System, 2000 . \n[68] J.S. Kim, K.J. Vinoy, V.K. Varadan, Wireless health monitoring of cracks in \nstructures with MEMS-IDT sensors, Proc. SPIE 4700 (2002) 342–353. \n[69] S.J. Burgett, M. Kranz, MEMS sensor systems developments at AMCOM for \nenvironmental conditions monitoring, in: Proc. 3 Rd Intern. Workshop on \nStructural Health Monitoring, 2001, pp. 1134 –1141 . \n[70] D. Chung, Structural health monitoring by electrical resistance measurement, \nJournal of smart materials and structures 10 (2001) 624–636. \n[71] L. Zhang, Novel self-healing CFRP composites with high glass transition \ntemperatures, Compos. Sci. Technol. 168 (2018) 96–103. \n[72] W.H. Prosser, T.L. Brown, S.E. Woodard, G.A. Fleming, E.G. Cooper, Sensor \ntechnology for integrated vehicle health management of aerospace vehicles, in: AIP \nConference Proceedings, vol. 657, 2003, p. 1582 . \n[73] P. Gaudenzi, M. Bernabei, E. Dati, G. De Angelis, M. Marrone, L. Lampani, On the \nevaluation of impact damage on composite materials by comparing different NDI \ntechniques, Compos. Struct. 118 (2014) 257–266. \n[74] X. Maldague, Theory and Practice of Infrared Thermography for Non Destructive \nTesting, John Wiley & Sons, Canada, 2001 . \n[75] S. Deane, Application of NDT thermographic imaging of aerospace structures, \nInfrared Phys. Technol. 97 (2019) 456–466. \n[76] D.I. Gillespie, Defect detection in aerospace sandwich composite panels using \nconductive thermography and contact sensors, Sensors 20 (2020) . \n[77] R.D. Finlayson, M. Friesel, M. Carlos, P. Cole, Health monitoring of aerospace \nstructures with acoustic emissions and acousto-ultrasonics, in: 15th World \nConference on Non-destructive Testing, October 2020 . \n[78] P. Daryabor, M.S. Safizadeh, Image fusion of ultrasonic and thermographic \ninspection of carbon/epoxy patches bonded to an aluminum plate, NDT E Int. 90 \n(2017) 1–10. \n[79] P. Gaudenzi, Smart Structures: Physical Behaviour, Mathematical Modelling and \nApplications, John Wiley Sons, 2009 . \n[81] [Online]. Available:, Elastic, November 2021. Accessed November 2021, http \ns://www.elastic.co/ . \n[82] Grafana [Online]. Available: Accessed November 2021, https://grafana.com/ , \nNovember 2021. \n[83] Ignite [Online]. Available: Accessed November 2021, https://ignite.apache.org/ , \nNovember 2021. \n[84] Kafka [Online]. Available: Accessed November 2021, https://kafka.apache.org/ , \nNovember 2021. \n[85] Kibana [Online]. Available: Accessed November 2021, https://www.elastic. \nco/kibana/ , November 2021. \n[86] Pytorch [Online]. Available: Accessed November 2021, https://pytorch.org/ , \nNovember 2021. \n[88] [Online]. Available:, Scikit Learn, November 2021. Accessed November 2021, \nhttps://scikit-learn.org/ . \n[89] Tensorflow [Online]. Available: Accessed November 2021, https://www.tensor \nflow.org/ , November 2021. \n[90] Redis [Online]. Available: Accessed November 2021, https://redis.io , November \n2021. \n[91] M. Li, “Spatial-Temporal Finite Element Analytics for CPS-Enabled Smart Factory: \nApplication in Hybrid Flow Shop, ” Procedia Manufacturing, 2020, pp. 1229 –1236 . \n[92] “Flink Flink [Online]. Available: Accessed November 2021, https://flink.apache. \norg/, November 2021. M. Eugeni et al.",
      "metadata": {
        "filename": "An industry 4.0 approach to large scale production of satellite 2022.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\An industry 4.0 approach to large scale production of satellite 2022.pdf",
        "file_size": 8180979,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:35.186178",
        "content_length": 85542
      }
    },
    "b9f85718-1117-4a9a-ad4a-1eade3ffcda1": {
      "id": "b9f85718-1117-4a9a-ad4a-1eade3ffcda1",
      "content": "[Página 1]\nAssessing business value of Big Data Analytics in European ﬁrms☆\nNadine Côrte-Real ⁎, Tiago Oliveira, Pedro Ruivo\nNOVA IMS, Universidade Nova de Lisboa, 1070-312, Lisboa, Portugal\nabstract article info\nAvailable online 9 August 2016 In the strategic management ﬁeld, dynamic capabilities (DC) such as organizational agility are considered to be\nparamount in the search for competitive advantage. Recent research claims that IT business value research\nneeds a more dynamic perspective. In particular, the Big Data Analytics (BDA) value chain remains unexplored.\nTo assess BDA value, a conceptual model is proposed based on a knowledge-based view and DC theories. Toempirically test this model, the study addresses a survey to a wide range of 500 European ﬁrms and their IT\nand business executives. Results show that BDA can provide business value to several stages of the value chain.\nBDA can create organizational agility through knowledge management and its impact on process and\ncompetitive advantage. Also, this paper demonstrates that agility can partially mediate the effect betweenknowledge assets and performance (process level and competitive advantage). The model explains 77.8% of\nthe variation in competitive advantage. The current paper also presents theoretical and practical implications\nof this study, and the study's limitations.\n© 2016 Elsevier Inc. All rights reserved.Keywords:\nBig Data Analytics (BDA)\nIT business value\nKnowledge Based View (KBV)Dynamic capabilities (DC)Organizational agilityCompetitive advantage\n1. Introduction\nIn the era of Big Data, ﬁrms in every sector are required to deal with a\nhuge amount of data. Data in vast amounts can offer invaluable insights\nand competitive advantage if the right technological and organizational\nresources support them ( Morabito, 2015 ). Recently, several academics\nand practitioners have stressed the need to understand how, why, and\nwhen Big Data Analytics (BDA) applications can be a valuable resource\nfor companies to gain competitive advantage ( Abbasi, Sarker, &\nChiang, 2016; Agarwal & Dhar, 2014; Corte Real, Oliveira, & Ruivo,\n2014; LaValle et al., 2011 ). Although BDA technologies have been\nrecognized as the “next big thing for innovation ”(i.e., a potential source\nof business value and competitive advantage), the BDA value chain\nremains relatively unexplored and needs further investigation. No\nempirical research exists assessing how BDA can bring business value\n(Abbasi et al., 2016 ), establishing a linkage between knowledge assets,\norganizational agility, and performance (process-level and competitive\nadvantage) ( Corte Real et al., 2014 ). Firms that inject BDA in their\nbusiness operations can surpass their peers by 5% in productivity and\n6% in pro ﬁtability ( Barton, 2012 ). For that reason, European ﬁrms are\ninvesting heavily in BDA technologies ( SAS, 2013; Sharma, Mithas, &\nKankanhalli, 2014 ). Nevertheless, this investment can only be valuableif organizations use the appropriate technology and organizational\nresources to achieve competitive advantage ( Manyika et al., 2011a ).\nIn response to the scarcity of research on this subject, this study\nexamines the impact of BDA on the business value chain in a\nEuropean context by empirically testing a new theoretical frame-\nwork that merges two strategic management theories (Knowledge\nB a s e dV i e w( K B V )a n dd y n a m ic capabilities (DC)) at ﬁrm-level. Not\nonly does this paper extend BDA research by transposing, merging,\nand examining hypotheses in IT innovations and management ﬁelds,\nbut also contributes to DC research by empirically assessing the ante-\ncedents and impacts of a speci ﬁc dynamic capability (organizational\nagility), when using BDA technologies. This is the ﬁrst paper that\nstudies the entire BDA value chain at ﬁrm-level, linking concepts of\nknowledge management, agility, and performance (process-level\nand competitive advantage). To clarify the role of agility on perfor-\nmance, this papers tests if agility is a mediator of knowledge assets\non performance (process-level performance and competitive\nadvantage). The study explores the following three research ques-\ntions (RQs):\nRQ1 –What are the BDA enablers for the creation of organizational\nagility?RQ2 –What are the impacts of this dynamic capability created by\nBDA on sustainable competitive advantage?\nRQ3 –Is agility a mediator of knowledge assets on performance\n(process-level performance and competitive advantage)?Journal of Business Research 70 (2017) 379 –390\n☆The author is grateful for the comments by anonymous reviewers, on earlier drafts of\nthis article.\n⁎Corresponding author.\nE-mail address: nreal@novaims.unl.pt (N. Côrte-Real).\nhttp://dx.doi.org/10.1016/j.jbusres.2016.08.011\n0148-2963/© 2016 Elsevier Inc. All rights reserved.\nContents lists available at ScienceDirect\nJournal of Business Research\n\n[Página 2]\nThis study offers guidance for executives and managers to assess the\nconditions under which BDA can add business value to organizations.\nManagers and IT executives can bene ﬁt from an evaluation instrument\nto assess the impact of BDA. Also, this paper provides valuable support\nto justify BDA investments and initiatives. Firms that have not yet\ndecided to adopt these technologies can obtain a view of potential\ngains from adopting and effectively using BDA. This research demon-strates how best to leverage the knowledge embedded in BDA systems,\nacquiring organizational agility capabilities that lead toward competi-\ntive advantage.\nThe remainder of this paper has the following structure: Section 2\nprovides an introduction to the BDA concept and a theoretical\nbackground to assess BDA initiatives; Section 3 presents the conceptual\nmodel and the hypotheses; Section 4 outlines the methodology; and\nSection 5 shows the empirical results. Finally, the paper presents a\ndiscussion and the conclusions from the ﬁndings.\n2. Background2.1. Big Data Analytics\nChen, Chiang ( Chen, Chiang, & Storey, 2012 ) coined the term Big\nData Analytics (BDA) as a related ﬁeld of business intelligence &\nanalytics (BI&A), referring to the BI&A technologies that mostly concern\ndata mining and statistical analysis. Authors de ﬁne BDA as “an e w\ngeneration of technologies and architectures, designed to economically\nextract value from very large volumes of a wide variety of data, by enabling\nhigh velocity capture, discovery and/or analysis. ”(IDC, 2011 ). BDA tech-\nnologies allow ﬁrms to improve existing applications by offering\nbusiness-centric practices and methodologies that provide a competi-\ntive advantage ( Chen et al., 2012; Davenport, 2006 ). The latest literature\nindicates that there is much room for further BDA research ( Abbasi\net al., 2016; Agarwal & Dhar, 2014; Erevelles, Fukawa, & Swayne,\n2016 ). There are already academic studies that re ﬂect the adoption\nand use of BDA (e.g., ( Malladi, 2013; Xu, Frankwick, & Ramirez, 2016;\nKwon, Lee, & Shin, 2014 )). Regarding value, most BDA academic studies\nfocus on analyzing business value from a data or system perspective\n(e.g., ( LaValle et al., 2011; Kwon et al., 2014 )). From the strategic\nmanagement perspective only one conceptual paper explores how\nBDA affects several marketing activities ( Erevelles et al., 2016 ). The\nremaining literature addresses industry primarily ( LaValle et al., 2011;\nRussom, 2011 ). As ﬁrms do not know how to capture business value\n(Barton, 2012; LaValle et al., 2011 ), some scholars ( Corte Real et al.,\n2014; Malladi, 2013 ) argue that BDA value research is scarce and\nneeds to extend beyond post-adoption stages toward competitiveness\n(Erevelles et al., 2016; Xu et al., 2016 ). Although numerous approaches\nassess IT Value at the process and ﬁrm levels (see Schryen ( Schryen,\n2013 ) for a review), this study extends IT business value research\nfrom the strategic management perspective, by empirically assessing\nthe BDA business value chain in European ﬁrms.\n2.2. Theoretical foundation\nMany studies in recent decades investigate IT business value and\ncompetitive advantage using the resource-based view (RBV) ( Barua,\nKriebel, & Mukhopadhyay, 1995; Bharadwaj, 2000; Mata, Fuerst, &\nBarney, 1995; Melville, Kraemer, & Gurbaxani, 2004; Ruivo, Oliveira, &\nNeto, 2015; Soh & Markus, 1995; Zhu & Kraemer, 2005 ). The limitations\nof RBV encourage the use of other theories such as DC and KBV ( Arend &\nBromiley, 2009; Wang & Ahmed, 2007 ). As DC theory constitutes the\nsecond foundation that supports knowledge-based thinking ( Pettigrew,\nThomas, & Whittington, 2001 ), this study combines these theories. KBV\nexplores a ﬁrm's potential to acquire competitiveness in a dynamic\nmarket context, but only DC theory can solve the problem of sustaining\ncompetitive advantage in turbulent environments ( Grant, 1996;\nVolberda, 1996 ).2.2.1. Knowledge Based View theory\nKBV states that a ﬁrm's knowledge resources are unique and\ninimitable and that the ﬁrm's primary function is to leverage them\ninto productive outcomes ( Grant, 1996; Nonaka, 1995 ). The possession\nof knowledge resources gives the ﬁrm basic foundations to renew or re-\nconﬁgure its resource base and to build dynamic capabilities ( Wu,\n2006 ), such as organizational agility. Companies that have high levels\nof staff knowledge and involvement can more skillfully identify the\nneed to make changes to existing resources and decide about the ac-\ntions necessary to implement these changes ( Nieves & Haller, 2014 ).\nKBV theory can help to conceptualize the performance effects of IT in-\nvestments ( Pavlou et al., 2005 ). Management studies use this theory\n(e.g., ( Nieves & Haller, 2014 )), as do studies in IT ﬁelds (e.g., ( Sher &\nLee, 2004 )) to understand the role of knowledge management in the\ncreation of DC. In BDA technologies, Xu, Frankwick ( Xu et al., 2016 )\nseek to understand the relationships among traditional marketing\nanalytics, BDA, and new product success. The current paper is the ﬁrst\nthat empirically tests KBV to understand the role of BDA in the creation\nof agility.\n2.2.2. Dynamic capability theory\nIn the past decade the DC perspective arose as one of the most\neffective theoretical lenses for the strategic management ﬁeld\n(Schilke, 2014 ), attracting the interest of scholars not only in business,\nbut also in the IT management ﬁeld ( Helfat et al., 2009; Protogerou,\nCaloghirou, & Lioukas, 2012 ). Rooted in RBV and KBV, DC argues that\nthe dynamic capabilities enable ﬁrms to modify their resource to\nadapt rapidly to changing conditions, helping them to sustain their\ncompetitive advantage over time ( Helfat & Peteraf, 2009; Teece,\nPisano, & Shuen, 1997 ). Although the literature has a broad range of\ndeﬁnitions for DC, one of the seminal papers de ﬁnes DC as “the ability\nto integrate, build, and recon ﬁgure internal and external competencies to\naddress rapidly-changing environments ”(Teece et al., 1997 ). DC\ndisaggregates into “the capacity (1) to sense and shape opportunities\nand threats, (2) to seize opportunities, and (3) to maintain competitive-ness through enhancing, combining, protecting, and, when necessary,\nrecon ﬁguring the business enterprise's intangible and tangible assets ”.\nSome authors argue that agility is an organizational dynamic\ncapability ( Blome, Schoenherr, & Rexhausen, 2013; Sambamurthy\net al., 2007; Zhou & Wu, 2010 ). Teece ( Teece, 2007 )d eﬁnes agility as a\nhigher-order dynamic capability that emerges over time, generally\ndeﬁning agility as a capability with which ﬁrms can identify and re-\nspond to environmental threats and opportunities and quickly adjust\ntheir behaviors ( Goldman, Nagel, & Preiss, 1995; Sambamurthy,\nBharadwaj, & Grover, 2003 ). This concept also relates to the operational\nﬂexibility of organizational processes and IT systems to support\nstructured or unstructured changes ( Chen et al., 2014 ). Achieving agility\ndemands processing a large and varied amount of information\n(Goldman et al., 1995 ). This process is possible with BDA applications.\nHowever, like IT applications ( Sambamurthy et al., 2003; Weill,\nSubramani, & Broadbent, 2002 ), BDA tools cannot automatically\nimprove agility. In fact, under certain conditions BDA tools can impede\nagility ( Chen et al., 2014 ). For this reason, the need exists to understand\nhow BDA applications can create agility.\nSeveral recent studies in the business management ﬁeld apply DC\ntheory to measure the in ﬂuence of DC in the creation of competitive ad-\nvantages (e.g., Schilke, 2014; Zott, 2003; Drnevich & Kriauciunas, 2011 ).\nIn the IT management ﬁeld, few empirical studies use this theory.\nAnalyzing the IT in ﬂuence on DC generically, ( Chen et al., 2014; Sher\n& Lee, 2004 ), researchers conclude that IT is an enabler of DC in\norganizations. Regarding agility, several studies assess the impact of IT\non organizational agility (e.g., Sambamurthy et al., 2007; Chen et al.,\n2014; Cai et al., 2013; Tallon & Pinsonneault, 2011; Liu et al., 2013; Lu\n& Ramamurthy, 2011 ). These studies demonstrate a positive relation-\nship between IT and agility. Chen ( Chen et al., 2014 ) recently concludes\nthat the IT business value essentially depends on how agile a ﬁrm is380 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390\n\n[Página 3]\nwith regard to managing business processes. Although the literature\naddresses the impact of IT on the creation of organizational agility, no\nstudy links BDA with this speci ﬁc DC. Apart from some qualitative stud-\nies in the area of business analytics (BA) ( Shanks & Bekmamedova,\n2013; Shanks & Sharma, 2011 ), only conceptual papers use DC theory\nto study BDA value ( Corte Real et al., 2014; Erevelles et al., 2016 ).\nFirms that do not develop the resources and capabilities to use BDA\napplications will struggle to develop a sustainable competitive advan-\ntage ( Erevelles et al., 2016 ). Given that agility is vital for companies´\nsurvival, and that BDA can support organizational business processes,\nthis study ﬁlls this academic gap and links the two concepts empirically.\n3. Conceptual model\nWith recourse to the two strategic management theories (KBV and\nDC) discussed above, this section explains the conceptual model and\nthe speci ﬁc hypotheses ( Fig. 1 ).\nRooted in an earlier conceptual model ( Corte Real et al., 2014 ), this\nresearch model empirically tests 12 propositions. The study assesses\nthe entire value chain starting with how BDA can leverage different\nforms of knowledge to create organizational agility ( H1,H2,H3). BDA\ntechnologies can provide organizational agility to the ﬁrm by using\neffective knowledge management. Firms owning this type of dynamic\ncapability can achieve competitive advantage directly ( H4a)o ri n d i r e c t -\nly through business processes ( H4b). Results obtained by using business\nprocesses will impact the overall organization ( H5). Agility can also\nmediate the relationship between knowledge assets and performance\n(H6a,b,c-H7a,b,c). BDA uses some controls such as country, industry,\ntechnological turbulence, and time.\n3.1. Hypothesis3.1.1. Knowledge assets\nOrganizational knowledge such as operational routines, skills, and\nknow-how constitutes a key source of competitiveness ( Grant, 1996 ).\nKnowledge management plays a critical role in pro ﬁciently managing\ndata and delivering it to the end users to support business processes\n(Rajpathak & Narsingpurkar, 2013 ). Knowledge management repre-\nsents a dimension supported by KBV ( Ruggles, 1998 ) and enables\ndynamic capabilities by offering speci ﬁc functional competences that\ncan improve business performance ( Teece et al., 1997 ). A naturalrelationship exists between KM and BDA. Both deal with intangible\nassets such as data, knowledge, and intelligence ( Erickson & Rothberg,\n2015 ). BDA is a source of knowledge management, allowing ﬁrms to\nadd value primarily at the beginning of the information value chain\nand helping knowledge to ﬂow to achieve business excellence ( Chau\n& Xu, 2012; Popovi čet al., 2012 ).\nBig data is a potential knowledge asset, contingent upon the proper\nuse of that knowledge ( Erickson & Rothberg, 2015\n). BDA represents\ntechnologies drivers of a strategic knowledge asset (big data). BDA\napplications have the potential to add value by providing more\ntransparent and accurate results to support decision-making in several\nbusiness areas ( Manyika et al., 2011a ).\nBDA strategy requires the capacity to sense, acquire, process, store,\nand analyze the data and convert that data into knowledge ( Rajpathak\n& Narsingpurkar, 2013 ). Several empirical studies state that the knowl-\nedge processes are antecedent dimensions of successful DC, by allowing\nﬁrms to continually renew their knowledge base and deliver business\nperformance ( Ambrosini & Bowman, 2009; Sher & Lee, 2004; Zheng,\nZhang, & Du, 2011 ). As DC are information-intensive ( Pavlou & El\nSawy, 2011 ), BDA may help in the creation of DC and organizational\nagility speci ﬁcally. Using BDA technologies helps to store and share\nknowledge, thereby allowing for an improvement of organizational\nknowledge by promoting ef ﬁciency within an organization, particularly\nby data integration and the use of analytical tools ( Russom, 2011 ). Some\nauthors argue that ﬁrms must combine endogenous and exogenous\nknowledge to achieve DC ( Sher & Lee, 2004 ). Zhao ( Cai et al., 2013 )\nargues that IT capability and KM capability are important in fostering\norganizational agility. Agility is promoted through knowledge manage-\nment by improving innovative responses, and can improve through the\nuse of IT and automated business processes ( Cai et al., 2013 ). In the\nsame way, organizations should be able to use BDA technologies to\nconvert knowledge into new routines and enhance organizational\nagility. Based on these ﬁndings, the hypotheses are:\nH1. BDA technologies allow an effective endogenous knowledge\nmanagement that positively in ﬂuences dynamic capabilities such as\norganizational agility.\nH2. BDA technologies allow an effective exogenous knowledge\nmanagement that positively in ﬂuences dynamic capabilities such as or-\nganizational agility.\nFig. 1. Proposed conceptual model.381 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390\n\n[Página 4]\nKnowledge sharing with key channel partners refers to the extent to\nwhich a ﬁrm shares insights and know-how about its business context\nwith its partners ( Saraf, Langdon, & Gosain, 2007 ). Channel partners\nare considered to be tactically and strategically important for\ncompanies. They can help to collect crucial market-related information\nwith which to ﬁne tune the strategy to meet customer needs, resulting\nin long-term ﬁnancial performance ( Lorenzoni & Lipparini, 1999 ).\nLiterature points out that the collaborative knowledge sharing capacity\nprovides an opportunity to increase value (e.g.,( Saraf et al., 2007 )) and\nenable DC (e.g., ( Della Corte & Del Gaudio, 2012 )). Considering that\nDC theory encompasses several levels of analysis, it is important to\nconsider the relational view, including the ability to collaborate with\nchannel partners ( Teece, 2007 ). Literature shows that agility needs the\nsupport of effective knowledge sharing ( Liu, Song, & Cai, 2014 ). Some\nstudies link the knowledge sharing capability through IT with agility\n(e.g., ( Cai et al., 2013; Liu et al., 2014 )). Such interactions can also\nbeneﬁt from the use of BDA technologies, consequently enhancing\norganizational agility by in ﬂuencing the capabilities to sense opportuni-\nties and threats, shape them, and seize them ( Della Corte & Del Gaudio,\n2012 ). Therefore, another hypothesis is:\nH3. BDA technologies allow an effective knowledge sharing with\npartners that positively in ﬂuences organizational dynamic capabilities\nsuch as organizational agility.\n3.1.2. Organizational agility\nDC can play a key role in determining a ﬁrm's competitive advantage\n(Teece et al., 1997; Zott, 2003 ). Agility is the “capacity of an organization\nto efﬁciently and effectively redeploy/redirect its resources to value cre-\nating and value protecting (and capturing) higher-yield activities as in-\nternal and external circumstances warrant ”(Teece, Peteraf, & Leih,\n2016 ). In the management ﬁeld several researchers recognize that DC\ndoes not lead directly to sustainable competitiveness, and that this\nvalue derives from improved business processes (e.g., ( Schilke, 2014;\nDrnevich & Kriauciunas, 2011 )). Some authors conclude that agility\ncan in ﬂuence organizational performance ( Cai et al., 2013; Liu et al.,\n2013; Tallon & Pinsonneault, 2011 ). Hence, additional hypotheses are:\nH4a. Organizational agility is a dynamic capability leveraged by BDA\nthat positively affects the creation of competitive advantages.\nH4b. Organizational agility is a dynamic capability leveraged by BDA\nthat positively in ﬂuences the process-level performance.\nBy engaging the business activities (e.g., sense customer needs, mar-\nket research, R&D) companies can increase the possibility of achieving\nprocess innovation success ( Zollo & Winter, 2002 ). In the IT ﬁeld some\nauthors focus on the importance of assessing how business processes\ncan bring value to ﬁrms (e.g., ( Chen et al., 2014; Tallon, 2007 )). Recent\nconceptual considerations are that BDA is a source of DC (organizational\nagility, speci ﬁcally) and that BDA are a way to provide business value to\nﬁrms ( Erevelles et al., 2016 ). Therefore, the hypothesis is:\nH5. Process-level performance has a positive effect on competitive\nadvantage.\n3.1.3. The mediating role of agility on the relationship between knowledge\nassets and performance\nEarlier IT literature considers that dynamic capabilities can establish\na link between knowledge assets and ﬁrm performance ( Sher & Lee,\n2004; Wang, Klein, & Jiang, 2007 ). In the management ﬁeld some\nauthors examine agility as a mediator between the management of\nknowledge assets and performance ( Chung, 2010; Liu et al., 2014 ).\nAlso, the proposed model suggests a potential mediating role of agility\nin the relationship between knowledge assets and two types ofperformance (process-level performance and competitive advantage).\nThus, additional hypotheses are:\nH6a. Agility positively mediates the relationship between endogenous\nknowledge management and competitive advantage.\nH6b. Agility positively mediates the relationship between exogenous\nknowledge management and competitive advantage.\nH6c. Agility positively mediates the relationship between knowledge\nsharing with partners and competitive advantage.\nH7a. Agility positively mediates the relationship between endogenous\nknowledge management and process-level performance.\nH7b. Agility positively mediates the relationship between exogenous\nknowledge management and process-level performance.\nH7c. Agility positively mediates the relationship between knowledge\nsharing with partners and process-level performance.\n3.1.4. Competitive advantage\nCompetitive advantage exists when a ﬁrm reveals having greater\nsuccess compared with its current or potential competitors ( Peteraf &\nBarney, 2003 ). To be consistent with this conceptualization, superior\nﬁrm performance relative to that of competitors constitutes an empiri-\ncal and common indicator of competitive advantage. ( Barnett, Greve, &\nPark, 1994; Schilke, 2014 ). Based on Schilke's construct ( Schilke, 2014 ),\ncompetitive advantage was operationalized as re ﬂective-re ﬂective type\n(Ringle, Sarstedt, & Straub, 2012 ), with the ﬁrst-order dimensions of:\n(1) strategic performance (qualitative dimension) and (2) ﬁnancial per-\nformance (quantitative dimension), both in comparison to competition.\n3.1.5. Controls\nAs literature widely supports, this study uses the industry and the\ncountry in which a ﬁrm competes as predictors of competitiveness\n(Schilke, 2014 ). BDA may be particularly useful to ﬁrms operating in\nturbulent technological environments ( Wade & Hulland, 2004 ), and\nconsequently, following the approach of Menguc and Auh ( Menguc &\nAuh, 2006 ) and Drnevich and Kriauciunas ( Drnevich & Kriauciunas,\n2011 ), the study includes turbulent technological environment as a con-\ntrol. A turbulent technological environment makes current technology\nobsolete and requires the development of new advances ( Menguc &\nAuh, 2006 ). Finally, we use the variable “time since adoption of BDA ”\nto control for the knowledge and experience that organizations gain\nby using BDA over time ( Elbashir et al., 2013 ). These controls explain\nall dependent variables (agility, process-level performance, and\ncompetitive advantage).\n4. Research design\n4.1. Measurement\nTo test the model ( Fig. 1 ) and the related hypotheses, the study per-\nforms a multi-country survey of European organizations from several\nindustries. Following the recommendations of Moore and Benbasat\n(Moore & Benbasat, 1991 ), the study uses a survey instrument drawing\nupon a comprehensive literature review. Regarding content validity,\nﬁve established academic IS researchers and two language experts\nreview each item on the questionnaire, assessing its content, scope,\nand purpose ( Brislin, 1970 ). To test the dif ﬁculty of the questions, to-\ngether with the reliability and validity of the scales, a pilot study uses\na sample of 30 executives from ﬁrms not part of the main survey.\nRemoval of some items reduces ambiguity and simpli ﬁes interpretation.\nThe survey instrument and measurement items are in Appendix A.382 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390\n\n[Página 5]\n4.2. Data\nThe survey was conducted in 2015 using an online survey tool. To\nguarantee the quality of the data, the respondent pro ﬁle uses the\nfollowing three criteria: deep knowledge of the organization strategy,\nmore than ﬁve years of experience in BI&A/BDA initiatives, and holding\nan IT/business executive or management position in the company. Themailing database comes from Dun & Bradstreet, one of the world's lead-\ningﬁrms for commercial information and business insight. The initial\nsample of 500 ﬁrm executives from European ﬁrms receives an email\nto participate in the survey.\nNinety-two valid responses were received in the ﬁrst month. To\nincrease the response rate a follow-up email was sent. During the\nfollowing months 83 additional valid responses were received from\nlate responders, totaling 175 usable responses (overall response rate\nof 35%). As seen in Table 1 , the sample comprises different industries\nof which almost half are ﬁnancial ﬁrms (40.5%). Regarding ﬁrm size,\nthe sample is equally distributed between mid-size and large compa-\nnies. Business (41.4%) and IT executives (58.6%) are well represented.\nNon-response bias was assessed using the sample distributions of the\nearly and late respondent groups compared with the Kolmogorov-\nSmirnov test ( Ryans, 1974 )( s e e Table 2 ). The early respondents were\nidenti ﬁed by selecting the respondents in the ﬁrst month. The test\nshows that the two groups do not differ statistically (5% signi ﬁcance\nlevel, pN0.05), demonstrating the absence of non-response bias\n(Ryans, 1974 ). Due to the fact that the study collects data simultaneous-\nly from a single source, for the sake of validity, common method bias\nneeds to be assessed. The study uses Harman's post hoc single-factor\nanalysis for this purpose. A factorial analysis of all indicators was con-\nducted and the ﬁrst extracted factors explain 36.9% of variance. This\nmeans that common method bias is unlikely to be an issue in the data\nPodsakoff et al., 2003 .\n5. Results\nTo estimate the conceptual model, the study uses the partial least\nsquares (PLS) method ( Hair, Ringle, & Sarstedt, 2011 ). PLS ful ﬁlls theresearch purpose by examining the validity of the constructs, without\nrequiring normal distributions for the variables. PLS requires a sample\nsize of ten times the number of the largest number of structural paths\ndirected at a particular construct ( Gefen & Straub, 2005 ). In the\nconceptual model the largest number of structural paths directed to a\nparticular construct is three, which means that the minimum sample\nsize should be 30. The sample is larger ( n=1 7 5 ) ,m e a n i n gt h a ti ti sa d -\nequate for PLS. Before testing the structural model, the study analyzes\nthe measurement model in order to assess reliability and validity.\n5.1. Measurement model\nThe study examines indicator reliability, construct reliability, con-\nvergent validity, and discriminant validity in order to assess the mea-\nsurement model. Tables 3 and 4 show the results of the measurement\nmodel. Regarding indicator reliability, only loadings above 0.7 were\nconsidered. Hence, four items (ENKM5, DC1, PLP3-4) were eliminated.\nAsTable 3 reveals, the instrument presents good indicator reliability,\nas the loadings are above 0.70. The composite reliability coef ﬁ\ncient as-\nsesses the construct reliability because construct reliability takes into\nconsideration indicators having different loadings ( Hair et al., 2011;\nHenseler, Ringle, & Sinkovics, 2009 ).Table 4 shows that all constructs\nhave composite reliability above 0.7, which suggests that the constructs\nare reliable. To test convergent validity, the study uses average variance\nextracted (AVE). The AVE should be higher than 0.5, (i.e., the latent var-\niable explains more than half of the variance of its indicators ( Henseler\net al., 2009; Fornell & Larcker, 1981 )).Table 4 shows that all constructs\nmeet this criterion. Regarding discriminant validity, the study uses two\nmeasures: the Fornell-Larcker criterion and cross-loadings. First, ac-\ncording to Fornell and Larcker ( Fornell & Larcker, 1981 ), the square\nroot of AVE should be greater than the correlations with other latent\nvariables. Table 4 shows that the square roots of AVEs (in bold) are\nhigher than the correlation between constructs. All the constructs\nshow evidence of acceptable discrimination. Second, the loading of\neach indicator should be greater than all cross-loadings ( Chin, 1998a )\n(see Table 3 ). Overall, the model has good indicator reliability, construct\nreliability, convergent validity, and discriminant validity. As these\ncriteria are met, the constructs can test the structural model.\n5.2. Structured model\nTo evaluate the structured model, we followed Hair's ﬁve-step\napproach ( Hair et al., 2013 ): (1) collinearity assessment, (2) structural\nmodel path coef ﬁcients, (3) coef ﬁcient of determination (R2value),\n(4) effect size f2,a n d( 5 )p r e d i c t i v er e l e v a n c eQ2and blindfolding.\nRegarding collinearity (1), the results suggest minimal collinearity\namong the constructs (the highest VIF among the explanatory variables\nis 2.95), which means the predictors in the structural model do not\nsuffer from this issue. To empirically assess the hypotheses postulated\ninSection 3 , the study examines the level of signi ﬁcance in pathTable 1\nSample pro ﬁle.\nSample characteristics (n = 175) Obs. (%)\nRespondent position\nIT executive\nChief Information Of ﬁcer (CIO) 22 12.5%\nIT Director 26 14.8%IT Manager 32 18.2%Other IT executive 23 13.1%\nBusiness executive\nChief Financial Of ﬁcer (CFO) 19 10.9%\nBusiness Manager - Strategic Planning 18 10.3%\nCentral Operations Of ﬁcer (COO) 14 8.0%\nOther Business executive 21 12.0%\nNo. of employees\nb50 14 8.0%\n50–250 76 43.4%\nN250 85 48.5%\nIndustry\nManufacturing 23 13.1%Electricity, gas and water supply activities 11 6.2%Wholesale and retail trade 19 10.8%Transports and telecommunications 18 10.2%Financial intermediation 71 40.5%Others 33 18.8%\nNotes: (1) The ﬁrm size is categorised based on European enterprises size classi ﬁcation\n[104]; (2) The industries of activity are in accordance with NACE (European standard clas-siﬁcation of productive economic activities).Table 2\nTesting possible response bias: early vs. late respondents.\nConstructs Full sample\nN = 175Early\nrespondentsN=9 2Late\nrespondentsN=8 3Kolmogorov-\nSmirnov test\nMean S.D. Mean S.D. Mean S.D. p-Value\nENKM 5.9 0.71 5.9 0.67 5.9 0.75 0.65\nEXKM 5.8 0.86 5.9 0.85 5.7 0.86 0.07\nKSP 4.8 0.89 4.8 0.80 4.7 0.98 0.30AG 6.1 0.93 6.1 0.78 6.0 1.07 0.72PLP 6.1 0.81 6.1 0.78 6.0 0.83 0.23CA 5.9 0.82 6.0 0.72 5.8 0.92 0.34SP 6.0 0.81 6.0 0.72 6.0 0.89 0.76FP 5.9 0.96 6.0 0.81 5.7 1.09 0.16383 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390\n\n[Página 6]\ncoefﬁcients (2) by means of a bootstrapping technique ( Hair et al.,\n2011; Henseler et al., 2009 ) with 5000 iterations of re-sampling, with\neach bootstrap sample constituted by the number of observations\n(i.e., 175 cases). To have more conservative outcomes, the study uses\ntheno sign change option ( Hair et al., 2013 ).Fig. 2 shows the estimated\nmodel (path coef ﬁcients, R2and Q2), and Table 5 summarizes the\nresults. Concerning R2values (3), all dependent variables present rea-\nsonable values. In addition, this study calculates the f2and q2effect\nsizes (4). Most of the values of f2effect size are small, with the exception\nof agility in process-level-performance and exogenous knowledge\nmanagement in agility (moderate effects). Last, based on a blindfolding\nprocedure, all Q2values are above zero, which means the model has\npredictive power concerning the dependent variables (see Fig. 2 ).\nFig. 2 summarizes the analysis results as follows: the conceptual\nmodel explains 61.8% of the variation in organizational agility. Endoge-\nnous Knowledge Management (EnKM) ( ^β= 0.155; pb0.01) and Exog-\nenous Knowledge Management (ExKM) ( ^β= 0.248; pb0.001) are\nstatistically signi ﬁcant in explaining organizational agility (AG). Thus,\nH1 and H2 are con ﬁrmed, whereas knowledge sharing partners (KSP)\n(H3) is not con ﬁrmed. Organizational agility (AG) ( ^β= 0.371;\npb0.001) is statistically signi ﬁcant in explaining Process-level Perfor-\nmance (PLP), and consequently H4b is supported. The conceptual\nmodel explains 57.8% of the variation in Process-level Performance\n(PLP). Agility (AG) contributes signi ﬁcantly to explain performance attwo levels: Process-level Performance (PLP) ( ^β= 0.371; p b0.001)\nand Competitive Advantage (CA) ( ^β=0 . 2 0 4 ;p b0.01), which con ﬁrms\nH4a and H4b .H5is not supported, as the effect is statistically not signif-\nicant (PLP- NCA). The conceptual model explains 77.8% of the variation\nin Competitive Advantage (CA). The conceptual model substantially ex-\nplains the variation of all three dependent variables ( Chin, 1998b;\nHenseler et al., 2009 ).\n5.3. Mediating effect testing\nBased on the guidelines of Hair ( Hair et al., 2013 ), Preacher\n(Preacher & Hayes, 2008 ), and Nitzl ( Nitzl, Roldán, & Cepeda,\n2016 ), the study evaluates the signi ﬁcance of the mediating effects\nof organizational agility. Mediation analysis is eligible if the indirect\neffect is signi ﬁcant. Table 6 presents the results, which ful ﬁll the nec-\nessary conditions to perform the mediator assessment. Also, the\nstudy calculates variance accounted for (VAF) to determine the size\nof the indirect effect in relation to the total effect ( Hair et al.,\n2013 ). The results show that agility can partially mediate the\nrelationship between knowledge assets (endogenous and exogenous\nknowledge) and performance (process-level performance andcompetitive advantage), thereby supporting H6a,b and H7a,b. No\nmediating effects were found between knowledge sharing withTable 3\nLoadings and cross-loadings for the measurement model.\nConstruct Item ENKM EXKM KSP AG PLP FP SP\nEndogenous knowledge management ENKM1 0.715 0.171 0.270 0.264 0.240 0.266 0.180\nENKM2 0.796 0.092 0.393 0.184 0.094 0.331 0.190\nENKM3 0.915 0.317 0.294 0.450 0.322 0.476 0.371\nENKM4 0.826 0.313 0.135 0.374 0.331 0.508 0.365\nExogenous knowledge management EXKM1 0.086 0.797 -0.183 0.390 0.365 0.328 0.345\nEXKM2 0.214 0.899 -0.136 0.495 0.477 0.446 0.403\nEXKM3 0.397 0.775 0.057 0.444 0.636 0.515 0.434\nKnowledge sharing partners KSP1 0.383 −0.012 0.873 −0.125 −0.140 −0.167 −0.156\nKSP2 0.324 −0.058 0.939 −0.145 −0.185 −0.116 −0.192\nKSP3 0.210 −0.140 0.960 −0.245 −0.276 −0.199 −0.300\nAgility AG2 0.395 0.453 −0.182 0.860 0.576 0.586 0.729\nAG3 0.397 0.482 −0.189 0.931 0.604 0.619 0.665\nAG4 0.402 0.538 −0.085 0.905 0.608 0.607 0.627\nAG5 0.327 0.494 −0.263 0.928 0.590 0.640 0.682\nPerformance at process level PLP1 0.315 0.629 −0.231 0.676 0.951 0.571 0.563\nPLP2 0.308 0.533 −0.204 0.558 0.939 0.525 0.552\nCompetitive advantage Financial performance FP1 0.445 0.501 −0.238 0.675 0.571 0.950 0.728\nFP2 0.531 0.496 −0.071 0.594 0.487 0.949 0.665\nFP3 0.477 0.518 −0.199 0.657 0.594 0.950 0.704\nStrategic performance SP1 0.343 0.363 −0.134 0.615 0.507 0.584 0.840\nSP2 0.327 0.445 −0.298 0.683 0.499 0.719 0.932\nSP3 0.321 0.485 −0.230 0.715 0.590 0.681 0.927\nTheﬁgures in bold represents the cross-loadings for the measurement model.\nTable 4\nCorrelation matrix, composite reliability (CR), and square root of AVEs.\nCR ENKM EXKM KSP AG PLP FP SP\nEndogenous knowledge management (ENKM) 0.89 0.82\nExogenous knowledge management (EXKM) 0.87 0.30 0.83\nKnowledge Sharing with Partners (KSP) 0.95 0.31 −0.09 0.93\nAgility (AG) 0.95 0.42 0.54 −0.20 0.91\nProcess level performance (PLP) 0.94 0.33 0.62 −0.23 0.66 0.95\nFinancial performance (FP) 0.97 0.51 0.54 −0.18 0.68 0.58 0.95\nStrategic performance (SP) 0.93 0.37 0.49 −0.25 0.75 0.59 0.74 0.90\n(1) First column are CR (composite reliability).\n(2) Diagonal elements are square root of average variance extracted (AVE).(3) Off-diagonal elements are correlations.The bold ﬁgures represent the square roots of AVEs.384 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390\n\n[Página 7]\npartners and performance (process-level performance and competi-\ntive advantage), which means H6c and H7c are not con ﬁrmed.\n6. Discussion\nAs BDA can generate value in several ways, the need exists to under-\nstand the entire chain. This study ﬁlls the research gap by assessing not\nonly the antecedents but also the effects of BDA initiatives in European\nﬁrms.\nThe results strongly support the claim that BDA applications can\nallow an effective internal and external knowledge management which\ncan help ﬁrms to create organizational agility. This agility exists in several\nways: (1) by sensing opportunities and threats (e.g., reacting to new\nproducts or services of competitors); (2) by seizing possible chances\n(e.g., expanding into new regional or international markets), and\n(3) by adjusting to the technological environment to attain competitive\nadvantage (e.g., adopting new technologies to produce products andservices more ef ﬁciently). This ﬁnding is consistent with earlier literature\n(Chen et al., 2014; Liu et al., 2014; Sher & Lee, 2004 ).\nRegarding the antecedents, the results demonstrate that BDA can\nsupport organizational knowledge management, allowing the crea-\ntion/enhancement of dynamic capabilities such as organizational agility.\nThisﬁnding is consistent with earlier studies applied to IT innovations\nand organizational management (e.g., ( Nieves & Haller, 2014; Sher &\nLee, 2004; Cai et al., 2013; Liu et al., 2014; Cepeda & Vera, 2007 )). The\nresults suggest that exogenous knowledge management deserves\nmore attention, which was considered more important than endoge-\nnous knowledge management. This outcome suggests that BDA\ntechnologies can provide business value by facilitating the acquisition\nof supply chain and marketing knowledge. While knowledge manage-\nment is important to explain BDA value creation, the way of sharing\nthis strategic asset among business partners is not statistically signi ﬁ-\ncant in this study. Although the hypothesis related to the knowledge\nshared with partners ( H3) seems plausible and consistent with earlier\nstudies for other IT innovations (e.g., ( Zhu & Kraemer, 2005; Zheng\nFig. 2. Estimated model. Note: ns = non-signi ﬁcant. ** |t| N=1.96 at p = 0.05; *** |t| N=2 . 5 7a tp=0 . 0 1l e v e l ;* * * *| t | N=3.29 at p = 0.001 level.\nTable 5\nSigniﬁcant testing results of the structural model path coef ﬁcients.\nStructural path Path coef ﬁcient (t-value) Effect size (f2) Effect size (q2) 95% con ﬁdence interval Conclusion\nEndKM →AG 0.155⁎⁎\n(2.562)0.038 0.024 [0.032; 0.268] H1supported\nExKM→AG 0.248 ⁎⁎⁎⁎\n(4.556)0.120 0.074 [0.149; 0.364] H2supported\nKSP→AG 0.010 ns\n(0.121)0.000 0.000 [ −0.145; 0.169] H3not supported\nAG→CA 0.204 ⁎⁎⁎\n(2.786)0.064 0.021 [0.065; 0.351] H4a supported\nAG→PLP 0.371⁎⁎⁎⁎\n(3.969)0.125 0.080 [0.173; 0.544] H4b supported\nPLP→CA 0.106 ns\n(1.579)0.021 0.007 [ −0.030; 0.234] H5not supported\nNote: ns = non-signi ﬁcant.\nThe values of f2and q2effects can be considered weak (0.02). moderate (0.15) and strong (0.35).\nConﬁdence level:\n⁎⁎|t|N=1.96 at p= 0.05 level.\n⁎⁎⁎|t|N=2 . 5 7a t p=0 . 0 1l e v e l .\n⁎⁎⁎⁎ |t|N=3.29 at p= 0.001 level.385 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390\n\n[Página 8]\net al., 2011; Ruivo, Oliveira, & Neto, 2014 )), this construct does not con-\ntribute to creating valuable organizational agility. An earlier study con-\ncludes that using this type of knowledge is not always useful and can\nharm speci ﬁc business processes in some situations. Moreover, this\nstudy shows that agility can partially mediate the positive effect of\nsome knowledge assets (exogenous and endogenous) and performance\n(process-level performance and competitive advantage) ( H6a,H6b and\nH7a,H6b). This ﬁnding is consistent with earlier studies ( Liu et al., 2013;\nLiu et al., 2014; Pavlou & El Sawy, 2006 ).\nCompetitive performance is not only about how much ﬁrms know,\nbut how they use what they know ( Haas & Hansen, 2005 ). A possible\nexplanation for this result is that ﬁrms are reluctant to share sensitive\ninformation that might compromise their competitive advantage. In\nfact, synergies with business partners can be bene ﬁcial (e.g.,( Setia,\nRichardson, & Smith, 2015 )), but careful attention is needed regarding\nthe shared information. The study shows that knowledge sharing with\npartners can be truly compromising in the areas of Production and Op-\nerations or Product and Service enhancement, which represent the core\nbusiness practices of a ﬁrm. An information sharing agreement might be\na solution to overcome this constraint.\nConcerning the effects of agility leveraged by BDA, the results indi-\ncate that this dynamic capability can positively impact competitive ad-\nvantage in different ways (via processes or organizationally), which is\nin line with the ﬁndings of other authors ( Drnevich & Kriauciunas,\n2011; Protogerou et al., 2012 )(H4a,b). Agility can also be more effective\nin improving speci ﬁc business processes than organizational perfor-\nmance, which is consistent with Drnevich and Kriauciunas ( Drnevich\n& Kriauciunas, 2011 ).The results demonstrate that no signi ﬁcant link\nexists between process-level performance and competitive advantage\n(H5). In this sense, Drnevich and Kriauciunas ( Drnevich & Kriauciunas,\n2011 ) argue that a ﬁrm's performance depends on a set of elements\nthat might fail due to miscommunication between the business areas\nand the top management. Although some business areas can behave\nin an ef ﬁcient way, this ef ﬁciency does not necessarily have a signi ﬁcant\neffect on the overall performance.\nAlthough BDA technologies are generaly associated with customer\nmanagement or marketing areas, results indicate that, in general,\nEuropean ﬁrms focus more on internally improving their assets\n(products and services) and the way that these are being produced to\noptimize costs. With Europe still showing signs of ﬁnancial crisis, this\nﬁnding might point the way to a change of survival strategy in compet-\nitive markets.\n6.1. Limitations and further research\nCertain limitations apply to the interpretation of the results of this\nstudy. First, the antecedents of agility do not extend beyond the speci ﬁc\nknowledge resources included in the model. Other factors can also\ndetermine the development of this dynamic capability in European\nﬁrms. Future studies may include these resources as variables of themodel or by moderating existing variables. Second, although the study\nconsiders constructs in the model embedding the impact of BDA at\nprocess-level, the model is ﬁrm-level. Before generalization is possible,\nresearchers should perform a longitudinal study based on the process\napproach. Future research should use speci ﬁcp r o c e s sc o n s t r u c t st o\nassess the impact of BDA on several business areas in detail. Third, due\nto the perceptual nature of the measures used, future studies should\nidentify the issues associated with cross-sectional research design.\nAlthough the use of objective measures to assess ﬁrm performance is\nimportant, in this study companies were reluctant to provide them.\nFourth, although the sample size is statistically adequate, a larger\nsample could be useful to reinforce the conclusions of this study.\nAs researchers generally accept that BDA can provide bene ﬁts to all\nEuropean ﬁrms ( European_Commission, 2015 ) across several indus-\ntries, reinforced on a McKinsey survey ( Manyika et al., 2011b )r e p o r t s\nthat most industries in Europe have the capacity to store and manipu-\nlate big data, and consequently the potential value of using big data\nresides mainly in developed countries. Therefore, data from ﬁve\nEuropean developed countries were collected. By conducting future\nstudies in more countries and industries, which may have different per-\nceptions of BDA and diverse external contexts, the understanding of\nBDA business value could likely improve. Due to their different cultures,\nresearch to perform a comparative study among European regions\n(e.g., Northern and Southern Europe) could be interesting.\n6.2. Theoretical implications\nThis study offers two key contributions that extend theory on BDA in\ntechnology and organizational management research:\n(1)BDA value chain understanding - Despite the potential bene ﬁts,\nsome ﬁrms fail to capture value from BDA initiatives ( Kaisler\net al., 2013 ). Recent papers focus on BDA research opportunities\n(Abbasi et al., 2016; Agarwal & Dhar, 2014 ), claiming that there is\na need to conduct assessments of the actual impact of BDA\ninvestments and use, and to understand how to achieve the\nbeneﬁts for performance. The BDA value chain remains relatively\nunexplored and requires further investigation. The current paper\nresponds to the calls of scholars by empirically assessing the\nvalue that BDA can bring to European ﬁrms. This study theoreti-\ncally proposes and empirically validates a conceptual model\nbased on strategic management theories (KBV and DC), never\nbefore combined for this purpose, to explain the full BDA value\nchain. Liu ( L i ue ta l . ,2 0 1 4 ) argues that literature about the\nrelationship among knowledge management, organizational\nagility, and ﬁrm performance is still limited. This is the ﬁrst\nstudy that empirically demonstrates that BDA applications\nbased on an effective knowledge management can help ﬁrms to\ncreate organizational agility leading to competitive advantage.\nFurther studies could bene ﬁcially use this theoretical framework\nto assess the business value in other IT innovations at a process-Table 6\nMediation test by bootstrapping approach.\nEffect of Direct effect (t-value) Indirect effect (t-value) Total effect VAF (%) Interpretation Conclusion\nEnKM→AG→CA 0.137 ⁎⁎(2.317) 0.053 ⁎⁎(2.156) 0.190 ⁎⁎⁎⁎(3.577) 27.89% Partial mediation H6a supported\nExKM→AG→CA 0.081 ns (1.506) 0.097 ⁎⁎⁎(2.617) 0.178 ⁎⁎⁎⁎(4.037) 54.49% Partial mediation H6b supported\nKSP→AG→CA 0.026 ns (0.464) −0.014 ns (0.607) 0.012 ns (0.199) na No mediation H6cnot supported\nEnKM→AG→PLP 0.141 ⁎⁎(1.988) 0.057 ⁎⁎(2.212) 0.198 ⁎⁎⁎(2.813) 28.79% Partial mediation H7a supported\nExKM→AG→PLP 0.344⁎⁎⁎⁎(5.412) 0.092⁎⁎⁎(3.041) 0.436⁎⁎⁎(7.219) 21.10% Partial mediation H7b supported\nKSP→AG→PLP −0.157⁎⁎(2.408) 0.003 ns (0.119) −0.154⁎⁎(2.172) na No mediation H7cnot supported\nNote: VAF = variance accounted for. The VAF N80% indicates full mediation. 20% ≤VAF≥80% show partial mediation. VAF b20% indicates no mediation. ns = non-signi ﬁcant. na = not\napplicable.\n⁎⁎|t|N=1.96 at p = 0.05 level.\n⁎⁎⁎|t|N= 2.57 at p = 0.01 level.\n⁎⁎⁎⁎ |t|N=3.29 at p = 0.001 level.386 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390\n\n[Página 9]\nlevel and ﬁrm-level. Academics can make use of this paper for\npedagogical support for teaching about BDA value chain.\n(2)DC literature –This paper contributes to DC research by empir-\nically testing agility business value in a BDA context ( Drnevich\n& Kriauciunas, 2011 ). The results strongly support the belief\nthat BDA technologies can trigger agility and that agility can af-\nfect competiveness in two ways (via processes or globally). AsBDA can signi ﬁcantly improve business processes ( Davenport,\n2006 ), business process enhancement driven by BDA is an im-\nportant research area ( Abbasi et al., 2016 ). Earlier studies focus\nonly on the link between agility and ﬁrm performance ( Chen\net al., 2014; Liu et al., 2014; Tallon & Pinsonneault, 2011 ), while\nthis study empirically demonstrates that an effect of agility exists\nat the process-level, too. In addition, despite an increasing use of\nmediation testing, most of the studies in PLS-SEM do not analyze\nmediation effects ( Hair et al., 2013; Nitzl et al., 2016 ). Under-\nstanding mediation issues can be crucial for researchers because\nthey can better explain or hinder the in ﬂuence of a third variable\nin the relationship between two variables in a model ( Cepeda &\nVera, 2007 ). This study demonstrates that agility can be a\nmediator between external and internal knowledge assets and\nperformance (process-level performance and competitive\nadvantage).\n6.3. Managerial implications\nFor practitioners (including executives and IT managers) this study\ndemonstrates how best to leverage the knowledge embedded in BDA\nsystems and initiatives and achieve capabilities that will help to main-\ntain competitive advantages. The paper provides support to justify\nBDA investments and initiatives. The results indicate that although\nBDA technologies call for substantial investment in implementation\nand maintenance, European ﬁrms are aware of BDA's potential value\nand bene ﬁts. Executives should apply these guidelines to their organiza-\ntional IT strategy.\nBDA can provide value at several stages: (1) knowledge; (2) dynamic\ncapability (organizational agility); (3) business process; and (4) com-\npetitive performance. To initiate the value creation process, ﬁrms\nshould invest in an effective BDA program. First, the value that BDA\ncan provide derives ﬁrst from the way ﬁrms use the technologies\navailable to manage knowledge. An effective training program can\nhelp to leverage the way users extract and manage knowledge. Second,\nby effectively using BDA, ﬁrms can acquire capabilities to innovate and\nrapidly adjust to external demands (e.g., optimize business processes).\nThird, these capabilities will encourage speci ﬁc business areas to\ninvolve the whole organization, when an effective bottom-up strategy\nis followed, supported by good communication practices. By applying\nthis framework to BDA speci ﬁcally, managers and IT executives can\nbeneﬁt from a performance metric that uniquely speci ﬁes the impact\nof BDA. By evaluating the organizational knowledge conversion into\nprocess and ﬁrm-level capabilities, practitioners can increase their\nproductivity. Software vendors of BDA can also gain a better under-\nstanding of how European ﬁrms can invest and experience the value\ncreated through BDA. They can natively embed BDA capabilities in\ntheir solutions as a way for their customers to achieve superior ﬁnancial\nand strategic performance. Finally, ﬁrms that have not yet decided to\nadopt these technologies can gain a perception of what is possible by\nadopting and effectively using BDA.\n6.4. Business research implications\nThe business community now sees big data as a potential tool of\nbusiness value for achieving competitive advantage. This value can\nonly be real if companies know how to effectively manage Big Data An-\nalytics (BDA) initiatives. This paper establishes a ﬁrst link between BDAprocess-level performance and competitive advantage, by merging the\nﬁeld of information systems and strategic management. By presenting\nand discussing strategic and organizational drivers and impacts of\nBDA, guidance to business researchers, practitioners, and scholars is\nprovided. As such, this paper extends knowledge by directly evaluating\nthe effect of BDA on the decision-making process to support an effective\nIT resource management, focusing on challenges for adoption, gover-nance, and evaluation.\nThe outcomes of this paper indicate that BDA can be an effective\naid to survival in competitive markets, particularly by supporting\nProduction and Operations or P roduct and Service enhancement.\nStriving to overcome damages of the ﬁnancial crisis, European\nﬁrms are using BDA tools to internally improve their assets (products\nand services) and the way that these are being produced to optimize\ncosts. European ﬁrms tend to attribute greater value to external\nknowledge provided by BDA applications than to internal knowledge\nmanagement. Sharing knowledge with business partners is poten-\ntially harmful to organizational productivity, so careful attention is\nin order when exchanging this type of core data between companies.\nAlso, this study concludes that organizational agility leads directly to\na better performance (process-level and competitive advantage) but\ncan mediate effects from knowledge assets on performance. This\nmeans that ﬁr m sm u s tb e a ri nm i n dt h a ts e v e r a lp a t h sc a nl e a dt o\ncompetitive advantage. First, managers should consider investing\nin BDA technologies to take advantage of internal and external\nknowledge resources. Second, by governing the knowledge extract-\ned by BDA, agility becomes the “ultimate ”organizational capability\nthat leads to sustainable compet itive advantages. Firms should\nconﬁdently invest in the development of agility supported by BDA\ntools.\n7. Conclusions\nAs Big Data Analytics (BDA) can offer value to companies in\nseveral ways, many scholars highlight the need to understand the\npath to competitive advantage. The main outcome emerging from\nthis paper has to do with understanding the value chain of BDA.\nGrounded on knowledge-based view (KBV) and dynamic capabilities\n(DC), this study ﬁlls a research gap from the strategic management\nperspective, by perceiving the antecedents (knowledge assets) and\nthe impacts (on process-level performance and competitive advan-\ntage) of BDA initiatives in European ﬁrms. The results show that\nthe model signi ﬁcantly explains all dependent variables (61.8% of\nagility variation, 57.8% of process-level performance variation, and\n77.8% of competitive advantage variation). The major conclusions\nof this study are:\na) BDA can be a strategic investment for European ﬁrms to enhance or-\nganizational agility and survive in competitive markets. Firms\nshould invest in the development of organizational agility supported\nby effective BDA applications.\nb) To create agility, European ﬁrms tend to believe that the external\nknowledge deriving from BDA applications can be more effective\nin the creation of agility than internal knowledge. Sharing knowl-\nedge with business partners is problematic, as sharing, is a potential\nbarrier for process-level performance.\nc) Regarding the impacts of agility, this capability leads directly to a\nbetter performance (process-level and competitive advantage) but\ncan mediate effects from knowledge assets on performance. This\nmeans that BDA initiatives can lead to better operational ef ﬁciency,\nbut several paths can lead to competitive advantage.\nThus, a crucial need exists for ﬁrms to have an integrated view of the\nBDA chain in order to be able to fully leverage the innovative power of\nBDA capabilities to achieve competitive advantage.387 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390\n\n[Página 10]\nAppendix A. Survey questionnaire\nReferences\nAbbasi, A., Sarker, S., & Chiang, R. H. (2016). Big Data research in information systems: To-\nward an inclusive research agenda. Journal of the Association for Information Systems ,\n17(2), 3.\nAgarwal, R., & Dhar, V. (2014). Editorial —Big Data, data science, and analytics: The oppor-\ntunity and challenge for IS research. Information Systems Research ,25(3), 443 –448.\nAmbrosini, V., & Bowman, C. (2009). What are dynamic capabilities and are they a useful\nconstruct in strategic management? International Journal of Management Reviews ,\n11(1), 29 –49.\nArend, R., & Bromiley, P. (2009). Assessing the dynamic capabilities view: spare change,\neveryone? Strategic Organization ,7(1), 75.\nBarnett, W. P., Greve, H. R., & Park, D. Y. (1994). An evolutionary model of organizational\nperformance. Strategic Management Journal ,15(S1), 11 –28.\nBarton, D. (2012). Making advanced analytics work for you. Harvard Business Review ,90,\n78–83.\nBarua, A., Kriebel, C. H., & Mukhopadhyay, T. (1995). Information technologies and busi-\nness value: An analytic and empirical investigation. Information Systems Research ,\n6(1), 3 –23.\nBharadwaj, A. S. (2000). A resource-based perspective on information technology\ncapability and ﬁrm performance: An empirical investigation. MIS Quarterly ,24(1),\n169–196.Blome, C., Schoenherr, T., & Rexhausen, D. (2013). Antecedents and enablers of supply\nchain agility and its effect on performance: A dynamic capabilities perspective.\nInternational Journal of Production Research ,51(4), 1295 –1318.\nBrislin, R. W. (1970). Back-translation for cross-cultural research. Journal of Cross-Cultural\nPsychology ,1(3), 185 –216.\nCai, Z., et al. (2013). Developing organizational agility through IT capability and KM\ncapability. The moderating effects of organizational climate .P A C I S .\nCepeda, G., & Vera, D. (2007). Dynamic capabilities and operational capabilities: A\nknowledge management perspective. Journal of Business Research ,60(5),\n426–437.\nChau, M., & Xu, J. (2012). Business intelligence in blogs: Understanding consumer interac-\ntions and communities. MIS Quarterly ,36(4), 1189 –1216.\nChen, H., Chiang, R., & Storey, V. (2012). Business intelligence and analytics: From Big\nData to big impact. MIS Quarterly ,36(4), 1165 –1188.\nChen, Y., et al. (2014). IT capability and organizational performance: The roles of business\nprocess agility and environmental factors. European Journal of Information Systems ,\n23(3), 326 –342.\nChin, W. W. (1998a). Commentary: Issues and opinion on structural equation modeling.\nJSTOR, 7 –16.\nChin, W. W. (1998b). The partial least squares approach for structural equation modeling.\nChung, T. R. (2010). Knowledge creation and ﬁrm performance. In e. (Ed.), Mediating\nprocesses from an organizational agility perspective .A M C I S .Constructs Items Source\nKnowledge assets Please indicate the extent to which these forms of knowledge are used in your organization.\nBDA technologies:\nEndogenous knowledge\nManagementENKM1. Reduce uncertainties of knowledge loss\nENKM2. Reduce dependence on speci ﬁc personnel\nENKM3. Are comprehensively utilized by members in organizationENKM4. Are comprehensively constructed in organization*(Sher & Lee, 2004 )\nExogenous knowledge\nManagementEXKM1. Facilitate acquisition of supply chain knowledge\nEXKM2. Facilitate processing of supply chain knowledge\nEXKM3. Facilitate processing of marketing knowledge(Sher & Lee, 2004 )\nKnowledge sharing with channel\npartnersKSP1. We frequently share knowledge about our business environment\n(e.g., other business relationships) with our channel partners.KSP2. Knowledge about all of our channel partners, competitors, etc., is shared with ourother channel partners.KSP3. Business insights are exchanged between us and our other channel partners.(Liu et al., 2014 )\nOrganizational agility (dynamic\ncapability)Please indicate the degree to which the use of BDA tools in the last three years has helped to:\nAG1. Respond to changes in aggregate consumer demand.*\nAG2. React to new product or service launches by competitors.\nAG3. Expand into new regional or international markets.AG4. Change (i.e., expand or reduce) the variety of products/services available for sale.AG5. Adopt new technologies to produce better, faster, and cheaper products and services.(Lu & Ramamurthy, 2011 )\nProcess-level performance To what extent has BDA been used to support critical business activities in each of the following\nprocesses in the last three years. A sampling of critical activities in each process is shown below.PLP1. Production and operations: improve throughout, boost labour productivity, improve ﬂexibility\nand equipment utilisation, and streamline operations.\nPLP2. Product and service enhancement: embed IT in products, increase pace of development/R&D,\nmonitor design cost, improve quality, support innovation.PLP3. Marketing and sales: spot market trends, anticipate customer needs, build market share,improve forecast accuracy, and evaluate pricing options.*PLP4. Customer relations: respond to customer needs, provide after-sales service and support, improvedistribution, create customer loyalty*(Peteraf & Barney, 2003 )\nCompetitive advantage Please indicate the degree to which you agree with the following statements.\nStrategic Performance\nSP1. We have gained strategic advantages over our competitorsSP2. We have a large market share.SP3. Overall, we are more successful than our major competitors.Financial performanceFP1. Our EBIT (earnings before interest and taxes) is continuously above industry average.FP2. Our ROI (return on investment) is continuously above industry average.FP3. Our ROS (return on sales) is continuously above industry average.(Schilke, 2014 )\nControl variables\nTime since BDA adoption Number of years since adoption (#)Country CountryIndustry Type of industryTechnological turbulence Please indicate the degree to which you agree with the following statements.\nTT1. Extent of technological turbulence in the environment.TT2. Leadership in product/process innovation.\nTT3. Impact of new technology on operations.(Brislin, 1970 )\nNotes: (1) * items eliminated due low loading. (2) Items were measured using a 7-point numerical scale (1 is Strongly Disagree and 7 is Strongly Agree).388 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390\n\n[Página 11]\nCorte Real, N., Oliveira, T., & Ruivo, P. (2014). Understanding the hidden value of business\nintelligence and analytics (BI&A). Twentieth American Conference of Information\nSystems . Savannah, Georgia: Association of Information Systems.\nDavenport, T. H. (2006). Competing on analytics. Harvard Business Review ,84,1–12.\nDella Corte, V., & Del Gaudio, G. (2012). Dynamic capabilities: A still unexplored issue\nwith growing complexity. Corporate Ownership and Control ,9,3 2 7 –338.\nDrnevich, P. L., & Kriauciunas, A. P. (2011). Clarifying the conditions and limits of the con-\ntributions of ordinary and dynamic capabilities to relative ﬁrm performance. Strategic\nManagement Journal ,32(3), 254 –279.\nElbashir, M. Z., et al. (2013). Enhancing the business value of business intelligence: The role\nof shared knowledge and assimilation. Journal of Information Systems ,27(2), 87 –105.\nErevelles, S., Fukawa, N., & Swayne, L. (2016). Big Data consumer analytics and the trans-\nformation of marketing. Journal of Business Research ,69(2), 897 –904.\nErickson, S., & Rothberg, H. (2015). Big Data and knowledge management: Establishing a\nconceptual foundation. Leading issues in knowledge management. Vol. Two . (pp. 204) 2.\nEuropean_Commission (2015). Towards a thriving data-driven economy. Accessed on:\n30th December 2015]; Available from http://ec.europa.eu/digital-agenda/en/\ntowards-thriving-data-driven-economy#Article\nFornell, C., & Larcker, D. F. (1981). Evaluating structural equation models with unobserv-\nable variables and measurement error. Journal of Marketing Research ,18,3 7 5 –381.\nGefen, D., & Straub, D. (2005). A practical guide to factorial validity using PLS-Graph: Tu-\ntorial and annotated example. Communications of the Association for Information\nSystems ,16(1), 5.\nGoldman, S. L., Nagel, R. N., & Preiss, K. (1995). Agile competitors and virtual organizations:\nStrategies for enriching the customer. Van Nostrand Reinhold.\nGrant, R. M. (1996). Prospering in dynamically-competitive environments: Organization-\nal capability as knowledge integration. Organization Science ,7(4), 375 –387.\nHaas, M. R., & Hansen, M. T. (2005). When using knowledge can hurt performance: The\nvalue of organizational capabilities in a management consulting company. Strategic\nManagement Journal ,26(1), 1 –24.\nHair, J. F., Ringle, C. M., & Sarstedt, M. (2011). PLS-SEM: Indeed a silver bullet. Journal of\nMarketing Theory and Practice ,19(2), 139 –152.\nHair, J. F., Jr., et al. (2013). A primer on partial least squares structural equation modeling\n(PLS-SEM). Sage Publications.\nHelfat, C., & Peteraf, M. (2009). Understanding dynamic capabilities: Progress along a de-\nvelopmental path. Strategic Organization ,7(1), 91.\nHelfat, C. E., et al. (2009). Dynamic capabilities: Understanding strategic change in organiza-\ntions. John Wiley & Sons.\nHenseler, J., Ringle, C. M., & Sinkovics, R. R. (2009). The use of partial least squares path\nmodeling in international marketing. Advances in International Marketing (AIM) ,20,\n277–320.\nIDC (2011). Big Data analytics. Future architectures, skills and roadmaps for the CIO .\nKaisler, S., et al. (2013). Big Data: Issues and challenges moving forward. In system sci-\nences (HICSS). 2013 46th Hawaii International Conference on System Sciences .I E E E .\nKwon, O., Lee, N., & Shin, B. (2014). Data quality management, data usage experience and\nacquisition intention of Big Data analytics. International Journal of Information\nManagement ,34(3), 387 –394.\nLaValle, S., et al. (2011). Big Data, analytics and the path from insights to value. MIT Sloan\nManagement Review ,52(2), 21 –31.\nLiu, H., Song, D., & Cai, Z. (2014). Knowledge management capability and ﬁrm performance:\nThe mediating role of organizational agility. PACIS.\nLiu, H., et al. (2013). The impact of IT capabilities on ﬁrm performance: The mediating\nroles of absorptive capacity and supply chain agility. Decision Support Systems ,\n54(3), 1452 –1462.\nLorenzoni, G., & Lipparini, A. (1999). The leveraging of inter ﬁrm relationships as a distinc-\ntive organizational capability: A longitudinal study. Strategic Management Journal ,\n20(4), 317 –338.\nLu, Y., & Ramamurthy, K. (2011). Understanding the link between information technology\ncapability and organizational agility: An empirical examination. MIS Quarterly ,35(4),\n931–954.\nMalladi, S. (2013). Adoption of business intelligence & analytics in organizations –An em-\npirical study of antecedents. 19th American Conference on Information Systems\n(AMCIS) Chicago, Illinois.\nManyika, J., et al. (2011a). In M.G. Institute (Ed.), Big Data: The next frontier for innovation,\ncompetition and productivity .M c K i n s e yG l o b a lI n s t i t u t e .\nManyika, J., et al. (2011b). Big Data: The next frontier for innovation competition and\nproductivity. McKinsey Global Institute.\nMata, F. J., Fuerst, W. L., & Barney, J. B. (1995). Information technology and sustained\ncompetitive advantage: A resource-based analysis. MIS Quarterly ,19(4), 487 –505.\nMelville, N., Kraemer, K., & Gurbaxani, V. (2004). Information technology and organiza-\ntional performance: An integrative model of IT business value. MIS Quarterly ,28(2),\n283–322.\nMenguc, B., & Auh, S. (2006). Creating a ﬁrm-level dynamic capability through capitaliz-\ning on market orientation and innovativeness. Journal of the Academy of Marketing\nScience ,34(1), 63 –73.\nMoore, G. C., & Benbasat, I. (1991). Development of an instrument to measure the percep-\ntions of adopting an information technology innovation. Information Systems\nResearch ,2(3), 192 –222.\nMorabito, V. (2015). Big Data and analytics: Strategic and organizational impacts. Springer.\nNieves, J., & Haller, S. (2014). Building dynamic capabilities through knowledge resources.\nTourism Management ,40,2 2 4 –232.\nNitzl, C., Roldán, J. L., & Cepeda, G. (2016). Mediation analyses in partial least squares\nstructural equation modeling. Helping researchers discuss more sophisticated models\n(pp. 3 –21).\nNonaka, I. (1995). The knowledge-creating company: How Japanese companies create the\ndynamics of innovation. Oxford University Press.Pavlou, P. A., & El Sawy, O. A. (2006). From IT leveraging competence to competitive ad-\nvantage in turbulent environments: The case of new product development.\nInformation Systems Research ,17(3), 198 –227.\nPavlou, P. A., & El Sawy, O. A. (2011). Understanding the elusive black box of dynamic ca-\npabilities. Decision Sciences ,42(1), 239 –273.\nPavlou, P. A., et al. (2005). Measuring the return on information technology: A\nknowledge-based approach for revenue allocation at the process and ﬁrm level.\nJournal of the Association for Information Systems ,6(7), 199 –226.\nPeteraf, M. A., & Barney, J. B. (2003). Unraveling the resource-based tangle. Managerial\nand Decision Economics ,24(4), 309 –323.\nPettigrew, A. M., Thomas, H., & Whittington, R. (2001). Handbook of strategy and manage-\nment. Sage.\nPodsakoff, P. M., et al. (2003). Common method biases in behavioral research: A critical\nreview of the literature and recommended remedies. Journal of Applied Psychology ,\n88(5), 879.\nPopovi č, A., et al. (2012). Towards business intelligence systems success: Effects of\nmaturity and culture on analytical decision making. Decision Support Systems ,54,\n729–739.\nPreacher, K. J., & Hayes, A. F. (2008). Asymptotic and resampling strategies for assessing\nand comparing indirect effects in multiple mediator models. Behavior Research\nMethods ,40(3), 879 –891.\nProtogerou, A., Caloghirou, Y., & Lioukas, S. (2012). Dynamic capabilities and their indirect\nimpact on ﬁrm performance. Industrial and Corporate Change ,21(3), 615 –647.\nRajpathak, T., & Narsingpurkar, A. (2013). Managing knowledge from Big Data analytics in\nproduct development. Tata Consulting, 11.\nRingle, C. M., Sarstedt, M., & Straub, D. (2012). A critical look at the use of PLS-SEM in MIS\nquarterly. MIS Quarterly (MISQ) ,3 6 ( 1 ) .\nRuggles, R. (1998). The state of the notion: Knowledge management in practice. California\nManagement Review ,40(3), 80 –89.\nR u i v o ,P . ,O l i v e i r a ,T . ,&N e t o ,M .( 2 0 1 4 ) . Examine ERP post-implementation stages of use\nand value: Empirical evidence from Portuguese SMEs. International Journal of\nAccounting Information Systems ,15(2), 166 –184.\nRuivo, P., Oliveira, T., & Neto, M. (2015). Using resource-based view theory to assess the\nvalue of ERP commercial-packages in SMEs. Computers in Industry ,73,1 0 5 –116.\nRussom, P. (2011). Big Data analytics. Fourth Quarter: TDWI Best Practices Report.\nRyans, A. B. (1974). Estimating consumer preferences for a new durable brand in an\nestablished product class. Journal of Marketing Research ,4 3 4 –443.\nSambamurthy, V., Bharadwaj, A., & Grover, V. (2003). Shaping agility through digital op-\ntions: Reconceptualizing the role of information technology in contemporary ﬁrms.\nMIS Quarterly ,2 3 7 –263.\nSambamurthy, V., et al. (2007). IT-enabled organizational agility and ﬁrms' sustainable\ncompetitive advantage. ICIS 2007 proceedings (pp. 91).\nSaraf, N., Langdon, C. S., & Gosain, S. (2007). IS application capabilities and relational value\nin inter ﬁrm partnerships. Information Systems Research ,18(3), 320 –339.\nSAS (2013). Big Data analytics. An assessment of demand for labour and skills, 2012 –2017 .\nSchilke, O. (2014). On the contingent value of dynamic capabilities for competitive advan-\ntage: The nonlinear moderating effect of environmental dynamism. Strategic\nManagement Journal ,35(2), 179 –203.\nSchryen, G. (2013). Revisiting IS business value research: What we already know, what\nwe still need to know, and how we can get there. European Journal of Information Sys-\ntems,22(2), 139 –169.\nSetia, P., Richardson, V., & Smith, R. J. (2015). Business value of partner's IT intensity:\nValue co-creation and appropriation between customers and suppliers. Electronic\nMarkets ,1–16.\nShanks, G., & Bekmamedova, N. (2013). Creating value with business analytics in the sup-\nply chain. European Conference of Information Systems. Utrecht: European Conference\non Information Systems .\nShanks, G., & Sharma, R. (2011). Creating value from business analytics systems: The im-\npact of strategy. 15th Paci ﬁc Asia Conference on Information Systems: Quality Research\nin Paci ﬁc, PACIS 2011 (pp. 1 –12). Queensland: Queensland University of Technology.\nSharma, R., Mithas, S., & Kankanhalli, A. (2014). Transforming decision-making processes:\nA research agenda for understanding the impact of business analytics on organisa-\ntions. European Journal of Information Systems ,23(4), 433 –441.\nS h e r ,P .J . ,&L e e ,V .C .( 2 0 0 4 ) . Information technology as a facilitator for enhancing\ndynamic capabilities through knowledge management. Information & Management ,\n41(8), 933 –945.\nSoh, C., & Markus, M. L. (1995). How IT creates business value: A process theory synthesis.\nInternational Conference of Information Systems . ICIS Proceedings.\nTallon, P. P. (2007). A process-oriented perspective on the alignment of information\ntechnology and business strategy. Journal of Management Information Systems ,\n24(3), 227 –268.\nTallon, P. P., & Pinsonneault, A. (2011). Competing perspectives on the link between stra-\ntegic information technology alignment and organizational agility: Insights from a\nmediation model. MIS Quarterly , 35(2).\nTeece, D. J. (2007). Explicating dynamic capabilities: The nature and microfoundations of\n(sustainable) enterprise performance. Strategic Management Journal ,28(13), 1319 –1350.\nTeece, D., Peteraf, M. A., & Leih, S. (2016). Dynamic capabilities and organizational agility:\nRisk, uncertainty and entrepreneurial management in the innovation economy. Un-\ncertainty and Entrepreneurial Management in the Innovation Economy (April 7, 2016) .\nTeece, D. J., Pisano, G., & Shuen, A. (1997). Dynamic capabilities and strategic manage-\nment. Strategic Management Journal ,18(7), 509 –533.\nVolberda, H. W. (1996). Toward the ﬂexible form: How to remain vital in hypercompet-\nitive environments. Organization Science ,7(4), 359 –374.\nWade, M., & Hulland, J. (2004). Review: The resource-based view and information sys-\ntems research: Review, extension, and suggestions for future research. MIS\nQuarterly ,28(1), 107 –142.389 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390\n\n[Página 12]\nWang, C. L., & Ahmed, P. K. (2007). Dynamic capabilities: A review and research agenda.\nInternational Journal of Management Reviews ,9(1), 31 –51.\nWang, E., Klein, G., & Jiang, J. J. (2007). IT support in manufacturing ﬁrms for a knowledge\nmanagement dynamic capability link to performance. International Journal of\nProduction Research ,45(11), 2419 –2434.\nWeill, P., Subramani, M., & Broadbent, M. (2002). Building IT infrastructure for strategic\nagility. MIT Sloan Management Review ,44(1), 57.\nWu, L. -Y. (2006). Resources, dynamic capabilities and performance in a dynamic envi-\nronment: Perceptions in Taiwanese IT enterprises. Information & Management ,\n43(4), 447 –454.\nXu, Z., Frankwick, G. L., & Ramirez, E. (2016). Effects of big data analytics and traditional\nmarketing analytics on new product success: A knowledge fusion perspective.\nJournal of Business Research ,69(5), 1562 –1566.Zheng, S., Zhang, W., & Du, J. (2011). Knowledge-based dynamic capabilities and innova-\ntion in networked environments. Journal of Knowledge Management ,15(6),\n1035 –1051.\nZhou, K. Z., & Wu, F. (2010). Technological capability, strategic ﬂexibility, and product in-\nnovation. Strategic Management Journal ,31(5), 547 –561.\nZhu, K., & Kraemer, K. (2005). Post-adoption variations in usage and value of e-business\nby organizations: Cross-country evidence from the retail industry. Information\nSystems Research ,16(1), 61 –84.\nZollo, M., & Winter, S. G. (2002). Deliberate learning and the evolution of dynamic\ncapabilities. Organization Science ,13(3), 339 –351.\nZott, C. (2003). Dynamic capabilities and the emergence of intraindustry differential ﬁrm\nperformance: Insights from a simulation study. Strategic Management Journal ,24(2),\n97–125.390 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390",
      "metadata": {
        "filename": "Assessing business value of Big Data 2017.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\Assessing business value of Big Data 2017.pdf",
        "file_size": 620244,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:35.562371",
        "content_length": 76355
      }
    },
    "fc9e80fa-1776-4165-b47f-a112395ab0c0": {
      "id": "fc9e80fa-1776-4165-b47f-a112395ab0c0",
      "content": "[Página 1]\nAnnals of Operations Research (2023) 328:1073–1103\nhttps://doi.org/10.1007/s10479-022-04955-2\nORIGINAL RESEARCH\nBig data analytics and the effects of government restrictions\nand prohibitions in the COVID-19 pandemic on emergency\ndepartment sustainable operations\nGörkem Sariyer1·Mustafa Gokalp Ataman2·Sachin Kumar Mangla3·\nYigit Kazancoglu4·Manoj Dora5\nAccepted: 29 August 2022 / Published online: 15 September 2022\n© The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022\nAbstract\nGrounded in dynamic capabilities, this study mainly aims to model emergency departments’(EDs) sustainable operations in the current situation caused by the COVID-19 pandemic byusing emerging big data analytics (BDA) technologies. Since government may impose somerestrictions and prohibitions in coping with emergencies to protect the functioning of EDs,it also aims to investigate how such policies affect ED operations. The proposed model isdesigned by collecting big data from multiple sources and implementing BDA to transformit into action for providing efﬁcient responses to emergencies. The model is validated inmodeling the daily number of patients, the average daily length of stay (LOS), and dailynumbers of laboratory tests and radiologic imaging tests ordered. It is applied in a case studyrepresenting a large-scale ED. The data set covers a seven-month period which collectivelymeans the periods before COVID-19 and during COVID-19, and includes data from 238,152patients. Comparing statistics on daily patient volumes, average LOS, and resource usage,both before and during the COVID-19 pandemic, we found that patient characteristics anddemographics changed in COVID-19. While 18.92% and 27.22% of the patients requiredlaboratory and radiologic imaging tests before-COVID-19 study period, these percentageswere increased to 31.52% and 39.46% during-COVID-19 study period. By analyzing theeffects of policy-based variables in the model, we concluded that policies might cause sharpdecreases in patient volumes. While the total number of patients arriving before-COVID-19was 158,347, it decreased to 79,805 during-COVID-19. On the other hand, while the averagedaily LOS was 117.53 min before-COVID-19, this value was calculated to be 165,03 min\nB Yigit Kazancoglu\nyigit.kazancoglu@yasar.edu.tr\n1Yasar University, Department of Business Administration, ˙Izmir, Turkey\n2Bakırçay University Çi˘ gli Region Training and Research Hospital, Department of Emergency\nMedicine, ˙Izmir, Turkey\n3Digital Circular Economy for Sustainbale Development Goals (DCE-SDG), Jindal Global Business\nSchool, O P Jindal Global University, Haryana, India\n4Yasar University, Department of Logistics Management, ˙Izmir, Turkey\n5Sustainable Production and Consumption School of Management Anglia Ruskin University, Cambridge,\nUK\n123\n\n[Página 2]\n1074 Annals of Operations Research (2023) 328:1073–1103\nduring-COVID-19 study period. We ﬁnally showed that the model had a prediction accuracy\nof between 80 to 95%. While proposing an efﬁcient model for sustainable operations manage-ment in EDs for dynamically changing environments caused by emergencies, it empiricallyinvestigates the impact of different policies on ED operations.\nKeywords Big data analytics ·Emergency department ·COVID-19 ·Machine learning ·\nSustainable operations\n1 Introduction\nMedical scientists and sociologists have widely researched the effects of the COVID-19pandemic on human physical and psychological health. Its impacts on operations and supplychain management have gained signiﬁcant attention from scholars (Choi, 2021 ; Queiroz et al.,\n2020 ;S a r k i s , 2021 ) and industry experts (Deloitte, 2020 ; Harvard Business Review, 2020 ).\nHowever, although the COVID-19 pandemic has affected operations and supply chains ona large scale and most the companies have faced disruptions (Fortune, 2020 ) since it has\nalso created emergency situations in many countries, its impact on health services is a highpriority and needs to be addressed.\nEfﬁcient and timely service delivery is a signiﬁcant burden for health services, and the\nimportance of providing rapid responses increases in emergencies. However, as experiencedduring the COVID-19 pandemic, this is very challenging, particularly for EDs, which areincreasingly used as gateways to hospital admissions and have been identiﬁed as one ofthe most overcrowded health services units. Besides, since most countries provide a 7/24ED service, non-urgent patients frequently occupy them, which has also been identiﬁed asan essential issue leading to increased overcrowding (Ataman & Sariyer, 2021 ). While the\nproblem of overcrowding in EDs is a major challenge for the service providers even in regulartimes (Sariyer & Ataman, 2020 ), pandemic environments push these services into bottlenecks\nsince the number of patients being infected increases uncontrollably. In addition to this sharpincrease in patient volumes, the proﬁles and demographics of patient admissions to hospitalEDs also vary signiﬁcantly. Under these circumstances, to protect the functioning of healthservices and EDs, governments are forced to impose widespread restrictions and prohibitions.To cope with the COVID-19 pandemic, the leaders of many countries declared sudden orphased lockdowns and quarantines and the closure of physical shops and businesses, transportbans, etc. Although these may help the functioning of EDs under emergencies and cause asudden decrease in patient volumes, it is crucial for ED service providers to rapidly adapt thesystem in response to such changes and be able to manage operations efﬁciently in highlydynamic conditions (Alinaghian & Goli, 2017 ; Hossain et al., 2021 ; Mondal & Roy, 2021 ;\nThakur et al., 2021 ). Thus, not only but especially under emergencies, EDs must have strong\ndynamic capabilities to manage these uncertain and dynamically changing environments.\nThese huge patient volumes and the extensive range of patient characteristics also create\nlarge volumes of data for EDs. Thus, these health services are additionally challenged bya ubiquitous context of big data, which has appeared as an exciting frontier of productivityand opportunity (Sanders & Ganeshan, 2018 ). In this era, data is also identiﬁed as a valuable\nasset of EDs, enabling insights and decision making (Feng & Shanthikumar, 2018 ). However,\nbig data requires the ability to process and arrange it to be used in decision-making. Thus,although the collected data is precious for EDs, unless they can analyze it and transform itinto useful information that can be turned into rapid action, it cannot go beyond useless data\n123\n\n[Página 3]\nAnnals of Operations Research (2023) 328:1073–1103 1075\nrecording that simply takes up storage capacity. At this point, BDA becomes increasingly\ncrucial for EDs in making efﬁcient and timely decisions in emergency situations.\nThe term ’BDA’ is used to refer to the techniques, technologies, systems, practices,\nmethodologies, and applications for analyzing big data sets and is deﬁned as a holistic processof collecting, managing, and investigating the ﬁve major dimensions of data: volume, variety,velocity, veracity, and value (Wamba et al., 2017 ). BDA can support operational and strategic\ndecision-making and turn to action in value creation for all organizational levels and enhanceoperational performance. BDA technologies have been implemented for various operationsand supply chain practices based on their superior performances (Gupta et al., 2021 ;K u m a r\net al., 2016 ,2020 ; Mari´ ce ta l . , 2021 ;M i s h r ae ta l . , 2018 ). In the big data era, BDA can be\nviewed as an organizational capability for EDs to cope with dynamically changing situa-tions. Thus, besides having strong dynamic capabilities, if an ED holds BDA capabilities tomanage big data, it should respond more actively to emergencies, increasing its efﬁciencyand performance in managing operations. Moreover, big data and BDA implementations inreal-time systems will have great importance in providing sustainable ED operations (Daset al., 2021 ;G o l ie ta l . , 2019 ,2021 ; Midya et al., 2021 ; Mondal & Roy, 2022 ). Having such\ncapabilities and advantages, BDA has attracted researchers, decision, and policymakers incoping with COVID-19 as a current global emergency (Abdel-Basset et al., 2021 ; Bag et al.,\n2021 ; Huang et al., 2020 ; Kapoor et al., 2021 ; Lee & Trimi, 2021 ; Mondal & Roy, 2021 ;\nPapadopoulos et al., 2020 ; Sharma et al., 2020 ; Sözen et al., 2022 ; Tirkolaee et al., 2022 ).\nAlthough these technologies are popular in the COVID-19 context, they have little use in\nthe ED operations decision-making processes in this pandemic period. On the other hand,since EDs are the main actors of health services in managing emergency environments,taking advantage of these technologies to improve EDs’ operations is critical in effectivelymanaging emergencies. Besides, since governmental reactions in ﬁghting COVID-19 havecaused sharp and signiﬁcant changes in the demand for EDs, investigating the effects of theseactions in EDs operations and putting these effects into account in decision-making modelsis another unique point. Therefore, this study aims to present a model implementing BDAtechnologies for managing four primary ED operations in COVID-19. By conducting inter-views with ED service providers and searching the related literature, the primary operationsthat are challenging for ED services in emergencies and even in regular times are deter-mined as managing daily patient volumes, average stay lengths of patients, and utilizationof laboratory radiologic imaging services. Besides proposing a generic model for managingED operations under emergencies and validating this model for different processes of EDs,taking the governmental actions as the main factors of this model and thus showing how theyaffect these operations is the novelty of this paper. Hence, we aim to answer the followingresearch questions in this paper:\nRQ1. How does BDA assist in making effective decisions for predicting daily patient\nvolumes, average stay lengths of patients, and resource utilization of EDs under dynam-ically changing conditions caused by emergencies?RQ2. How do government-imposed restrictions and prohibitions affect daily patientvolumes, average stay lengths, and ED resource utilization of EDs in emergencies?\nSince the current emergency having worldwide effects is the COVID-19 pandemic, we\nfocus on modeling ED operations during COVID-19 and identify the restrictions and prohi-bitions imposed to cope with this pandemic. To address these research questions, we proposea BDA-driven model and implement machine learning techniques as one of the most potentsub-set of BDA. More speciﬁcally, we implement neural networks-based techniques and mul-tilayer perceptron (MLP) algorithms to develop required predictions on daily patient volumes,\n123\n\n[Página 4]\n1076 Annals of Operations Research (2023) 328:1073–1103\naverage stay lengths, and daily utilization of laboratory and imaging services of EDs. In vali-\ndating this model in different ED operations, we deﬁne the output variables for each operationas previously stated and identify two sets of factors (input variables). While in the ﬁrst set, weidentify possible operation-speciﬁc factors that may affect the output variable of this oper-ation. We deﬁne additional elements representing different types of government restrictionsand prohibitions in the second set. These factors are similarly used for each operation. Withthe proposed model and implemented MLP algorithm by obtaining 80% to 95% accuraciesfor predicting the output values of four ED operations, we answered the RQ1 of this studysince such accurate predictions play a crucial role in making efﬁcient decisions EDs underemergencies. By investigating the signiﬁcance of the relations between the output variablesand the set of input factors representing the government-imposed restrictions and prohibitionsand analyzing the directions of these relations, we answered the RQ2 of this study.\nThe organization of this paper is as follows. In Sect. 2, we discuss the theoretical back-\nground of this paper. We present the proposed model in Sect. 3and introduce the case study,\nand data set characteristics, data pre-processing steps, and results of the proposed model inSect. 4. Section 5discusses the ﬁndings of this study. We present the theoretical, managerial,\nand policy implications in Sect. 6. Section 7offers concluding remarks, limitations of this\nstudy, and the future research directions.\n2 Theoretical background\n2.1 The dynamic capabilities view\nDynamic capabilities deﬁne an organization’s ability to innovate, adapt to change, andimprove in a good way for its customers (Teece et al., 2016 ). Zollo and Winter ( 2002 ,\np. 340) deﬁned dynamic capability as a \"learned and stable pattern of collective activitythrough which the organization systematically generates and modiﬁes its operating routinesto pursue improved effectiveness.\"\nThe dynamic capabilities utilize an organization’s internal and external resources in the\nbest possible manner to respond appropriately to environmental uncertainties (Teece et al.,1997 ). Emergencies cause environmental or external uncertainties, and managing opera-\ntions in EDs, particularly under emergencies, requires real-time information whereby serviceproviders can arrive at critical decisions. The dynamic capabilities help integrate primaryresources through the availability of this information and then further help to modify ED oper-ating routines and procedures appropriately. Therefore, we based our research on the dynamiccapability view. Positioning the resources correctly is the prime requisite for coping with theseuncertainties and the chaotic environments related to emergencies. Dynamic capabilities arethe main processes for sensing, integrating, learning, and reconﬁguring resources and capa-bilities (Birkinshaw et al., 2016 ) and stress an organization’s capacity to create, extend or\nmodify its resources purposefully. These are also crucial in managing ED operations, par-ticularly in emergencies, since aligning the capabilities and resources and reconﬁguring theprocesses may help dynamically deal with changing patient volumes and proﬁles. To dealwith unexpected increases in patient volumes in COVID-19, many countries reconﬁguredtheir health systems, so pandemic services were opened to provide patients. The resourcesand capacities of these services, such as doctors, nurses, and other health staff, required med-ical equipment (medicines, beds, intensive care units, respiratory devices), were provided bymany different hospital departments and mainly from the EDs. In some countries where pan-demic services were not opened, EDs served as these services and encountered COVID-19\n123\n\n[Página 5]\nAnnals of Operations Research (2023) 328:1073–1103 1077\npatients. For such countries, the increased need for medical staff and resources was satisﬁed\nby reconﬁguring the hospital’s other services and aligning them with the pandemic services.\nIn the health services operations and supply chain management literature, many stud-\nies base their theoretical backgrounds on the dynamic capability perspective (Rubbio et al.,2020 ). In the era of big data, health systems are one of the primary services that deal with\nbig data sets of the high volume, variety, and velocity of patient data. Thus, we move furthertowards BDA capability (BDAC), which has evolved from the dynamic capability perspec-tive. We, therefore, highlight the importance of having BDAC for managing health servicesoperations, particularly in emergencies.\n2.2 Big data analytics capability\nDuring the COVID-19 pandemic, BDA has been used to detect surface indicators related tothe pandemic (Guo et al., 2020 ). Real-time big data-driven insights have helped scholars and\ndecision-makers to comprehend the impact of this pandemic. COVID-19 trackers provide anessential source of data to help scholars research and make more informed decisions on copingwith this pandemic by collecting and aggregating big data (Verma & Gustafsson, 2020 ). Such\nsituations increase the volume and the variety of patients’ characteristics in health services.Besides, many external factors may come into play, changing the system dynamics. Undersuch circumstances, it is necessary for health services providers to rapidly adapt the systemto the changing conditions to provide timely and effective services to patients. Thus, the roleof BDAC in healthcare operations gained increased attention (Yu et al., 2021 ).\nWe propose a system for managing ED operations, such as forecasting patient volumes,\nanalyzing patient LOS, and modeling the use of primary resources in emergencies. Even inregular times, the main challenge faced by ED service providers is the overcrowded environ-ment of these services, which creates vast volumes and varieties of patients. An emergencyis an external challenge that may cause an unexpected and sharp increase in patient volumesand varieties, thus straining the system and making managing operations much more difﬁcult.Government is a prominent actor as a system enabler in this era. To protect the functioningof these services and respond to emergencies, governments impose some policies, such asrestrictions and prohibitions, which may cause a sudden decrease in patient volumes but stillchange the characteristics and increase the system’s randomness. All these create dynami-cally changing environments, and the service providers must adopt the system appropriatelyand effectively in response to these rapidly changing conditions. Since by their nature anddue to all these sudden changes, ED services include a huge volume, variety, velocity, andveracity of data, these services may take advantage of BDA to help operations cope withsuch rapid changes in the system. We summarise the theoretical framework of our researchin Fig. 1.\nAs seen in Fig. 1, based on huge volumes, velocities, and varieties of patients, the\ndata inherent in the EDs exhibits a dynamic feature. Since emergencies are also featuredwith rapidly changing conditions, these increase the randomness in the EDs and, therefore,stalemate decision-making processes in EDs. This study attempts to contribute to dynamiccapability theory and BDAC by extending their usage for the decision-making processes ofone of the most important actors of health services, EDs, under emergencies. By presentingthe rapidly changing features of the EDs in emergencies and presenting a model highlightinga need for BDAC, this study aims to contribute to the context of these theories.\n123\n\n[Página 6]\n1078 Annals of Operations Research (2023) 328:1073–1103\nFig. 1 Theoretical framework of this research\n3 Proposed models\nIn this paper, we propose models for managing the primary operations of EDs, particu-\nlarly in emergencies. These models include ﬁve main sequential steps: Data Collection,Pre-processing, Modelling, Testing & Model Evaluation, and Providing Managerial & Pol-icy Implications. As discussed earlier, ED environments contain big data sets that can beprocessed with BDA, and valuable information can be obtained in decision-making. Thus,an essential initial step for adapting these emerging technologies into proposed models andsystems is bringing data sets related to the context. A data set can be obtained using differentsources within this research framework. To get the related data of the proposed models, werequired data triangulation. Valuable data sets for the proposed models are secondary datareceived from a case ED covering the period before and during COVID-19; governmentreports; documentary analysis; and interviews with ED service providers. Case study datamay include relevant information about patients arriving at this ED during the study period.Government reports and documentary analyses should be checked to identify the types ofrestrictions and prohibitions imposed by the government to cope with the emergency. Finally,interviews and documents should be used to decide on the main challenges to ED operations,making planning and managing operations more difﬁcult in emergencies. Related metrics andtargeted values of these metrics can also be identiﬁed by collecting data through interviewsand a literature search.\nSince the collected data is raw data, which in its current form is not suitable for analyz-\ning and modeling, different data pre-processing tasks must be performed. It is necessary todeﬁne the input and output variables of the model, deﬁne the periodicity (hourly, daily, weekly,monthly, etc.) of the analysis, and determine ways to measure the values of the variables. Datatransformation may also involve measuring the values of the variables. One of the main pre-processing tasks in big data studies is cleaning the data set to remove redundant or inappropri-ate data, missing values, and outliers. After all these tasks have been performed, the structureddata set, which can further be processed with BDA tools and techniques, is obtained.\nOnce the structured data set of the model is ready, the modeling step comes next. The\nobtained data set is split into two train and test sets. Train data sets include the values ofall the input and output variables, whereas since the test data set will be used to evaluatethe model’s prediction accuracies, it does not include the values of the output variables. The\n123\n\n[Página 7]\nAnnals of Operations Research (2023) 328:1073–1103 1079\ntrain data set is further processed with machine learning as one of the most widely used BDA\ntechniques. Machine learning presents algorithms to extract knowledge and make efﬁcientdecisions by learning from given data sets. Researchers widely prefer these algorithms basedon their ﬂexibility in using data to capture complex and non-linear behaviors (Choi et al.,2018 ). Among various machine learning algorithms, MLP neural networks have received\nsigniﬁcant attention since these are appropriate and efﬁcient for function approximation,pattern classiﬁcation, and prediction. Incorporating hidden layers between input and outputlayers is one of the other parser properties of these algorithms. When required by extendingthe number of hidden layers, MLP neural networks can expand the number of input featurecombinations to improve the model’s learning ability, ﬁnally increasing the prediction power.Although many other BDA techniques have been widely implemented in the literature, themachine learning-based MLP neural network algorithm is integrated into the proposed modelbased on these properties and superiorities.\nThe testing and model evaluation step comes next in the proposed model. The obtained\nMLP algorithm with the optimized parameters is applied to the test data set to get the predictedvalues of the output variables of interest. The predicted values are then compared with theactual values, and the mean errors and accuracies of the prediction should be calculated. Theseperformances should then be compared with the target values. If the targets are achieved orthe model performance goes beyond the targeted one, the model can be proposed for real-life applications. The results on the signiﬁcance and impacts of government restrictions andprohibitions may also be discussed in detail, and implications should be recommended topolicymakers. Suppose the model performance cannot achieve the targets. In that case, it isnecessary to go back to the data pre-processing step and re-deﬁne the model input and outputvariables. The modeling, testing, and evaluation steps must be repeated until proper modelshave been obtained. The proposed model is shown in Fig. 2.\nFig. 2 Flowchart of the proposed model\n123\n\n[Página 8]\n1080 Annals of Operations Research (2023) 328:1073–1103\n4 Case study\n4.1 Case study specification\nWe collected the data set of this study from an ED of a research and training hospital located\nin a metropolitan region in Izmir, Turkey. The daily number of patients or visits to this EDis more than 1,000. This huge patient volume is due to several reasons. First, as mentionedpreviously, overcrowding is a common problem in EDs. Second, due to the vast volumes ofnon-urgent patient visits, this problem can be more severe in some countries, such as Turkey,compared to many other countries. Third, many patients may choose to be treated in thishospital due to its type. Fourth, since this is a public hospital, receiving service from EDs isfree of charge. Fifth, since it is located in a metropolitan region and is very close to publictransport stations and the city center, it is also easily accessible for ambulances. Sixth butnot least, since this ED provides uninterrupted service (7 days and 24 h) while many of theother departments of this hospital provide service only within working hours on weekdays,this causes additional visits of patients of different departments to EDs out of the workinghours. These characteristics created huge volumes, velocities, and varieties in the data set.\nIn Turkey, the ﬁrst COVID-19 case was reported on March 10, 2020, in Istanbul city, and\nthe virus then spread quickly to the whole country. In Turkey, the COVID-19 was encounteredlater than in many other countries. Thus, public awareness had already been created aboutthis virus and the pandemic. Public awareness was a crucial initial step in coping with thisvirus. Since it ﬁrst appeared in Turkey, the government started announcing policies like\"social distancing,\" \"hygiene,\" and \"stay at home.\" However, raising public awareness fromthe outset and making announcements was not enough to prevent the spread of the virus.Then, the government imposed other types of restrictions and prohibitions. Restrictions forthe elderly, inter-city transport bans and restrictions for the young were imposed startingfrom the end of March. In addition, starting from the middle of April, total curfews wereimposed at weekends (for two days) and for extended weekends in some of the weeks, whichcould last up to three or four days. The number of cases and deaths started to fall by May.Then the period of normalization began at the beginning of June. Although restrictions andprohibitions were still in use during this month, they were more relaxed.\nHaving high volumes, velocities, and varieties in patient sizes and characteristics, the\nselected ED was identiﬁed as proper for this study’s theoretical framework and methodology.Besides, since in different periods (such as before March and during April) and days (suchas weekdays and weekends), government-imposed actions were highly changing during thestudy period, the case ED allowed to investigate the impact of these actions on ED operations.\n4.2 Data set characteristics\nThe data set covers seven months, from December 2019 to June 2020, and includes 238,152patients. Data from between March 10 to the end of June 2020 represents data collected duringthe period of COVID-19’s ﬁrst peak in Turkey. To have a similar number of days before theCOVID-19 period, the related data set was started in December 2019. Thus, before COVID-19 and during COVID-19 periods cover around 3.5 months of data. For each arriving patient,records of the ED case include the following information: patient ID, gender, age, arrivaltype, triage level, date of arrival, time of arrival, diagnostic tests for treatment-if required,related times for diagnostic tests, assigned diagnosis type by a doctor after treatment, andtime of departure. The patient ID is unique for each patient arrival. Gender is recorded as\n123\n\n[Página 9]\nAnnals of Operations Research (2023) 328:1073–1103 1081\nmale and female. Age is recorded as it is in a continuous form. The arrival type represents\nif a patient arrived by themselves or by ambulance, so it is recorded as one of two options:\"walk-in\" or \"by ambulance.\" When a patient comes to this ED, they are ﬁrst met by a triagenurse, who triages the patient based on his complaints and clinical acuities. This ED uses the3-level Emergency Severity Index for patient triage.\nFurthermore, trauma patients are treated in a different zone. Thus, arriving patients are\nassigned to one of four zones labeled green, yellow, red, and trauma zones. The arrivaldate represents the full date of the patient’s arrival in a day, month, and year form. Time ofarrival shows the exact time of arrival in an hour, minute, and second form. Many diagnostictests can be ordered in EDs for patient diagnosis. The label of the requested test, and therelated ordering time, approval time, and result time are recorded in the next three rows inan hour, minute, and second form. When doctors diagnose the patients, they assign the typeof diagnosis based on the International Classiﬁcation of Diagnosis 10th version (ICD-10).Thus, the diagnosis cell includes the diagnosis based on the ICD-10 codes, which can have22 different categories. The last cell consists of the departure time of the patient in an hour,minute, and second form.\nThe data set includes additional attributes to represent government restrictions and prohi-\nbitions. The four main restrictions and prohibitions imposed in Izmir city are considered inthe proposed models. During the COVID-19 study period, total curfew (lockdowns), curfewfor the young (age ≤20), curfew for the elderly (age ≥65), and transport bans were imposed.\nThese are also adopted in the proposed models as model input variables, as discussed in thenext section on data pre-processing.\nAs presented in Fig. 2, selecting the study variables is an important initial step of the\nproposed model. However, it should be kept in mind that these variables are not ﬁxed andrigid and may depend on the selected case studies. Different variables may deﬁne the system’sinternal and external dynamics for other cases.\n4.3 Data pre-processing\nWe implement the proposed model with four different ED operations to investigate how theimposed policies have changed and affected the primary operations and resource usage. Theﬁrst and second operations, Operation 1 and Operation 2, respectively predict the daily num-ber of patients arriving and the average LOS of these patients (LOS is deﬁned as the timebetween the patient’s arrival and their departure) for each day during-COVID-19 period.Different diagnostic tests can be mainly grouped into either laboratory tests or radiologicimaging tests. Thus, we also implement the model for two other operations to analyze theprimary resource usage. Operation 3 and Operation 4 predict daily numbers of ordered labo-ratory tests and radiologic imaging tests for diagnosing patients. Regarding output variablesor attributes of the model for each operation, these are deﬁned adequately as the daily numberof patients, average daily LOS of patients, the daily number of laboratory tests ordered, andthe daily number of radiologic imaging tests ordered during-COVID-19 period.\nSince the aim is to model and manage related daily values, the data set was initially trans-\nformed. In this process, we eliminated the repetitive values from the data set. More than oneICD-10 encoded diagnosis can be assigned to a patient. Different laboratory tests (hemogram,biochemistry, enzyme, hormone, etc.) or radiologic imaging tests (X-ray, tomography, ultra-sound, magnetic resonance imaging, etc.) can also be ordered for a patient with a unique ID.While obtaining the corresponding daily value of the models, we eliminated these repetitiveor redundant values.\n123\n\n[Página 10]\n1082 Annals of Operations Research (2023) 328:1073–1103\nBesides the policy-based attributes, some other input variables were also deﬁned to adopt\nthe system characteristics in the proposed models. These variables were used to represent thesystem dynamics in normal circumstances. Previous studies showed that the day of the weekhas a signiﬁcant effect on patient volume and LOS (Sarıyer et al., 2020 ). Existing literature\nalso presented that the patient volume, LOS, and numbers of diagnostic tests ordered differedsigniﬁcantly between categories of demographic variables (Sarıyer & Ataman, 2020 ). We,\ntherefore, identiﬁed these factors as internal factors to represent the ED environment in normalcircumstances. To measure the values of these inputs, we used the study’s data set coveringthe before-COVID-19 period. As in output variables, we made the required transformationsto obtain the daily values of these input variables. The data set is described in Table 1.\nWe performed data pre-processing by dropping missing values in the dataset by using the\ndropna() function of the pandas module in Python. After this, based on standardization, we\nremoved the outliers from the data set by using the zscore() function of the pandas module\nTable 1 Deﬁnitions and measurement scales of the model variables\nOperation Deﬁned output variables\n(symbol, deﬁnition, scale)Operation-speciﬁc input\nvariables representingsystem dynamics(symbol, deﬁnition,scale)Common input vari-\nables(symbol, deﬁnition,scale)\n1: Managing daily\nnumbers of patientY1: The daily number of\npatients arriving eachd a yi nt h eduring-COVID-19study period(numerical)X1: The average daily\nnumber of patientsarriving for each day ofthe week—Mondaythrough to Sunday(numerical)Representing govern-\nment restrictions andprohibitions\nX2: The whole curfew\nexists in the day to bepredicted or not (cat-egorical)\nX3: Curfew for young\nexists in the day to bepredicted or not (cat-egorical)\nX4: Curfew for the\nelderly exists in theday to be predictedor not (categorical)\nX5: Transport ban\nexists in the day tobe predicted or not(binary)\n2: Managing daily\naverage LOS ofpatientsY2: Average daily LOS\nof patients arriving eachd a yi nt h eduring-COVID-19study period(numerical)X7-X8: average daily\nLOS of female-malepatients for each day ofthe week (numerical)\nX9-X10-X11: Average\ndaily LOS of agegroups—[0–14],[15–64], ≥65—for\neach day of the week(numerical)\n123\n\n[Página 11]\nAnnals of Operations Research (2023) 328:1073–1103 1083\nTable 1 (continued)\nOperation Deﬁned output variables\n(symbol, deﬁnition, scale)Operation-speciﬁc input\nvariables representingsystem dynamics(symbol, deﬁnition,scale)Common input vari-\nables(symbol, deﬁnition,scale)\nX12 through X15:\nAverage daily LOS oftriage groups—red,yellow, green, traumazones—for each day of\nthe week (numerical)\nX16 through X37:\nAverage daily LOS ofICD-10 encodeddiagnosis, for 21\ngroups\n*, for each day\nof the week(numerical)\n3: Managing daily\nnumbers of ordered\nlaboratory testsY3: The daily number of\nlaboratory tests ordered\nin the\nduring-COVID-19study period(numerical)X38-X39: Average daily\nnumbers of laboratory\ntests ordered for\nfemale-male patientsfor each day of theweek (numerical)\nX40-X41-X42: Average\ndaily numbers of\nlaboratory testsordered for agegroups—[0–14],[15–64], ≥65—for\neach day of the week(numerical)\nX43-X44: Average daily\nnumbers of laboratorytests ordered for arrivaltype groups—byambulance orwalk-in—for each dayof the week(numerical)\nX45 through X48:\nAverage daily numbersof laboratory testsordered for triagegroups; red, yellow,green, trauma zones,for each day of theweek (numerical)\n123\n\n[Página 12]\n1084 Annals of Operations Research (2023) 328:1073–1103\nTable 1 (continued)\nOperation Deﬁned output variables\n(symbol, deﬁnition, scale)Operation-speciﬁc input\nvariables representingsystem dynamics(symbol, deﬁnition,scale)Common input vari-\nables(symbol, deﬁnition,scale)\nX49 through X69:\nAverage daily numbersof laboratory testsordered for ICD-10encoded diagnosis, for\n21 groups\n*, for each\nday of the week(numerical)Representing system\ndynamics\nX1-fcast: Predicted\ndaily number ofpatients with Model\n1 on each day\nduring-COVID-19study period(numerical) –used in2\nnd,3rd,a n d4th\noperations modeling\n4: Managing daily\nnumbers of orderedradiologic imagingtestsY4: The daily number of\nradiologic imaging testsordered in theduring-COVID-19study period(numerical)X70-X71: Average daily\nnumbers of radiologicimaging tests orderedfor female-malepatients for each day ofthe week (numerical)\nX72-X73-X74: Average\ndaily numbers ofradiologic imagingtests ordered for agegroups—[0–14],[15–64], ≥65—for\neach day of the week(numerical)\nX75-X76: Average daily\nnumbers of radiologicimaging tests orderedfor arrival typegroups—by ambulanceor walk-in—for eachday of the week(numerical)\nX77 through X80:\nAverage daily numbersof radiologic imagingtests ordered for triagegroups—red, yellow,green, traumazones—for each day ofthe week (numerical)\nX81 through X101:\nAverage daily numbersof radiologic imagingtests ordered forICD-10 encoded\ndiagnosis, for 21\ngroups\n*, for each day\nof the week(numerical)\n123\n\n[Página 13]\nAnnals of Operations Research (2023) 328:1073–1103 1085\nin Python. We initiated the categorical conversion of the input variables with the Categorical\nclass initializer of the pandas module in Python. We used the Categorical class to encode\nnumerical values as categorized by the capability of initializing the corresponding variableswith categorical values. After these pre-processing steps, we obtained the structured data setfor further modeling with the MLP neural network.\nAs seen in Table 1, we identiﬁed the government policies as common input variables in each\noperation to analyze their effects on each of the deﬁned output variables for the correspondingoperations. However, once we predicted the daily number of patients in Operation 1, we usedthese predictions to describe system characteristics in all other models. The daily number ofpatients may affect the average daily LOS, and the number of each diagnostic test ordered.\n5 Results\n5.1 Descriptive results\nThe study period covering the before-COVID-19 period included 100 days of data, andthe total number of patients arriving during these days was 158,347. Laboratory tests wereordered for 29,953 of these patients and 43,106 radiologic imaging tests. On the other hand,the study period covering the during-COVID-19 period included 113 days of data, and thetotal number of patients arriving during these days was 79,805. The number of laboratoryand radiologic imaging tests ordered during this period was 25,154 and 31,488. The averagedaily LOS was 117.53 min in the before-COVID-19 period and 165,03 min in the during-COVID-19 period. Daily values for the number of patients, average LOS, and numbers ofeach type of diagnostic test ordered in the whole study period are depicted in Fig. 3.\nThese results show that while daily and total numbers of patients and diagnostic tests\nordered sharply decreased, average LOS values increased during the during-COVID-19period compared to before-COVID-19. However, although decreases are seen in three ofthe operations’ output variables (1, 3, 4), the sharpest decline was seen in Operation 1’s out-put, the daily number of patients. The decrease in patient numbers may have also caused thedecline in the number of tests ordered. On the other hand, it should be noted that, althoughpatient and diagnostic test numbers decreased, average LOS values increased. All these criti-cal numerical ﬁndings could be due to the change in the system dynamics, which were mainlycaused by patients who occupied EDs unnecessarily and did not need an emergency service.\nWe categorized the patients into three groups to support this idea by numerical ﬁndings\nconsistent with our model boundaries and comparatively presented the related statistics for\nFig. 3 Daily values of the models’ output variables in the study period\n123\n\n[Página 14]\n1086 Annals of Operations Research (2023) 328:1073–1103\neach of these. These categories were: patients requiring no diagnostic tests, laboratory tests,\nand radiologic imaging tests. Since diagnostic tests are one of the most critical resources fordiagnosing patients, we believe most patients for whom no tests are ordered can representthe cases that occupy EDs for non-urgent conditions.\nFor these categories, the average daily numbers of patients and their average LOS are\nshown for each day of the week before-COVID-19 and during-COVID-19 periods in Fig. 4.\nFigure 4shows that while average daily values for patient numbers decreased in each of the\nthree categories in the during-COVID-19 period compared to the before-COVID-19 period,the majority of the decrease is related to the category of patients requiring no diagnostic test.Although it is worth noting that reductions were seen in the number of patients requiring nodiagnostic test, some increases were seen in their average LOS values in the during-COVID-19 period. This ﬁnding mainly supports our hypothesis. On the other hand, at least some\nFig. 4 Daily average patient numbers and LOS values for each day of the week\n123\n\n[Página 15]\nAnnals of Operations Research (2023) 328:1073–1103 1087\ndecreased levels were observed in the average LOS values of patients requiring diagnostic\ntests during the pandemic period. This could be due to the decreases in resource utilization.When resource utilization decreases, it accelerates access to resources and enables moreefﬁcient use. Based on the daily distributions of patient numbers, one other ﬁnding should benoted. In the patients requiring no diagnostic test category, while Saturdays and Sundays, thatis, the weekend, had the highest daily patient numbers compared to weekdays in the before-COVID-19 period, daily numbers were the highest on Mondays in the during-COVID-19period. The impact of government restrictions and prohibitions on ED operations is directlyseen in this ﬁnding. Since most of the weekends, total curfews were imposed during thisperiod, patient volume, particularly in the patients requiring no diagnostic test category,sharply decreased at weekends.\nTable 2shows the total number of patients arriving at this ED based on the categories of\nthe considered demographics (gender, age, triage, arrival types, diagnosis) for the before-and during-COVID-19 study periods comparatively.\nFrom the values of Table 2, it should be seen that the distribution of patient numbers\nbased on gender changed in the during-COVID-19 period compared to the before-COVID-19period, as the number of male patients increased. Differences were also depicted based on agedistributions. For each of the three categories, in the young group, age:[0–14], patient numbersand distributions sharply decreased in the during-COVID-19 period, and in the elderly group,age≥65. In contrast, distributions fell in the patients requiring diagnostic tests category\noverall. There was some increase in this age category. Additionally, for all three types,the distribution of patients arriving by ambulance increased in the during-COVID-19 studyperiod. Another important ﬁnding showed that, while distributions of green zone patientssigniﬁcantly decreased in the patients requiring no diagnostic test category, the distributionof green zone patients increased in some other categories. Finally, signiﬁcant differenceswere observed between 22 different ICD-10 encoded diagnosis types on the distributions ofthe four main groups. These ICD-10 codes were J00-J99 (disease of the respiratory system),M00-M99 (disease of musculoskeletal system and connective tissue), R00-R99 (symptoms,signs, and abnormal clinical and laboratory ﬁndings, not elsewhere classiﬁed), and U00-U85(codes for special purposes, COVID-19 here). The signiﬁcant differences in the distributionsof these diagnosis types are associated with the COVID-19 pandemic and the season.\n5.2 Model results\nThe proposed model was implemented in the obtained data sets of the corresponding casestudy. Since we focus on four primary ED operations, the model was tested repetitively fourtimes for Operations 1 through 4, which increased the model’s validity.\nIn this section, the relation between the identiﬁed input variables and the corresponding\noutput variables for each ED operation of interest will be presented based on the results ofthe Pearson correlation analysis. The statistical association between the model variables ispresented in a heat-map structure in the Appendix for each operation. In Table 3,w es h o w e d\nthe direction, magnitude, and signiﬁcance level of the relationships, notably the signiﬁcantinput variables of the model for each operation.\nFrom the values of Table 3, it is observed that the deﬁned input variables of Operation\n1, X1 through X5, were all signiﬁcantly related to the output variable Y1. Besides, therelations were in a negative direction. This demonstrates how policy-based restrictions andprohibitions reduce the predicted number of daily patients in the during-COVID-19 period.Nonetheless, while it is observed that the system dynamics related to input variable X1 had a\n123\n\n[Página 16]\n1088 Annals of Operations Research (2023) 328:1073–1103\nTable 2 Distributions of each patient demographic variable for three categories in the before- and during-\nCOVID-19 periods\nVariable Levels Patients requiring no\ndiagnostic testPatients requiring\nlaboratory testsPatients requiring\nradiology tests\nBefore During Before During Before During\nn (%) n (%) n (%) n (%) n (%) n (%)\nGender Female 47,670\n(47.742)14,784\n(42.444)16,636\n(55.540)12,407\n(49.324)21,894\n(51.177)14,899\n(47.316)\nMale 52,179\n(52.258)20,048\n(57.556)13,317\n(44.460)12,747\n(50.676)20,887\n(48.823)16,589\n(52.684)\nAge age: [0–14] 20,722\n(20.753)3,683\n(10.574)4,726\n(15.778)1,715\n(6.818)7,991\n(18.679)2,951\n(9.372)\nage:\n(15–64)70,980\n(71.087)27,538\n(79.059)17,730\n(59.193)18,310\n(72.792)26,814\n(62.677)23,309\n(74.025)\nage≥65 8,147\n(8.159)3,611\n(10.367)7,497\n(25.029)5,129\n(20.390)7,976\n(18.644)5,228\n(16.603)\nTriage\nlevelgreen room 68,335\n(68.438)12,122\n(34.801)2,624\n(8.760)6,490\n(25.801)7,746\n(18.106)7,279\n(23.117)\nyellow\nroom23,212\n(23.247)14,888\n(42.742)20,542\n(68.581)12,037\n(47.853)21,406\n(50.036)12,280\n(38.999)\nred room 2,313\n(2.316)2,076\n(5.960)5,904\n(19.711)5,737\n(22.808)4,833\n(11.297)4,950\n(15.720)\ntrauma\nroom5,989\n(5.998)4,904\n(14.079)883\n(2.948)890\n(3.538)8,796\n(20.561)6,979\n(22.164)\nArrival\ntypewalk in 98,553\n(98.702)33,148\n(95.165)24,508\n(81.822)19,224\n(76.425)37,374\n(87.361)25,642\n(81.434)\nby ambu-\nlance1,296\n(1.298)1,684\n(4.835)5,445\n(18.178)5,930\n(23.575)5,407\n(12.639)5,846\n(18.566)\nICD-10\nencodeddiagno-sisA00-B99 3,095\n(3.100)755\n(2.168)241\n(0.805)193\n(0.767)156\n(0.365)96 (0.305)\nC00-D49 32\n(0.032)24\n(0.069)49\n(0.164)31\n(0.123)43\n(0.101)24 (0.076)\nD50-D89 135\n(0.135)139\n(0.399)75\n(0.250)88\n(0.350)37\n(0.086)51 (0.162)\nE00-E89 108\n(0.108)122\n(0.350)131\n(0.437)117\n(0.465)74\n(0.173)81 (0.257)\nF01-F99 696\n(0.697)515\n(1.479)223\n(0.744)183\n(0.728)132\n(0.309)\n124\n(0.394)\nG00-G99 1,211\n(1.213)540\n(1.550)335\n(1.118)221\n(0.879)415\n(0.970)277\n(0.880)\nH00-H59 646\n(0.647)453\n(1.301)10\n(0.033)7 (0.028) 12\n(0.028)6 (0.019)\n123\n\n[Página 17]\nAnnals of Operations Research (2023) 328:1073–1103 1089\nTable 2 (continued)\nVariable Levels Patients requiring no\ndiagnostic testPatients requiring\nlaboratory testsPatients requiring\nradiology tests\nBefore During Before During Before During\nn (%) n (%) n (%) n (%) n (%) n (%)\nH60-H95 1,541\n(1.543)576\n(1.654)61\n(0.204)36\n(0.143)63\n(0.147)46 (0.146)\nI00-I99 1,113\n(1.115)730\n(2.096)1,192\n(3.980)857\n(3.407)959\n(2.242)715\n(2.271)\nJ00-J99 36,073\n(36.128)5,368\n(15.411)3,174\n(10.597)5,223\n(20.764)4,427\n(10.348)4,913\n(15.603)\nK00-K95 3,925\n(3.931)1,789\n(5.136)1,580\n(5.275)935\n(3.717)1,184\n(2.768)753\n(2.391)\nL00-L99 1,384\n(1.386)1,154\n(3.313)69\n(0.230)67\n(0.266)38\n(0.089)47 (0.149)\nM00-M99 13,190\n(13.210)7,625\n(21.891)2,459\n(8.210)1,933\n(7.685)14,039\n(32.816)8,924\n(28.341)\nN00-N99 2,050\n(2.053)1,206\n(3.462)2,434\n(8.126)1,562\n(6.210)1,673\n(3.911)1,195\n(3.795)\nO00-O9A 28\n(0.028)25\n(0.072)17\n(0.057)18\n(0.072)54\n(0.126)28 (0.089)\nP00-P96 49\n(0.049)51\n(0.146)50\n(0.167)39\n(0.155)5 (0.012) 4 (0.013)\nQ00-Q99 3 (0.003) 5 (0.014) 4 (0.013) 5 (0.020) 5 (0.012) 6 (0.019)\nR00-R99 11,797\n(11.815)3,544\n(10.175)13,110\n(43.769)7,599\n(30.210)12,321\n(28.800)6,957\n(22.094)\nS00-T88 2,556\n(2.560)1,790\n(5.139)193\n(0.644)179\n(0.712)632\n(1.477)537\n(1.705)\nU00-U85 0 (0.000) 644\n(1.849)0 (0.000) 2,106\n(8.372)0 (0.000) 1,971\n(6.260)\nV00-Y99 1,448\n(1.450)1,286\n(3.692)517\n(1.726)426\n(1.694)1,509\n(3.527)801\n(2.544)\nZ00-Z99 18,769\n(18.797)6,491\n(18.635)4,029\n(13.451)3,329\n(13.234)5,003\n(11.694)3,932\n(12.487)\nsigniﬁcant relation with the model output variable, the relations of the policy-based variables,\nparticularly X5, X2, and X3, were more substantial. However, for Operation 2, we observedthat most of the selected input variables were not signiﬁcantly related to Y2. We observedthat only X1-fcast and X5 were related considerably to Y2. As also seen in Table 3,m o s to f\nthe selected input variables of the model were signiﬁcant while modeling Operations 3 and4. We also observed that some of the selected policy-based variables had signiﬁcant negativerelations with Y3 and Y4. This result demonstrated that such policies caused substantialdecreases in resource usage of EDs during-COVID-19 period.\nAfter analyzing the effects of the identiﬁed input variables on the operations, we further\nprocessed the obtained data sets using the MLP neural networks. MLPRegressor in the neuralnetwork package of the sklearn module in Python was initialized to process the data sets of the\nmodels. The solver function of the algorithm chosen was adam() and the activation function\n123\n\n[Página 18]\n1090 Annals of Operations Research (2023) 328:1073–1103\nTable 3 Correlation results for signiﬁcant input parameters of the model for each of the operations\nModeling daily patient\nnumbers: Operation 1Modeling average\ndaily LOS: Operation2Modeling daily numbers\nof ordered laboratorytests: Operation 3Modeling daily\nnumbers of orderedradiologic imagingtests: Operation 4\nrY1−X1=-0.25**\nrY1−X2=-0.40**\nrY1−X3=-0.43**\nrY1−X4=-0.22*\nrY1−X5=-0.76**rY2−X1−fc a s t =\n0.18*\nrY2−X5=− 0.29**rY3−X1−fc a s t =\n0.39**\nrY3−X2=-0.36**\nrY3−X38=0.24**\nrY3−X39=0.33**\nrY3−X40=0.19*\nrY3−X41=0.27**\nrY3−X43=0.29**\nrY3−X46=0.33**\nrY3−X49=0.28**\nrY3−X51=0.39**\nrY3−X53=0.19*\nrY3−X55=− 0.28**\nrY3−X58=0.37**\nrY3−X59=0.22\nrY3−X62=− 0.24**\nrY3−X63=− 0.28**\nrY3−X64=− 0.38**\nrY3−X66=0.27**\nrY3−X67=0.39**rY4−X1−fc a s t =\n0.87**\nrY1−X2=-0.42**\nrY1−X3=-0.31**\nrY1−X5=-0.66**\nrY1−X70=0.34**\nrY1−X71=0.36**\nrY1−X72=0.42**\nrY1−X73=0.38**\nrY1−X74=0.30**\nrY1−X75=0.42**\nrY1−X77=0.26**\nrY1−X78=0.40**\nrY1−X79=0.32**\nrY1−X80=0.28**\nrY1−X82=0.32**\nrY1−X83=0.30**\nrY1−X87=0.25**\nrY1−X88=0.19*\nrY1−X90=0.36**\nrY1−X91=0.32**\nrY1−X92=-0.30**\nrY1−X93=0.37**\nrY1−X95=− 0.20*\nrY1−X98=0.30**\nrY1−X99=0.24**\n*Correlation is signiﬁcant in 95%CI\n**Correlation is signiﬁcant in 99%CI\nselected was relu() . The train test split was used for experimentation, and the separation was\napplied randomly. The train/test split value of 0.8 was applied. The experiment was repeatedseveral times to obtain the optimal model parameters for learning rate, momentum, and thenumber of hidden layers. The prediction performances of the models were tested on the testdata sets based on the mean absolute percentage error (MAPE), and the root mean squareerror (RMSE) statistics. The optimal model parameters speciﬁc to each model and modelperformances are represented in Table 4.\nTable 4shows that the proposed model performs well for managing ED operations in\nthe COVID-19 periods. The model, tested in four different operations, achieved around 90%accuracy in two of these operations and 95% accuracy in one. On the other hand, in one of the\n123\n\n[Página 19]\nAnnals of Operations Research (2023) 328:1073–1103 1091\nTable 4 MLP neural network performances on ED operations predictions during-COVID-19\nED operations during-COVID-19 and\nrelated modelOptimized parameters (learning\nrate-LR, momentum-M, number ofhidden layers-HLModel\nperformance\nMAPE RMSE\nModelling daily patient numbers:\nOperation 1LR=0.01, M =0.01, HL =2 10.573 88.624\nModelling daily average LOS:\nOperation 2LR=0.5, M =0.2, HL =3 19.309 40.473\nModelling daily numbers of ordered\nlaboratory tests: Operation 3LR=0.001, M =0.125, HL =4 9.884 28.325\nModelling daily numbers of ordered\nradiologic imaging tests: Operation 4LR=0.019, M =0.19, HL =3 5.924 20.324\noperations modeling average daily LOS, the model performance was lower, having around\n80% accuracy. The model results are also consistent with the ﬁndings on the relationshipbetween model attributes. Since lower relations were observed between variables on LOSmodeling, prediction performance could not achieve the modeling performances on otheroperations with higher correlation levels between the variables. Nonetheless, the achievedaccuracies were still acceptable and practically implementable compared with related studiesand targeted levels.\n6 Discussion\nThis study emphasizes implementing emerging technologies, particularly BDA, in manag-ing health services’ operations. As noted in the literature (Akter & Wamba, 2019 ; Donthu &\nGustaffson, 2020 ), we believe that the challenges posed by COVID-19 can be tackled using\nthese technologies. Grounded in dynamic capabilities and the related context of BDAC, weproposed a model for the management of ED operations in emergencies. To show the valid-ity of the proposed model, we tested it in four different primary operations of EDs. Whiledeﬁning the model variables, besides using the system dynamics-related factors, we imple-mented additional variables to represent the effect of government restrictions and prohibitionsimposed to cope with emergencies. Thus, we contribute to the literature by proposing an efﬁ-cient system for managing ED operations in emergencies by implementing emerging BDAtechnologies and investigating the effects of these policy-based factors on ED operations.\nThe model has been validated using real-life data from a large-scale ED operating in\n˙Izmir city, Turkey. Although the overcrowded environments of EDs are a global problem,\nthis problem is worse in some countries, such as Turkey, in which EDs are frequently occupiedunnecessarily by non-emergent patients. By comparing the daily and total patient volumes inthe before- and during-COVID-19 study periods, the descriptive ﬁndings on the case data setmainly represent the signiﬁcance of this problem in this ED since patient volumes sharplydecreased during-COVID-19 period. By classifying patients into three categories—patientsrequiring no diagnostic tests, laboratory tests, and radiologic imaging tests—and identifyingthat the reduction in patient volume was mainly caused by the ﬁrst category (patients requir-ing no diagnostic tests), we also provide evidence to support this ﬁnding. We additionallysupport this ﬁnding by observing increases in the average LOS values of patients who do not\n123\n\n[Página 20]\n1092 Annals of Operations Research (2023) 328:1073–1103\nrequire any diagnostic tests. Contrarily, the average LOS values were observed to decrease\nfor patients requiring diagnostic tests during-COVID-19 period. All these ﬁndings demon-strate that most patients make unnecessary visits to this ED. This result supports the existingstudies reporting a substantial decrease in ED visits during the COVID-19 (Jeffery et al.,2020 ; Schereyer et al., 2020 ). We also contribute to the literature by linking this result to one\nof the biggest operational challenges of EDs and demonstrating that unnecessary visits arethe leading cause of overcrowded ED environments. Besides, from the practical viewpoint,the decrease in patient numbers and diagnostic test orders during COVID-19 may be usedfor hospital managers’ better scheduling and allocation of ED resources. Although a sharpdecline was observed in these values, a signiﬁcant increase was observed in patients’ averageLOS values, meaning that arriving patients to EDs during-COVID-19 required more andlonger interventions and treatments. Thus, better planning and allocation of ED resourceswill be essential for functioning these services during emergencies.\nSigniﬁcant decreases in patient volume during-COVID-19 period may be related to two\nmain factors. First, the pandemic created stress in patients. To protect themselves from beinginfected, they may have avoided visiting EDs if they did not have emergent or urgent sit-uations. Second, due to the government restrictions and prohibitions imposed, people werepartially obliged to stay at home if they did not need an emergent or urgent health service.Since the ﬁrst factor is more behavioral, it is beyond the scope of this study. However, weaimed to identify the impacts of policy-based factors on ED operations by adopting our modelinto a case study representing the overcrowding of ED environments and frequently unneces-sary ED visits. This result supports the existing studies reporting decreased patient volumesdue to the governmental actions taken in ﬁghting COVID-19 (Kendzerska et al., 2021 ; Sözen\net al., 2022 ). It also enhances literature by considering this effect in developing prediction\nmodels for patient volumes, average stay lengths of patients, and resource utilization of EDsduring this pandemic period.\nThe depicted decreases in the average LOS values of patients requiring laboratory or\nradiologic imaging tests in the during-COVID-19 period compared to the before-COVID-19period highlights another essential ﬁnding of this study. While this ﬁnding has been widelypresented in the literature (Houshyar et al., 2020 ; Jeffery et al., 2020 ), by proposing an\nefﬁcient data-driven model for predicting the daily utilization of these services during thispandemic, once again, this study differs from the existing studies. As an interpretation, itshould be noted that the decrease in the utilization of EDs’ resources accelerates the accessto resources and enables more efﬁcient use of them, and solves another challenge of longwaiting times in EDs.\nA critical step in devising the proposed model was determining the model inputs appro-\npriately. In the case study implementation, input variables are deﬁned in two categories as(i) variables representing system dynamics and (ii) government restrictions and prohibitions.While policy-based variables are deﬁned commonly in implementing the proposed modelfor considered ED operations, system dynamics-based variables are explicitly deﬁned foreach operation. The primary demographics, such as gender, age, triage level, arrival type,and ICD-10 encoded diagnosis in the ED patients’ database, were used and appropriatelytransformed to identify operation-speciﬁc input variables. The values of these variables weremeasured based on the data set for the before-COVID-19 study period.\nAfter forming data sets in this manner, the proposed model was tested for the considered\nED operations of managing the daily number of patients, average daily LOS, daily numbersof laboratory tests ordered, and daily numbers of radiologic imaging tests ordered. Whenthe relations between the speciﬁed input variables and the daily number of patients during-COVID-19 period were analyzed, it was concluded that policy-based attributes have more\n123\n\n[Página 21]\nAnnals of Operations Research (2023) 328:1073–1103 1093\nsigniﬁcant effects on the daily number of patients compared to the identiﬁed system dynamics-\nrelated input variables. Some relations were observed between the deﬁned input variables,such as transport bans and restrictions on the elderly, and the daily average LOS during-COVID-19. While policy-based variables, such as total curfew, are related to the daily numberof laboratory tests ordered during-COVID-19 period, some other system dynamics-relatedinput variables also have relations with the corresponding output variable. Finally, bothpolicy-based attributes, namely, curfews and restrictions and transport bans, and most systemdynamics-related variables seemed to relate to the daily number of radiologic imaging testsordered. It is also noted that the depicted correlations between policy-based input variablesand the corresponding output variables had negative signs showing that such policies maydecrease patient volume and the utilization of primary ED resources. From these ﬁndings, itis concluded that the restrictions and prohibitions imposed by the government in coping withCOVID-19 have had signiﬁcant impacts on the management of ED operations. This resultis in line with the existing studies (Akter & Wamba, 2019 ; Haldane & Morgan, 2021 ; Sözen\net al., 2022 ). Our ﬁndings contribute to the literature by investigating the effects of system\ndynamics-related and government-imposed actions together and comparatively for differentoperations of EDs.\nThe obtained data sets were then used to implement the proposed model in the four primary\nED operations using MLP neural networks. Neural network algorithms have been presentedin the literature for automatic COVID-19 detection (Qayyum et al., 2021 ) and infection rate\npredictions (Wieczorek et al., 2020 ; Sozen, Sariyer & Ataman, 2021). By implementing this\nalgorithm in multi real-life operations of EDs, the used contexts of this BDA technique havebeen extended in this paper. The model has high prediction accuracies for managing dailypatient numbers and daily use of resources during a pandemic. Besides achieving or exceedingthe prediction performances of models in the literature in this context (Whitt & Zhang, 2019 ),\nthese results achieved the targeted value (85%) set by this ED’s service providers. Althoughthe model’s performance is lower in predicting daily average LOS values, it can still matchthe performance of previous studies (Ataman & Sariyer, 2021) and achieve the targeted valueof 75% accuracy. This operation’s targeted value is smaller than others since modeling LOS ismore complex. Thus, with the proposed model, which utilizes BDA, we believe that even themost challenging health services operations may be managed efﬁciently, and the difﬁcultiesposed by emergencies can be handled.\n7 Implications\n7.1 Theoretical implications\nThe study underpins the dynamic capability theory in two folds. The emergencies are featuredwith the rapidly changing conditions and parameters. Hence, the data inherent in the crisesexhibits a dynamic feature. Eventually, the properties of the data set are subject to change.Therefore, DC theory arises as an ideal theoretical structure to embrace dynamically changingenvironments caused by emergencies. While such situations cause rapid changes in patientvolumes, varieties, and characteristics, from different viewpoints, the government’s policies,such as restrictions and prohibitions in ﬁghting these situations, create additional modiﬁ-cations in the system environment. For instance, during emergencies caused by pandemicillnesses, volumes of infected patients may signiﬁcantly increase. The total patient volumein health services may also be decreased due to panic and stress factors created by being\n123\n\n[Página 22]\n1094 Annals of Operations Research (2023) 328:1073–1103\ninfected and based on governmental policies such as stay-home warnings and curfews. All of\nthis support how emergencies create dynamically changing environments. This implicationis strengthened by comparing the main features of the health system data before-COVID19and during-COVID19 periods. Hence, the study’s ﬁndings state that DC is applicable inemergencies.\nThe second fold of the theoretical implication can be asserted that dynamically changing\nenvironments caused by emergencies affect decision-making processes. As the propertiesof the data set act in a dynamic manner, it forces the decision-making process to be in linewith this rapid change. Even though the big data nature of the data sets stays the same,the time pressure on the decision-makers is higher due to the fast and dynamic change ofdata. Thus, the need for rapid decision-making increases the need for the capabilities relatedto data analytics. Therefore, BDAC is a crucial structure for building the decision-makingmechanism within emergencies. Once again, the study’s ﬁndings support this implication byhighlighting the signiﬁcant changes in patient volumes, demographics (such as distributionson gender, age, triage, arrival type, and diagnosis categories), and diagnostic test requirements(resource usage) between the before and during pandemic periods. Being aware of changes insuch parameters and having capabilities of shaping ED services rapidly in response to thesechanges provide signiﬁcant advantages in ﬁghting emergencies. Thus, it can be depicted thatBDAC is applicable in emergencies.\nThus, although dynamic capability theory and the recent view of BDAC have been well\npresented in management literature, this study attempts to extend their usage in the healthcontext, particularly under emergencies. By discussing the rapidly changing parameters andfeatures of the health system environments in emergencies, proposing a model highlightinga need for BDAC, and implementing this model in a real-life big data study, this study aimsto contribute to the context of these theories.\n7.2 Managerial implications\nOur main suggestion is that the decision-makers of health services have BDAC and use bigdata sets of their system environments effectively to create meaningful knowledge, whichshould then be turned rapidly into actions. Adopting the system to dynamically changing con-ditions caused by emergencies quickly and efﬁciently should be achieved by taking advantageof the emerging technologies and by being able to implement these technologies in practicefor planning and managing operations. Based on the results of this study, we showed howthe current emergency, COVID-19, and the government policies change the patient volumes,varieties, and characteristics. Since such changes may signiﬁcantly affect ED operations, andbecause it is essential to provide rapid responses to these changing situations, it should alsobe noted that understanding and identifying the main factors that impact their operations iscritical. Suppose system-related factors are characterized and appropriately measured, andexternal factors that may arise from the emergencies are carefully followed and identiﬁed.All these factors can be collectively used in modeling ED operations by taking advantageof BDA technologies. Hence, the system may function efﬁciently even in emergencies. Thechallenges arising in the ED environment and posed by emergencies can be easily managed insuch conditions. Based on such models, the managers will be able to make rapid and correctdecisions and adapt the system efﬁciently to dynamically changing conditions.\nWe also highlight the importance of data recording in health services. Although BDA and\nBDAC are signiﬁcant technologies and capabilities for health services and particularly emer-gency departments, all these do not make any sense if there exist no data sets to analyze, create\n123\n\n[Página 23]\nAnnals of Operations Research (2023) 328:1073–1103 1095\nknowledge, and use in decision making. Therefore, we suggest that the ED decision-makers\nfocus on electronic recording and data storage processes and should not avoid investing inthese processes and systems. Since the quantity and quality of the data allow meaningful andactionable knowledge, the decision-makers should spend time and effort testing the quality ofrecording processes. Assuring the existence of valid and reliable big data sets is the primaryprior condition for an ED decision-maker to take advantage of BDA in ﬁghting against thechallenges and uncertainties posed by emergencies. This is also very important for satisfyingthe sustainable monitoring in ED processes and real-time emergency response applications.\n7.3 Policy implications\nThis study mainly emphasized the overcrowded ED environments and the signiﬁcance of thisproblem in our ED, even regularly. Based on the ﬁndings, we noted that this overcrowdingmight be primarily associated with the redundant use of these services, particularly for patientswho occupy them for non-urgent situations. These types of patients generally perceive EDsas gateways to hospitals. To not make an appointment and wait in line for polyclinic servicesor receive a health service at weekends or nights, as EDs provide a 7/24 service, patientsmay choose to visit EDs. However, providing a timely and efﬁcient service becomes morechallenging in these crowded environments based on limited resources. If ED operationscannot be appropriately managed, patients even in emergent and urgent situations may have towait to be treated, which may have signiﬁcant consequences. To cope with this overcrowdingproblem, different government actions should be taken.\nThis study also analyzes the effects of government restrictions and prohibitions in coping\nwith emergencies, particularly COVID-19. It should be highlighted that imposing these poli-cies is crucial in emergencies to protect the functioning of EDs. Government policies, suchas curfews (lock-downs), transport bans, and partial restrictions on the elderly or the young,may decrease patient volumes, redundant ED visits, and resource utilization.\nIn today’s era that requires awareness of big data and the related contexts of BDA and\nBDAC, we also advise policymakers to invest in data storage and analysis in governmentagencies. Governments must create awareness of these emerging concepts and technologiesin public institutions. Governments should pay time, effort, and budget to regularly controlthe agencies based on their data storage capabilities, qualities, quantities, and reliabilities.It may be necessary to impose sanctions on institutions deﬁcient in these concepts duringthese controls. Creating high-quality, reliable, and robust data sets in government institutionswill improve more accurate and timely decision-making processes in emergency and routinesituations. This may also help governments integrate sustainability orientation in health careoperations and ﬂexibility for managing emergencies.\n8 Conclusion\nWhile emergencies precisely demonstrate dynamically changing environments, health ser-vices are the main actors in coping with those situations. Governments are another leadingactor; they are the enablers of the system and may impose restrictions and prohibitions toprotect the functioning of health services. We, therefore, propose a model, which is groundedin the dynamic capabilities and related context of BDAC, for managing operations of one ofthe most crucial health services units, namely, EDs, during emergencies. With this model,we aim not only to manage ED operations sustainably but also to investigate the effects\n123\n\n[Página 24]\n1096 Annals of Operations Research (2023) 328:1073–1103\nof imposed restrictions and prohibitions on these operations. Besides proposing a generic\nmachine learning integrated model for managing ED operations under emergencies and vali-dating this model for different operations of EDs, taking the governmental actions as the mainfactors of this model and thus showing how they affect these operations is the main contri-bution of this paper. This study also contributes to dynamic capability theory and BDAC byextending their usage for the decision-making processes of one of the most important actorsof health services, EDs, under emergencies. We also believe that the proposed BDA-drivenmodel or more general big data and BDA implementations in real-life operations may helpsatisfy sustainable operations in EDs.\nThe proposed model adopts one of the most popular BDA techniques: multilayer per-\nceptron neural networks. The model is implemented in a real-life data set representing alarge-scale ED with daily patient volumes of more than 1,000. The current COVID-19 pan-demic represents a focused emergency. The model is validated in four different primaryoperations of EDs: managing daily numbers of patients, daily average stays of patients anddaily usage of resources (laboratory services and radiologic imaging services). The predic-tion performance of the proposed model varies between 80 to 95% for the correspondingoperations. This study also showed that policy-based factors might signiﬁcantly affect EDoperations. Such restrictions and prohibitions may cause sharp decreases in patient volumesand resource utilisations in EDs, which are challenged by overcrowding. Thus, imposingsuch policies is crucial to protect ED functioning in emergencies.\nThe main limitation of this study was that its experimental evaluation was based on data\ncollected from a single case study, and its ﬁndings may, therefore, not generalize to emer-gency departments with signiﬁcantly different patient populations, characteristics, volumes,and varieties. Generalizing these results to other emergency departments with different oper-ational processes, guidelines, and dynamics may also be impossible. Operationally, to ensurerobustness, it is critical to check for variations in patient and system dynamics patternsobserved in this case study to transfer the proposed model to other emergency departments.Future studies should include a broader set of operations, measurements, internal and exter-nal variables, and outcomes from multiple emergency departments to support the robustnessof the proposed model. Finally, we expect that the implementation of deep learning tech-niques can potentially further improve the predictive performance of the proposed model forconsidered operations of EDs.\nAppendix 1\nCorrelation matrices of the identiﬁed variables of the models for corresponding ED opera-tions.\nSee Fig. 5.\n123\n\n[Página 25]\nAnnals of Operations Research (2023) 328:1073–1103 1097\nFig. 5 Operation 1: Modelling daily numbers of ED patients during COVID-19\n123\n\n[Página 26]\n1098 Annals of Operations Research (2023) 328:1073–1103\nAppendix 2\nSee Fig. 6.\nFig. 6 Operation 2: Modelling daily average LOS of ED patients during COVID-19\n123\n\n[Página 27]\nAnnals of Operations Research (2023) 328:1073–1103 1099\nAppendix 3\nSee Fig. 7.\nFig. 7 Operation 3: Modelling daily numbers of laboratory tests ordered\n123\n\n[Página 28]\n1100 Annals of Operations Research (2023) 328:1073–1103\nAppendix 4\nSee Fig. 8.\nFig. 8 Operation 4: Modelling daily numbers of radiologic imaging tests ordered\nReferences\nAbdel-Basset, M., Chang, V ., & Nabeeh, N. A. (2021). An intelligent framework using disruptive technologies\nfor COVID-19 analysis. T echnological F orecasting and Social Change, 163 , 120431.\nAkter, S., & Wamba, S. F. (2019). Big data and disaster management: A systematic review and agenda for\nfuture research. Annals of Operations Research, 283 (1), 939–959.\nAlinaghian, M., & Goli, A. (2017). Location, allocation and routing of temporary health centers in rural\nareas in crisis, solved by improved harmony search algorithm. International Journal of Computational\nIntelligence Systems, 10 (1), 894–913.\nAtaman, M. G., & Sarıyer, G. (2021). Predicting waiting and treatment times in emergency departments using\nordinal logistic regression models. The American Journal of Emergency Medicine, 46 , 45–50.\nBag, S., Gupta, S., Choi, T. M., & Kumar, A. (2021). Roles of innovation leadership on using big data analytics\nto establish resilient healthcare supply chains to combat the COVID-19 pandemic: A multimethodologicalstudy. IEEE Transactions on Engineering Management .https://doi.org/10.1109/TEM.2021.3101590\n123\n\n[Página 29]\nAnnals of Operations Research (2023) 328:1073–1103 1101\nBirkinshaw, J., Zimmermann, A., & Raisch, S. (2016). How do ﬁrms adapt to discontinuous change? Bridging\nthe dynamic capabilities and ambidexterity perspectives. California Management Review, 58 (4), 36–58.\nChoi, T. M. (2021). Fighting against COVID-19: What operations research can help and the sense-and-respond\nframework. Annals of Operations Research .https://doi.org/10.1007/s10479-021-03973-w\nChoi, T. M., Wallace, S. W., & Wang, Y . (2018). Big data analytics in operations management. Production\nand Operations Management, 27 (10), 1868–1883.\nDas, S. K., Pervin, M., Roy, S. K., & Weber, G. W. (2021). Multi-objective solid transportation-location problem\nwith variable carbon emission in inventory management: A hybrid approach. Annals of Operations\nResearch .https://doi.org/10.1007/s10479-020-03809-z\nDeloitte. (2020). COVID -19: Managing supply chain risk and disruption . Retrieved November 10,\n2020, from https://www2.deloitte.com/global/en/pages/risk/articles/covid-19-managing-supply-chain-\nrisk-anddisruption.html .\nDonthu, N., & Gustafsson, A. (2020). Effects of COVID-19 on business and research. Journal of Business\nResearch, 117 , 284.\nFeng, Q., & Shanthikumar, J. G. (2018). How research in production and operations management may evolve\nin the era of big data. Production and Operations Management, 27 (9), 1670–1684.\nFortune. (2020). 94% of the F ortune 1000 are seeing coronavirus supply chain disruptions: Report .\nRetrieved November 10, 2020, from https://fortune.com/2020/02/21/fortune-1000-coronavirus-china-\nsupply-chain-impact/ .\nGoli, A., Zare, H. K., Tavakkoli-Moghaddam, R., & Sadeghieh, A. (2019). Hybrid artiﬁcial intelligence and\nrobust optimization for a multi-objective product portfolio problem Case study: The dairy productsindustry. Computers and Industrial Engineering, 137 , 106090.\nGoli, A., Khademi-Zare, H., Tavakkoli-Moghaddam, R., Sadeghieh, A., Sasanian, M., & Malekalipour\nKordestanizadeh, R. (2021). An integrated approach based on artiﬁcial intelligence and novel meta-\nheuristic algorithms to predict demand for dairy products: a case study. Network Computation in Neural\nSystems, 32 (1), 1–35.\nGuo, M., Zhang, Q., Liao, X., Chen, F. Y ., & Zeng, D. D. (2020). A hybrid machine learning framework for\nanalyzing human decision-making through learning preferences. Omega, 101 , 102263.\nGupta, S., Justy, T., Kamboj, S., Kumar, A., & Kristoffersen, E. (2021). Big data and ﬁrm marketing per-\nformance: Findings from knowledge-based view. T echnological F orecasting and Social Change, 171 ,\n120986.\nHaldane, V ., & Morgan, G. T. (2021). From resilient to transilient health systems: The deep transformation of\nhealth systems in response to the COVID-19 pandemic. Health Policy and Planning, 36 (1), 134–135.\nHarvard Business Review. (2020). Coronavirus is proving we need more resilient supply chains . Retrieved\nNovember 5, 2020, from https://hbr.org/2020/03/coronavirus-is-proving-that-we-need-moreresilient-\nsupply-chains .\nHossain, M. K., Thakur, V ., & Mangla, S. K. (2021). Modeling the emergency healthcare supply chains:\nResponding to the COVID-19 pandemic. Journal of Business and Industrial Marketing .https://doi.org/\n10.1108/JBIM-07-2020-0315\nHoushyar, R., Tran-Harding, K., Glavis-Bloom, J., Nguyentat, M., Mongan, J., Chahine, C., Loehfelm, T.\nW., Kohli, M. D., Zaragoza, E. J., Murphy, P. M., & Kampalath, R. (2020). Effect of shelter-in-place\non emergency department radiology volumes during the COVID-19 pandemic. Emergency radiology,\n27(6), 781–784.\nHuang, H., Peng, Z., Wu, H., & Xie, Q. (2020). A big data analysis on the ﬁve dimensions of emergency\nmanagement information in the early stage of COVID-19 in China. Journal of Chinese Governance,\n5(2), 213–233.\nJeffery, M. M., D’onofrio, G., Paek, H., Platts-Mills, T. F., Soares, W. E., Hoppe, J. A., Genes, N., Nath, B.,\n& Melnick, E. R. (2020). Trends in emergency department visits and hospital admissions in health caresystems in 5 states in the ﬁrst months of the COVID-19 pandemic in the US. JAMA internal medicine,\n180(10), 1328–1333.\nKapoor, K., Bigdeli, A. Z., Dwivedi, Y . K., & Raman, R. (2021). How is COVID-19 altering the manufac-\nturing landscape? A literature review of imminent challenges and management interventions. Annals of\nOperations Research .\nhttps://doi.org/10.1007/s10479-021-04397-2\nKendzerska, T., Zhu, D. T., Gershon, A. S., Edwards, J. D., Peixoto, C., Robillard, R., & Kendall, C. E. (2021).\nThe effects of the health system response to the COVID-19 pandemic on chronic disease management:A narrative review. Risk Management and Healthcare Policy, 14 , 575.\nKumar, A., Shankar, R., Choudhary, A., & Thakur, L. S. (2016). A big data MapReduce framework for\nfault diagnosis in cloud-based manufacturing. International Journal of Production Research, 54 (23),\n7060–7073.\n123\n\n[Página 30]\n1102 Annals of Operations Research (2023) 328:1073–1103\nKumar, A., Shankar, R., & Aljohani, N. R. (2020). A big data driven framework for demand-driven forecasting\nwith effects of marketing-mix variables. Industrial Marketing Management, 90 , 493–507.\nLee, S. M., & Trimi, S. (2021). Convergence innovation in the digital age and in the COVID-19 pandemic\ncrisis. Journal of Business Research, 123 , 14–22.\nMari´ c, J., Galera-Zarco, C., & Opazo-Basáez, M. (2021). The emergent role of digital technologies in the\ncontext of humanitarian supply chains: A systematic literature review. Annals of Operations Research .\nhttps://doi.org/10.1007/s10479-021-04079-z\nMidya, S., Roy, S. K., & Yu, V . F. (2021). Intuitionistic fuzzy multi-stage multi-objective ﬁxed-charge solid\ntransportation problem in a green supply chain. International Journal of Machine Learning and Cyber-\nnetics, 12 (3), 699–717.\nMishra, D., Gunasekaran, A., Papadopoulos, T., & Childe, S. J. (2018). Big Data and supply chain management:\nA review and bibliometric analysis. Annals of Operations Research, 270 (1), 313–336.\nMondal, A., & Roy, S. K. (2021). Multi-objective sustainable opened-and closed-loop supply chain under\nmixed uncertainty during COVID-19 pandemic situation. Computers & Industrial Engineering, 159 ,\n107453.\nMondal, A., & Roy, S. K. (2022). Application of Choquet integral in interval type-2 Pythagorean fuzzy\nsustainable supply chain management under risk. International Journal of Intelligent Systems, 37 (1),\n217–263.\nPapadopoulos, T., Baltas, K. N., & Balta, M. E. (2020). The use of digital technologies by small and medium\nenterprises during COVID-19: Implications for theory and practice. International Journal of Information\nManagement, 55 , 102192.\nSarkis, J. (2021). Supply chain sustainability: Learning from the COVID-19 pandemic. International Journal\nof Operations & Production Management, 41 (1), 63–73.\nSchreyer, K. E., Daniel, A., King, L. L., Blome, A., DeAngelis, M., Stauffer, K., Desrochers, K., Donahue, W.,\nPolitarhos, N., Raab, C., & McNamara, R. (2020). Emergency department management of the Covid-19\npandemic. The Journal of emergency medicine, 59 (6), 946–951.\nThakur, V ., Mangla, S. K., & Tiwari, B. (2021). Managing healthcare waste for sustainable environmental\ndevelopment: A hybrid decision approach. Business Strategy and the Environment, 30 (1), 357–373.\nTirkolaee, E. B., Goli, A., Ghasemi, P., & Goodarzian, F. (2022). Designing a sustainable closed-loop supply\nchain network of face masks during the COVID-19 pandemic: Pareto-based algorithms. Journal of\nCleaner Production, 333 , 130056.\nQayyum, A., Razzak, I., Tanveer, M., & Kumar, A. (2021). Depth-wise dense neural network for automatic\nCOVID19 infection detection and diagnosis. Annals of Operations Research .https://doi.org/10.1007/\ns10479-021-04154-5\nQueiroz, M. M., Ivanov, D., Dolgui, A., & Wamba, S. F. (2020). Impacts of epidemic outbreaks on supply\nchains: Mapping a research agenda amid the COVID-19 pandemic through a structured literature review.Annals of Operations Research .https://doi.org/10.1007/s10479-020-03685-7\nSanders, N. R., & Ganeshan, R. (2018). Big data in supply chain management. Production and Operations\nManagement, 27 (10), 1745–1748.\nSarıyer, G., & Ataman, M. G. (2020). The likelihood of requiring a diagnostic test: Classifying emergency\ndepartment patients with logistic regression. Health Information Management Journal, 51 (1), 13–22.\nSarıyer, G., Ataman, M. G., & Kızılo˘ glu, ˙I. (2020). Analyzing main and interaction effects of length of stay\ndeterminants in emergency departments. International Journal of Health Policy and Management, 9 (5),\n198–205.\nSözen, M. E., Sarıyer, G., & Ataman, M. G. (2022). Big data analytics and COVID-19: Investigating the\nrelationship between government policies and cases in Poland, Turkey, and South Korea. Health Policy\nand Planning, 37 (1), 100–111.\nSharma, M., Luthra, S., Joshi, S., & Kumar, A. (2020). Developing a framework for enhancing survivability\nof sustainable supply chains during and post-COVID-19 pandemic. International Journal of Logistics\nResearch and Applications, 25 (4–5), 433–453.\nRubbio, I., Bruccoleri, M., Pietrosi, A., & Ragonese, B. (2020). Digital health technology enhances resilient\nbehaviour: Evidence from the ward. International Journal of Operations and Production Management,\n40(1), 34–67.\nTeece, D. J., Pisano, G., & Shuen, A. (1997). Dynamic capabilities and strategic management. Strategic\nManagement Journal, 18\n(7), 509–533.\nTeece, D., Peteraf, M., & Leih, S. (2016). Dynamic capabilities and organizational agility: Risk, uncertainty,\nand strategy in the innovation economy. California Management Review, 58 (4), 13–35.\nVerma, S., & Gustafsson, A. (2020). Investigating the emerging COVID-19 research trends in the ﬁeld of busi-\nness and management: A bibliometric analysis approach. Journal of Business Research, 118 , 253–261.\n123\n\n[Página 31]\nAnnals of Operations Research (2023) 328:1073–1103 1103\nWamba, S. F., Gunasekaran, A., Akter, S., Ren, S. J. F., Dubey, R., & Childe, S. J. (2017). Big data analytics\nand ﬁrm performance: Effects of dynamic capabilities. Journal of Business Research, 70 , 356–365.\nWhitt, W., & Zhang, X. (2019). Forecasting arrivals and occupancy levels in an emergency department.\nOperations Research for Health Care, 21 , 1–18.\nWieczorek, M., Siłka, J., & Wo´ zniak, M. (2020). Neural network powered COVID-19 spread forecasting\nmodel. Chaos, Solitons & Fractals, 140 , 110203.\nYu, W., Zhao, G., Liu, Q., & Song, Y . (2021). Role of big data analytics capability in developing integrated\nhospital supply chains and operational ﬂexibility: An organizational information processing theory per-\nspective. T echnological F orecasting and Social Change, 163 , 120417.\nZollo, M., & Winter, S. G. (2002). Deliberate learning and the evolution of dynamic capabilities. Organization\nScience, 13 (3), 339–351.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and\ninstitutional afﬁliations.\nSpringer Nature or its licensor holds exclusive rights to this article under a publishing agreement with the\nauthor(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article issolely governed by the terms of such publishing agreement and applicable law.\n123",
      "metadata": {
        "filename": "Big data analytics 2022.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\Big data analytics 2022.pdf",
        "file_size": 2950376,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:35.837648",
        "content_length": 86402
      }
    },
    "932927b3-d9fe-4477-9ec0-bbf37f794ab3": {
      "id": "932927b3-d9fe-4477-9ec0-bbf37f794ab3",
      "content": "[Página 1]\nExpert Systems With Applications  115 (2019) 543–556  \nContents  lists available  at ScienceDirect  \nExpert  Systems  With  Applications  \njournal  homepage:  www.elsevier.com/locate/eswa  \nBIGOWL:  Knowledge  centered  Big Data  analytics  /p82 \nCristóbal  Barba-González,  José García-Nieto  ∗, María del Mar  Roldán-García,  \nIsmael  Navas-Delgado,  Antonio J.  Nebro,  José F.  Aldana-Montes  \nDepartmento  de Lenguajes  y Ciencias de la Computación,  University  of Málaga, ETSI Informática,  Campus de Teatinos, Málaga 29071, Spain \na r t i c l e i n f o \nArticle history: \nReceived  5 April 2018 \nRevised 26 July 2018 \nAccepted  14 August 2018 \nAvailable  online 23 August 2018 \nKeywords:  \nOntology  \nBig Data analytics  \nSemantics  \nKnowledge  extraction  a b s t r a c t \nKnowledge  extraction  and incorporation  is currently  considered  to be beneﬁcial  for eﬃcient  Big Data an- \nalytics. Knowledge  can take part in workﬂow  design, constraint  deﬁnition,  parameter  selection  and con- \nﬁguration,  human interactive  and decision-making  strategies.  This paper proposes  BIGOWL,  an ontology  \nto support  knowledge  management  in Big Data analytics.  BIGOWL  is designed  to cover a wide vocab- \nulary of terms concerning  Big Data analytics  workﬂows,  including  their components  and how they are \nconnected,  from data sources to the analytics  visualization.  It also takes into consideration  aspects such \nas parameters,  restrictions  and formats.  This ontology  deﬁnes not only the taxonomic  relationships  be- \ntween the different  concepts,  but also instances  representing  speciﬁc individuals  to guide the users in \nthe design of Big Data analytics  workﬂows.  For testing purposes,  two case studies are developed,  which \nconsists  in: ﬁrst, real-world  streaming  processing  with Spark of traﬃc Open Data, for route optimization  \nin urban environment  of New York city; and second, data mining classiﬁcation  of an academic  dataset on \nlocal/cloud  platforms.  The analytics  workﬂows  resulting  from the BIGOWL  semantic  model are validated  \nand successfully  evaluated.  \n©2 0 1 8 Elsevier  Ltd. All rights reserved.  \n1. Introduction  \nIn accordance  with the recent Gartner’s  report, 1 an emerging  \nchallenge  in Big Data is to construct  data-driven  intelligent  appli- \ncations  that capture  and inject domain  knowledge  in the analyt-  \nical processes,  including  context  and using a standardized  format.  \nContext  refers to all the relevant  (meta)-information  to support  the \nanalysis  and to help interpreting  its results.  This will facilitate  the \nintegration  (in a standardized  way) with third parties’  data, algo- \nrithms,  business  intelligence  (BI) and visualization  services.  \nThe use of semantics  as contextual  information  will enhance  \nthe analytical  power of the algorithms,  as well as the reuse of \nsingle components  in data analytics  workﬂows  ( Ristoski  & Paul- \n/p82 This work has been partially funded by Grants TIN2014-58304,  TIN2017-86049-  \nR (Spanish  Ministry of Education  and Science) and P12-TIC-1519  (Plan Andaluz de \nInvestigación,  Desarrollo  e Innovación).  Cristóbal  Barba-González  is supported  by \nGrant BES-2015-072209  (Spanish  Ministry of Economy  and Competitiveness).  José\nGarcía-Nieto  is the recipient  of a Post-Doctoral  fellowship  of “Captación  de Talento \npara la Investigación” Plan  Propio at Universidad  de Málaga. \n∗Corresponding  author. \nE-mail addresses:  cbarba@lcc.uma.es  (C. Barba-González),  jnieto@lcc.uma.es  \n(J. García-Nieto),  mmar@lcc.uma.es  (M.d.M. Roldán-García),  ismael@lcc.uma.es  (I. \nNavas-Delgado),  antonio@lcc.uma.es  (A.J. Nebro), jfam@lcc.uma.es  (J.F. Aldana-  \nMontes).  \n1 https://www.gartner.com/doc/3656517/adopt-datadriven-approach-  \nconsolidating-infrastructure  . heim, 2016 ). Therefore,  the development  of ways to make the do- \nmain knowledge  explicit  and usable is needed  to improve  the \ndata processing  and analysis  tasks. The Semantic  Web technolo-  \ngies can be used to annotate  not only the knowledge  domain  \nof the data, but also the analytics’  meta-data  ( Keet, Ławrynow-  \nicz, d’Amato,  Kalousis,  Nguyen,  Palma, Stevens,  & Hilario,  2015 ), \nincluding:  algorithms’  parameters,  input variables,  tuning experi-  \nences, expected  behaviors  and taxonomies.  This will facilitate  the \nreuse and composition  of Big Data analytics  in a proper manner,  as \nwell as to enhance  the quality  of consumed  and produced  data. \nIn this regard,  ontologies  describe  concepts,  relationships,  \nclasses,  individuals,  formal logic axioms  and objects  of a particu-  \nlar domain  ( Gruber,  1995 ). The objects  refer to entities  and events \n(concepts)  in the real world, and their relations  represent  the se- \nmantic  links between  these entities.  A series of studies  have been \nappearing  in the last few years, in which ontological  approaches  \nare suggested  to enhance  Big Data analytics  ( Konys, 2016; Kuiler, \n2014 ). However,  they are presented  as conceptual  frameworks,  still \nin an early stage of development,  and mostly oriented  to the spe- \nciﬁc domain  of health system  applications.  \nThis motivates  us to propose  an ontology-driven  approach  to \nsupport  knowledge  management  in Big Data analytics  workﬂows.  \nThe proposed  ontology  is called BIGOWL  (BIG data analytics  OWL 2 \n2 OWL refers to the Web Ontology  Language  described  in Section 2.1 . \nhttps://doi.org/10.1016/j.eswa.2018.08.026  \n0957-4174/© 2018 Elsevier Ltd. All rights reserved.\n\n[Página 2]\n544 C. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 \nontology),  which acts as a formal schema  for the representation  \nand consolidation  of knowledge  in Big Data analytics.  Knowledge  \nincorporation  is in turn beneﬁcial  for an eﬃcient  algorithmic  per- \nformance,  by taking part in operator’s  design,  parameter  selection,  \nhuman  interactive  and decision-making  strategies.  \nOur scientiﬁc  hypothesis  is as follows:  “The semantic  annotation  \nof Big Data sources,  components  and algorithms  can acts as a link to \ncapture  and incorporate  the domain  knowledge  to guide and enhance  \nthe analytical  processes  ”. In addition,  the semantic  annotation  can \nprovide  the background  for reasoning  methods  based on axiomatic  \nand rule logic recommendations.  \nTo test this hypothesis,  a semantic  model has been gener- \nated, which comprises  an RDF 3 (Resource  Description  Framework)  \nrepository  that follows  the BIGOWL  scheme.  This repository  can be \nqueried  by high level algorithms  using SPARQL.  The goal is to prop- \nerly feed artiﬁcial  intelligence  procedures  capable  of guiding  the \ndesign of Big Data analytics  workﬂows.  \nAs a proof-of-concept,  we show how BIGOWL  can be used to \nguide the design of real-world  and academic  analytic  workﬂows.  \nA ﬁrst case study consists  in optimizing  vehicular  routes based on \nNew York real-time  Open Data about urban traﬃc (average  speeds \nof vehicles,  traﬃc densities,  etc.). 4 The data source is managed  by \nstreaming  processing  tasks (Kafka and Spark),  after which they are \noptimized  (jMetalSP  5 ) and visualized.  The second  case study is a \nclassiﬁcation  workﬂow  modeled  by using the popular  Weka 6 li- \nbrary for data mining,  as well as the BigML in-cloud  service.  7 \nThe main contributions  of this study are: \n•The proposed  ontology,  BIGOWL,  has been designed  and imple- \nmented  for the representation  and consolidation  of knowledge  \nin Big Data analytics.  It considers  a large and complemented  set \nof concepts,  attributes  and relationships  that have been taken \nfrom Big Data ecosystem.  \n•A semantic  approach  has been implemented  to annotate  (i.e. \nto “semantize”)  all the involved  meta-data  from multiple  data \nsources,  processing  components  and analytic  algorithms.  The \nmeta-data  are integrated  following  the BIGOWL  structure  and \nstored in a common  RDF repository.  \n•The semantic  model is evaluated  in the context  of two realis- \ntic use cases: real-time  routing  calculation  in urban traﬃc and \nclassical  classiﬁcation  with decision  trees. The proof-of-concept  \nlead us to test our initial hypothesis.  \nThe remaining  of this paper is structured  as follows.  In \nSection  2 , background  concepts  and literature  overview  are pre- \nsented.  Section  3 presents  current  practices  in Big Data analyt-  \nics. Section  4 describes  the semantic  model, comprising  the on- \ntology, RDF repository,  mappings  and workﬂow  composition  assis- \ntant. Section  5 presents  the use case for testing and validation.  In \nSection  6 , a series of discussions  are included.  Conclusions  and fu- \nture work are drawn in Section  7 . \n2. Background  and related  work \nTo make this paper self-contained,  this section  describes  back- \nground  concepts  in the Semantic  Web ﬁeld. A review of the state \nof the art is also provided  to point out the main differences  of the \nrelated  works with the proposed  approach.  \n3 RDF in W3C https://www.w3.org/RDF/  . \n4 https://www.data.cityofnewyork.us/Transportation/Real-  Time- Traﬃc- Speed- Data/ \nxsat-x5sa  . \n5 http://www.jmetal.sourceforge.net/  . \n6 https://www.cs.waikato.ac.nz/ml/weka/  . \n7 https://www.bigml.com/  . Table 1 \nBasic OWL-DL semantic  syntax used to formally deﬁne the proposed  \nontology.  \nDescriptions  Abstract syntax DL syntax \nOperators  intersection  ( C 1 , C 2 , /22c5/22c5/22c5, C n ) C 1 /2293C 2 /2293/22c5/22c5/22c5/2293C n \nunion ( C 1 , C 2 , /22c5/22c5/22c5, C n ) C 1 /2294C 2 /2294/22c5/22c5/22c5/2293C n \nRestrictions  for at least 1 value V from C ∃ V.C \nfor all values V from C ∀ V.C \nR is Symmetric  R ≡R −\nClass Axioms A partial ( C 1 , C 2 , /22c5/22c5/22c5, C n ) A /subsetsqequal C 1 /2293C 2 /2293/22c5/22c5/22c5/2293C n \nA complete  ( C 1 , C 2 , /22c5/22c5/22c5, C n ) A ≡C 1 /2293C 2 /2293/22c5/22c5/22c5/2293C n \n2.1. Background  concepts  \n•Ontology.  In accordance  with Noy, McGuinness  et al. (2001) , an \nontology  provides  a formal representation  of the real world. \nIt deﬁnes  an explicit  description  of concepts  in a domain  of \ndiscourse  (classes  or concepts),  properties  of each concept  de- \nscribing  various  features  and attributes  of the concept  (proper-  \nties) and restrictions  on properties.  Ontologies  are part of the \nW3C standard  stack of the Semantic  Web. 8 An ontology  to- \ngether with a set of individual  instances  of classes  constitutes  a \nknowledge  base and offer services  to facilitate  interoperability  \nacross multiple  heterogeneous  systems  and databases.  \n•RDF. Resource  Description  Framework  ( McBride,  2004 ) is a \nW3C recommendation  that deﬁnes  a language  for describ-  \ning resources  on the web. RDF describes  resources  in terms \nof triples,  consisting  of a subject,  predicate  and object. RDF \nSchema  (RDFS) ( Staab & Studer,  2013 ) describes  vocabularies  \nused in RDF descriptions.  \n•OWL. The Ontology  Web Language  is used to deﬁne ontolo-  \ngies on the Web, which extends  RDF and RDFS, but adding  a \nvocabulary.  From a formal description,  OWL is equivalent  to a \nvery expressive  description  logic DL, where an ontology  cor- \nresponds  to a Tbox ( Gruber  et al., 1993 ). In this sense, OWL- \nDL is syntactic  description  that gives maximum  expressive-  \nness while retaining  computational  completeness  and decid- \nability ( McGuinness,  Van Harmelen  et al., 2004 ). In this work, \nwe use OWL-DL  syntax summarized  in Table 1 to formalize  the \nproposed  ontology.  \n•SPARQL  is a query language  for easy access to RDF \nstores. It is the query language  recommended  by \nW3C ( Harris, Seaborne,  & Prud’hommeaux,  2013 ) to work \nwith RDF graphs ( Prud, Seaborne  et al., 2006 ), then supporting  \nqueries  and web data sources  identiﬁed  by URIs. \n•SWRL. The Semantic  Web Rule Language  provides  the \nOWL-based  ontologies  with procedural  knowledge,  which \ncompensates  for some of the limitations  of ontology  in- \nference,  particularly  in identifying  semantic  relationships  \nbetween  individuals  ( Horrocks,  Patel-Schneider,  Bechhofer,  \n& Tsarkov,  2005 ). SWRL uses the typical logic expres-  \nsion “Antecedent  ⇒ Consequent  ”t o represent  semantic  rules. \nBoth antecedent  (rule body) and consequent  (rule head) \ncan be conjunctions  of one or more atoms written  as \n“atom 1 ∧ atom 2 ∧ /22c5/22c5/22c5∧ atom n ”. Each atom is attached  to one or \nmore parameters  represented  by a question  mark and a vari- \nable (e.g., ? x ). The most common  uses of SWRL include  trans- \nferring characteristics  and inferring  the existence  of new indi- \nviduals  ( Grosof & Poon, 2004 ). 9 \n8 https://www.w3.org/standards/semanticweb/  . \n9 https://www.w3.org/Submission/SWRL/  .\n\n[Página 3]\nC. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 545 \n2.2. Related  work \nIn the last decade,  there have been appearing  a series of stud- \nies in which ontological  approaches  are deﬁned  to express  the \nknowledge  domain  in data mining  and optimization  algorithms.  A \nrepresentative  set of these works are compiled  in a recent sur- \nvey ( Dou, Wang, & Liu, 2015 ), in which they are organized  by \ncategories  of algorithms  and applications:  association  rule discov-  \nery ( Marinica  & Guillet,  2010 ), classiﬁcation  ( Allahyari,  Kochut,  & \nJanik, 2014 ) and clustering  ( Jing, Ng, & Huang,  2010 ). In these ap- \nplications,  semantics  is used with different  objectives,  such as: to \nreduce the search space by specifying  restrictions,  to ﬁlter results \nin the post-processing  stage, and to annotate  the results of data \nmining  processes.  \nFollowing  with this research  line, some recent works include  \nontologies  to guide the processes  in machine  learning  tasks. For \nexample,  in Pinto, Scioscia,  Loseto,  and Ruta (2015) and Roldán-  \nGarcía, García-Nieto,  and Aldana-Montes  (2017) , two different  on- \ntologies  are used in the classiﬁcation  process  to infer incon- \nsistencies  between  concepts  by means of semantic  reasoning.  \nIn Phan, Dou, Wang, Kil, and Piniewski  (2015) , an ontology-driven  \ndeep learning  model is proposed  to predict  human  behavior.  \nIn the ﬁeld of optimization,  an interesting  approach  has been \nrecently  proposed  in Yaman,  Hallawa,  Coler, and Iacca (2017) , \nwhere the ECO ontology  is deﬁned  to formally  represent  knowl-  \nedge in evolutionary  computation  algorithms.  This ontology  can \nbe used for suggesting  strategies  for solving  optimization  prob- \nlems. At the same time, an OWL ontology  has been pro- \nposed in Li, Yevseyeva,  Basto-Fernandes,  Trautmann,  Jing, and \nEmmerich  (2017) to model and systematize  the knowledge  of \npreference-based  multi-objective  evolutionary  algorithms.  These \nontologies  are validated  in use cases focused  on algorithmic  and \nparameter  selection  in academic  problems.  \nFrom a different  point of view, a parallel  line of research  focuses  \non deﬁning  ontologies  for the semantic  annotation  of data analytic  \nworkﬂows.  The main objective  is to model the input and output \nof algorithms  involved  in data mining  and knowledge  base discov-  \nery (KDD) workﬂows  to generate  valid compositions.  To this end, \nseveral  OWL ontologies  such as: KDDONTO  ( Diamantini,  Potena,  & \nStorti ), DMWF  ( Kietz, Serban,  Bernstein,  & Fischer,  2010 ) and KD \n( Záková,  Kremen,  Zelezny,  & Lavrac,  2011 ), were proposed.  How- \never, they did not describe  the problem  domain,  or those basic \nconcepts  (algorithm,  type of analysis,  task, dataset,  attribute,  etc.) \nthat can be combined  to deﬁne entities  or constraints.  In fact, \nthese ontologies  were not designed  with the objective  of opti- \nmizing  the performance  of the data mining  algorithms,  since they \ndo not offer detail enough  to provide  support  to what is known  \nas meta-learning.  In Nguyen,  Hilario,  and Kalousis  (2014) , meta- \nlearning  is deﬁned  as the KDD procedure  to improve  performance  \nin data mining  processes,  using information  collected  during the \nexperimentation  phase of these algorithms.  In this regard,  the use \nof semantics  is considered  not only for the algorithmic  composi-  \ntion, but also for the improvement  of data mining  processes,  taking \nadvantage  of acquired  knowledge  from past experience.  \nIn this context,  the EU-FP7  European  initiative  e-LICO 10 pro- \nposed the DMOP ontology  ( Keet et al., 2015 ), which is de- \nﬁned to support  the analytic  workﬂow  composition  by follow-  \ning the standard  CRISP-DM  ( Shearer,  20 0 0 ). DMOP is used to de- \nﬁne analytical  workﬂows,  as well as to describe  algorithms,  pa- \nrameters,  inputs/outputs  and a large amount  of meta-data  in- \ncluded in typical data mining  processes.  A step further  was taken \nby Kumara,  Paik, Zhang, Siriweera,  and Koswatte  (2015) that use \n10 http://www.e-lico.eu/  . Automatic  Service  Composition  to automate  the analytic  workﬂow  \ngeneration.  \nAs a summary,  Table 2 outlines  the main features  of the related  \nwork with regards  to the semantic  approach  proposed  here. These \nfeatures  consist  of specifying  whether  the existing  approaches:  fo- \ncus on data mining  or optimization,  are oriented  to Big Data, pro- \nvide proof-of-concepts,  align with other ontologies,  use OWL/RDF  \nin the semantic  model and/or describe  workﬂow  composition  \ntasks. Then, it is possible  to identify  the actual contributions  of the \nproposed  semantic  model beyond  the state of the art, as follows:  \n•BIGOWL  is conceived  to semantically  model data analytics  in \nBig Data environments.  Similarly  to other ontologies  in the \nliterature,  it is oriented  to general  KDD procedures,  although  \nconsidering  those Big Data ecosystem  elements  with class in- \nstances,  e.g., ontology  individuals.  \n•It is aligned  with the DMOP ontology,  which is in turn aligned  \nwith CRISP-DM.  They have been validated  to construct  data \nmining  workﬂows.  \n•Besides  data mining,  BIGOWL  is also focused  on optimization  \nalgorithms,  although  with special  interest  on covering  multi- \nobjective  metaheuristics  in Big Data environments.  \n•The proposed  approach  is validated  on two real-world  use- \ncases consisting  of classical  data mining  and streaming  data \nprocessing  for multi-objective  optimization.  \n3. Current  practices  in Big Data analytics  \nIn current  Big Data technology  ecosystems,  when facing a spe- \nciﬁc data analytic  task, it is usual to support  on already  existing  \ntools. Some of those consist  in commercial  services  often provided  \nthrough  cloud computing  Software-as-a-Service  (SaaS), which can \nbe used by no skilled people by means of workﬂow  compositions  \n(e.g., Azure ML, Amazon  ML, BigML, Data Mining  Cloud Frame-  \nwork, and Kognitio);  other tools are open-source  frameworks  re- \nquiring  skilled users who prefer to program  their application  using \nmore technical  approaches.  Additional  factors (such as: data for- \nmat, data source,  volume  and velocity  required  to analyse  data) are \nalso determinant  when choosing  the proper technology  ( Zomaya  & \nSakr, 2017 ). Hadoop  ecosystem  represents  the most used frame- \nwork for developing  distributed  Big Data analytic  applications.  \nHowever,  it is conceived  for high skilled users, so even the stan- \ndard workﬂow  composition  service  of Hadoop  (Oozie)  requires  cer- \ntain programming  ability to be properly  used. \nBesides  technological  or commercial  aspects,  current  Big Data \nplatforms  still follow the common  procedure  when facing data an- \nalytics tasks ( ACM-SIGKDD,  2014 ), which comprises  typical steps \nof classical  KDD: data collection,  data transformation,  data mining,  \npattern  evaluation,  and knowledge  presentation.  \nKeeping  this in mind, the proposed  semantic  approach  is ori- \nented to general  KDD procedures,  then leading  the underlying  \nBig Data technological  platform  to be semantically  annotated  with \nclass instances,  e.g., individuals  in the ontology.  \n4. Semantic  model \nOne of the main goals in this study is to capture  all the needed  \nsemantics  to guide the smart design of Big Data analytics  work- \nﬂows and to enhance  their performance.  For this reason,  we opted \nto design an OWL 2 ontology  to describe  analytic  algorithms,  \ndatasets,  problems,  and workﬂows  in the Big Data context.  \nTo this end, the standard  Ontology  101 development  pro- \ncess ( Noy & McGuinness,  2001 ) has been followed,  which com- \nprises seven steps: \n1. Determine  the domain  and scope of the ontology  . The main scope \nof BIGOWL  is data processing  and data analytics  in Big Data en-\n\n[Página 4]\n546 C. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 \nTable 2 \nSummary  ontologies’  features.  \nFeature/Ontology  CRISP-DM  KDDONTO  PMOEA ECO (Pinto’2015)  (Phan’2015)  DMWF KD DMOP BIGOWL \nData Mining /check /check /check /check /check /check /check /check \nOptimization  /check /check /check \nBig Data environments  /check \nProof of concepts  /check /check /check /check /check /check /check /check \nAligned to other ontology  /check /check \nOWL/RDF  /check /check /check /check /check /check /check /check \nWorkﬂow  composition  /check /check /check /check \nFig. 1. Overview  of the BIGOWL ontology.  Continuous  arrows refer to subclasses,  whereas dotted ones refer to properties.  \nvironments.  This considers  not only classical  data analytic  pro- \ncedures,  but also speciﬁc  data processing  and underlying  soft- \nware platform  features  oriented  to Big Data. \n2. Consider  reusing  existing  ontologies  . As commented  before,  the \nproposed  ontology  is aligned  with DMOP, which has been \nsuccessfully  validated  to construct  data mining  workﬂows.  \nDMOP is in turn aligned  with the foundational  ontology  \nDOLCE ( Masolo,  Borgo, Gangemi,  Guarino,  & Oltramari,  2003 ) \nand follows  the standard  CRISP-DM  in the deﬁnition  of data \nmining  processes.  \n3. Enumerate  important  terms in the ontology  . Important  terms \nwere selected  from the literature  related  to Big Data and op- \ntimization.  In addition,  terms from the ontologies  aligned  ( Keet \net al., 2015; Yaman et al., 2017 ) were also incorporated.  Exam- \nples of such terms are: Component,  Workﬂow,  Task, Data, Dat- \naProcessing  and Software  . \n4. Deﬁne the classes and the class hierarchy  . We have followed  a \ntop-down  approach  in developing  the class hierarchy.  This fact \nfacilitates  among  others, the alignment  with DMOP and DOLCE,  \nthe design of annotation  mappings  and the use of a seman-  \ntic reasoner.  Fig. 1 shows the ontology  core classes  and hier- \narchy. For instance,  the class Component  has several  subclasses,  \nincluding  DataAnalysing  and DataCollection  . Classes  modeling  al- \ngorithms,  components  and workﬂows  are aligned  with the class \ndmop:DataType  . BIGOWL  has been developed  using Protégé11 \nand OWL 2. \n11 https://protege.stanford.edu/  . 5. Deﬁne the properties  of classes and slots . With the purpose  of \nrelating  classes  and deﬁning  attributes,  we have included  ob- \nject and data properties.  A representative  set of properties  are \nshown in Table 3 , where the class Component  is related  to class \nAlgorithm  by means of the object property  hasAlgorithm  . Data \nproperties  of class Component  are path, author, numberOfInputs  \nand numberOfOutputs  . \n6. Deﬁne the facets of the slots . This step includes  the deﬁnition  of \ncardinality  constraints  and value restrictions  for the ontology’s  \nproperties.  For example,  the range of the property  order is re- \nstricted  to integer  (to specify  in which step this task is carried  \nout), when the class Task is its domain.  \n7. Create instances  . Instances  or individuals  in BIGOWL  are \nspeciﬁc  of the Big Data analytics  domain.  For exam- \nple, GeneratorDataTraﬃc  is an instance  of the class Kafka , \nwhich is a subclass  of DataIngestion  . The class Kafka has a \nproperty  topicKafka  (with range “string”)  to indicate  streams  of \nrecords  of Apache  Kafka 12 services.  \n4.1. The BIGOWL  ontology  \nBIGOWL  has been developed  following  the steps described  \nabove, producing  184 classes,  16 object properties  (binary  re- \nlationships  between  individuals),  20 data properties  (individ-  \nual attributes),  488 axioms,  66 individuals  and growing.  It is \nworth mentioning  that classes  DM-DataClass  ≡DMDataClass  and IO- \n12 Data Streaming  Processing  https://www.kafka.apache.org/  .\n\n[Página 5]\nC. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 547 \nTable 3 \nComponent:  object and data properties.  \nObject properties  Description  logic \nhasAlgorithm  ∃ hasAlgorithm.Thing  /subsetsqequal Component  \nhasParameter  ∃ hasParameter.Thing  /subsetsqequal Workﬂow  /2294Algorithm  /2294Component  \nisConnected  ∃ isConnected.Thing  /subsetsqequal Algorithm  /2294Component  /2294Task \nisCorrect  ∃ isCorrect.Thing  /subsetsqequal Algorithm  /2294Component  \nspeciﬁesInputClass  ∃ speciﬁesInputClass.Thing  /subsetsqequal Algorithm  /2294Component  /2294Task \nspeciﬁesOutputClass  ∃ speciﬁesOutputClass.Thing  /subsetsqequal Algorithm  /2294Component  /2294Task \nData Properties  Description  Logic \nauthor ∃ author.Datatype  Literal /subsetsqequal Workﬂow  /2294Algorithm  /2294Component  /2294Problem /2294Software  \nhasDataValue  ∃ hasDataValue.Datatype  Literal /subsetsqequal DataType  /2294IO-Class /2294Parameter  /2294Workﬂow  \n/2294Algorithm /2294Component /2294Problem \nnumberOfInputs  ∃ numberOfInputs.Datatype  Literal /subsetsqequal Algorithm  /2294Component  \nnumberOfOutputs  ∃ numberOfOutputs.Datatype  Literal /subsetsqequal Algorithm  /2294Component  \npath ∃ path.Datatype  Literal /subsetsqequal IO-Class /2294Algorithm  /2294Component  \nTable 4 \nTask: object and data properties.  \nObject properties  Description  logic \ncompatibleWith  ∃ compatibleWith.Thing  /subsetsqequal Task /latticetop /subsetsqequal ∀ compatibleWith.Task  \nhasComponent  /latticetop /subsetsqequal ∀ hasComponent.Component  \nisConnected  ∃ isConnected.Thing  /subsetsqequal Algorithm  /2294Component  /2294Task \nspeciﬁesInputClass  ∃ speciﬁesInputClass.Thing  /subsetsqequal Algorithm  /2294Component  /2294Task \nspeciﬁesOutputClass  ∃ speciﬁesOutputClass.Thing  /subsetsqequal Algorithm  /2294Component  /2294Task \nData Properties  Description  Logic \norder ∃ order.Datatype  Literal /subsetsqequal Task /latticetop /subsetsqequal ∀ order.Datatype  \nClass ≡Data are declared  as equivalent  (with relation  ≡) to align \nwith those classes  from other ontologies  (DMOP)  that describe  \nsimilar  concepts.  We use OWL-DL  syntax (see Table 1 ) to formal-  \nize the proposed  ontology.  The complete  ontology  is developed  in \n“bigowl.owl  ”ﬁ l e and available  in the GitHub  repository.  13 \nA representative  set of the main classes  are described  here, to- \ngether with their object and data properties.  These classes  are: \nComponent,  Task, Algorithm,  Data , and Workﬂow  . Each class has de- \nﬁned a set of properties  or conditions  in order to be conceptual-  \nized. That is, an individual  that satisﬁes  those properties  is consid-  \nered to be a member  of that class. \n- Component  . This class represents  each processing  step in the \nanalytic  workﬂow.  It is used to encapsulate  one concrete  function-  \nality, its parameters  and the corresponding  inputs and outputs  it \nconsiders.  The class Component  has four subclasses  that are ori- \nented to deﬁne speciﬁc  functionalities  in typical data analytics  pro- \ncessing  chains:  DataCollection  , to connect  to data sources;  DataPro-  \ncessing , to clean, curate, fuse and consolidate  data; DataAnalysis  , to \nperform  the algorithmic  function;  and DataSink  , to represent  ﬁnal \nsteps in the data ﬂow, e.g., store and visualization.  Table 3 con- \ntains the object and data properties  deﬁned  for Component  . In ac- \ncordance  with these, a component  can specify  Input classes  and \nOutput  classes,  to deﬁne the type of data it is accepting  and gener- \nating, respectively.  Therefore,  a component  can connect  with other \none if their linking  inputs and outputs  are compatible  among  them. \n- Task . A task represents  an instance  of a component  that is \nused in a workﬂow  and can be run. As shown in Table 4 , the class \nTask has similar  properties  to those of Component  , but including  \nthe object property  compatibleWith  , to specify  compatibility  among  \nconnected  tasks, and the data property  order , which indicates  the \nspeciﬁc  step of execution  in which this task is scheduled,  in the \nscope of the workﬂow.  A Component  is then a template  for one or \nmore tasks, which will be used to carry out its speciﬁc  functional-  \nity in a workﬂow.  \n13 URL link https://www.github.com/KhaosResearch/BIGOWL  . - Algorithm  . This class is devoted  to cover all possible  kinds It \nhas two main subclasses:  DataMiningAlgorithm  and OptimizationAl-  \ngorithm  ; which are used to distinguish  between  these two fami- \nlies of algorithms.  The former  one is included  in form of equiv- \nalence with the class DM-Algorithm  , which is linked from DMOP. \nThis way, all subclasses  deriving  from this class in DMOP are also \nused in BIGOWL.  For the later, i.e., OptimizationAlgorithm  , a new hi- \nerarchical  classiﬁcation  of classes  has been elaborated  in this study \nfor the annotation  of this family, which comprises:  Exact, Heuristic  , \nand Metaheuristic  algorithms  as main subclasses.  \nTable 5 includes  the object and data properties  of Algorithm  . \nAmong  its main object properties  it is worth mentioning:  imple- \nments , which is referred  to a learning  model or search strategy;  \nmanages  , to annotate  the type of data it works;  and resolves  , which \nis related  to the Problem  it is oriented  to solve. This is a use- \nful mechanism  to relate classes  Algorithm  and Problem  , which also \nshare the data property  dealWith  that indicates  the speciﬁc  fea- \ntures an algorithm  should fulﬁll to deal with a problem.  \nIn this regard,  the class Problem  deﬁnes  a series of data proper-  \nties like: numberOfConstraints,  numberOfObjectives,  encodedBy  , and \nnumberOfVariables  , that will lead a future reasoner  to recommend  \nthe correct  algorithm  to solve it. These two classes  have to be \ndeclared  as DisjointWith  , in order to avoid future inconsistencies  \nwhen querying  the annotated  data in a workﬂow.  \n- Data . The class Data is devoted  to annotate  all the data ﬂow- \ning throughout  the analytic  workﬂow.  It is declared  as EquivalentTo  \nIO-Class  of DMOP. This aligning  enables  datatypes  deﬁned  by third \nparties’  ontologies  to be contextualized  in the analysis.  Table 6 \ncontains  the main data properties  deﬁned  for this class, namely:  \npath , to annotate  the origin of data; and hasDataType  , which de- \nﬁnes the relation  with class DataType  . This last is used to deﬁne \nthe type of data, i.e. PrimitiveType  (Double,  Integer,  Boolean,  etc.) \nor StructuredType  (Graph,  Tree, Matrix,  Vector,  Tuple, etc.). \n- Workﬂow  . It is used to guide the correct  orchestration  of \nthose tasks involved  in a data analysis  job. Its main object prop- \nerties are hasTask  and hasParameter  , which are formally  described  \nin Table 7 . These properties  are used by the workﬂow  to obtain the \nexecution  order, as well as the input/output  speciﬁcations  of each\n\n[Página 6]\n548 C. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 \nTable 5 \nAlgorithm:  object and data properties.  \nObject properties  Description  logic \nhasComponent  /latticetop /subsetsqequal ∀ hasComponent.Component  \nhasParameter  ∃ hasParameter.Thing  /subsetsqequal Workﬂow  /2294Algorithm  /2294Component  \nspeciﬁesInputClass  ∃ speciﬁesInputClass.Thing  /subsetsqequal Algorithm  /2294Component  /2294Task \nspeciﬁesOutputClass  ∃ speciﬁesOutputClass.Thing  /subsetsqequal Algorithm  /2294Component  /2294Task \nimplements  Transitive  Property  implements  ∃ implements.Thing  /subsetsqequal Algorithm  /latticetop /subsetsqequal ∀ implements.Strategy  \nmanages  ∃ manages.Thing  /subsetsqequal Algorithm  /latticetop /subsetsqequal ∀ manages.DataType  \nresolves ∃ resolves.Thing  /subsetsqequal Algorithm  /latticetop /subsetsqequal ∀ resolves.Problem  \nData Properties  Description  Logic \nauthor ∃ author.Datatype  Literal /subsetsqequal Workﬂow  /2294Algorithm  /2294Component  /2294Problem /2294Software  \nhasDataValue  ∃ hasDataValue.Datatype  Literal /subsetsqequal DataType  /2294IO-Class /2294Parameter  /2294Workﬂow  \n/2294Algorithm /2294Component /2294Problem \nnumberOfInputs  ∃ numberOfInputs.Datatype  Literal /subsetsqequal Algorithm  /2294Component  \nnumberOfOutputs  ∃ numberOfOutputs.Datatype  Literal /subsetsqequal Algorithm  /2294Component  \ndealWith  ∃ dealWith.Datatype  Literal /subsetsqequal Algorithm  /latticetop /subsetsqequal ∀ dealWith.Datatype  \nTable 6 \nData: object and data properties.  \nObject properties  Description  logic \nhasDataType  ∃ hasDataType.Thing  /subsetsqequal Parameter  /2294Data /latticetop /subsetsqequal ∀ hasDataType.DataType  \npath ∃ path.Datatype  Literal /subsetsqequal IO-Class /2294Algorithm  /2294Component  \nTable 7 \nWorkﬂow:  object and data properties.  \nObject properties  Description  logic \nhasTask ∃ hasTask.Thing  /subsetsqequal Workﬂow  /latticetop /subsetsqequal ∀ hasTask.Task  \nhasParameter  ∃ hasParameter  Thing /subsetsqequal Workﬂow  /2294Algorithm  /2294Component  \nData Properties  Description  Logic \nauthor ∃ author.Datatype  Literal /subsetsqequal Workﬂow  /2294Algorithm  /2294Component  /2294Problem /2294Software  \nhasDataValue  ∃ hasDataValue.Datatype  Literal /subsetsqequal DataType  /2294IO-Class /2294Parameter  /2294Workﬂow  \n/2294Algorithm /2294Component /2294Problem \nisCorrectWorkﬂow  ∃ isCorrectWorkﬂow.Datatype  Literal /subsetsqequal Workﬂow  /latticetop /subsetsqequal ∀ isCorrectWorkﬂow.Datatype  \nnumTasks  ∃ numTask.Datatype  /subsetsqequal Workﬂow  /latticetop /subsetsqequal ∀ numTask.Datatype  \ntask. This information,  together  with the data properties  numTasks  \nand isCorrectWorkﬂow  , is then used in reasoning  time to check \nwhether  the workﬂow  is correctly  composed  or not, i.e., to address  \nsemantic  validation  of the analytic  workﬂow.  \n4.2. Overall approach  \nAn overview  of the proposed  semantic  model is illustrated  in \nFig. 2 , which is arranged  together  with the underlying  operational  \nmodel, hence enabling  actual composition  of analytic  workﬂows.  \nIn this approach,  BIGOWL  is the ontological  scheme  driving  the \nwhole process.  It is the terminological  box (TBox) that deﬁnes  the \nvocabulary  with concepts  and properties  in the domain  of Big Data \nanalysis.  As explained  before,  BIGOWL  is developed  in OWL 2 ac- \ncording  to which, concepts  are represented  by classes  and relations  \nare represented  by data properties  or object properties.  As repre- \nsented in Fig. 2 , BIGOWL  is conceived  as an abstract  top-level  on- \ntology that enables  not only subontology  replication  e.g., to focus \non speciﬁc  use cases or algorithmic  families,  but also linkage  with \nexternal  domain  knowledge  ontologies,  which are oriented  to the \nspeciﬁc  problem  domain  (Smart Cities, Biology,  etc.). \nAt bottom-level,  the Assertional  Box (ABox) deﬁnes  all the in- \nstances  in the knowledge  domain  (in OWL 2 an instance  is rep- \nresented  by an individual)  involving  the analytic  workﬂows’  meta- \ndata. These instances  are stored in RDF triple format in a Stardog  14 \nrepository,  which is a commercial  version  of the Pellet OWL 2 rea- \nsoner ( Sirin, Parsia, Grau, Kalyanpur,  & Katz, 2007 ), but enhanced  \nwith persistence  capabilities.  Once the ontology  (Tbox) has been \n14 http://www.stardog.com/  . loaded together  with SWRL rules, a series of reasoning  tasks are \nlaunched  by using the Stardog  OWL 2 reasoner  to derive new infor- \nmation  that is not explicitly  expressed  in the knowledge  base. The \nnew information  will indicate,  when applicable  and among  others, \nwhether  an analytic  workﬂow  is correctly  composed,  or not. \nIn this model, the Annotation  Module  is used to populate  the \nRDF repository  with new instances  that involve  the required  meta- \ndata (annotated)  to be used in workﬂows,  for example:  algorithms,  \noperators,  parameters,  input/output  (paths),  data sources,  database  \nconnections,  data sinks, software,  execution  order, etc. \nThe Operational  Model will make use of these annotated  meta- \ndata for driving  the workﬂow  composition.  In this process,  each \nstep a new component  is to be selected  and used, a SPARQL  query \nis launched  to obtain the required  meta-data  and to suggest  the \nnext possible  component/s  to be included.  \nA very simple (hypothetical)  case of use would comprise  the \nfollowing  steps: \n(i) A user desires  to extract  patterns  from a dataset  and visual- \nize the results;  \n(ii) Then, the user selects one algorithm  from a list of data \nmining  algorithms  (in form of analysis  component)  queried  \nthroughout  the semantic  model;  \n(iii) The selected  algorithm  requires  speciﬁc  input parameters  \nand data to train, so the semantic  model will supply them; \n(iv) The initial dataset  should be then formatted  in form of data \ncollection  task; \n(v) In case collected  data need transformation,  an intermediate  \ndata processing  component  is included  between  collection  \nand analysis;\n\n[Página 7]\nC. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 549 \nFig. 2. General overview  of the semantic  model that follows the ontology’s  scheme of BIGOWL.  The analytic operational  model address the workﬂow  composition  driven by \nthe semantic  model \n(vi) The semantic  model will suggest  suitable  output component  \n(visualization)  to be linked after the analytic  algorithm.  \nIt is worth mentioning  that each step in the workﬂow  is instan-  \ntiated by a task, which entails an execution  order. Then, the entire \nworkﬂow  is arranged  according  to all the ordering  values in tasks. \nIn summary,  the semantic  model acts as a mediator  between  \ndata provider  components  and data consumers.  It also acts as a \ndata source and meta-data  registry  with functions  to make “agree-  \nments” on  the provision  and traceability  of the whole data value \nchain. \n5. Validation  \nFor validation  purposes,  two different  cases of study have been \ndeveloped  to show how the proposed  semantic  approach  is used \nfor driving  the composition  of data analytic  workﬂows.  The ﬁrst \none is focused  on Big Data streaming  processing  and optimiza-  \ntion of real-world  traﬃc routes in the domain  of Smart Cities. The \nsecond  case study is centered  on classic data mining  analysis  on \nacademic  problem  instances,  although  considering  local and cloud \ncomputing  environments.  In this way, we aim at covering,  as much \nas possible,  different  aspects  in Big Data applications:  algorithmic  \nanalyses  (optimization  and data mining),  velocity  and volume  is- \nsues (streaming  processing),  real-world  and academic  data prob- \nlems, and Big Data ecosystems  (Apache  Spark local and on-premise  \ncluster,  BigML cloud SaaS API). \nIn these two cases, a similar  semantic  annotation  and query- \ning procedure  has been followed,  which consists  in the man- \nual annotation  (guided  by domain  experts)  of: algorithms,  tech- \nnological/platform  features,  and attributes  of problem  domain  of \nknowledge;  and automatic  querying  by means of SPARQL  sen- \ntences.  To distinguish  individuals  belonging  to each case study, \ntwo different  namespaces  has been deﬁned,  i.e. traﬃc: http:// \nwww.khaos.uma.es/perception/traﬃc/khaosteam#  and weka:  http: \n//www.khaos.uma.es/perception/weka/khaosteam#  , respectively.  5.1. Case study 1: streaming  processing  of New York City traﬃc \nopen-data  \nThe ﬁrst case study consists  in a dynamic  version  of the \nbi-objective  Traveling  Salesman  Problem  (TSP), to minimize  the \n“travel time” and  the “distance” to  cover certain  routing  points \nin a urban area. The algorithm  for solving  it is a dynamic  variant  \nof the well-known  multi-objective  metaheuristic  NSGA-II  provided  \nin jMetalSP  ( Barba-González,  García-Nieto,  Nebro, Cordero,  Durillo,  \nNavas-Delgado,  & Aldana-Montes,  2017 ), 15 which allows parallel  \nprocessing  of evaluation  functions  in Apache  Spark environment.  \nIn the case of the dynamic  bi-objective  TSP, which is formu-  \nlated in terms of a distance  matrix and a time travel matrix,  the \nperiodic  changes  can affect any of them. Our particular  dynamic  \nTSP problem  instance  is based on real-world  data. Speciﬁcally,  it \nis feed from the Open Data API provided  by the New York City \nDepartment  of Transportation,  16 which updates  traﬃc information  \nseveral  times per minute.  The information  is provided  as a text ﬁle \nwhere each line includes  the average  speed to traverse  the two end \npoints deﬁning  a link in the most recent interval.  The goal is then, \ngiven a list of nodes in New York city and the distances  between  \neach pair of nodes, calculate  the shortest  possible  route that visits \neach node. \nNew York’s traﬃc data is read periodically  by an external  appli- \ncation that writes a ﬁle in HDFS whenever  new data are acquired,  \nso we have implemented  a streaming  data component  for that pur- \npose. This component  reads periodically  the new data appeared  \nin the speciﬁc  directory  (this is done automatically  by Spark) and \nmakes a simple processing:  if a change  in a link is detected  (time \nor distance),  then the corresponding  problem  matrices  are up- \ndated. \nThe analysis  of the streaming  data sources  can be carried  out \nin parallel  by using Spark. In fact, we used a Hadoop  cluster com- \n15 https://www.github.com/jMetal/jMetalSP  . \n16 https://www.data.cityofnewyork.us/Transportation/Real-  Time- Traﬃc- Speed- Data/ \nxsat-x5sa  .\n\n[Página 8]\n550 C. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 \nFig. 3. Workﬂow  for dynamic  bi-objective  optimization  of TSP problem instance with Open Data New York \nposed of 100 cores in the previous  study where the Big Data op- \ntimization  model was presented  ( Barba-González  et al., 2017 ). In \naddition,  two other streaming  data sources  where used as sepa- \nrate components,  which based on Twitter  and Kafka. In the ﬁrst \none, tweets are read from Twitter  API with the topic “New York \ntraﬃc” and  a processing  of each tweet is simulated,  so the prob- \nlem is updated  in accordance  with it (for testing purposes  we set \nrandom  changes  in traﬃc scenario).  This way, we combine  a differ- \nent streaming  source with the possibility  of adjusting  the process-  \ning time, which will serve for performance  evaluation  purposes.  In \nthe second  source,  the idea is to enrich the case study with an- \nother data source that will produce  artiﬁcial  data. Then we created  \na Kafka message  producer  that generates,  following  uniform  and \nnormal  distributions,  a series of random  messages  with data to up- \ndate the problem.  Every 5 s at least 10 0 0 messages  are produced,  \nbut on average  about 10,0 0 0 messages  are created.  Both the Twit- \nter and Kafka streaming  source classes  have the same behavior  as \nthe HDFS based one: they iteratively  collect and analyze  the data \nto somehow  update  the problem.  \nAfter data processing,  the analytic  task is then carried  out, \nwhich entails dynamic  optimization  computed  by NSGAII  algo- \nrithm of the jMetalSP  library.  The results of the analysis  are used \nto feed data sinks. In this case study, we consider  two of them: \none that stores the produced  Pareto fronts in HDFS, and other one \nthat visualizes  information  about the Pareto front approximation  \n(as the number  of solutions  and the number  of generated  fronts) \nusing R-plot library.  \nThe workﬂow  implementing  this case study is represented  in \nFig. 3 , 17 where all the components  are arranged  according  to data \nﬂow. In this workﬂow,  the numeric  indexes  (1)–(7)  correspond  to \nthose steps as indicated  in Table 8 , which contain  the required  \nSPARQL  queries  the semantic  model apply to recommend  forth- \ncoming  component/s  to use, in design time. For this case study, \nthe main set of individuals  annotated  in the semantic  model and \ntheir relationships,  are shown in Fig. 4 . Then it is possible  to follow \nthe complete  process  step-by-step:  \n•Step (1) . The workﬂow  designer  fetch all the optimization  prob- \nlems from BIGOWL  to select the implementation  that better \nﬁts the required  model for TSP instances.  Interestingly,  they are \nall subclasses  of OptimizationProblem  , which is integrated  from \nDMOP. As a result, (s)he selects TSP. \n•Step (2) . Given a problem  to solve, TSP in this case, the seman-  \ntic model recommends  a series of optimization  algorithms  that \ncould deal with it, i.e., those annotated  algorithms  that better \n17 Ontology  instances  available  at https://www.github.com/KhaosResearch/  \nBIGOWL/blob/master/traﬃc.owl  . adapt to the problem  in terms of properties,  such as: solution  \nencoding,  manages,  dealWith  , etc. After this, the designer  selects \nNSGAII.  \n•Step (3) . This is an intermediate  step followed  by the semantic  \nmodel to recommend  speciﬁc  annotated  component  and task \ninstancing  the underlying  software  that implements  TSP and \nNSGAII.  \n•Step (4) . Now, the objective  of this query is to obtain the spe- \nciﬁc data model to properly  host data in problem  and algorithm  \ntasks. This step is thought  to use speciﬁc  domain  knowledge  \ninformation  (traﬃc routes in this case) coming  from external  \nontologies.  The resulting  annotated  instance  here is MatrixNY  , \nwhich refers to a data model comprising  a matrix of points and \ndistances  in the scenario  of New York city. \n•Step (5) . Once the workﬂow  designer  has a clear idea about \nthe data model, (s)he can set data sources  and connect  them \nto feed the analysis.  The semantic  model is then queried  to \nshow all possible  data collectors,  i.e., those previously  anno- \ntated. Among  all the resulting  possibilities,  ReadWebNYDataTraf-  \nﬁc, DataCollectionDataTraﬃcKafka  and DataCollectionTwitter  are \nselected  for this case study. \n•Step (6) . Before connecting  data sources  to analytic  component,  \na previous  task is required  for data processing  and consolida-  \ntion. In this case study, the corresponding  component  is im- \nplemented  as a Spark processing  task to join Kafka messages,  \nTweets  and traﬃc data streams.  \n•Step (7) . Last steps usually  correspond  to data sink tasks to al- \nlocate results from analyses.  For this case study, Visualization-  \nTask and HDFSStoreTask  are selected,  which implement  R-plot \nvisualization  and storage  in HDFS, respectively.  \n•Step (8) . Finally,  the semantic  model is queried  to obtain \nthe corresponding  task instances  that are mutually  compati-  \nble among  them. The analytic  workﬂow  is now ready to be \nlaunched  on the underlying  running  platform.  \nMoreover,  once the whole process  is completed,  a further  rea- \nsoning procedure  can now be started  to check whether  the gen- \nerated workﬂow  is semantically  consistent,  or not. This reasoning  \ntask will be explained  in Section  5.3 . \n5.2. Case study 2: classiﬁcation  with Iris ﬂower dataset  \nAs commented  before,  the second  case study consists  in the \nacademic  problem  of Irish ﬂower classiﬁcation  by means of deci- \nsion tree J48, a classical  algorithm  for data mining  analytics.  For \nmaterialization,  two different  approaches  have been used in this \ncase: the well-known  library for data mining  Weka and the BigML \nSaaS API for analysis  on-cloud.  The aim is to illustrate  how similar  \nannotation  and querying  procedures  with BIGOWL  can be used to\n\n[Página 9]\nC. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 551 \nTable 8 \nSPARQL queries for case study of streaming  processing  of New York city traﬃc open-data.  \nStep SPARQL Result\n(1)SELECT DISTINCT ?problem WHERE {\n?problem rdf:type ?type .\n?type rdfs:subClassOf* dmop:OptimizationProblem .}TSP, ZDT1, ZDT2, ZDT3, ZDT4,\nZDT5, ZDT6, Kursawe..\n(2)SELECT DISTINCT ?algorithm\n(count(DISTINCT ?propertiesAlgorithm) AS numProperties)\nWHERE {\ntraffic:TSP bigowl:encodedBy ?solution.?algorithm rdf:type ?type.?type rdfs:subClassOf* bigowl:OptimizationAlgorithm.\n?entity bigowl:manages ?solution .\n?algorithm bigowl:dealWith ?propertiesAlgorithm .\ntraffic:TSP bigowl:hasFeature ?propertiesTSP .\nFILTER ( ?propertiesTSP in (?propertiesAlgorithm)).\n} GROUP BY ?algorithm ORDER BY DESC(?numProperties)NSGAII, MOCell,\nSMSEMOA,SPEA2, IBEA, PAES,\nPESA2, WASFGA\n(3)SELECT distinct ?comp ?task WHERE {\n?comp bigowl:hasProblem traffic:TSP .\n?comp bigowl:hasAlgorithm traffic:NSGAII .\n?comp rdf:type bigowl:Optimization .?task rdf:type bigowl:Task . ?task bigowl:hasComponent ?comp. }OptmimizationComponent,\nOptimizationTask\n(4)SELECT distinct ?data WHERE {\n?comp bigowl:hasProblem traffic:TSP .\n?comp bigowl:hasAlgorithm traffic:NSGAII .\n?comp rdf:type bigowl:Optimization .\n?task rdf:type bigowl:Task . ?task bigowl:hasComponent ?comp.\n?task bigowl:specifiesInputClass ?data . }MatrixNY\n(5)SELECT distinct ?dataCollection WHERE {\n?dataCollection rdf:type ?type.\n?type rdfs:subClassOf* bigowl:DataCollection.}ReadWebNYDataTraﬃc,\nDataCollectionHDFS,\nDataCollectionDataTraﬃcKafka,\nDataCollectionTwitter,\nDataCollectionDB, ...\n(6)SELECT distinct ?taskProcessing ?compProcessing WHERE {\n?taskCollection bigowl:hasComponent bigowl:ReadNYDataTraffic.\n?taskCollection bigowl:specifiesOutputClass ?out.\n?dataProcessing rdf:type ?typeProcessing .\n?typeProcessing rdfs:subClassOf* bigowl:DataProcessing.\n?taskProcessing bigowl:hasComponent ?dataProcessing .?taskProcessing bigowl:specifiesInputClass ?out.\n?taskProcessing bigowl:specifiesOutputClass traffic:MatrixNY. }SparkTask, ComponentSpark\n(7)SELECT distinct ?dataSink WHERE {\n?dataSink rdf:type ?type.\n?type rdfs:subClassOf* bigowl:DataSink.}VisualizationPlot,\nDataSinkHDFSStore,\nDataSinkOracleStore, ...\n(8)SELECT distinct ?task1 ?task2 WHERE {\n?task1 rdf:type bigowl:Task . ?task2 rdf:type bigowl:Task .\n?task1 bigowl:specifiesOutputClass ?output .\n?task2 bigowl:specifiesInputClass ?output . }GeneratorDataTraﬃcTask,\nSparkTask, TwitterCollectorTask,\nKafkaMGTask,\nReadNYDataTraﬃcTask,\nOptimizationTask, VisualizationTask\ncompose  workﬂows  on different  platforms  when solving  the same \nproblem.  \nFig. 5 shows the individuals  (and their relationships)  anno- \ntated in the ontology,  and Fig. 6 18 represents  graphically  the an- \nalytic workﬂow  for this case study. The numeric  labels (1)–(5)  are \n18 Ontology  instances  available  at https://www.github.com/KhaosResearch/  \nBIGOWL/blob/master/weka.owl  . aligned  with their corresponding  steps in Table 9 that contain  the \nSPARQL  queries  used and their results.  \nIn a nutshell,  steps (1)–(3)  are used to guide the workﬂow  de- \nsigner on the selection  of data model, algorithm,  and analysis  com- \nponents  and tasks, respectively.  Step (4) is used to query suit- \nable data collector  components,  in this case the designer  selects \nDataCollectionBigML  for BigML API instance  and DataCollectorFS  for \nWeka instance  dataset.  Step (5) queries  are devoted  to select possi- \nble data sink components,  and speciﬁcally  DataSinkFSStore  and Vi-\n\n[Página 10]\n552 C. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 \nFig. 4. BIGOWL’s  individuals  annotated  in the workﬂow  for dynamic  bi-objective  optimization  of TSP problem \nFig. 5. BIGOWL’s  individuals  in workﬂow  for Irish ﬂower classiﬁcation  with J48 decision tree instanced  from Weka \nFig. 6. Workﬂow  for Irish ﬂower classiﬁcation  with J48 decision tree instanced  from \nWeka and BigML. sualizationPlot  , which implement  orders to save results in ﬁle sys- \ntem and API method  for plotting  in BigML, respectively.  Finally,  \nstep (6) obtains  the corresponding  task instances  that are mutu- \nally compatible  among  them throughout  the complete  workﬂow.  \n5.3. Reasoning  with BIGOWL  \nReasoning  procedure  is built in BIGOWL  with formulation  of se- \nmantic  rules on top of the OWL ontology,  to deduce  new informa-  \ntion from the existing  knowledge.  These rules are formulated  in \nSWRL and used to perform  semantic  reasoning  jobs mainly  de- \nvoted to check correctness  of workﬂows,  e.i., to discover  those \ncomponents  and tasks with (non-)compatible  connectivity  of in- \nputs/outputs,  execution  orders,  data domains,  data formats,  data\n\n[Página 11]\nC. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 553 \nTable 9 \nSPARQL queries for case study Irish ﬂower classiﬁcation  on Weka, as well as on BigML. \nStep SPARQL Result\n(1)SELECT DISTINCT ?individual\nWHERE {\n?individual rdf:type ?type .\n?type rdfs:subClassOf* bigowl:DMDataClass .\n}Iris, Contact-lens, CPU, Diabetes,\nGlass, Ionosphre, Labor,\nReutersCorn, Segment,..\n(2)SELECT ?algorithm\nWHERE {\nweka:Iris rdf:type ?typeD .?typeD rdfs:subClassOf* ?classSomePropertyAlgorithm.?algorithm rdf:type ?type.?type rdfs:subClassOf* bigowl:DataMiningAlgorithm.\nbigowl:DataMiningAlgorithm rdfs:subClassOf* [\na owl:Restriction ;\nowl:onProperty bigowl:manages ;\nowl:someValuesFrom ?classSomePropertyAlgorithm ] .\n}J48, LogisticRegression, NaiveBayes,\nRepTree, IBk, LinearNNSearch,\nSMO, ...\n(3)SELECT distinct ?comp ?taskWHERE {\n?comp bigowl:hasAlgorithm weka:J48 .?task rdf:type bigowl:Task .\n?task bigowl:hasComponent ?comp. } ClassiﬁcationJ48Component,\nClassiﬁcationJ48Task\n(4)SELECT distinct ?dataCollection WHERE {\n?dataCollection rdf:type ?type.\n?type rdfs:subClassOf* bigowl:DataCollection.}DataCollectionOpenData,\nDataCollectionBigML,\nDataCollectionHDFS,\nDataCollectorFS, ...\n(5)SELECT distinct ?dataSink WHERE {\n?dataSink rdf:type ?type.\n?type rdfs:subClassOf* bigowl:DataSink.}VisualizationPlot,\nDataSinkHDFSStore,\nDataSinkOracleStore,\nDataSinkFSStore, ...\n(6)SELECT distinct ?task1 ?task2 WHERE {\n?task1 rdf:type bigowl:Task . ?task2 rdf:type bigowl:Task .\n?task1 bigowl:specifiesOutputClass ?output .?task2 bigowl:specifiesInputClass ?output . }ClassAsignerIrisTask,\nClassiﬁcationJ48Task,\nClassiﬁerPerformanceEvaluatorTask,\nCrossValidaionFolderMarkerTask,\nTextViewerTask\ntypes, etc. SWRL rules are then evaluated  by the reasoner  after \nclassifying  Big Data components  in accordance  with axioms,  as de- \nﬁned in Table 1 . In concrete,  there are two types of axioms  associ-  \nated with OWL-DL  classes  for reasoning,  namely:  subClassOf  , which \nis used to deﬁne the necessary  conditions  for a class to be consid-  \nered a member  of a given OWL class; and equivalentClass  , for an- \nnotating  when two classes  can be considered  as equivalent,  if they \ncomply  the conditions.  \nBIGOWL  imports  subClassOf  axioms  from DMOP to specify  tax- \nonomy  classiﬁcation  of Data Mining  contexts  and their data. In this \nsense, subclasses  are also the natural  way of describing  hierarchy  \nof algorithmic  families  and versions  in optimization  analyses.  For \ninstance,  Genetic  Algorithms  are subclasses  of Evolutionary  Algo- \nrithms and these in turn, are subclasses  of Population  Based Algo- \nrithms.  This structural  information  is then considered  in reasoning  \ntime for algorithm  recommendation.  The main axioms  for subclass  \nclassiﬁcation  are deﬁned  in Table 10 , which correspond  to Data \nMining  and Optimization  algorithmic  families.  \nFurthermore,  a series of speciﬁc  SWRL rules are described  for \nassessing  the compatibility  of components.  As commented  before,  \nthe main goal is to address  the generation  of well-formed  Big Data \nworkﬂows.  A description  of these rules is as follows:  - Compatibility  between  task, component  and Data Mining  \nalgorithm  . This rule is used to check that input data model is com- \npatible  with the task that is indeed  an instance  (or implementa-  \ntion) of a component.  In this speciﬁc  case, the used component  \nrefers to a Data Mining  Algorithm  to perform  a speciﬁc  analysis.  \nIn short, this rule is used by the reasoner  to validate  compatibility  \nbetween  data mining  component  and data source.  The result is a \npredicate  indicating  that data “feeding” the  component  are com- \npatible  with the analytic  algorithm,  so a task can be launched  to \nrun it on the underlying  platform.  \nbigowl:specifiesInputClass(?task, ?data) ˆ\nbigowl:hasComponent(?task, ?comp) ˆbigowl:hasAlgorithm(?comp, ?alg) ˆbigowl:DataMiningAlgorithm(?alg) ˆ\nbigowl:DMDataClass(?data)\n-> bigowl:isCorrect(?alg, ?data)\nNote that a similar  rule is deﬁned  in the semantic  model to \nconsider  optimization  algorithms.  \n- Compatibility  between  tasks of a workﬂow  . This rule is ap- \nplied to a complete  workﬂow.  It is used to check that input/output  \ndata connections  of each pair of consecutive  tasks are “semanti-\n\n[Página 12]\n554 C. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 \nTable 10 \nOWL axioms for algorithmic  subclass classiﬁcation.  \nClass Classiﬁcation rule\nOptimization AlgorithmOptimizationAlgorithm subClassOf\n((implements some OptimizationStrategy) and\n(resolves some OptimizationProblem)) or Algorithm\nDataMining AlgorithmOptimizationAlgorithm subClassOf\n(manages some DMDataClass) or Algorithm\nOptimization ComponentOptimization subClassOf (hasAlgorithm only\n(OptimizationAlgorithm or MachineLearning))\nDataMining ComponentDataMining subClassOf (hasAlgorithm only\n(DataMiningAlgorithm or MachineLearning))\ncally” similar.  The outcome  is a new predicate  indicating  whether  \neach two consecutive  tasks are mutually  compatible,  or not. \nWorkflow(?w) ˆ\nbigowl:hasTask(?w, ?task1) ˆbigowl:order(?task1, ?ord1) ˆbigowl:hasTask(?w, ?task2) ˆ\nbigowl:order(?task2, ?ord2) ˆ\nswrlb:add(?ord2, ?ord1, 1) ˆbigowl:specifiesInputClass(?task2, ?data)ˆ\nbigowl:specifiesOutputClass(?task1, ?data)\n-> bigowl:compatibleWith(?task1, ?task2)\n- Connectivity  between  tasks and data . Similarly  to the pre- \nvious one, this rule is used to indicate  that two instances  of tasks \nare properly  linked, that is to say, it checks that the input data of \ntask2  are covered  with the output data of task1  , according  to \nthe execution  order established  in the workﬂow.  \nWorkflow(?w) ˆbigowl:hasTask(?w, ?task1) ˆbigowl:order(?task1, ?ord1) ˆbigowl:hasTask(?w, ?task2) ˆ\nbigowl:order(?task2, ?ord2) ˆ\nswrlb:add(?ord2, ?ord1, 1) ˆbigowl:specifiesInputClass(?task2, ?data) ˆ\nbigowl:specifiesOutputClass(?task1, ?data)\n-> bigowl:isConnected(?task2, ?data)\n- Workﬂow  correctness  . Finally,  this rule validates  that all the \ncomponents,  instanced  by corresponding  tasks and data sources,  \nare correctly  arranged  and connected.  The result is then a new \npredicate  indicating  whether  the complete  workﬂow  is correct,  or \nnot. \nWorkflow(?w) ˆbigowl:hasTask(?w, ?task) ˆbigowl:numberOfInput(?task, ?nIn) ˆbigowl:isConnected(?task, ?data).\nsqwrl:makeSet(?set, ?data) ˆ\nsqwrl:groupBy(?set, ?task).sqwrl:size(?cont, ?set) ˆ\nswrlb:equal(?cont, ?nIn)\n-> sqwrl:select(?cont, ?nIn, ?task) ˆbigowl:isCorrectWorkflow(?w, true)In summary,  these case studies  are used as a “proof of concept  ”\nto somehow  highlight  that the proposed  semantic  model is able to \nsupport  in the design of Big Data analytics.  In this regard,  BIGOWL  \nenables  automatic  SPARQL  querying  for component  recommenda-  \ntion, as well as reasoning  procedures  for workﬂow  validation.  \n6. Discussions  \nOne of the main research  ﬁndings  we claim with the design \nand implementation  of BIGOWL  is the ability to represent  and con- \nsolidate  knowledge  involving  Big Data analytics.  This semantic  ap- \nproach  allows us to annotate  (i.e. to “semantize”)  all the meta- \ndata ﬂowing  from multiple  data sources,  processing  components  \nand analytic  algorithms.  The meta-data  are integrated  following  \nthe BIGOWL  structure  and stored in an RDF repository.  \nOn the one hand, the results obtained  in the two case stud- \nies indicate  that, driven by the ontological  model, it is possible  \nto progressively  deliver  component  recommendations  for the con- \nstruction  of Big Data analytics  workﬂows.  The resulting  workﬂows  \nare indeed  enhanced  with semantic  knowledge  that explicitly  de- \nscribes  and registers  the data lineage  (data provenance  in database  \nsystems),  from sources  to results.  It also would enable to replay \nspeciﬁc  portions  or inputs of the data ﬂow for step-wise  debug-  \nging or regenerating  lost outputs.  In the BIGOWL  semantic  model, \ndata linage is mapped  with RDF triples referring  to records  of the \ninputs, entities,  systems,  algorithms  and processes  that inﬂuence  \ndata of interest,  hence providing  a historical  record of the data ob- \ntained (as results)  and its origins  (as sources).  \nBased on the analysis  provided  in the two cases studies,  the \nuser is able to identify  the correct  path the data follow and how \nthey are modiﬁed  to obtain added value, for a given domain  of \nknowledge.  For example,  in the ﬁrst case study, a series of data \nsources  involving  information  about urban traﬃc in the city of \nNew York (with geo-locations,  travel times, densities,  tweets,  etc.) \nare semantically  related  (or linked)  to the results obtained,  in form \nof optimized  routes in a problem  characterization  of the classical  \nTSP. In this case study, the outputs  are encoded  in form of routes, \nwhere the travel time and the routing  distance  are optimized.  This \nway, the resulting  routes are linked to the traﬃc densities  and the \nTwitter  messages,  so the data lineage  is registered  with semantic  \nannotations.  \nSimilarly,  in the second  case study, it is possible  to connect  \nprediction  accuracies  with classiﬁcation  algorithms,  for the Irish \nﬂower database.  In addition,  the running  experiences  acquired  \nwhen using different  execution  frameworks,  e.g., in-house/in-cloud,  \nare also annotated  as results.\n\n[Página 13]\nC. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 555 \nAnother  important  ﬁnding  lies in the possibility  of using the \nsemantic  knowledge-base,  now consolidated  in the RDF repository,  \nto perform  reasoning  tasks, hence to infer new knowledge.  In this \nstudy, a series of SWRL rules are used to train the reasoner.  In this \nstudy, a reasoner  is used to evaluate  a set of SWRL rules deﬁned  \nfor the speciﬁc  task of workﬂow  validation.  In this regard,  the val- \nidation  analysis  performed  by the reasoner  required  644 ms for \ncase study 1 and 673 ms for case study 2. Taking into account  that \nwe used the Stardog  OWL 2 reasoner,  the time spent in reasoning  \ntasks is acceptable  for workﬂow  validation.  \nOn the other hand, the main constraint  of the proposed  seman-  \ntic model is that it needs a domain  ontology  to cover the prob- \nlem knowledge  domain.  This domain  ontology  contains  the spe- \nciﬁc concepts  for a given case, so it can be reused in domains  \nwhere previous  efforts provided  such model. However,  if such on- \ntology is not available,  then its design is required.  As explained  \nin Section  4.1 , the class Data in BIGOWL  is used, not only to an- \nnotate all the data ﬂowing  in the analytic  workﬂow,  but also to \nallow alignment  with third parties’  ontologies  covering  the spe- \nciﬁc problem  domain  of knowledge.  Additionally,  the general  on- \ntology could miss concepts  that would be needed  in some cases \nand are not described  in the current  model. This constraint  can be \nsolved by proposing  an extension,  in form of new version  release  \nof BIGOWL,  though  a collaborative  portal. In this sense, BIGOWL  is \npublicly  available  at WebProtégé,  19 where any registered  user can \nintroduce  changes.  These changes  will be reviewed  in a regular  ba- \nsis to approve  or reject them. The last stable version  of the ontol- \nogy will be provided  in the project  GitHub  repository.  20 \nIn addition,  a secondary  constraint  arises when a new workﬂow  \nis generated  or executed  by a user, since a series of new annota-  \ntions are required  to store all the meta-data  involved  in the data \nanalytic  process,  in form of RDF triples.  This makes the RDF repos- \nitory to increase  signiﬁcantly,  which would promote,  not only fu- \nture reasoning  procedures  to infer new knowledge  from these data, \nbut also their connection  with other Linked Data. In this sense, \nthe eﬃcient  management  of large RDF repositories  has become  a \nchallenging  task attracting  many scholars  to research  ( Zomaya  & \nSakr, 2017 ), which means a clear implication  for academia.  \nIn terms of practical  implications,  the proposed  semantic  model \nrepresents  an initial demonstrator  for the experimental  piloting  of \nBig Data frameworks  enhanced  with semantics.  The objective  is to \nobtain “Smart  Data” and  promote  the data value chain in industry  \nprocesses,  which is a key challenge  nowadays  as reﬂected  in the \nStrategic  Research  and Innovation  Agenda  of the Big Data Value As- \nsociation  (EU SRIA 4.0 BDVA).  21 Several  industrial  projects  in this \nassociation,  like BigDataEurope  22 and BigOceanData,  23 are focused  \non exploiting  semantics  in Big Data analytics,  so they could par- \ntially take advantage  of BIGOWL  as reference  ontological  model. \n7. Conclusions  \nIn this work, an ontological  approach  called BIGOWL  is pro- \nposed to provide  a conceptual  framework  for the annotation  of \nBig Data analytics.  The proposed  semantic  model is materialized  \nby means of an RDF repository,  and programmatic  querying  and \nreasoning  functions.  \nTo test the initial hypothesis,  two case studies  have been devel- \noped, which consist  in: (1) real-world  streaming  traﬃc data pro- \ncessing  for route optimization  in urban environment,  and (2) aca- \ndemic data mining  classiﬁcation  on local/on-cloud  platforms.  The \n19 WebProtégé https://www.goo.gl/F6fYUc  . \n20 GitHub https://www.github.com/KhaosResearch/BIGOWL  . \n21 http://www.bdva.eu/sites/default/ﬁles/BDVA  _ SRIA _ v4 _ Ed1.1.pdf  . \n22 https://www.big-  data- europe.eu/  . \n23 http://www.bigoceandata.com/  . experience  on these cases revealed  that BIGOWL  approach  is useful \nwhen integrating  knowledge  domain  concerning  a speciﬁc  analytic  \nproblem.  Consequently,  the integrated  knowledge  is used for guid- \ning the design of Big Data analytics  workﬂows,  by recommending  \nnext components  to be linked, and supporting  ﬁnal validation.  \nIt is worthy  to declare  that the proposed  semantic  model is cur- \nrently populated  with those annotated  elements  required  to set the \ncase studies  reported  in this work, although  it can be feed with \nnew instances  regarding  other Big Data workﬂows.  \nThis motivates  our future research  agenda,  which entails a \nﬁrst phase to provide  automatic  facilities  for ontology  population,  \nhence to enrich the semantic  approach;  second,  to provide  new \nmechanisms  to promote  the use of contextual  domain  of knowl-  \nedge in the generation  of Big Data analytic  solutions;  and third, to \ngenerate  new and heterogeneous  use cases of analytics  workﬂows  \nthat would led us to ﬁnd and solve new possible  deﬁciencies,  as \nwell as to enrich the knowledge  base. \nReferences  \nACM-SIGKDD  (2014). Data mining curriculum.  ACM SIGKDD 2006-04-30.  Retrieved  \n2014-01-27.  \nAllahyari,  M. , Kochut, K. , & Janik, M. (2014). Ontology-based  text classiﬁcation  into \ndynamically  deﬁned topics. In 2014 IEEE international  conference  on semantic  \ncomputing  (pp. 273–278)  . \nBarba-González,  C. , García-Nieto,  J. , Nebro, A. J. , Cordero, J. A. , Durillo, J. J. , \nNavas-Delgado,  I. , et al. (2017). Jmetalsp:  A framework  for dynamic  multi-ob-  \njective big data optimization.  Applied Soft Computing  . In–Press–Online  \nDiamantini,  C., Potena, D., & Storti, E.. Ontology-driven  kdd process composition.  \nDou, D. , Wang, H. , & Liu, H. (2015). Semantic  data mining: A survey of ontolo- \ngy-based  approaches.  In Semantic  computing  (icsc), 2015 ieee international  con- \nference on (pp. 244–251).  IEEE . \nGrosof, B. N. , & Poon, T. C. (2004). SweetDeal:  Representing  agent contracts  with \nexceptions  using semantic  web rules, ontologies,  and process descriptions.  In- \nternational  Journal of Electronic  Commerce,  8 (4), 61–97 . \nGruber, T. R. (1995). Toward principles  for the design of ontologies  used for \nknowledge  sharing?  International  Journal of Human-Computer  Studies, 43 (5–6), \n907–928  . \nGruber, T. R. , et al. (1993). A translation  approach  to portable ontology  speciﬁca-  \ntions. Knowledge  Acquisition,  5 (2), 199–220  . \nHarris, S. , Seaborne,  A. , & Prud’hommeaux,  E. (2013). Sparql 1.1 query language.  W3C \nRecommendation,  21 (10) . \nHorrocks,  I. , Patel-Schneider,  P. F. , Bechhofer,  S. , & Tsarkov, D. (2005). OWL rules: \nA proposal  and prototype  implementation.  Web Semantics:  Science, Services and \nAgents on the World Wide Web, 3 (1), 23–40 . \nJing, L. , Ng, M. , & Huang, J. (2010). Knowledge-based  vector space model for text \nclustering.  Knowledge  and Information  Systems, 25 (1), 35–55 . \nKeet, C. , Ławrynowicz,  A. , d’Amato,  C. , Kalousis,  A. , Nguyen, P. , & Palma, R. (2015). \nThe data mining optimization  ontology.  Web Semantics,  32 , 43–53 . \nKietz, J. , Serban, F. , Bernstein,  A. , & Fischer, S. (2010). Data mining workﬂow  tem- \nplates for intelligent  discovery  assistance  and auto-experimentation.  In Proceed- \nings- of the ecml/pkdd:  10 (pp. 1–12) . \nKonys, A. (2016). Ontology-based  approaches  to big data analytics.  In International  \nmulti-conference  on advanced  computer  systems (pp. 355–365)  . \nKuiler, E. W. (2014). From big data to knowledge:  An ontological  approach  to big \ndata analytics.  Review of Policy Research,  31 (4), 311–318 . \nKumara, B. T. G. S. , Paik, I. , Zhang, J. , Siriweera,  T. H. A. S. , & Koswatte,  K. R. C. (2015). \nOntology-based  workﬂow  generation  for intelligent  big data analytics.  In 2015 \nieee international  conference  on web services (pp. 495–502)  . \nLi, L. , Yevseyeva,  I. , Basto-Fernandes,  V. , Trautmann,  H. , Jing, N. , & Em- \nmerich, M. (2017). Building and using an ontology  of preference-based  multi- \nobjective  evolutionary  algorithms.  In H. Trautmann,  G. Rudolph,  K. Klamroth,  \nO. Schütze, M. Wiecek, Y. Jin, & C. Grimme (Eds.), Evolutionary  multi-criterion  \noptimization:  9th international  conference,  EMO 2017, Münster, Germany,  March \n19–22, 2017, proceedings  (pp. 406–421).  Cham: Springer International  Publish- \ning . \nMarinica,  C. , & Guillet, F. (2010). Knowledge-based  interactive  postmining  of associa- \ntion rules using ontologies.  IEEE Transactions  on Knowledge  and Data Engineering,  \n22 (6), 784–797  . \nMasolo, C. , Borgo, S. , Gangemi,  A. , Guarino,  N. , & Oltramari,  A. (2003). Wonderweb  \ndeliverable  d18, ontology  library (ﬁnal). ICT Project, 33052 . \nMcBride,  B. (2004). The resource description  framework  (rdf) and its vocabulary  de- \nscription  language  rdfs. In Handbook  on ontologies  (pp. 51–65). Springer . \nMcGuinness,  D. L. , Van Harmelen,  F. , et al. (2004). Owl web ontology  language  \noverview.  W3C Recommendation,  10 (10), 2004 . \nNguyen, P. , Hilario, M. , & Kalousis,  A. (2014). Using meta-mining  to support data \nmining workﬂow  planning  and optimization.  Journal of Artiﬁcial Intelligence  Re- \nsearch, 51 , 605–644  . \nNoy, N. , & McGuinness,  D. L. (2001). Ontology  development  101: A guide to creating \nyour ﬁrst ontology.  Technical  report .\n\n[Página 14]\n556 C. Barba-González  et al. / Expert Systems With Applications  115 (2019) 543–556 \nNoy, N. F., McGuinness,  D. L. et al. (2001). Ontology  development  101: A guide to \ncreating your ﬁrst ontology.  \nPhan, N. , Dou, D. , Wang, H. , Kil, D. , & Piniewski,  B. (2015). Ontology-based  deep \nlearning for human behavior  prediction  in health social networks.  In Proceed- \nings of the 6th ACM conference  on bioinformatics,  computational  biology and health \ninformatics  (pp. 433–442).  ACM . \nPinto, A. , Scioscia, F. , Loseto, G. , Ruta, M. , Bove, E. , & Sciascio, E. D. (2015). A seman- \ntic-based  approach  for machine  learning data analysis. In 2015 IEEE international  \nconference  on semantic  computing  (ICSC) (pp. 324–327)  . \nPrud, E. , & Seaborne,  A. (2006). Sparql query language  for rdf. W3C Recommendation  . \nRistoski, P. , & Paulheim,  H. (2016). Semantic  web in data mining and knowledge  \ndiscovery:  A comprehensive  survey. Web Semantics:  Science, Services and Agents \non the World Wide Web, 36 , 1–22 . \nRoldán-García,  M. , García-Nieto,  J. , & Aldana-Montes,  J. F. (2017). Enhancing  seman- \ntic consistency  in anti-fraud  rule-based  expert systems. Expert Systems with Ap- \nplications,  90 (Supplement  C), 332–343  . Shearer, C. (20 0 0). The crisp-dm  model: The new blueprint  for data mining. Journal \nof Data Warehousing,  5 (4), 13–22 . \nSirin, E. , Parsia, B. , Grau, B. C. , Kalyanpur,  A. , & Katz, Y. (2007). Pellet: A practical  \nowl-dl reasoner.  Web Semantics:  Science, Services and Agents on the WWW, 5 (2), \n51–53 . \nStaab, S. , & Studer, R. (2013). Handbook  on ontologies  . Springer Science & Business  \nMedia . \nYaman, A. , Hallawa, A. , Coler, M. , & Iacca, G. (2017). Presenting  the ECO: Evolution-  \nary computation  ontology.  In European  conference  on the applications  of evolu- \ntionary computation  (pp. 603–619)  . \nZáková, M. , Kremen, P. , Zelezny, F. , & Lavrac, N. (2011). Automating  knowledge  dis- \ncovery workﬂow  composition  through ontology-based  planning.  IEEE Transac- \ntions on Automation  Science and Engineering,  8 (2), 253–264  . \nZomaya, A. Y. , & Sakr, S. (2017). Handbook  of big data technologies  (1st). Springer \nInternational  Publishing  .",
      "metadata": {
        "filename": "BIGOWL2019.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\BIGOWL2019.pdf",
        "file_size": 2271408,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:36.608723",
        "content_length": 74849
      }
    },
    "036993c2-b385-494a-a35a-12b1a21af260": {
      "id": "036993c2-b385-494a-a35a-12b1a21af260",
      "content": "[Página 1]\nTesting of big data analytics systems by benchmark \n \nMingang Chen \nShanghai Key Laboratory of Computer Software Testing \nand Evaluating  \nShanghai Development Center of Computer Software \nTechnology \nShanghai, China \ncmg@ssc.stn.sh.cn Wenjie Chen, Lizhi Cai \nShanghai Key Laboratory of Computer Software Testing \nand Evaluating \nShanghai Development Center of Computer Software \nTechnology \nShanghai, China \ncwj@ssc.stn.sh.cn, clz@ssc.stn.sh.cn \n \n \nAbstract —With the rapid development of big data \ntechnologies and applications, various big data analytics systems \nhave been released by open source communities and industry. So \ntesting and evaluating the overall performance of these big data \nanalytics systems has become an important research topic. The \npaper analyzes in detail the challenges of testing big data \nanalytics systems and proposes the method and strategies for the \ntesting. Furthermore, the paper presents two cases of testing big \ndata analytics systems by benchmark. \nKeywords—testing; big data; benchmark; TPC-DS; TPCx-\nBigBench. \nI.  INTRODUCTION  \nIn recent years, big data has become a hot topic for \ngovernments and enterprises, and it is considered as a new \ndriving force for innovation in the information era. This is \nbased on the following two facts: firstly, in the past ten years, \nthe speed of data generating is becoming faster and faster, and \nwe have already entered the big data era; secondly, big data \ncontains huge values, and has brought about revolutionary \ndevelopments in many fields, such as e-commerce, finance, \ntransportation, medical and health service, etc. \nHowever, the “3V” characteristics (volume, variety, and \nvelocity) of big data make challenges for data processing and \nanalytics.  Recently, industry and academia have launched a \nvariety of big data analytics system to cope with the challenges, \nsuch as open source Apache Hive [1], Apache Spark [2], and \ncommercial Transwarp Inceptor, Cloudera Impala, IBM Big \nSQL and so on. More and more enterprises or organizations \nuse big data analytics system to build the business application \nand obtain decision support from data. Therefore, testing and \nevaluating big data analytics systems has become one of the \nimportant research subjects of the big data fields. \nTesting of big data analytics system mainly has the \nfollowing three roles. (1) We can verify the correctness of \nfunctionalities and the reliability of the big data analytics \nsystem before it is deployed and put to use. (2) We can carry \nout a fair comparison of the performance of different big data \nanalytics systems. (3) We can optimize the performance of big \ndata analytics systems by testing. \nPresently, testing of big data analytics system mainly uses \nbenchmarks, and by benchmark testing, we can analyze and evaluate the functionalities, performance, reliability, and \ncompatibility of the system. There are three categories of \nbenchmark in the testing of big data analytics systems. The first \ncategory is the micro benchmark. This category of benchmark \nprincipally aims at testing a certain component of the big data \nanalytics system thus is also called component-level \nbenchmark. Such as TeraSort can only be used to test the \nsystem’s performance for sorting text data, and GridMax can \nonly be used to test the performance of various MapReduce job \nin the Hadoop clusters. Therefore, the micro benchmark cannot \nevaluate the performances of big data analytics system entirely. \nThe second category is the comprehensive benchmark. This \ncategory of the benchmark can test more than one components \nof big data analytics system. For example, Hibench is a \ncomprehensive benchmark, and its workload including micro \nbenchmarks, web search, SQL query and machine learning [3]. \nThe third category is the application oriented benchmark, \nwhich is characterized by simulating the scenario of big data \napplications in the enterprise. TPC-DS is a benchmark for \ntesting big data decision support systems [4, 5]. TPCx-\nBigBench [6, 7] is the first end-to-end, application-level big \ndata benchmark based on TPC-DS. Due to the standardization \nand usability of TPC-DS and TPCx-BigBench, more and more \norganizations begin to use these two benchmarks to test, \nevaluate and compare the overall performance of big data \nanalytics systems. \nThis paper will discuss in detail the challenges of testing big \ndata analytics systems in Part II, and propose method and \nstrategies of how to test big data  analytics systems in Part III. \nIn Part IV, two cases of testing will be presented, that is testing \nof Transwarp Inceptor by TPC-DS and performance \ncomparison of Hive and Spark SQL by TPCx-BigBench. In \naddition, some preliminary analysis will be made on how to \noptimize the performance of Spark SQL by benchmark testing. \nFinally, we conclude the paper in section V. \nII. THE CHALLENGES OF TESTING BIG DATA ANALYTICS SYSTEM  \nDue to the “3V” characteristics of big data and the \ncomplexity of big data analytics system, this brings about \nchallenges for testing big data analytics system. \nFirst is the complexity of the technologies on big data \nanalytics system. It generally adopts distributed architectures, \nsuch as master-slave or peer-to-peer. And factors that will \nThis work was funded by Science and Technology Commission of \nShanghai Municipality Program (16511101202, 17411952800).  \n2312018 IEEE International Conference on Software Testing, Verification and Validation Workshops\n0-7695-6432-1/18/$31.00 ©2018 IEEE\nDOI 10.1109/ICSTW.2018.00054\n\n[Página 2]\naffect the performance of the system under test are complex, \nsuch as network environment, hardware configurations, system \nconfiguration parameters, and virtualization etc. For instance, \nHadoop system has over 200 configuration parameters. \nSecond is the complexity of test datasets. The test datasets \nof big data analytics system need not only to meet the “3V” \ncharacteristics of big data but also to represent typical business \nscenes. \nThird are the challenges of testing methods and tools, such \nas the traditional testing tools can no longer be appropriate, \nlacking automatic testing methods and the customization of \ntesting and diagnosing schemes. Different modules in the big \ndata analytics require different testing techniques. For example, \nwe test the performance of Spark SQL by SQL’s queries, while \nwe test throughput and latency of Spark Streaming by loading \nstreaming data. \nFourth, the testing of big data analytics system requires \nmore professional and more comprehensive testing abilities.  \nTesters not only need to have the testing expertise but also need \nto master the big data analysis and processing technology. For \nexample, testers need to know how to load data from Hadoop \nHDFS into a Hive table and verify if the loading is correct.  \nIII. BENCHMARK TESTING METHOD OF BIG DATA ANALYTICS \nSYSTEM  \nThe testing of big data analytics system with benchmark \ncan generally be divided into 6 phases, that is requirement \nanalysis for testing big data analytics systems, preparing the \ntesting environment, preparing the test datasets and workload, \nloading the test datasets, testing for the big data analytics \nsystem and analysis of the testing result, as shown in Fig.1.  \n \nFig.1. Benchmark testing method of big data analytics system  \nA. Requirement analysis of testing for big data analytics \nsystems \nThe phase of requirement analysis for testing big data \nanalytics systems is by and large same as traditional software \ntesting, including specifying the objects of testing, the purposes \nof testing, the environment of testing, the datasets of testing,  \ntechnology and tools of testing and the risk of testing, etc. But the key point of testing big data analytics system is the \nperformance and reliability of the system. For example, how \nefficient is the system's processing and analysis of data with \nlarge-scale datasets? Whether tasks of data processing can be \nmigrated automatically or not when a node in the cluster goes \ndown? Will the data be lost in a distributed environment when \na node crashes? \nB. Preparing the testing environment \nIn order to test a big data analytics system, we need to \nprepare a cluster of distributed data storage and computing, at \nthe same time a sufficient storage space is required to store and \nanalyze the large-scale datasets. It is worth noticing that the \nstorage space here not only refers to the hard disk space but \nalso memory space, especially in testing Apache Spark, due to \nthe 60% occupation of memory is used for buffering RDD (the \ndata structure of Spark), so enough memory space should be set \napart for the testing program. We should be careful that, the \ntesting environment should be ensured “clean”. In other words, \nwe should ensure that there is no other applications running in \nthe cluster, the CPU and memory of the node in the cluster are \nboth at their minimum utilization. \nC. Preparing the test datasets and workload \nThe datasets for testing big data analytics system comes \nfrom two sources: one is the real data from business, such as \ndata from weblogs or database of business; the other is \nsimulated data generated by big data benchmarking tools. TPC-\nDS and TPCx-BigBench are two benchmarks that have been \nnominated in the industry. It should be noted that we should set \nappropriate data scale, data type, and data model according to \nthe requirement of the testing. The workload is the core of \nperformance testing of big data analytics system. It needs to \nreflect business scenarios and data analytical techniques. The \nworkload in TPC-DS or TPCx-BigBench is the set of queries to \nbe executed against the test datasets.  \nD. Loading the test datasets \nDuring the phase of loading test datasets, we should verify \nif the data has been loaded correctly into the distributed storage \nsystem. For example, whether the data is loaded into the right \nHDFS storage directory? Is the size of data file correct? If the \ndata need to be loaded into the distributed database system, we \nshould verify if the data can be load into the table in the \ndatabase correctly. \nE. Testing of the big data analytics system \nTesting of the big data analytics system needs to focus on \nsystem’s functionality, performance, reliability, and \ncompatibility. \n1) Functionality testing \nThe Functionality testing of big data analytics system mainly \nverifies whether functions of the system in data storage, data \nprocessing, data I/O etc. are correct? For example, whether \ndata processing based on MapReduce is correct? Whether the \nresults of SQL queries on the SQL-On-Hadoop system are \ncorrect? And whether the data I/O is complete? \n232\n\n[Página 3]\n2) Performance testing \nThe performance testing of big data analytics system needs \nto test the performance of data I/O, data processing and \nanalytic and the performance of SQL query on the system and \nso on. For example, we can test the reading and writing \nperformance of Hadoop HDFS using single large data file or \nmultiple large data files.  For SQL-On-Hadoop systems, the \nperformance of SQL query is the most important performance \nmetric.  \n3) Reliability testing \nThe reliability testing of big data analytics system needs to \nfocus on the following two aspects:  \n• If the task can be automatically migrated when a task \nof data analytics failed at a certain node (may be due to lack of \nmemory), so as to ensure the task is executed correctly? \n•  If one or some nodes in the cluster go down, will the \ntask of the data analytic be executed correctly due to the fault-\ntolerant mechanism of the system? \n4) Compatibility Testing \nThe compatibility testing of big data analytics system needs \nto verify the compatibility of the file system, the compatibility \nof data storage format, the compatibility of SQL syntax and so \non. \nF. Analysis of the testing result \nDuring the phase of analysis of the testing result, we need \nto analyze system’s testing metrics (functionality, performance, \nreliability, and compatibility) comprehensively according to the \ntesting requirement and finish the testing report. \nIV. CASES OF TESTING BIG DATA ANALYTICS  \nAccording to the test method described in Part ċ, in this \nsection, we present two cases of testing big data analytics \nsystem. \nA. Testing for Transwarp Inceptor by TPC-DS  \n1) Requirement analysis of testing Transwarp Inceptor  \nThe purpose of testing Transwarp Inceptor is to verify the \nfunctionality of ETL ˈand evaluate the performance of SQL \nquery and compatibility of SQL syntax through automated \ntesting scripts. The method of testing follows the TPC-DS \nspecification. \n2) The system under test and environment \na) Transwarp Inceptor big data analytics system \nInceptor is a commercial big data analytics system \ndeveloped by Transwarp Technology Co., Ltd. It provides \nhigh-speed SQL analytics based Apache Spark. It can help \nbusinesses to build high-speed, scalable data warehouses, and \nperform interactive analysis, real-time reporting, and \nvisualization of data. Transwarp Inceptor has a three-tier \nstructure from bottom to top: the storage layer, the distributed computing engine layer and the interface layer, as is shown in \nFig.2. \n \nFig.2. Architecture of Transwarp Inceptor \nb) Test environment \nThe test environment consists of four physical servers, and \nthe configurations of servers are same, as is shown in Table I. \nFour servers make up a Transwarp cluster through Gigabit \nnetwork.  \nTABLE I.  THE HARDWARE CONFIGURATION OF THE TESTING SERVERS  \n Node1 Node2 Node3 Node4 \nModel Dell PowerEdge R720 \nCPU Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz \n ( 2 CPU x 6 cores) \nMemory \n(GB) 256 256 256 256 \nStorage 24 TB HDD hard drive \nOperating \nSystem Red Hat Enterprise Linux 6.5 \nHadoop Transwarp DataHub v3.4 Hadoop 2.2 \nInceptor Transwarp Inceptor v4.0 \nRoles Primary \nNameNode, \nInceptor Server, \nDataNode Secondary \nNameNode, \nInceptor \nMetaStore, \nDataNode  \nDataNode  \nDataNode \n3) Generating test datasets and workload by TPC-DS \na) TPC-DS \nTPC-DS is testing benchmark for decision support system \nproposed by TPC (Transaction Processing Performance \nCouncil). TPC-DS models the decision support functions of a \nretail product supplier. The business model of benchmark \nsimulates sales and returns of the three main channels (stores, \nonline retailers, and catalogs). The business model contains 7 \nfact tables and 17 dimension tables, and tables are organized by \nstar and snowflake mixed model. A reduced business model of \nTPC-DS is shown in Fig.3.  \n233\n\n[Página 4]\nFig.3. TPC-DS database schema \nTPC-DS allows users to generate the different scale of \ndatasets from 100G to 100T according to the user’s test \nrequirements and test environment. In general, the TPC-DS \nbenchmark has following characteristics: \n• A large amount of business data and test cases (SQL \nqueries) can answer real business problems. \n• A total of 99 SQL queries follow the SQL 99 and SQL \n2003 core syntax standard, and SQL queries are \ncomplex. \n• The test cases include a variety of business models, \nsuch as interactive query, statistical analysis, iterative \nOLAP and data mining. \n• Almost all of the test cases need high I/O loading and \nCPU computing. \nb) The generation of test datasets and workload \nIn this phase, we use the data generation and query \ngeneration tools (DSTools v1.3.0) provided by the TPC-DS \nbenchmark to generate 500GB test datasets and 99 SQL \nqueries through automated shell scripts, and the script fragment \nis as follows. \n# Generate 500GB test datasets in the specified HDFS directory \n1: dbgen2 -scale 500 -dir HDFS_LOCATION \n \n# Generate 99  queries compatible with Oracle syntax for 500GB \ndatasets through the query template \n2: qgen2 –query99.tpl –directory QUERY_TEMPLATE –dialect \noracle  -scale 500  \nThe 500GB test datasets consist of 24 tables of the database \n(7 fact tables and 17 dimension tables) mentioned above. The \n99 SQL queries implement business intelligence by answering \nreal business questions. \n4) Data loading \nIn the data loading phase, we first create 24 tables in \nTranswarp Inceptor to build the data warehouse for testing. The \nschemas of tables are provided by the TPC-DS benchmark. \nThen we load the datasets that have been generated in the \nHDFS into tables. The following script fragment shows how to \nload datasets in HDFS into the inventory table. \n# load inventory.dat into the inventory table \n1: LOAD DATA  inpath '/tpc_ds/data/inventory.dat' INTO TABLE \ninventory;  \n5) Testing for Transwarp Inceptor \nThe core of the TPC-DS based benchmark testing is the \nexecution of 99 SQLs one by one. In testing, we verify the \ncorrectness of the test results and record the execution time of \nSQL. We execute 99 SQLs with automated scripts by three rounds and take the average time of three rounds as SQL’s \nexecution time. The following script fragment shows how to \nexecute 99 SQL queries sequentially in Transwarp Inceptor. \n# Execute all 99 SQL queries one by one  \n1: for(i = 1; i<=99; i++ ){ \n2:   sql = \"query\"+  i + \".sql\";  \n3:   system( \"transwarp -t -h localhost  -f ./sql/\" + sql);  \n4:}\n6) Testing Analysis \nIn the case of the 500GB test datasets, the four categories \nof SQL execution time are shown in Table II. Test results \nshow that 96 out of 99 SQL queries can be run directly in \nTranswarp Inceptor. There only 3 SQL queries need minor \nmodification to be compatible with SQL compiler of \nTranswarp Inceptor. Considering that the TPC-DS \nspecification allows SQL’s minor modification, so Transwarp \nInceptor has good compatibility with SQL 2003 standard.  \nTABLE II.  SQL  QUERIES ’ EXECUTION TIME OF TRANSWARP INCEPTOR  \nSQL \nCategories The number \nof SQL The total \nexecution time \n(seconds) The average \nexecution time \n(seconds) \nInteractive \nquery 9 197 21.9 \nStatistical \nanalysis 69 7705 111.7 \nIterative OLAP 10 4232 423.2 \nData mining 11 3502 318.4 \nB. Testing  Hive vs. Spark SQL by TPCx-BigBench  \n1) Requirement analysis of te sting Hive vs. Spark SQL \nTesting Hive vs. Spark SQL has two purposes. One is to \nutilize TPCx-BigBench as a benchmark for evaluating and \ncomparing the performance of two SQL-On-Hadoop analytics \nsystems. The other is to tune system parameters for optimizing \nanalytics system’s performance. \n2) Systems under test and test environment \na) Hive  \nHive is one of the first data analytics engines to be built on \ntop of MapReduce. It was originally developed by Facebook to \nsupport data analysts to analyze large datasets in Hadoop by \nqueries in a SQL-like declarative query language. This SQL-\nlike language is called HiveQL and is based on the SQL \nlanguage, but does not strictly follow the SQL 99 standard. \nHive has now become the foundation of new SQL on Hadoop \nprojects, such as Impala, Presto, and Spark SQL. Hive \nmetadata has become the de facto standard for users to store \nand manage metadata (table names, column names, and types, \netc.) in Hadoop ecosystem. \nAlthough Hive is a widely used project, historically its \nbiggest drawback has been performance. Most of the \nperformance problems can be attributed to Hive's use of \nMapReduce as its execution engine. MapReduce is not a good \nchoice for running ad hoc, interactive queries. The main reason \n234\n\n[Página 5]\nis that MapReduce reads and writes to disk extensively, and \nthere is a high startup cost for MapReduce jobs. \nb) Spark SQL \nApache Spark is a cluster computing platform designed to \nbe fast and general-purpose. Spark extends the popular \nMapReduce model to efficiently support more types of \ncomputations, including interactive queries and stream \nprocessing. One of the main features of Spark is to be able to \nrun computing in memory, so Spark has faster computing \nspeed than MapReduce. \nSpark SQL [8] is the component that Spark uses to \nmanipulate structured data. It allows querying data via SQL as \nwell as the HiveSQL and it supports many sources of data, \nincluding Hive tables, Parquet, and JSON. Spark SQL is fully \ncompatible with Hive. Spark SQL supports HiveSQL and Hive \nmetastore, so we can compare the performance of Hive and \nSpark SQL under the same test datasets.  \nSpark SQL also seamlessly integrates with Spark machine \nlearning libraries MLlib and Spark ML. For example, in a \nmachine learning application, the DataFrame API provided by \nSpark SQL can easily be used for data cleaning and feature \nengineering. \nc) Test environment \nThe test environment is a Cloudera Data Hub (CDH) \ncluster with 4 nodes connected directly through Gigabit \nnetwork, and detail hardware and software are shown in Table \nIII. Cloudera CDH 5.10 with default configurations was used \nfor all tests.  \nTABLE III.  TEST ENVIRONMENT FOR TESTING HIVE VS . SPARK  \n Node1 Node2 Node3 Node4 \nCPU Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz (8 cores) \nMemory \n(GB) 64 80 80 80 \nStorage 4TB HDD hard drive \nOperating \nSystem CentOS 6.7 x86_64 \nHadoop Cloudera Data Hub 5.10.0 (Hadoop 2.6.0) \nHive  Hive 1.1.0 \nSpark Spark 2.1.0 (--driver-memory 10g –execuotr-memory 20g ) \nRoles HDFS \nNameNode, \nResourceManager HDFS DataNode \nNodeManager \n3) Generating test datasets and workload by TPCx-\nBigBench \nBigBench covers the “3Vs” characteristics of the big data \nsystem. The initial implementation of BigBench was at the \nTeradata Aster platform in 2014. Later on, BigBench was \nstandardized by TPC in Nov. 2016, and TPC released TPCx-\nBigBench v1.2.0 as the benchmark for big data analytics \nsystem. BigBench benchmark consists of the data model, the \ndata generator and the specification of the workload. \na) Data model of BigBench The data model of BigBench includes structured data, semi-\nstructured data, and unstructured data, as shown in Fig.4. The \nstructured data of BigBench is adapted from TPC-DS. The \nsemi-structured data is composed of clicks made by customers \nand guest users visiting the retailer’s website. The unstructured \ndata is covered by product reviews submitted by actual \ncustomers or guest users. Therefore, BigBench satisfies the \n“variety” property of big data. \n \nFig.4. Data model of TPCx-BigBench \nb) Data generator of BigBench \nThe data generator of BigBench is based on an extension of \nPDGF [9] and allows generating data in accordance with the \ndata model. It can not only generate the structured data but also \ngenerate the semi-structured and unstructured data. PDGF is a \nparallel data generator that is capable of generating large \namounts data based on a scale factor. So, the “volume” \nproperty of big data is reflected in BigBench. In addition, the \n“velocity” property of big data is implemented through a \nperiodic refreshing scheme that continually adds new data to \ndifferent tables in the data model. The following script \nfragment shows how to set data storage directory and generate \n50GB datasets parallel by BigBench. \n# Set dataset’s HDFS storage path in  userSettings.conf \n1: export BIG_BENCH_HDFS_ABSOLUTE_PATH \n=\"/user/$BIG_BENCH_USER\"  \n2: export BIG_BENCH_HDFS_RELATIVE_HOME \n=\"benchmarks/bigbench\" \n# Generate 50GB test datasets with TPCx-BigBench \n1: $INSTALL_DIR/bin/bigBench runBenchmark –f 50 –m 8  –i \nDATA_GENERATION  \n-f  <scale factor of dataset> \n-m [number of map tasks for data generation] \n-i  <benchmark phases to perform > \nc) Query workload of BigBench \nThe BigBench query workload includes 30 queries, which \nare defined as questions about the business model. Ten of them \nhave been taken from the TPC-DS workload. The other 20 \nqueries were adapted from a McKinsey big data use cases and \nopportunities report. The 30 queries of BigBench can be \nclassified from two aspects: data types and analysis methods, \nas shown in Table IV and Table V. Analysis methods can be \ngrouped into four categories: Pure Hive Queries(Pure HQL), \nHive Queries with MapReduce programs, Hive Queries using \nnatural language processing(NLP/UDF/UDTF), and Queries \nusing Apache Spark MLLIB(Machine Learning).  \n235\n\n[Página 6]\nTABLE IV.  DATA TYPES OF BIGBENCH ’S WORKLOAD  \nData type Queries Number \nStructured data query1,query6, query7, query9, \nquery11, query13, query14, query15, \nquery16, query17, query20, query21, \nquery22, query23, query24, query25, \nquery26, query29 18 \nSemi-structured data query2, query3, query4, query5, \nquery8, query12, query30 7 \nUnstructured data query10, query18, query19, query27, \nquery28 5 \nTABLE V.  ANALYTIC METHOD OF BIGBENCH ’S WORKLOAD  \nAnalytic method Queries Number \nPure HQL query6, query7, query9, query11, \nquery12, query13, query14, query15, \nquery16, query17, query21, query22, \nquery 23, query 24 14 \nMapReduce query2, query3, query4, query8, \nquery30 5 \nMachine Learning query5, query20, query25, query26, \nquery28 5 \nNLP/UDF/UDTF query1, query10, query18, query19, \nquery27, query29 6 \n4) Data Loading in BigBench \nData loading in BigBench refers to load test datasets into \nHive tables. The following script fragment shows how to load \ntest datasets created in the phase of “DATA_ GENERATION” \ninto Hive tables. We can verify whether data loading was \nsuccessful or not by Hive’s shell command. \n# Load test datasets into Hive tables \n1: $INSTALL_DIR/bin/bigBench runBenchmark –i LOAD_TEST \n \n# Verify the test datasets was loaded successfully \n2: hive> use bigbench; \n3: hive> show tables; \n5) Testing for Hive vs. Spark SQL \nIn order to compare the performance of Hive and Spark \nSQL, we use Hive engine and Spark engine respectively. We \nexecute 30 queries in sequence to compare the execution time, \nas shown in Table Ď.  T h e  s c r i p t  f r a g m e n t  i s  a s  f o l l o w s .  I t  i s  \nworth noting that before using Spark engine we need to ensure \nthat Spark had access to the tables in Hive.  \n# Test Hive performance with BigBench \n1: $INSTALL_DIR/bin/bigBench runBenchmark –i POWER_TEST \n \n# Test Spark SQL performance with BigBench \n2: $INSTALL_DIR/bin/bigBench runBenchmark –i POWER_TEST \n–e spark_sql  \nTABLE VI.  EXECUTION TIME FOR ALL QUERIES WITH SF 50(50G  DATA ) \nQuery No. Analytic method Execution time (seconds) \nHive Spark SQL \nquery1 UDF/UDTF 296 124 \nquery2 MapReduce 3904 1634 \nquery3 MapReduce 1046 568 \nquery4 MapReduce 3932 989 \nquery5 Machine Learning 535 344 \nquery6 Pure HQL 603 238 \nquery7 Pure HQL 897 260 query8 MapReduce 680 251 \nquery9 Pure HQL 1123 138 \nquery10 NLP/UDF/UDTF 1133 1868 \nquery11 Pure HQL 242 110 \nquery12 Pure HQL 271 146 \nquery13 Pure HQL 361 152 \nquery14 Pure HQL 93 92 \nquery15 Pure HQL 151 124 \nquery16 Pure HQL 823 236 \nquery17 Pure HQL 230 118 \nquery18 NLP/UDF/UDTF 1066 903 \nquery19 NLP/UDF/UDTF 401 317 \nquery20 Machine Learning 341 322 \nquery21 Pure HQL 613 175 \nquery22 Pure HQL 160 128 \nquery23 Pure HQL 254 145 \nquery24 Pure HQL 307 118 \nquery25 Machine Learning 483 350 \nquery26 Machine Learning 249 291 \nquery27 NLP/UDF/UDTF 121 201 \nquery28 Machine Learning 456 510 \nquery29 UDF/UDTF 237 154 \nquery30 UDF/UDTF/MapReduce 3769 922 \n6) Performance analysis of Hive vs. Spark SQL \nAccording to Table Ď, Fig.5 and Fig.6, Spark SQL \nperformance is 1-8 times that of Hive under 14 Pure HQL \nqueries and 5 Hive queries with MapReduce.  The main reason \nis that Spark SQL uses memory computing and optimized SQL \nengine. So Spark SQL is more efficient than Hive that uses \nMapReduce as a computing engine. \n \nFig.5. Hive and Spark SQL performance comparison by Pure HQL query \n \nFig.6. Hive and Spark SQL performance comparison by MapReduce query \n236\n\n[Página 7]\nFor machine learning workload 㸪Hive and Spark SQL are \nsimilar in performance, since both Hive and Spark SQL use \nSpark MLLIB as a machine learning engine, as shown in Fig.7. \n \nFig.7. Hive and Spark SQL performance comparison by machine learning \nSince NLP programs were written in the Python language, \nneither Hive nor Spark SQL can take advantage of the system’s parallel computing features. As a result, for NLP/UDF/UDTF workload, Hive and Spark SQL performance’s gap is not large, and Hive outperformed Spark SQL even on query 10 and query 27, as shown in Fig.8. \n \nFig.8. Hive and Spark SQL performance comparison by NLP/UDF/UDTF \nFor query10, we modify the parameter of \nspark.sql.shuffle.partition  from the default of 200 to 50 to \noptimize the performance of Spark SQL. In Spark SQL, a large number of shuffle partitions means more tasks when shuffle operation occurs. More tasks in Spark SQL will increase the overhead of tasks startup and decrease the performance of the system. As shown in Fig.9, by optimizing Spark SQL’s parameter, query 10 reduce its run time from 1868 seconds to 1376 seconds. \n \nFig.9. Spark SQL performance improvement through optimization  \nV. CONCLUSION  \nWith the continuous development of big data applications \nand technologies, industry and academia pay more and more attention to the benchmark testing of big data analytics systems. It not only equitably compares the performance of multiple big data analytics systems, but also allows you to tune system parameters and optimize system performance. The paper analyzes the challenges of testing big data analytics system and summarizes the methods and strategies of the test. And the paper presents two cases of benchmark testing for big data analytics systems. In case 1, we present an automated system testing solution for Transwarp Inceptor by TPC-DS in detail, and the test includes system’s functionality, performance, reliability and compatibility of SQL. In case 2, we test and compare the performance of Hive and Spark SQL by TPCx-BigBench, an application oriented end-to-end benchmark. Test results show that the performance of Spark SQL significantly better than Hive on the workload of pure HQL and query with MapReduce. In the future, we will further research new technologies of big data benchmarks [10], such as testing and evaluation of streaming analytics and graph analytics systems. \n \nR\nEFERENCES  \n \n[1] A.Thusoo, J.S. Sarma, N. Jain, et al, “Hive-a petabyte scale data \nwarehouse using hadoop”, IEEE 26th International Conference on Data \nEngineering. IEEE, 2010, pp.996-1005. \n[2] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, et al, “Spark: \nCluster Computing with Working Sets”, Usenix Conference on Hot \nTopics in Cloud Computing, Boston, USA, 2010. \n[3] S. Huang, J. Huang, J. Dai, et al, “The HiBench Benchmark Suite: \nCharacterization of the MapReduce-Based Data Analysis”. ICDE \nWorkshops, 2010, pp. 41 - 51. \n[4] R. O. Nambiar, M. Poess, “The making of TPC-DS”, Proceedings of the \n32nd international conference on Very large data bases. VLDB \nEndowment, 2006, pp.1049-1058. \n[5] M. Poess, R. O. Nambiar, D. Walrath,“Why you should run TPC-DS: a \nworkload analysis”, Proceedings of the 33rd international conference on \nVery large data bases. VLDB Endowment, 2007, pp.1138-1149. \n[6] A. Ghazal, T. Rabl, M. Hu, et al, “BigBench: towards an industry \nstandard benchmark for big data analytics”, Proceedings of the 2013 ACM SIGMOD international conference on Management of data, 2013, \npp.1197-1208. \n237\n\n[Página 8]\n[7] TPCx-BigBench Standard Specification Version 1.2.0, November 2016, \nhttp://www.tpc.org/  \n[8] M. Armbrust, R. S. Xin, C. Lian, et al, “Spark sql: Relational data \nprocessing in spark”, Proceedings of the 2015 ACM SIGMOD \nInternational Conference on Management of Data, 2015, pp.1383-1394. \n[9] T. Rabl, M. Frank, H. M. Sergieh, et al, “A Data Generator for Cloud-\nScale Benchmarking”,  TPCTC, 2010, pp.41-56. [10] T. Rabl, M. Frank, M. Danisch, et al, “The vision of BigBench 2.0”, \nProceedings of the Fourth Workshop on Data analytics in the Cloud., \nACM, 2015. \n \n \n \n238",
      "metadata": {
        "filename": "chen2018.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\chen2018.pdf",
        "file_size": 325155,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:36.809043",
        "content_length": 31571
      }
    },
    "94553f7c-0219-4683-8566-938f0d311229": {
      "id": "94553f7c-0219-4683-8566-938f0d311229",
      "content": "[Página 1]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n1\nAutoDiagn: An Automated Real-time Diagnosis\nFramework for Big Data Systems\nUmit Demirbaga, Zhenyu Wen\u0003Member, IEEE , Ayman Noor, Karan Mitra, Member, IEEE , Khaled\nAlwasel, Saurabh Garg, Albert Zomaya, Fellow, IEEE , Rajiv Ranjan, Senior Member, IEEE\nAbstract—Big data processing systems, such as Hadoop and Spark, usually work in large-scale, highly-concurrent, and multi-tenant\nenvironments that can easily cause hardware and software malfunctions or failures, thereby leading to performance degradation.\nSeveral systems and methods exist to detect big data processing systems’ performance degradation, perform root-cause analysis, and\neven overcome the issues causing such degradation. However, these solutions focus on speciﬁc problems such as stragglers and\ninefﬁcient resource utilization. There is a lack of a generic and extensible framework to support the real-time diagnosis of big data\nsystems. In this paper, we propose, develop and validate AutoDiagn. This generic and ﬂexible framework provides holistic monitoring of\na big data system while detecting performance degradation and enabling root-cause analysis. We present an implementation and\nevaluation of AutoDiagn that interacts with a Hadoop cluster deployed on a public cloud and tested with real-world benchmark\napplications. Experimental results show that AutoDiagn can offer a high accuracy root-cause analysis framework, at the same time as\noffering a small resource footprint, high throughput and low latency.\nIndex Terms—Root-cause analysis, Big data systems, QoS, Hadoop, Performance\nF\n1 I NTRODUCTION\nThe rapid surge of data generated through sectors like\nsocial media, ﬁnancial services and industries has led to\nthe emergence of big data systems. Big data systems enable\nthe processing of massive amounts of data in relatively\nshort time frames. For instance, the Netﬂix big data pipeline\nprocesses approximately 500 billion events and 1.3 petabytes\n(PB) of data per day, further, during peak hours, it processes\napproximately 11 million events and 24 gigabytes (GB) of\ndata on a per-second basis. Facebook has one of the largest\ndata warehouses in the world, capable of executing more\nthan 30,000 queries over 300 PB data every day. However,\nthe enormousness and complexity of the big data system\nruns in heterogeneous computing resources, multiple tenant\nenvironments, as well as has many concurrent execution of\nbig data processing tasks, which makes it a challenge to\nutilize the big data systems efﬁciently and reliably[1]. For\nexample, Fig. 1 shows that the performance degrades at\nleast 10% when the resources are not utilized efﬁciently with\nSetting 2.\n\u000fU. Demirbaga is with Newcastle University, United Kingdom and Bartin\nUniversity, Turkey. E-mail: u.demirbaga2@newcastle.ac.uk\n\u000fZ. Wen is with Newcastle University, United Kingdom. E-mail:\nzhenyu.wen@newcastle.ac.uk, corresponding author.\n\u000fA. Noor is with Newcastle University, United Kingdom and Taibah\nUniversity, Saudi Arabia. E-mail: anoor@taibahu.edu.sa\n\u000fK. Mitra is with Lule˚ a University of Technology, Sweden. E-mail:\nkaran.mitra@ltu.se\n\u000fK. Alwasel is with Newcastle University, United Kingdom and Saudi\nElectronic University, Saudi Arabia. E-mail: kalwasel@gmail.com\n\u000fS. Garg is with University of Tasmania, Australia. E-\nmail:Saurabh.Garg@utas.edu.au\n\u000fA. Zomaya is with Sydney University, Australia, E-mail: al-\nbert.zomaya@sydney.edu.au\n\u000fR. Ranjan is with Newcastle University, United Kingdom. E-mail:\nraj.ranjan@newcastle.ac.uk\n 0 50 100 150 200 250 300 350\nWordCountGrepTPC-HTPC-DS K-means PageRankMakespan (sec)\nBig data applicationsSetting 1 Setting 2Fig. 1. Six big data applications are executed in a cloud-based Hadoop\ncluster with two settings: 1) the input data and jobs are allocated in\nthe same node; 2) the input data and jobs are allocated in different\nnodes. In Setting 2, the execution time of each application is delayed\nby transmitting data across nodes.\nTo overcome this, it is imperative to continuously mon-\nitor and analyze all available system resources at all times\nin a systematic, holistic and automated manner. These re-\nsources include CPU, memory, network, I/O and the big\ndata processing software components.\nMost of the commercial [2][3][4] and academic big\ndata monitoring systems mainly focus on visualizing task\nprogress, and the system’s resource utilization [5]. How-\never, they do not focus on the interaction between multiple\nfactors and performing root-cause analysis for performance\ndegradation [6][7]. Moreover, works such as [8], [9] aim to\nﬁnd the best parameters to optimize the performance of\nManuscript received ???; revised ???\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 2]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n2\nbig data processing systems, they do not focus on the root-\ncause analysis that may indicate the viable reasons behind\nperformance degradation and may provide intuitions for\nparameter tweaking.\nMantri [10] presents a systematic method that catego-\nrizes the main reasons causing outliers in a big data system.\nThe authors’ work was focused on the MapReduce pro-\ngramming framework in the Hadoop system; they do not\ndiscuss how Mantri can be applied to other big processing\nframeworks (e.g., Apache Spark1, and Apache Flink2). Gar-\nraghan et al. [11] proposed an online solution to detect long-\ntail issues in a distributed system. However, these solutions\nwere built for speciﬁc scenarios with much scope left for\nanalyzing a variety of problems that can exist in a large\nscale big data processing system.\nTo the best of our knowledge, there is a lack of a generic\nand comprehensive solution for the detection of a wide\nrange of anomalies and performance of root-cause analysis\nin big data systems. Developing a general and extensible\nframework for diagnosing a big data system is not trivial.\nIt requires well-deﬁned requirements which could enable\nthe broader adoption of root-cause analysis for the big\ndata systems, ﬂexible APIs to interact with an underlying\nmonitoring system and integration of multiple solutions for\ndetecting performance reduction problems while enabling\nthe automatic root-cause analysis. In this paper, we tackle\nthis research gap, and design and develop AutoDiagn to au-\ntomatically detect performance degradation and inefﬁcient\nresource utilization problems, while providing an online\ndetection and semi-online root-cause analysis for a big data\nsystem. Further, it is designed as a microservice architecture\nthat offers the ﬂexibility to plug a new detection and root-cause\nanalysis module for various types of big data systems.\nThe contributions of this paper are as follows:\n\u000fAn online and generic framework: We develop a general\nframework called AutoDiagn which can be adapted for\nthe detection of a wide range of performance degrada-\ntion problems while pinpointing their root-causes in big\ndata systems.\n\u000fA case study: We develop a novel real-time stream pro-\ncessing method to detect symptoms regarding outliers\nin a big data system. After that, we develop a set of\nquery APIs to analyze the reasons that cause the outlier\nregarding a task.\n\u000fA comprehensive evaluation: We evaluate the feasibility,\nscalability and accuracy of AutoDiagn through a set of\nreal-world benchmarks over a real-world cloud cluster.\nThe paper is organized as follows. The design require-\nments and idea are outlined in §2. In §3, we illustrate the\nhigh-level system architecture. §4 presents a case study that\nwe implemented and the case study is evaluated in §5. §6\ndiscusses the limitations of this paper and highlights our\nfurther work . Before drawing a conclusion in §8, we discuss\nthe related work in §7.\n1. https://spark.apache.org/\n2. https://ﬂink.apache.org/2 R EQUIREMENTS AND DESIGN IDEA\nIn this section, we analyze the key requirements of the\nreal-time big data diagnosis system, extracting the essential\nfeatures from the literature. Next, we present the key idea\nof the framework design.\n2.1 Fundamental prerequisite for diagnosing big data\nprocessing systems\nIn order to design a generic framework for diagnosing big\ndata processing systems, we classiﬁed the fundamental re-\nquirements of building a diagnosis system on such systems\nas follows:\n\u000fInfrastructure monitoring: Collecting the information\nabout the underlying system, such as network condi-\ntions, CPU utilization, memory utilization, and disk\nI/O status.\n\u000fTask execution monitoring: Collecting the task infor-\nmation, including execution time, progress, location,\nlocation of its input data, input data size, output data\nsize, CPU/memory usage, and process state (running,\nwaiting, succeeded, failed, killed).\n\u000fAbnormal behavior or fault detection: Detecting ab-\nnormal behaviors in big data processing systems, such\nas slowing tasks, failed tasks, very high/low resource\nusage, and experiencing very high response time for the\nrequests.\n\u000fRoot-cause analysis: Finding the root cause of perfor-\nmance reduction in big data processing systems, such\nas the reasons why: tasks are slowing down, resource\nutilization is low, the response time is high, or when the\nnetwork latency is high.\n\u000fVisualization: Visualizing the collected metrics and\nthe results of root-cause analysis of any failures caus-\ning performance reduction in the cluster with a user-\nfriendly interface in real-time.\n2.2 Key design idea\nMotivated by the above-mentioned requirements and in-\nspired by medical diagnosis, we highlight the design idea\nof root-cause analysis for big data processing systems as\nshown Fig. 2, which aims to provide holistic monitoring\nand root cause analysis for big data processing systems.\nFirst, a set of Symptom Detectors is deﬁned and developed in\nSymptom Detection to detect the abnormalities of the big\nsystem by processing collected system information stream\nin real-time. Once a symptom (abnormality) is detected,\ntheDiagnosis Management may launch the corresponding\nDiagnosers to troubleshoot the cause of the symptom. One\nsymptom may correspond to root causes. Finally, the deci-\nsions are made based on the root-cause analysis results.\n2.3 The generalizability of AutoDiagn\nModern big data processing systems consists of two main\ntypes: Big data analytics (e.g., Hadoop, Spark) and Stream\nprocessing (e.g., Flink, Spark Stream). Based on our de-\nsign idea, our AutoDiagn is an independent framework\nthat can be deployed alongside existing big data cluster\nmanagement systems (e.g., Apache YARN), and ideally it\nis suitable for root-cause analysis of any big data processing\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 3]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n3\nsystem. However, for the scope of this paper and practi-\ncal certainty, the implementation of AutoDiagn focuses on\ndebugging root causes of performance degradation (e.g.,\nslow task execution time) in Hadoop due to faults such as\ndata locality, cluster hardware heterogeneity, and network\nproblems (e.g., disconnection). Although we have validated\nthe functionality of AutoDiagn in the context of Hadoop and\nconsidering different classes of workload (e.g., WordCount,\nGrep, TPC-H, TPC-DC, K-means clustering, PageRank), it is\ngeneralizable to other big data processing systems executing\nsimilar classes of workload.\n3 A UTODIAGN ARCHITECTURE\nFollowing the design idea laid out in §2, we introduce Auto-\nDiagn, a novel big data diagnosing system. We ﬁrst illustrate\nthe high-level system architecture and then describe the\ndetails of each component. AutoDiagn is implemented in\nJava and all source code is open-source on GitHub3.\n3.1 Architecture overview\nAutoDiagn provides a systematic solution that automati-\ncally monitors the performance of big data systems while\ntroubleshooting the issues that cause performance reduc-\ntion. Fig. 3 shows its two main components: AutoDiagn\nMonitoring and AutoDiagn Diagnosing. AutoDiagn Monitoring\ncollects the deﬁned metrics (logs) and feeds AutoDiagn Diag-\nnosing with them in real-time. Once the abnormal symptoms\nare detected by analyzing the collected metrics, a deeper\nanalysis is conducted to troubleshoot the cause of abnormal\nsymptoms.\nAutoDiagn Monitoring. AutoDiagn Monitoring is a de-\ncentralized real-time stream processing system that collects\ncomprehensive system information from the big data system\n(e.g., Hadoop Cluster). The Collected Metrics is a set of\npre-deﬁned monitoring entities (e.g., CPU usage, memory\nusage, task location, task status) used to detect the abnormal\nsymptoms. Moreover, the system information, required for\nunderstanding the cause of detected abnormal symptoms,\nis collected in this modular.\nAutoDiagn Diagnosing. AutoDiagn Diagnosing is an event\nbased diagnosing system. First, the carefully crafted metrics\nare injected into the Symptom Detection Engine which is a\nreal-time stream processing module to detect the abnormal\nsymptoms in a big data system. In this paper, we use\nthe outlier which is a common symptom for performance\nreduction in a Hadoop cluster as a case study to demon-\nstrate the proposed framework. §4.1 illustrates the details\nof technology that we developed for symptom detection.\nMoreover, our system follows the principle of modular\nprogramming; the new symptom detection method can be\neasily plugged in. Diagnoser Plugins is a component for\ntrouble-shooting the reasons behind the detected symptom.\nA set of Diagnosers is instantiated by the Diagnoser Manager\nwhen their corresponding symptoms are detected. Then\nthe instantiated Diagnosers query a time series database to\nobtain the required input and their outputs illustrate the\ncause of the detected symptoms.\n3. https://github.com/umitdemirbaga/AutoDiagn3.2 AutoDiagn monitoring framework\nAutoDiagn monitoring framework is a holistic solution for\ncontinuous information collection in a big data cluster.\nThe framework needs to have a fast, ﬂexible and dynamic\npipeline to transfer the collected data as well as a high per-\nformance, large scale storage system. We now describe an\nimplementation of the framework for a big data computer\ncluster, and the high-level system architecture is shown in\nFig. 4.\nInformation Collection. In each compute node, we develop\nand deploy an Agent to collect real-time system information.\nFor the worker node, the Agent collects the usage of com-\nputing resource via SIGAR APIs4, including CPU, memory,\nnetwork bandwidth, and disk read/write speeds. Moreover,\ntheAgent in the master node collects the usage of computing\nresource as well as the job and tasks information. The Filter\nis developed by using GSon Library5to remove the less im-\nportant information obtained from ResourceManager REST\nAPI’s6, thereby reducing the size of data transmission. The\ncollected information is sent to RabbitMQ7cluster which is\na lightweight and easy-to-deploy messaging system in each\ntime interval via Publisher.\nStorage. The acquired information is time series data, we\ntherefore choose InﬂuxDB8for data storage. InﬂuxDB is a\nhigh performance, scalable and open source time series data\nbase which provides a set of ﬂexible open APIs for real-time\nanalytics. The Consumer subscribes the related stream topics\nfrom RabbitMQ and interacts with InﬂuxDB APIs to inject\nthe information to the data base.\nInteracting with AutoDiagn Diagnosing. The information\nrequired for symptom detection is directly forwarded and\nprocessed in AutoDiagn diagnosing via a consumer. If a\nsymptom is detected, InﬂuxDB will be queried by AutoDi-\nagn diagnosing for root-cause analysis. Finally, the analysis\nresults are sent back to the database to be stored.\nUser visualization. The user visualization allows the users\nto have a visible way to monitor their big data system. We\nutilize InﬂuxDB’s client libraries and develop a set of REST-\nful APIs to allow the users to query various information,\nincluding resource utilization, job and task status, as well as\nroot cause of performance reduction.\n3.3 AutoDiagn diagnosing framework\nIn this section, we discuss the core components of the\nAutoDiagn Diagnosing framework (see Fig. 3), as well as the\ninteractions with each other and the AutoDiagn Monitoring\nframework.\nSymptom Detection Engine. The symptom detection en-\ngine subscribes a set of metrics from the real-time streaming\nsystem. §4.1 illustrates the technique that we developed\nfor outlier detection. This component follows microservices\narchitecture to which new symptom detection techniques\ncan be directly attached to our AutoDiagn, interacting with\nother existing techniques to detect new symptoms.\n4. https://github.com/hyperic/sigar\n5. https://github.com/google/gson\n6. https://hadoop.apache.org/docs/r3.2.1/hadoop-yarn\n7. https://www.rabbitmq.com/\n8. https://www.inﬂuxdata.com/\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 4]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n4\nSymptom Detection Diagnosis Management Decision MakingSymptoms \n(N)\nRoot -cause 1\n•\n•\n••\n•\n•Root -cause 2\nRoot -cause M•\n•\n•Root -cause 3Diagnosis \n(M)\nRoot -cause 4MetricsSymptom \nDetector 2\nSymptom \nDetector NSymptom \nDetector 1Diagnoser 1\nDiagnoser 2\nDiagnoser 3\nDiagnoser 4\nDiagnoser MDecision 1\nDecision 2\n•\n•\n•\nDecision N Root -cause M -1 Diagnoser M -1\nFig. 2. The key design idea of root-cause analysis for big data processing systems\nAutoDiagn Diagnosing\nDiagnoser Plugins\nDiagnoser 1\nTask\nInput\nOutput\n…\nDiagnoser N\nTask\nInput\nOutputAutoDiagn Monitoring\nSymptom\nDetection\nEngine\nDiagnosis \ndecisionsCollected \nmetricsDetected\nSymptoms\nRoot -causes of the symptoms \nDiagnoser \nManager\nFig. 3. The high-level architecture of the AutoDiagn system\nDiagnoser Manager. The diagnoser manager is the core\nentity responsible for selecting the right diagnosers to ﬁnd\nthe reasons that cause the detected symptoms. Additionally,\nthe diagnoser manager is developed as a front-end com-\nponent, triggered by various detected symptoms (events)\nvia a RESTful API, exposing all diagnosing actions within\nour framework. The API includes general actions such as\nstarting, stopping or loading a diagnoser dynamically, and\nspeciﬁc actions such as retrieving some metrics. Importantly,\nthe diagnoser manager is able to compose a set of diagnosers\nto complete the diagnosing jobs that may require the coop-\neration of different diagnosers.\nDiagnoser Plugins. The diagnoser plugin contains a set of\ndiagnosers; and a diagnoser is the implementation of the\nspeciﬁc logic to perform root-cause analysis of a symptom.\nEach diagnoser refers to a set of metrics stored in a time\nseries database as the input of its analysis logic. Whenever\nit is activated by the diagnoser manager, it will perform\nan analysis, querying the respective metrics, executing the\nanalytic algorithm, and storing the results. §4.2 discusses the\nalgorithms to detect the outlier problems, for example, in aHadoop cluster. The diagnoser plugin is also designed as\na microservice architecture which has two advantages: i) a\nnew diagnoser can be conveniently plugged or unplugged\non-the-ﬂy without affecting other components; ii) new root-\ncause analysis tasks can be composed by a set of diagnosers\nvia RESTful APIs.\n3.4 AutoDiagn diagnosing interfaces for Hadoop\nAutoDiagn exposes a set of simple interfaces for system\nmonitoring, symptom detection and root-cause analysis.\nTable 1 shows that two types of APIs are deﬁned: high-\nlevel APIs and low-level APIs. The high-level APIs consist\nofSymptom Detection, Diagnoser and Decision Making.\nThe Symptom Detection APIs are a set of real-time stream\nprocessing functions used to detect the deﬁned symptoms\ncausing the performance reduction in the Hadoop system.\nEach Diagnoser is a query or a set of queries, which aim\nto ﬁnd one of the causes of a symptom. For example,\nQueryNonLocal() tries to ﬁnd all non-local tasks within a\ntime interval, which is one of the reasons that causes an out-\nlier. Finally, the Decision Making APIs are used to analyze\nthe results from each Diagnoser and make the conclusion.\nThese high-level APIs have to interact with the low-level\nAPIs (Information Collection) to obtain system information\nincluding resource usage, and the execution information of\nthe big data system (e.g., ask and job status in a Hadoop\nsystem). Based on this ﬂexible design, users can deﬁne\nand develop their own Symptom Detection, Diagnoser and\nDecision Making APIs and plug them into AutoDiagn.\n3.5 Example applications\nWe now discuss several examples for big data system root\ncause applications using AutoDiagn API.\nOutliers. Outliers are the tasks that take longer to ﬁnish\nthan other similar tasks, which may prevent the subse-\nquent tasks from making progress. To detect these tasks,\nthe real-time stream query QueryOutlier() is enabled\nin the Symptom Detection Engine. This function consumes\neach task’s completion rate (i.e., progress) and the executed\ntime to identify the outlier tasks (detailed in §4.1). Next,\nthree APIs QueryNonlocal(), QueryLessResource()\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 5]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n5\nComputer Cluster\nMaster Node\nPublisher FilterCollector AgentResource\nInformationTask\nInformation\n…Message \nBrokerAutoDiagn \nDiagnosingManagement Node\nUser \nVisualization\n StorageConsumer\nConsumer\nWorker Node 1Publisher\nCollectorAgent\nResource\nInformation\n…Task 1 Task N\nWorker Node 2Publisher\nCollectorAgent\nResource\nInformation\n…Task 1 Task N\nWorker Node NPublisher\nCollectorAgent\nResource\nInformation\n…Task 1 Task N\nFig. 4. The high-level architecture of the monitoring framework\nandQueryNodeHealth(), corresponding to three Diag-\nnosers that are used to analyze the reasons causing the de-\ntected symptom, are executed. QueryNonlocal() queries\nwhether the input data is allocated on the node on which\nan outlier task is processed. In addition, QueryLessRe-\nsource() investigates whether outlier tasks are running\non the nodes that have less available resource. Moreover,\nQueryNodeHealth() examines if an outlier task is the\ntask that is a restarted task due to the disconnected nodes\nfrom the network. Finally, RootcauseOutlier() is used\nto process the results from the three Diagnosers and make\nthe conclusion. All the APIs are shown in Table 1 and the\ntechnical details are illustrated in §4.\nInefﬁcient resource utilization. In our case this means that\nsome tasks are pending (or waiting) to be on worker nodes;\nat the same time, some worker nodes are idle, e.g., low CPU\nand memory usage. There are many reasons that cause this\nissue, but here we consider two key causes: task heterogeneity\nand resource heterogeneity. The type of tasks in a big data sys-\ntem are various, including CPU intensive tasks, IO intensive\ntasks and memory intensive tasks. However, the underlying\ncomputing resources are typically equally distributed to\nthese tasks, thereby causing inefﬁcient resource utilization.\nThe latter is caused by the heterogeneous underlying com-\nputing resources due to the multiple concurrent processing\ntask environments and the queues are built on the saturated\nnodes.\nTo detect the inefﬁcient resource utilization in a big data\nsystem, the real-time stream query QueryResourceU-\ntil() is used within a deﬁned time interval. We com-\npute the mean and standard deviation of the usage re-\nsources of the whole cluster. If the standard deviation\nis far from the mean, we will further query whether\nthe tasks are queued on the nodes which have high\nresource usage rates. If inefﬁcient resource utilization\nis detected, two Diagnosers, QueryOversubscribed()\nand QueryDiskIOboundTasks(), which are the root-\ncause analysis APIs shown in Table 1, are executed toperform root-cause analysis. QueryOversubscribed()\nchecks the type of tasks queuing on the saturated nodes.\nTheQueryDiskIOboundTasks() checks whether the sat-\nurated nodes have less available computing resource,\nwhile processing the allocated tasks. The conclusion of the\ncause of inefﬁcient resource utilization is made in Root-\ncauseResInef().\n3.6 Parallel execution\nFollowing the key design idea, the diagnosers are triggered\nby the corresponding detected symptom. However, we are\nable to parallelize the execution of each symptom detector\nand its diagnosers by partitioning the input data. For ex-\nample, if one symptom detector needs to process too many\ndata streams, we can use two of the same instances of the\nsymptom detector to process the data streams and aggregate\nthe results from two symptom detectors. The diagnoser can\nfollow the same strategy for parallel execution.\n3.7 Reliability analysis\nAutoDiagn follows the centralized design for data collec-\ntion, which simpliﬁes the implementation of the Symptom\nDetection, Diagnosis Management and Decision Making. They\ncan easily obtain the required information from one place,\ninstead of interacting with the entire big data system. More-\nover, the centralized design does not mean unreliability, due\nto the high-availability of RabbitMQ. The RabbitMQ cluster\ncan overcome the node fail in the message queuing system\nwhile ensuring scalability.\n4 C ASESTUDY\nIn the previous section, we have discussed that our frame-\nwork supports detection of multiple types of symptoms\n(e.g., outliers, inefﬁcient resource utilization). However, de-\ntecting these symptoms is non-trivial; and each symptom\ncan be detected by using different algorithms with different\ninput metrics. In this section, we present a case study that\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 6]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n6\nTABLE 1\nAutoDiagn diagnosing interface. See §3.4 for deﬁnitions and examples\nSymptom Detection (High-level APIs) Description\nQueryOutlier() Execute a Query that returns the list of outliers if any.\nQueryResourceUtil() Execute a Query that returns the list of the worker nodes in which the computing resources are not uti-\nlized effectively if any.\nDiagnoser (High-level APIs) Description\nQueryNonLocal() Execute a Query that return the list of non-local tasks if any.\nQueryLessResource() Execute a Query that returns false if the cluster is not homogeneous in terms of having resource capacity (CPU/memory).\nQueryNodeHealth() Execute a Query that returns the list of disconnected worker nodes in the cluster if any.\nQueryOversubscribed() Execute a Query that returns the list of the oversubscribed tasks if any.\nQueryDiskIOboundTasks() Execute a Query that returns the list of the disk- or IO-bound tasks if any.\nDecision Making (High-level APIs) Description\nRootcauseOutlier() Execute a Query that illustrate the main reason of the cause of the outlier.\nRootcauseResInef() Execute a Query that illustrate the main reason of the cause of inefﬁcient resource utilization.\nInformation Collection (Low-level APIs) Description\ntaskExecTime() Return the execution time since the task started in sec.\ntaskProgress() Return the progress of the running task as a percentage.\ntaskInput() Return the input data size of the running task in mb.\ntaskBlock() Return the block id this task process.\ntaskHost() Return the name of the node thistask ran on.\ntaskCPUusage() Return the CPU usage of the task.\ntaskMemoryUsage() Return the memory usage of the task.\ntaskContainerCPU() Return the allocated CPU to the container this task ran on.\ntaskContainerMemory() Return the allocated memory to the container this task ran on.\nblockHost() Return the names of the nodes that host the block.\npendingTasks() Return the number of the tasks waiting to be run.\nnodeTotalCoreNum() Return the number of the CPU core number of the node.\nnodeCPUUsage() Return the CPU utilization of the node.\nnodeTotalMem() Return the total memory capacity of the node.\nrestartedTasks() Return the name of the restarted tasks due to nodes that got disconnected from the network.\nnodeMemUsage() Return the memory utilization of the node.\nnodeDiskReadSpeed() Return the disk read speed of the node.\nnodeDiskWriteSpeed() Return the disk write speed of the node.\nnodeUploadSpeed() Return the network upload speed of the node.\nnodeDownloadSpeed() Return the network download speed of the node.\ndetails the technology of detecting outliers and the root-\ncauses analysis for the detected outliers. The notations used\nin this paper are summarized in Table 2.\nTABLE 2\nA summary of symbols used in the paper\nSymbols Description\nJp Job progress\nN Name of the task\nNl List ofN\nP Performance of the N\nPl List ofP\nO Progress of theN\nOl List ofO\nT Execution time of the N\nTl List ofT\nmed The performance of median task\nD Non-local tasks\nDl List of Non-local task\nR Task running on the node with less resources\nRl List ofR\nW Restarted tasks due to the nodes’ network failure\nWl List ofW\nSl List of outlier task\nSd Non-local outlier\nSdl List of Sd\nSr Outlier stemming from the resource variation\nSrl List of Sr\nSw Outlier stemming from disconnected nodes\nSwl List of Sw\nF Factor value of 1.5 used to ﬁnd the S4.1 Symptom detection for outliers\nAnanthanarayanan et al. [10] deﬁned the outlier tasks’ run-\ntime to be 1.5 times higher than that of the median task\nexecution time; their method is based on the assumption\nthat all tasks are started at the same time and are the same\ntype (i.e., the same input data and the same processing\ncode), which is not suitable for real-time symptom detection,\nbecause in a time interval the tasks may be submitted at\ndifferent times; the input data size of the tasks and the code\nfor tasks are not always the same. In this paper, we use\nPerformance (P) to measure the outlier as shown in Eq 1. O\nrepresents the normalized value of the task progress in terms\nof percent work complete, and Tis the normalized value of\nthe task execution time.\nP=O\nT(1)\nEq 2 is used to normalize the OandT, where xmin and\nxmax are the minimal and maximal values of the given\nmetrics (eg., task progress and execution time) in a time\ninterval. We set b= 1 anda= 0:1 to restrict the normalized\nvalues within the range from 0.1 to 1 [12].\nxnorm =a+(x\u0000xmin)(b\u0000a)\nxmax\u0000xmin(2)\nMoreover, we deﬁne the outlier tasks which have 1.5\ntimes less performance value than the median performance\nvalue in each time interval. Fig. 5 shows a snapshot of a time\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 7]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n7\nAlgorithm 1: Automated symptom detection for\noutliers\nInput: Jp- job progress in percentage,\nF- factor,\nN- name of the running task,\nNl- list ofN,\nO- progress of the task,\nOl- list ofO,\nT- execution time of the task,\nTl- list ofT.\nOutput: Sl- list of outliersS.\n1// Create a list Slto store theS\n2Sl Sl[0]\n3// Initialize the med\n4med med[0]\n5while Jp<100.0 do\n6 //Clear the SlandPl\n7 Sl Clear (Snew\nl ,Sl)\n8 Pl Clear (Pnew\nl ,Pl)\n9 foreachNinNldo\n10 //ComputeP\n11P=O\nT\n12 //Insert thePinto the Pl\n13 Pl.add(P )\n14 end\n15 //Get themedfrom thePl\n16 med Median value of Pl\n17 foreach value of Pldo\n18 if(P*F)< m edthen\n19 //Insert theNinto theSl\n20 Sl.add(N )\n21 end\n22 end\n23 //Update the SlinDiagnosis Generation component\n24 Sl Update (Snew\nl ,Sl)\n25 //Update the Nl,Ol,Tl,Jp\n26 Nl Replace (Nnew\nl ,Nl)\n27 Ol Replace (Onew\nl ,Ol)\n28 Tl Replace (Tnew\nl ,Tl)\n29 Jp Replace (Jnew\np ,Jp)\n30end\ninterval (e.g., three seconds), and two mappers are identiﬁed\nas outliers. More evaluations will be discussed in §5.\nAlgorithm 1 demonstrates the proposed ASD (auto-\nmated symptom detection) algorithm in the AutoDiagn\nsystem. It is fed by the streaming data provided by the\nAutoDiagn Monitoring system during job execution. First,\nthe performance of each running task is calculated (see\nAlgorithm 1, Line 11) using Eq 1. Next, the median value\nof the performance of all tasks is taken to be used to detect\noutliers (see Algorithm 1, Line 16). Then, the tasks whose\nperformance is 1.5 times less than the performance of the\nmedian task are selected as outliers (see Algorithm 1, Line\n20). As a ﬁnal step, these tasks detected as outliers are sent to\ntheDiagnosis Generation component for root-cause analysis\n(see Algorithm 1, Line 24).\n4.2 Root cause analysis for outliers\nWhen the detected symptoms are passed to the Diagnoser\nManager, the corresponding Diagnosers are executed for\ntrouble-shooting. The following subsection illustrates the\ntechnologies that we have developed for analyzing the\ncauses of outliers in a Hadoop cluster.\n4.2.1 Root cause of outliers\nIn this paper, we follow the three main reasons that cause\noutliers, discussed in [10], i.e., Data locality, Resource het-\nerogeneity, and Network failures.\nProgress (%)Execution time (sec) 0 1 2OutliersMedian=1.11Performance levels \n 30 35 40 45 50 55 60 65  14 16 18 20 22 24 26 28 30 32\nPerformance 0.2 0.4 0.6 0.8 1 1.2 1.4\nFig. 5. Performance evaluation of the tasks\nData locality. Hadoop Distributed File System (HDFS)\nstores the data in a set of machines. If a task is scheduled to\na machine which does not store its input data, moving data\nover the network may introduce some overheads to cause\nthe outliers issue.\nResource heterogeneity. The machines in a Hadoop cluster\nmay be homogeneous with the same hardware conﬁgura-\ntion, but the run-time computing resources are very hetero-\ngeneous due to the multiple talents environment, multiple\nconcurrent processing task environment, machine failures,\nmachine overloaded etc. If a task is scheduled to a bad\nmachine (e.g., has less computing resource) it may cause\nan outlier issue. Moreover, resource management systems\nfor a large-scale cluster like YARN split the tasks over the\nnodes equally without considering the resource capacities of\nthe nodes in the cluster, but only takes into account sharing\nthe node’s resources among the tasks running on the node\nequally by default [13]. That is more likely to raise an outlier\nproblem in the cluster.\nNetwork failure. In Hadoop clusters, the network discon-\nnection can cause the running tasks allocated on a discon-\nnected node to be restarted on other nodes, which may lead\nto the task becoming an outlier and, increase the completion\ntime. The following illustrates the three algorithms that\nwe developed to identify the outliers caused by the three\nreasons.\n4.2.2 Detecting data locality issues\nWe assume that a non-local task (D ) (e.g., mapper) is ex-\necuted on a node where its input data is not stored (In the\nfollowing, we use Sdto represent non-local outliers). To detect\nthese tasks, we develop Algorithm 2 to check whether a set\nof outliers is caused by a data locality issue. The input of\nour algorithm is a list of detected outliers during the time\ninterval from ttot+ 1 and one of its outputs is a list of\noutliers which also belongs to the non-local tasks. First, we\nquery our time series database to obtain all non-local tasks\nwithin the given time interval (see Algorithm 2, Line 2).\nHere, QueryNonLocal(), a root-cause analysis API, is\nused to ﬁnd the non-local ones among the running tasks\nin that period of time. It compares the location where the\ntask is running (host node of the task) with the nodes\nwhere the data block is replicated for fault tolerance via\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 8]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n8\ninformation collection APIs shown in Table 1, taskHost()\nandblockHost(). If the task is not running on any of\nthese nodes (nodes hosting a copy of the block), this task\nis marked as a non-local task. In the second step (Algorithm\n2, Line 4), we obtain the common elements of list DlandSl.\nThese elements symbolize the non-local outliers stemming\nfrom a data locality issue.\n4.2.3 Detecting resource heterogeneity issues\nAlgorithm 2 is designed to identify the outliers caused by\nthe resource heterogeneity. The tasks running on the nodes\nwhich have less computing resource (R ) tend to be outliers\n[14] (in the following, we use Srto represent outliers running\non the nodes which have less computing resource). In Algorithm\n2, the list of detected outliers during the time interval from\nttot+ 1 is used as input and one of the outputs of the\nalgorithm is a list of outliers which also belongs to the tasks\nrunning on the node with less computing resource. The time\nseries database is queried to obtain all the tasks running on\nthe node with less computing resource within the given time\ninterval (see Algorithm 2, Line 6).\nHere, QueryLessResource(), a root-cause analysis\nAPI, is used to check the heterogeneity of the nodes that host\nonly the running tasks based on the resource speciﬁcations\nof them in that period of time. It detects the nodes with less\nresource capacity in terms of CPU core numbers and the to-\ntal amount of memory among the nodes hosting the running\ntasks. The resource speciﬁcations of the nodes (i.e., CPU\ncore numbers, total amount of memory) are obtained from\neach node via information collection APIs shown in Table 1,\nnodeTotalCoreNum() andnodeTotalMem() APIs. As a\nsecond step (Algorithm 2, Line 8), we obtain the common\nelements of list RlandSl. These elements symbolize the\noutliers stemming from a cluster heterogeneity issue.\n4.2.4 Detecting network failure issues\nSince Slis obtained from Algorithm 1, a Diagnoser is exe-\ncuted via QueryNodeHealth() to ﬁnd all restarted tasks\ndue to the nodes disconnected by network failure within the\ngiven time interval (see Algorithm 2, Line 10). The low-level\nAPIrestartedTasks() is called which distinguishes the\nrestarted tasks due to network failure from the speculation\nof straggler tasks by analyzing the information of the tasks\nthat is provided by the monitoring agent. Thereafter, we\ncompute the list Swlthat contains the outlier tasks caused\nby the network failure (see Algorithm 2, Line 12).\n4.2.5 Decision making\nIn this case study, we use a simple decision make method\nthat compares the lists Sdl,SrlandSwland the probability\nof the reasons causing the outliers by using the number\nof the elements of a list divided the total number of out-\nlier tasks. For instance, the probability of the performance\nreduction caused by data locality isjSdlj\njSlj. More advanced\nmethods such as deep learning models can be used for pro-\ncessing more complicated decision making tasks in future\nwork.Algorithm 2: Root-cause analysis of outliers\nInput: Sl- list of outliers in time interval from ttot+ 1\nOutput: Sdl- list of non-local outliers Sd,\nSrl- list of outliers stemming from resource variation Sr,\nSwl- list of outliers stemming from disconnected nodes Sw.\n1// Find allDwithin the given time interval\n2Dl QueryNonLocal(t, t+1)\n3//Find the common elements in the DlandSl, and add them\ninto theSdl\n4Sdl RetainAll (Dl,Sl)\n5// Find allRwithin the given time interval\n6Rl QueryLessResource(t, t+1)\n7//Find the common elements in the RlandSl, and add them\ninto theSll\n8Srl RetainAll (Rl,Sl)\n9// Find allWwithin the given time interval\n10Wl QueryNodeHealth(t, t+1)\n11//Find the common elements in the WlandSl, and add them\ninto theSwl\n12Swl RetainAll (Wl,Sl)\n5 E VALUATION\nIn this section, we present a comprehensive evaluation\nshowing the capacity and the accuracy rate of AutoDiagn,\nas well as a analysis of its resource consumption and over-\nheads.\n5.1 Experimental setup\nEnvironments. We set up the Hadoop YARN clusters over\n31 AWS nodes with 1 master and 30 slaves with the Oper-\nating system of each node being Ubuntu Server 18.04 LTS\n(HVM). The Hadoop version is 3.2.1 and the Hive version\nis 3.1.1. To meet our experimental requirements, we built\ntwo types of cluster. In Type I each node has the same\nconﬁguration (i.e., 4 cores and 16 GB memory). In Type II,\n25 nodes have 4 cores and 16 GB memory and 6 nodes have\n2 cores and 4 GB memory.\nBenchmarks and workload. We used six well-known\nHadoop benchmarks in our evaluations namely: Word-\nCount9, Grep10, TPC-H11, TPC-DS12, K-means clustering13,\nand PageRank14. The input of each benchmark application\nis 30GB.\nMethodology. Our experiments aim to evaluate the effec-\ntiveness of AutoDiagn. To this end, we manually inject the\nabove-mentioned three main reasons to cause the outliers,\nwhich can be summarized as three types of execution en-\nvironment. EnvA: we perform all benchmark experiments\nin the cluster Type I. EnvB: we perform all benchmark\nexperiments in the cluster Type I, but skew the input size\nstored on different nodes. EnvC: we perform all benchmark\nexperiments in the cluster Type II (a heterogeneous cluster).\nEnvH: we perform all benchmark experiments in the cluster\nType I, and disconnect some nodes’ network during execu-\ntion. Each benchmarking is repeated 5 times and results are\nreported as the average and standard deviation. In total,\nthere are 90 experiments conducted in our evaluation.\n9. http://wiki.apache.org/hadoop/WordCount\n10. http://wiki.apache.org/hadoop/Grep\n11. http://www.tpc.org/tpch/\n12. http://www.tpc.org/tpcds/\n13. https://en.wikipedia.org/wiki/K-means clustering\n14. https://en.wikipedia.org/wiki/PageRank\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 9]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n9\nTABLE 3\nThe accuracy of symptom detection for non-local outliers in a\nhomogeneous cluster\nBenchmark Total\ntasksD Outliers\n(detected as Sd)Accuracy\n(%)Error\n(\u001b)\nWordCount 234 32 29 90.63 3.9\nGrep 236 37 33 89.19 4.8\nTPC-H 102 13 12 92.31 6.72\nTPC-DS 126 13 12 92.31 6.1\nK-means 234 34 29 85.29 1.25\nPageRank 235 28 25 89.29 6.2\nTABLE 4\nThe accuracy of symptom detection for the outliers stemming from\nresource variation in a heterogeneous cluster\nBenchmark Total\ntasksR Outliers\n(detected as Sr)Accuracy\n(%)Error\n(\u001b)\nWordCount 234 37 33 89.19 2.77\nGrep 236 26 24 92.31 4.77\nTPC-H 102 9 8 88.89 5.47\nTPC-DS 126 13 12 92.31 6.9\nK-means 234 36 33 91.67 2.88\nPageRank 235 30 28 93.33 5.35\n5.2 Diagnosis detection evaluation\nIn this section, we evaluate the accuracy of our symptom\ndetection method. To this end, we execute our benchmarks\ninEnvBto increase number of Sdtasks (see §4.2.2). Next,\nto increase the issue of resource heterogeneity (Sr referring\nto §4.2.3), we run the benchmarks in EnvC. Thereafter,\nwe run the benchmarks in EnvHto emulate the network\nfailure (Sw referring to §4.2.4). Finally, we compare the\ndetected Outlier tasks with the ground truths that are the\ndata locality, resource heterogeneity, and network failure\nissues observed by the AutoDiagn diagnosing system.\nTable 3, Table 4, and Table 5 summarize all the results. All\nbenchmarks achieve high accuracy by using our proposal\nsymptom detection method. The highest accuracy for both\nSdand Srare 92.3%, and for Swis 94.7% and the overall\naccuracy for outlier detection is 91.3%, where the Error\nrepresents the variation of the accuracy depending on the\nrepeated experiments.\nWe compute the accuracy of our symptom detection\nmethod by using the number of detected outlier tasks di-\nvided by the actual number of the tasks that can cause the\noutlier issue. Table 3, for example, Dis the total number of\nnon-local tasks and Outliers (Sd) is the number of detected\noutlier tasks that belong to non-local task. Therefore, the\naccuracy isSd\nD. Table 4 and Table 5 follow the same approach\nto compute the accuracy.\nOutlier veriﬁcation. To further verify the Sd,Sr, and Sw\nare the main reasons causing the outliers, we conduct the\nfollowing comparison experiments: 1) comparing the exe-\ncution time of local tasks and non-local tasks; 2) comparing\nthe execution time of the tasks running in EnvAand Env\nC; and 3) comparing the execution time of normal tasks and\nrestarted tasks due to network failure. Fig. 6(a) proves that\nnon-local tasks consume more time than local tasks due to\nthe overload introduced by data shufﬂing. Additionally, weTABLE 5\nThe accuracy of symptom detection for the outliers stemming from\nnetwork failures\nBenchmark Total\ntasksW Outliers\n(detected as Sw)Accuracy\n(%)Error\n(\u001b)\nWordCount 234 11 10 90.91 1.83\nGrep 236 13 12 92.31 6.73\nTPC-H 102 13 12 92.31 6.54\nTPC-DS 126 15 14 93.33 5.43\nK-means 234 17 16 94.12 4.33\nPageRank 235 19 18 94.74 4.23\ncompare the throughput of the local tasks and non-local\ntasks in terms of how much data can be processed in each\nsecond. Fig. 7 reveals that the throughput of non-local tasks\nis only 70% that of local tasks.\nMoreover, Fig. 6(b) shows that the execution time of\nthe tasks running on EnvAis less than that on EnvC.\nThis is because the tasks are equally distributed to all\ncomputing nodes and the less powerful nodes are saturated.\nFurthermore, Fig. 9(a) shows that the CPU usage of less\npowerful hosts reaches 100%, thereby building a task queue\nin these hosts, increasing the overall execution time. How-\never, Fig. 9(b) reveals that the powerful hosts have sufﬁcient\ncomputing resources for processing the allocated tasks.\nFurthermore, Fig. 6(c) shows that the execution time of\nthe restarted tasks are longer than the normal tasks. As\nFig. 8 illustrates, we compute the execution time of the\nrestarted task by adding the execution time of the task in\nthe disconnected node and that in the rescheduled node.\n5.3 Performance and overheads\nPerformance evaluation. We evaluate the performance of\nAutoDiagn by measuring the end-to-end response time of\nsymptom detection and root-cause analysis. Since they are\nnot affected by the types of benchmark, we report the\naverage of the response time. Fig. 10(a) shows that the\nreal-time symptom detection can achieve a low response\ntime, which only has 96 milliseconds and 1059 milliseconds\nwith 100 tasks and 1000 tasks, respectively. Although the re-\nsponse time increases linearly, the parallel execution method\ndiscussed in §3.6 can be applied to reduce the latency. The\nresponse time for root cause analysis is higher than that\nof symptom detection. For 100 tasks and 1000 tasks, their\nresponse times are 0.354 seconds and 5.974 seconds, respec-\ntively. Unlike the symptom detection which is very sensitive\nto latency because of the follow-up processes, triggering the\nfurther root-cause analysis or alerting the system managers,\nRoot-cause analysis aims to provide a holistic diagnosing of\na big system and the analysis results may help to improve\nthe system performance in future. As a result, the real-time\nroot-cause analysis is not compulsory.\nSystem overheads. To evaluate the system overhead intro-\nduced by AutoDiagn, we measure the CPU and memory\nusage of AutoDiagn Monitoring (agent) and AutoDiagn\nDiagnosing. Table 6 shows that -AutoDiagn Monitoring only\nconsumes approximately 2.52% memory and 4.69% CPU;\nwhile -AutoDiagn Diagnosis uses 2.08% memory and 3.49%\nCPU.\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 10]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n10\n 0 5 10 15 20 25 30 35 40\nWordCountGrepTPC-HTPC-DS K-means PageRankExecution time (sec)\nTypes of BenchmarkingLocal tasks running on Env A\nNon-local tasks (D) running on Env B\n(a) Local tasks vs Non-local tasks\n 0 5 10 15 20 25 30 35 40\nWordCountGrepTPC-HTPC-DS K-means PageRankExecution time (sec)\nTypes of BenchmarkingTasks running on Env A\nTasks (R) running on Env C (b) Homogeneous cluster vs Heterogeneous\ncluster\n 0 10 20 30 40 50 60\nWordCountGrepTPC-HTPC-DS K-means PageRankExecution time (sec)\nTypes of BenchmarkingTasks running on Env A\nTasks (W) running on Env H(c) Normal tasks vs Restarted tasks caused by\nnetwork failure\nFig. 6. Comparison of execution time of the tasks\n 0 1 2 3 4 5 6\nWordCountGrepTPC-HTPC-DS K-means PageRankThroughput (MB/s)\nTypes of BenchmarkingLocal tasks Non-local tasks\nFig. 7. The throughput of AutoDiagn\n 0 20 40 60 80 100\n0510152025303540455055Progress (%)\nElapsed time (sec)\nFig. 8. The life cycle of the restarted task\nFig 10(b) shows the network overhead of AutoDiagn.\nThe extra communication cost introduced by our tool is\nsmall but it increases when the number of parallel tasks\nincreases. For example, when the number of parallel task is\n100, there are about 45 messages per second sent from agents\nto RabbitMQ cluster and the total size of these messages is\n13.5 KB/s. The message rate and network overhead increase\nto 615 per second and 223 KB/s, respectively, when the\nnumber of parallel tasks is 1000.\nStorage overheads. AutoDiagn needs to dump the system\ninformation to a database which may consume extra storage\nresource. In our evaluation experiments, it only cost 3.75\nMB disk space in total. Obviously, increasing the types\nof symptom detection and root cause analysis will also\nconsume more storage resources. We discuss the potentialTABLE 6\nResource overhead caused by AutoDiagn components\nComponents Mem (%) CPU (%)\nAutoDiagn Monitoring 2.52 4.69\nAutoDiagn Diagnosing 2.08 3.49\nfuture work in §6.\n6 D ISCUSSION AND FUTURE WORK\nPopulating applications. In this paper, we propose a gen-\neral and ﬂexible framework to uncover the performance\nreduction issues in a big data system. In particular, we\ndevelop and evaluate big data applications for outliers. New\napplications (including symptom detection and root-cause\nanalysis) are required to populate our system for future\nwork.\nOverhead cost reduction. Our system is designed in a\nloosely-coupled manner, the processing components can\nbe easily scaled. However, the storage overhead increases\nwith the number of applications increasing. [15] proposed a\ncaching method to aggregate the information before sending\nto destination nodes. We will explore this direction in future\nwork to reduce the storage overhead and network overhead.\nPerformance improvement. Mantri [10] utilized the outputs\nof the root cause analysis to improve the resource allocation\nin Hadoop clusters. Thus, one open research direction is to\nbuild a system which can react to analysis results, thereby\nimproving the performance of the big data system.\n7 R ELATED WORK\nMuch recent work in big data systems focuses on improving\nworkﬂows [16], [17], [18], programming framework [19],\n[20], [21], task scheduling [22], [23], [24].\nRoot-cause analysis. There is a large volume of published\nstudies describing the role of root-cause analysis. The au-\nthors of [10], [25], [26] take the next step of understanding\nthe reasons for performance reduction. Mantri [10] charac-\nterizes the prevalence of stragglers in Hadoop systems as\nwell as troubleshooting the cause of stragglers. Dean and\nBarroso [25] analyze the issues causing tail latency in big\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 11]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n11\n 0 20 40 60 80 100 CPU usage (%)\nTimelineCPU utilization Outliers\n(a) CPU utilization of less powerful hosts and outliers\n 0 20 40 60 80 100 CPU usage (%)\nTimelineCPU utilization (b) CPU utilization of high power hosts\nFig. 9. CPU utilization of two nodes running simultaneously. Outliers are most likely to occur in the nodes which have less computing resource.\n 0 1 2 3 4 5 6\n50100 200 300 400 500 600 700 800 9001000Response time (sec)\nNumber of tasks running in parallelSymptom detectionRoot-cause analysis\n(a) The end-to-end response time of AutoDiagn diag-\nnosis system\n 0 100 200 300 400 500 600\n501002003004005006007008009001000 0 50 100 150 200 250Messages per second\nData rate (KB/s)\nNumber of tasks running in parallelMessage rates\nSize (KB/s)(b) The message rates and network overhead\nFig. 10. Performance evaluation and network overhead of AutoDiagn\ndata systems. Garraghan et al. [11], [27] proposed a new\nmethod to identify long tail behavior in big data systems\nand evaluated in google data trace. The authors in [28] use\nofﬂine log analysis methods to identify the root cause of\noutliers in a large-scale cluster consisting of thousands of\nnodes by tracking the resource utilization. Similarly, Zhou\net al. [29] use a simple but efﬁcient rule based method to\nidentify the root cause of stragglers.\nAlong with these similar works, there are some re-\nsearchers using statistical and machine learning methods for\nroot-cause analysis. The authors of [30] introduce a Regres-\nsion Neural Network (RNN) based algorithm to trouble-\nshoot the causes of stragglers by processing Spark logs.\nMore algorithms such as the associated tree and fuzzy data\nenvelopment analysis [31] and Reinforcement Learning [32]\nare applied for ﬁnding the reasons of stragglers in Hadoop\nand Spark.\nIn [33], a Pearson coefﬁcient of correlation is used for\nroot cause analysis to measure linear correlation between\nsystem metrics, workload and latency. However, these\nworks lack a systematic solution for root cause analysis for\nbig data processing systems and the proposed methods are\nnot applicable for real-time systems.\nDifferent to other work, the authors of [34] propose a\nnew algorithm that aims to reduce the proportion of strag-\ngler tasks in machine learning systems that use gradient-\ndescent-like algorithms. This work offers an idea to develop\nnew Diagnosers for machine learning systems using our\nframework.\nAnomaly detection and debugging. The authors in [35] pro-\npose a rule-based approach to identify anomalous behaviorsin Hadoop ecosystems by analyzing the task logs. This\nwork only analyzes the task logs, which fails to capture the\nperformance reduction issues caused by inefﬁcient utilizing\nthe underlying resources. Next, Khoussainova et al. [36]\nbuild a historical log analysis system to study and track\nthe MapReduce jobs which cause performance reduction\nbased on their relevance, precision and generality principles.\nHowever, this cannot be performed for real-time anomaly\ndetection. Du et al. [37] train a machine learning model from\nthe normal condition data by using Long Short-Term Mem-\nory (LSTM) and this trained model is used for detecting\nin Hadoop and OpenStack environments. Our AutoDiagn\nprovides infrastructure into which the trained models can\nbe plugged to enrich the applications.\nReal-time operational data analytic system. Agelastos et al.\n[38] propose a monitoring system for HPC systems, which\ncan capture the cases of applications competing for shared\nresources. However, this system does not consider root-\ncause analysis of the performance reduction. The authors\nof [5], [39] do not only provide the feature of real-time\nmonitoring, but are also able to identify the performance\nissues and trouble-shoot the cause of the issues. In addition\nto them, [40] uses a type of artiﬁcial neural network called\nautoencoder for anomaly detection. They ﬁrst monitor the\nsystem in real-time and collect the normal data for training\nthe model used to discern between normal and abnormal\nconditions in an online fashion. However, these systems are\ndeveloped for HPC clusters and are not suitable for big data\nsystems.\nTable 7 presents a brief overview of various monitoring\ntools for big data frameworks.\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 12]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n12\nTABLE 7\nThe features supported by existing tools and AutoDiagn\nFeature DataDog\n[2]Sequence\nIQ [3]Sematext\n[4]TACC\n[5]Mantri\n[10]DCDB\n[39]Nagios\n[41]Ganglia\n[42]Chukwa\n[43]DMon\n[44]AutoDiagn\nReal-time monitor-\ningYes Yes Yes Yes Yes Yes Yes Near\nreal-timeYes Near real-\ntimeYes\nRoot-cause analysis No No No No Yes Yes No No No Yes Yes\nBigData frameworks\nsupportGood Poor Good No Poor No Poor Poor Poor Good and\nExtensibleGood and\nExtensible\nUnderlying resource\nmonitoringYes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes\nReal-time monitor-\ning for big data tasksYes Yes Yes No Yes No No No Yes Yes Yes\nAuto-scaling Yes Yes Yes Yes Yes Yes No No Yes Yes Yes\nAlerts Yes No Yes No No No Yes No No No Yes\nVisualization of big\ndata tasksYes No Yes No No No No Yes No No Yes\nUser customized\nroot-cause analysisNo No No No No No No No No No Yes\n8 C ONCLUSION\nIn this paper, we have presented AutoDiagn, a framework\nfor enabling diagnosing of large-scale distributed systems\nto ascertain the root cause of outliers, with the core purpose\nof unravelling the concretization of complicated models\nfor system management. After making a comprehensive\nliterature review and identifying the requirements for real-\nworld problems, we conceived its design. The combination\nof user-deﬁned functions powered by APIs and the agent-\nbased monitoring system along with the ﬁndings obtained\nfrom an empirical analysis of the experiments we conducted\nplay a fundamental role in the development of the system.\nAutoDiagn can be applied to most big data systems along\nwith the monitoring systems. We have also presented the\nimplementation and integration of the AutoDiagn system to\nthe SmartMonit [45], real-time big data monitoring system,\ncombined in our production environment. In our implemen-\ntation on a large cluster, we ﬁnd AutoDiagn very effective\nand efﬁcient.\nOutliers are one of the main problems in big data sys-\ntems that overwhelm the whole system and reduce perfor-\nmance considerably. AutoDiagn embraces this problem to\nreveal the bottlenecks alongside their root causes.\nACKNOWLEDGEMENT\nThis research is funded by the Turkish Ministry of Na-\ntional Education. This research is partially funded by\nthe following UKRI projects: SUPER (EP/T021985/1),\nPACE (EP/R033293/1), and Centre for Digital Citizens\n(EP/T022582/1). This work is also supported by the grant\nof National Natural Science Foundation of China (62072408)\nand Zhejiang Provincial Natural Science Foundation of\nChina (LY20F020030).\nREFERENCES\n[1] A. Noor, K. Mitra, E. Solaiman, A. Souza, D. N. Jha, U. Demirbaga,\nP . P . Jayaraman, N. Cacho, and R. Ranjan, “Cyber-physical appli-\ncation monitoring across multiple clouds,” Computers & Electrical\nEngineering, vol. 77, pp. 314–324, 2019.[2] Datadog. Accessed: 2020-07-13. [Online]. Available: https:\n//www.datadoghq.com/\n[3] Sequenceiq. Accessed: 2020-07-14. [Online]. Available: https:\n//github.com/sequenceiq\n[4] Sematext. Accessed: 2020-07-13. [Online]. Available: https:\n//sematext.com/\n[5] R. T. Evans, J. C. Browne, and W. L. Barth, “Understanding\napplication and system performance through system-wide moni-\ntoring,” in 2016 IEEE International Parallel and Distributed Processing\nSymposium Workshops (IPDPSW). IEEE, 2016, pp. 1702–1710.\n[6] G. Iuhasz, D. Pop, and I. Dragan, “Architecture of a scalable\nplatform for monitoring multiple big data frameworks,” Scalable\nComputing: Practice and Experience, vol. 17, no. 4, pp. 313–321, 2016.\n[7] I. Dr ˘agan, G. Iuhasz, and D. Petcu, “A scalable platform for\nmonitoring data intensive applications,” Journal of Grid Computing,\nvol. 17, no. 3, pp. 503–528, 2019.\n[8] S. Babu, “Towards automatic optimization of mapreduce pro-\ngrams,” in Proceedings of the 1st ACM symposium on Cloud com-\nputing, 2010, pp. 137–142.\n[9] R. S. Xin, J. Rosen, M. Zaharia, M. J. Franklin, S. Shenker, and\nI. Stoica, “Shark: Sql and rich analytics at scale,” in Proceedings of\nthe 2013 ACM SIGMOD International Conference on Management of\ndata, 2013, pp. 13–24.\n[10] G. Ananthanarayanan, S. Kandula, A. G. Greenberg, I. Stoica,\nY. Lu, B. Saha, and E. Harris, “Reining in the outliers in map-\nreduce clusters using mantri.” in Osdi, vol. 10, no. 1, 2010, p. 24.\n[11] P . Garraghan, X. Ouyang, P . Townend, and J. Xu, “Timely long\ntail identiﬁcation through agent based monitoring and analytics,”\nin2015 IEEE 18th International Symposium on Real-Time Distributed\nComputing. IEEE, 2015, pp. 19–26.\n[12] J. Han, J. Pei, and M. Kamber, Data mining: concepts and techniques.\nElsevier, 2011.\n[13] T. Renner, L. Thamsen, and O. Kao, “Coloc: Distributed data and\ncontainer colocation for data-intensive applications,” in 2016 IEEE\nInternational Conference on Big Data (Big Data). IEEE, 2016, pp.\n3008–3015.\n[14] A. Rasooli and D. G. Down, “Guidelines for selecting hadoop\nschedulers based on system heterogeneity,” Journal of grid com-\nputing, vol. 12, no. 3, pp. 499–519, 2014.\n[15] A. Rabkin, M. Arye, S. Sen, V . S. Pai, and M. J. Freedman,\n“Aggregation and degradation in jetstream: Streaming analytics in\nthe wide area,” in 11thfUSENIXg Symposium on Networked Systems\nDesign and Implementation (fNSDIg 14), 2014, pp. 275–288.\n[16] Z. Wen, T. Lin, R. Yang, S. Ji, R. Ranjan, A. Romanovsky, C. Lin,\nand J. Xu, “Ga-par: Dependable microservice orchestration frame-\nwork for geo-distributed clouds,” IEEE Transactions on Parallel and\nDistributed Systems, vol. 31, no. 1, pp. 129–143, 2019.\n[17] Z. Wen, J. Cała, P . Watson, and A. Romanovsky, “Cost effective,\nreliable and secure workﬂow deployment over federated clouds,”\nIEEE Transactions on Services Computing, vol. 10, no. 6, pp. 929–941,\n2016.\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 13]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n13\n[18] Z. Wen, R. Qasha, Z. Li, R. Ranjan, P . Watson, and A. Romanovsky,\n“Dynamically partitioning workﬂow over federated clouds for\noptimising the monetary cost and handling run-time failures,”\nIEEE Transactions on Cloud Computing, 2016.\n[19] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn,\nN. Leiser, and G. Czajkowski, “Pregel: a system for large-scale\ngraph processing,” in Proceedings of the 2010 ACM SIGMOD Inter-\nnational Conference on Management of data, 2010, pp. 135–146.\n[20] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, I. Stoica\net al., “Spark: Cluster computing with working sets.” HotCloud,\nvol. 10, no. 10-10, p. 95, 2010.\n[21] M. Abadi, P . Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin,\nS. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for\nlarge-scale machine learning,” in 12thfUSENIXg symposium on\noperating systems design and implementation (fOSDIg 16), 2016, pp.\n265–283.\n[22] M. Isard, V . Prabhakaran, J. Currey, U. Wieder, K. Talwar, and\nA. Goldberg, “Quincy: fair scheduling for distributed computing\nclusters,” in Proceedings of the ACM SIGOPS 22nd symposium on\nOperating systems principles, 2009, pp. 261–276.\n[23] N. J. Yadwadkar and W. Choi, “Proactive straggler avoidance\nusing machine learning,” White paper, University of Berkeley, 2012.\n[24] A. Badita, P . Parag, and V . Aggarwal, “Optimal server selection\nfor straggler mitigation,” IEEE/ACM Transactions on Networking ,\nvol. 28, no. 2, pp. 709–721, 2020.\n[25] J. Dean and L. A. Barroso, “The tail at scale,” Communications of the\nACM, vol. 56, no. 2, pp. 74–80, 2013.\n[26] K. Ousterhout, R. Rasti, S. Ratnasamy, S. Shenker, and B.-G. Chun,\n“Making sense of performance in data analytics frameworks,”\nin12thfUSENIXg Symposium on Networked Systems Design and\nImplementation (fNSDIg 15), 2015, pp. 293–307.\n[27] P . Garraghan, X. Ouyang, R. Yang, D. McKee, and J. Xu, “Straggler\nroot-cause and impact analysis for massive-scale virtualized cloud\ndatacenters,” IEEE Transactions on Services Computing, vol. 12, no. 1,\npp. 91–104, 2016.\n[28] X. Ouyang, P . Garraghan, R. Yang, P . Townend, and J. Xu, “Re-\nducing late-timing failure at scale: Straggler root-cause analysis in\ncloud datacenters,” in Fast Abstracts in the 46th Annual IEEE/IFIP\nInternational Conference on Dependable Systems and Networks. DSN,\n2016.\n[29] H. Zhou, Y. Li, H. Yang, J. Jia, and W. Li, “Bigroots: An effective\napproach for root-cause analysis of stragglers in big data system,”\nIEEE Access, vol. 6, pp. 41 966–41 977, 2018.\n[30] S. Lu, X. Wei, B. Rao, B. Tak, L. Wang, and L. Wang, “Ladra:\nLog-based abnormal task detection and root-cause analysis in big\ndata processing with spark,” Future Generation Computer Systems,\nvol. 95, pp. 392–403, 2019.\n[31] Z. He, Y. He, F. Liu, and Y. Zhao, “Big data-oriented product infant\nfailure intelligent root cause identiﬁcation using associated tree\nand fuzzy dea,” IEEE Access, vol. 7, pp. 34 687–34 698, 2019.\n[32] H. Du and S. Zhang, “Hawkeye: Adaptive straggler identiﬁcation\non heterogeneous spark cluster with reinforcement learning,”\nIEEE Access, vol. 8, pp. 57 822–57 832, 2020.\n[33] J. P . Magalh ˜aes and L. M. Silva, “Root-cause analysis of perfor-\nmance anomalies in web-based applications,” in Proceedings of the\n2011 ACM Symposium on Applied Computing, 2011, pp. 209–216.\n[34] R. Bitar, M. Wootters, and S. El Rouayheb, “Stochastic gradient\ncoding for straggler mitigation in distributed learning,” IEEE\nJournal on Selected Areas in Information Theory, vol. 1, no. 1, pp.\n277–291, 2020.\n[35] A. M. Chacko, J. S. Medicherla, and S. M. Kumar, “Anomaly\ndetection in mapreduce using transformation provenance,” in\nAdvances in Big Data and Cloud Computing. Springer, 2018, pp.\n91–99.\n[36] N. Khoussainova, M. Balazinska, and D. Suciu, “Perfx-\nplain: debugging mapreduce job performance,” arXiv preprint\narXiv:1203.6400, 2012.\n[37] M. Du, F. Li, G. Zheng, and V . Srikumar, “Deeplog: Anomaly\ndetection and diagnosis from system logs through deep learning,”\ninProceedings of the 2017 ACM SIGSAC Conference on Computer and\nCommunications Security, 2017, pp. 1285–1298.\n[38] A. Agelastos, B. Allan, J. Brandt, P . Cassella, J. Enos, J. Fullop,\nA. Gentile, S. Monk, N. Naksinehaboon, J. Ogden et al., “The\nlightweight distributed metric service: a scalable infrastructure\nfor continuous monitoring of large scale computing systems and\napplications,” in SC’14: Proceedings of the International Conferencefor High Performance Computing, Networking, Storage and Analysis.\nIEEE, 2014, pp. 154–165.\n[39] A. Netti, M. M ¨uller, C. Guillen, M. Ott, D. Tafani, G. Ozer, and\nM. Schulz, “Dcdb wintermute: Enabling online and holistic op-\nerational data analytics on hpc systems,” in Proceedings of the 29th\nInternational Symposium on High-Performance Parallel and Distributed\nComputing, 2020, pp. 101–112.\n[40] A. Borghesi, A. Bartolini, M. Lombardi, M. Milano, and L. Benini,\n“Anomaly detection using autoencoders in high performance\ncomputing systems,” in Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, vol. 33, 2019, pp. 9428–9433.\n[41] Nagios. Accessed: 2020-07-15. [Online]. Available: https://www.\nnagios.org/\n[42] Ganglia. Accessed: 2020-07-15. [Online]. Available: http://ganglia.\ninfo/\n[43] Apache chukwa. Accessed: 2020-07-14. [Online]. Available:\nhttps://chukwa.apache.org/\n[44] Dmon. Accessed: 2020-07-12. [Online]. Available: https://github.\ncom/Open-Monitor/dmon\n[45] U. Demirbaga, A. Noor, Z. Wen, P . James, K. Mitra, and R. Ranjan,\n“Smartmonit: Real-time big data monitoring system,” in 2019 38th\nSymposium on Reliable Distributed Systems (SRDS). IEEE, 2019, pp.\n357–3572.\nUmit Demirbaga (Member, IEEE) is a PhD stu-\ndent in the School of Computing, Newcastle\nUniversity, UK. He received an MSc degree in\nComputer Science from Newcastle University,\nUK in 2017 and the BSc degree in Electronics\nand Computer Education from Marmara Univer-\nsity, Turkey in 2011. His research interests in-\nclude big data analytics, cloud computing and\ndistributed systems. He was awarded Outstand-\ning Performance Award with Best Team Project\nAward in his MSc in 2017.\nZhenyu Wen (Member, IEEE) received MSc and\nPhD degrees in Computer Science from New-\ncastle University, Newcastle upon Tyne, UK, in\n2011 and 2016, respectively. He is currently a\nPostdoc Researcher with the School of Com-\nputing, Newcastle University, UK. His current re-\nsearch interests include IoT, crowd sources, AI\nsystem, and cloud computing. For his contribu-\ntions to the area of scalable data management\nfor the Internet of Things. He was awarded the\nIEEE TCSC Award for Excellence in Scalable\nComputing (Early Career Researchers) in 2020.\nAyman Noor is a PhD student in Computer\nScience at Newcastle University, UK. His cur-\nrent research interests include cloud computing,\nmonitoring, and machine learning. He earned a\nMaster of Science in Computer and Information\nScience from Gannon University, PA, USA in\n2013 and a Bachelor in Computer Science from\nthe College of Computer Science and Engineer-\ning from Taibah University, Madinah, SA in 2006.\nKaran Mitra is an Assistant Professor at Lule ˚a\nUniversity of Technology, Sweden. He received\nhis Dual-badge PhD from Monash University,\nAustralia and Lule ˚a University of Technology in\n2013. His research interests include cloud and\nmobile cloud computing, performance bench-\nmarking of distributed systems, context-aware\ncomputing and QoE. He is a member of the IEEE\nand ACM.\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 14]\n0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE\nTransactions on Computers\n14\nKhaled Alwasel has a BS and MS in informa-\ntion technology from Indiana University-Purdue\nUniversity Indianapolis (2014) and Florida Inter-\nnational University (2015), USA. He is currently\nworking toward a PhD in the School of Com-\nputing Science at Newcastle University (UK).\nKhaled’s interests lie in the areas of software-\ndeﬁned networking (SDN), big data, IoT, edge\ncomputing, and cloud computing\nSaurabh Garg is a lecturer at the University of\nTasmania, Hobart, Tasmania. He has published\nmore than 30 papers in highly cited journals\nand conferences with H-index 24. He has gained\nabout three years of experience in industrial re-\nsearch while working at IBM Research Australia\nand India. His areas of interest are distributed\ncomputing, cloud computing, HPC, IoT, big data\nanalytics, and education analytics.\nAlbert Y. Zomaya is currently the Chair Pro-\nfessor of High Performance Computing & Net-\nworking in the School of Computer Science,\nUniversity of Sydney. He is also the Director of\nthe Centre for Distributed and High Performance\nComputing which was established in late 2009.\nProfessor Zomaya was an Australian Research\nCouncil Professorial Fellow during 2010-2014\nand held the CISCO Systems Chair Professor\nof Internetworking during the period 2002–2007\nand also was Head of School for 2006–2007.\nRajiv Ranjan is a Full professor in Comput-\ning Science at Newcastle University, UK. Before\nmoving to Newcastle University, he was Julius\nFellow (2013-2015), Senior Research Scientist\nand Project Leader in the Digital Productivity and\nServices Flagship of Commonwealth Scientiﬁc\nand Industrial Research Organization (CSIRO\nC Australian Government’s Premier Research\nAgency). Prior to that he was a Senior Research\nAssociate (Lecturer level B) in the School of\nComputer Science and Engineering, University\nof New South Wales (UNSW). Dr Ranjan has a PhD (2009) from\nthe department of Computer Science and Software Engineering, the\nUniversity of Melbourne.\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore.  Restrictions apply.",
      "metadata": {
        "filename": "demirbaga2022.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\demirbaga2022.pdf",
        "file_size": 3420259,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:37.106306",
        "content_length": 76788
      }
    },
    "9ae75c26-80ce-494f-af6d-7db9503ae926": {
      "id": "9ae75c26-80ce-494f-af6d-7db9503ae926",
      "content": "[Página 1]\nBigBench: TowardsanIndustryStandardBenchmarkfor\nBi\ngDataAnalytics\nAhmadGhazal1,5,TilmannRabl2,6,MinqingHu1,5,\nFrancois Raab4,8,MeikelPoess3,7,AlainCrolotte1,5,Hans-Arno Jacobsen2,9\n1TeradataCorp.,2UniversityofToronto,3OracleCorp.,4InfoSizing,Inc.\n5{ahmad.ghazal,minqing.hu,alain.crolotte}@teradata.com,6tilmann@msrg.utoronto.ca\n7meikel.poess@oracle.com,8francois@sizing.com,9jacobsen@eecg.toronto.edu\nABSTRACT\nThere is a tremendous interest in big data by academia,\nindustryanda large user base. Several commercial andopen\nsource providers unleashed a variety of products to support\nbig data storage and processing. As these products mature,\nthere is a need to evaluate and compare the performance of\nthese systems.\nIn this paper, we present BigBench, an end-to-end big\ndata benchmark proposal. The underlying business model\nof BigBench is a product retailer. The proposal covers a\ndata model and synthetic data generator that addresses the\nvariety, velocity and volume aspects of big data systems con-\ntaining structured, semi-structured and unstructured data.\nThe structured part of the BigBench data model is adopted\nfrom the TPC-DS benchmark, which is enriched with semi-\nstructured and unstructured data components. The semi-\nstructured part captures registered and guest user clicks\non the retailer’s website. The unstructured data captures\nproduct reviews submitted online. The data generator de-\nsigned for BigBench provides scalable volumes of raw data\nbased on a scale factor. The BigBench workload is designed\naround a set of queries against the data model. From a busi-\nness prospective, the queries cover the diﬀerent categories of\nbig data analytics proposed by McKinsey. From a technical\nprospective, the queries are designed to span three diﬀerent\ndimensions based on data sources, query processing types\nand analytic techniques.\nWe illustrate the feasibility of BigBench by implement-\ning it on the Teradata Aster Database. The test includes\ngenerating and loading a 200 Gigabyte BigBench data set\nand testing the workload by executing the BigBench queries\n(written using Teradata Aster SQL-MR)and reporting their\nresponse times.\nCategoriesandSubjectDescriptors\nD.2.8[Software Engineering ]: Metrics— performance mea-\nsures\nPermission to make digital or hard copies of all or part of this work for\npersonal or classroom use is granted without fee provided that copies are\nnot madeor distributed for proftor commercial advantage and that copies\nbearthisnoticeandthefullcitation onthefrstpage. Tocopyotherwise,to\nrepublish,topostonserversortoredistributetolists,requirespriorspecifc\npermissionand/or afee.\nSIGMOD’13, June22–27,2013,NewYork,NewYork,USA.\nCopyright 2013ACM978-1-4503-2037-5/13/06 ...$15.00.Keywords\nBenchmarking; big data; map reduce\n1. INTRODUCTION\nToday’s data explosion, fueled by emerging applications,\nsuch as social networking, micro blogs, and the“crowd intel-\nligence”capabilities of many sites, has led to the“big data”\nphenomenon. It is characterized by increasing volumes of\ndata of disparate types (i.e., structured, semi-structuredand\nunstructured)from sources that generate new data at a high\nrate (e.g., click streams captured in web server logs). This\nwealth of data provides numerous new analytic and business\nintelligence opportunitieslike fraud detection, customer pro-\nﬁling, and churn and customer loyalty analysis.\nConsequently, there is tremendous interest in academia\nand industry to address the challenges in storing, access-\ning and analyzing this data. Several commercial and open\nsource providers already unleashed a variety of products to\nsupport big data storage and processing. These tools are\nmostly parallel database management systems (e.g., Green-\nplum[4], Netezza’s TwinFin[9], Teradata[8], Oracle[6]) or\nMapReduce (MR) based systems (e.g., Hadoop [1], Cloud-\nera’s CDH [3], Hive[2] and many other systems like those in\n[15, 17, 24, 27]).\nAs big data systems mature, the pressure to evaluate and\ncompare performance and price performance of these sys-\ntems rises. However, to date there are no standard bench-\nmarks available. This takes us back to the middle of the\n1980’s, when the lack of standard database benchmarks led\nmanydatabasemanagementsystemvendorstopracticewhat\nis now referred to as“benchmarketing”– a practice in which\norganizations makeperformanceclaims basedonself-deﬁned,\nhighly biased benchmarks. The goal of publishing results\nfromsuchtailoredbenchmarkswastostatemarketingclaims,\nregardless of the absence of relevant and veriﬁable technical\nmerit. In essence, these benchmarks were designed as for-\ngone conclusions to ﬁt a pre-established marketing message.\nSimilarly, vendors would create conﬁgurations, referred to\nas “benchmark specials”, that were speciﬁcally designed to\nmaximize performance against a speciﬁc benchmark with\nlimited beneﬁt to real-world applications.\nTowards the end of the 1980’s, as a response to this grow-\ning practice, benchmark consortia such as the Transaction\nProcessing Performance Council (TPC) and the Standard\nPerformance Corporation (SPEC) were founded. Inﬂuenced\nbyacademic databaseexpertsandwell-known industrylead-\n1197\n\n[Página 2]\nUnstructured \nDa\nta \nSemi-Structured Data Structured Data \nSales \nCustomer \nItem \n Marketprice \nWeb Page \nWeb Log \nReviews \nAdapted \nTP\nC-DS \nBigBench \nSpecific \nFigure 1: Big Data Benchmark Data Model\ner\ns, industry standard benchmarks such as TPC-A, TPC-C\nand TPC-D were engineered and rules around publishing\nresults were agreed upon.\nRecently a few eﬀorts in the area of big data benchmarks\nemerged, such as YCSB[16], PigMix[7], GridMix [5] and\nGraySort [20]. These eﬀorts are island solutions and not\npoliced by any industry consortia. While some are focused\non one or a subset of components and tasks typical for big\ndata systems, others are based on speciﬁc map-reduce-style\nsystems.\nWebelieveanindustrystandardbigdatabenchmarkmust\nbe an end-to-end benchmark covering all major characteris-\ntics in the lifecycle of a big data system including the three\nVs described by Douglas Laney[21]: (i) volume(larger data\nset sizes), (ii) velocity (higher data arrival rates, such as\nclick streams) and (iii) variety(increased data type dispar-\nity, such as structured data from relational tables, semi-\nstructured data from key-value web clicks and un-structured\ndata from social media content).\nIn this paper, we present our proposal for an end-to-end\nbig data benchmark. After a presentation of initial ideas for\nthe benchmark at the ﬁrst Workshop on Big Data Bench-\nmarking1a group formed that collaborated on building the\nspeciﬁcation. We call it “BigBench”. It is based on a ﬁcti-\ntious retailer who sells products to customers via physical\nand online stores. The proposal covers a data model, syn-\nthetic data generator and workload description. The work-\nload queries are speciﬁed in English, since no clear standard\nfor big data systems has yet emerged. We also suggest di-\nrections for big data metrics speciﬁc to data loading and\nworkload execution. Furthermore, the feasibility of the pro-\nposal is validated by implementing it on the Teradata Aster\nDBMS (TAD). This experiment involves generating 200 Gi-\ngabyte of raw data and loading it into TAD. The English\nlike workload queries are implemented using TAD’s SQL-\nMR syntax and executed as a single stream of queries.\nTheﬁrstmajor componentofBigBenchisthespeciﬁcation\nof a data model that focuses on volume, variety and velocity.\nThe variety property of our model is illustrated in Figure\n1. The structured part of BigBench is adapted from the\nTPC-DS data model, which also depicts a product retailer\n[23]. We borrowed the store and online sales portion from\nthat model and added a table for prices from the retailer’s\ncompetitors.\nThe structured part is enriched with semi-structured and\n1WBDB, May 2012, San Jose – http://clds.ucsd.edu/\nwbdb2012un-structured data shown in the lower and right hand side\nof Figure 1. The semi-structured part is composed by clicks\nmade by customers and guest users visiting the retailer’s\nweb site. Our design assumes the semi-structured data to\nbe in a key-value format similar to Apache’s web server log\nformat. The un-structured data in our model is covered by\nproduct reviews that can be submitted by guest users or\nactual customers.\nWe also provide the design and implementation of a data\ngenerator for the proposed BigBench data model. Our data\ngenerator is based on an extension of PDGF [29]. PDGF is\na parallel data generator that is capable of producing large\namounts data for an arbitrary schema. The existing PDGF\ncan be used to generate the structured part of the BigBench\nmodel. However, it is not capable of producing neither the\nsemi-structured web clicks nor the unstructured product re-\nviewstext. Partofourcontributioninthispaperistoextend\nPDGF to coverthesemi-structuredandun-structuredparts.\nWe enhanced PDGF to produce a key-value data set for a\nﬁxed set of required and optional keys. This is suﬃcient to\ngenerate the web logs part of BigBench.\nThe main challenge in generating product reviews is to\nproduceun-structuredtext. Wedevelopedandimplemented\nan algorithm that produces synthetic text based on some\nsample inputtext. The algorithm usesa MarkovChain tech-\nnique that extracts key words and builds a dictionary based\non these key words. The new algorithm, called TextGen, is\napplied or our retailer model by using some real product re-\nviews from amazon.com for the initial sample data. PDGF\ninteracts with TextGen through an API sending product\ncategory as input and getting a product review text for that\ncategory.\nThe volumedimension of ourmodel is far simpler thanthe\nvariety discussion and previous data generators had a good\nhandle on that. PDGF handles the volume well since it can\nscale the size of the data based on a scale factor. It also\nruns eﬃciently for large scale factors since it runs in parallel\nand can leverage large systems dedicated for the benchmark.\nWe also address big data velocity by establishing a periodic\nrefresh scheme that constantly adds data to the diﬀerent\nareas of the data model.\nThe second major component of BigBench is the speci-\nﬁcation of workload queries applied on the BigBench data\nmodel. In terms of business questions, we found that the\nbig data retail analytics by McKinsey [22] serves our pur-\npose given that BigBench is about retail. In [22] ﬁve major\nareas, or business levers, of big data analytics are identiﬁed:\nmarketing, merchandising, operations, supplychainandnew\nbusiness models.\nIn addition to the big data retail business levers above,\nwe looked at three diﬀerent technical dimensions the Big-\nBench queries should span. The ﬁrst technical dimension is\nabout the type of data used in queries. This implies mak-\ning sure that structured types, semi-structured types, un-\nstructured types and their combinations are each covered\nin the queries. The second technical dimension covers the\ntwo common paradigms of declarative processing (SQL and\nsimilar constructs like HQL) and procedural MR processing.\nTo that end, some queries are best suited to be declarative,\nothers to be procedural and others to be a mix of both. The\nthirdtechnical dimensionis aboutthediﬀerentalgorithms of\nanalytic processing as described by the Apache MAHOUT\nsystem. Examples of these algorithms are classiﬁcations,\n1198\n\n[Página 3]\npattern matching, clustering, regression, dimensional redu c-\ntion, etc.\nIn summary, our key contributions are as follows:\n1. Wepresentthe ﬁrstend-to-endbenchmarkfor big data\nanalytics while previous work focused on few selected\ntypes of data or processing. BigBench implements the\ncomplete use-case of a realistic retail business.\n2. We specify 30 queries that cover all important aspects\nof big data analytics. The queries are speciﬁed in En-\nglish as well as TAD’s SQL-MR syntax.\n3. We develop and implement a novel technique for pro-\nducing un-structured text data and integrate it with a\ntraditional structured data generator.\n4. We conduct a proof of concept implementation and\nevaluation of BigBench by executing the benchmark\non the Teradata Aster DBMS.\nThe remainder of this paper is organized as follows. Sec-\ntion 2 covers previous work related to big data benchmark-\ning. Section 3 gives a detailed description of the BigBench\nbenchmark. The data model and data generation are de-\nscribed in detail in Sections 3.1 and 3.2. We describe the\nworkload queries in Section 3.3 and the benchmark metrics\nin Section 3.4. We present our proof of concept implemen-\ntation of BigBench using TAD in Section 4 including results\ninvolving 200 Gigabyte database. Finally, Section 5 summa-\nrizes the paper and suggests future directions.\n2. RELATEDWORK\nThe requirement for well deﬁned benchmarks that mea-\nsuretheperformanceofDBMSdealingwithverylarge amounts\nof data emerged when the ﬁrst generation of commercial\nsystems appeared in the 1980’s by Teradata Corporation\nand other more traditional DBMS vendors, who followed.\nDriven by vendor’s needs to compare commercial systems,\nthe Transaction Processing Performance Council developed\na series of data warehouse end-to-end benchmarks starting\nwith TPC-D in the beginning of the 90’s and TPC-H and\nTPC-R in the dawn of 2000 (all speciﬁcations available from\nthe TPC website2). These benchmarks, restricted to ter-\nabyte data sizes, emphasized single and multi-user perfor-\nmance of complex SQL query processing capabilities with\nsome updates on an enterprise data warehouse. Even ear-\nlier, academia started developing micro benchmarks such as\nthe Wisconsin benchmark, the OO7 [12] and BUCKY [13]\nbenchmarks for object-oriented DBMSs, XMark [31] and\nEXRT [14] benchmarks for XML-related DBMS technolo-\ngies.\nAs data volumes grew from megabytes of data and simple\ndata models (small number of tables with few relationships)\nover time to petabytesandcomplex data models (large num-\nber of tables with many complex relationships) the TPC\nresponded with the development of its next generation deci-\nsion support benchmark, TPC-DS [23], in the early 2000’s.\nStill based on the SQL programming language it contains\nmany big data elements, such as very large data and system\nsizes. Although the current limit is 100 terabyte, the data\ngenerator and schema can be extended to petabytes. It also\n2TPC -http://www.tpc.orgcontains very complex analytical queries using sophisticated\nSQL structures and a concurrent update model.\nIn parallel, academia as well as emerging big data com-\npanies have started deﬁning the next generation big data\nbenchmarks, which are mostly component and micro bench-\nmarks. Yahoo! developed its cloud serving benchmark,\nYCSB, to evaluate NoSQL data stores [16]. It is a ﬂexi-\nble multiuser benchmark with two tiers, a performance tier\n(testing latency) and a scalability tier. In the original paper,\nthree workloads were runagainst four diﬀerent data stores:\nHBase, Cassandra, PNUTs, and MySQL. Other evaluations\nfollowed that extended the scope of YCSB [30, 25]. The\nCALDA eﬀort [26] deﬁned a micro-benchmark for big data\nanalytics based on Google’s MapReduce paper and com-\npared Hadoop with two RDBMS systems, one that is row\nand one that is column organized. Another widely used\nbenchmark is the TeraSort or GraySort benchmark [20],\nwhich can be considered a micro benchmark that sorts a\nlarge number of 100-byte records doing considerable amount\nof computation, networking, and storage I/O. Other bench-\nmarks are the GridMix [5] and PigMix [7].\nTPC-DS [23, 28] is TPC’s latest decision support bench-\nmark. It covers the major three disciplines in the life-cycle\nof a relational decision support benchmark, namely (i) load-\ning the initial database (ii) executing queries in both single-\nand multi-user modes (iii) refreshing the database. TPC-DS\nhandles some aspects of big data like volume and some as-\npects of velocity. Still, it lacks key components of big data\nlike semi-structured and unstructured data and their asso-\nciated analytics.\nIn summary, previous benchmarks described in this sec-\ntion are mostly micro and component benchmarks. Others\nlike TPC-DS lack key big data characteristics. This brings\na need for an end-to-end benchmark for big data processing.\n3. BIGDATABENCHMARK\nThis section covers the major parts of the BigBench speci-\nﬁcation. Due to space restrictions, not all details can be pre-\nsented here. Additional details can be found in an extended\nversion of this paper, to be made available at publication\ntime.\n3.1 DataModel\nThe three cornerstone aspects of big data systems are vol-\nume,variety,velocity. Big data systems need to be able\nto deal with large volumes of data, sometimes in the mul-\ntiple petabyte range. We deal with the volume aspect in\nthe following section about data scaling. Variety refers to\nthe ability to deal with diﬀerently organized data, from un-\nstructured to semi-structured and structured data. The fol-\nlowing section about variety lays out a structure that cov-\ners all the types of data integrated in one model. Velocity\nrefers to the ability of a big data system to stay current\nthrough periodic refreshes, commonly referred to as extrac-\ntion,transformation andload(ETL). A big data system is\nnot a one-time snapshot of a business operations database\nnor is it a database where OLTP applications are running\nconcurrently. Hence, staying current with the operational\nside is a very important aspect of analytical systems, and\neven more so in the context of a big data system.\nIn the following subsection, we develop the data model\nshowing how the 3 Vs in big data are addressed in Big-\nBench. We show how volumeis addressed by using scale\n1199\n\n[Página 4]\n127.0.0.1 - - [Jun/23/2003:05:59:23 +0200]\n\"G\nET/page33.html?wcs_click_date=2452814\n&wcs_click_ time=21563&wcs_user_id=95789\n&wcs_web_page_sk=32&wcs_item_sk=28 HTTP/1.1\" 200 2256\n\"http://www.someurl.org\" \"Mozilla/5.0\"\nFigure 2: Example of a web log entry\nfactors in the data generators to scale data up to petabytes\nof data, how varietyis addressed through the usage of data\nfrom many sources and how velocityis achieved by periodic\nrefreshes of the data repository.\n3.1.1 Variety\nThe general benchmark data model is summarized in Fig-\nure 1, which shows the three data components of the bench-\nmark namely structured data, semi-structured data and un-\nstructureddatatogetherwiththerelationshipsbetweenthem.\nThe structured component of BigBench is adapted from\nthe TPC-DS benchmarkrecently publishedbytheTPC [10].\nA description of this benchmark can be found in [23, 28].\nBigBench is however not a simple extension of TPC-DS.\nInstead, BigBench focuses chieﬂyon the analytics associated\nwith semi-structured and unstructured data.\nWith a few exceptions most of the tables contained in\nTPC-DS are used by BigBench; the main focus being store\nand web sales, which only contain structured data. These\ntables cover data relating to the purchases made in stores\nand over the web, but also related tables such as itemde-\nscribing the items oﬀered by the retailer, customer and its\nancillary tables containing all relevant client data, webpage\nan\ndwebsitedescribing pages and web sites used by on-\nl\nine clients and all associated dimension tables. To better\nsupport our functional design, we also added a new table\ncalleditemmarketprices to the structured data. It contains\ncompetitor names and prices for each item so that price\ncomparisons performed by online users who are interested\nin particular items could also be captured.\nThe semi-structured data focuses on click-streams, con-\ntained in web log ﬁles. While some of the clicks result in\nsales thereby necessitating a link to structured area tables\ncontaining online sales, item, web pages, customer and asso-\nciated dimensions, the large majority of these clicks are as-\nsociated with browsing activity not resulting in sales. These\nclicks focus on items and are associated with registered users\nor guests.The format retained for the clicks is that of Apache\nlogs. A typical entry of such a log associated with a regis-\ntered user could look like the example in Figure 2.\nWeb logs can be processed either directly at run time (late\nbinding) or parsed and stored into a structured table/ﬁle.\nSince all values are surrogate keys referring to the struc-\ntured schema, the above record once processed could look\nlike Table 1.\nThe unstructured data resembles written text associated\nwith product reviews of items oﬀered by the retailer. Such\nreviews could be from several sources, namely guest users,\nregistered users withapurchaseandregistered userswithout\na purchase. This implies a relationship between reviews and\nstructred data like customer, sales and item tables. The\nreviews and its relationship with the structred data can be\ncaptured bya table/ﬁle. The table/ﬁle capturesthe primary\nkeys of the referenced tables. The review itself is containedin a large variable character ﬁeld containing free form text,\nthe rating score and the date and time of the review are also\ncontained in the table/ﬁle.\n3.1.2 Volume\nThe size of the structured area is based on the size of the\ntables involved, using a well-understood and known quan-\ntity similar to the scale factor in TPC-DS. The size of the\nsemi-structured and unstructured areas are also based on\nthis scale factor. Consequently, the size of the complete\nBigBench data set is based on a single scale factor and is\npredictable and deterministic at any volume.\nFor the item marketprice table, it is assumed that an av-\ner\nage of 5 competitor prices are stored for each item. Thus,\nthe sizing of item marketprice is |it em|×5.\nThe size of web logs dependson the number of clicks made\nby buyers (making entries in web sales) and visitors who do\nno\nt endupbuying. Each rowin web sales represents a single\nli\nne item, thus the number of clicks per sale is comprised of\nthe number of clicks per item and the number of clicks to\nmake a sale (i.e. login, go to cart, checkout). The number\nof clicks for buyers cbcan be speciﬁed with the following\nequation:\ncb=|websales|×( pages per item+pages per buy\nitems per sale)\nAs\nsumingbothpages per itemandpages per buytobe equal\nto 4 on average and setting the avergae value of items per\nsale to be 12 (from TPC-DS), the value of cbis simpliﬁed to\ncb=|websales|×4.3 3\nWe assume that 80% of surfers are visitors (20% buyers)\nwhich makes the ratio of visitors to buyers to be 4:1. We\nalso assume that on average visitors browse items the same\nway as buyers. Based on these assumptions, the formula for\nthe number of clicks for visitors cvis:\ncv= (|websales|×p ages per item) ×visitor ratio\ncv=|websales|×1 6\nOverall, the size of the web log is cb+cvand can be ex-\npressedasamultipleofthesizeofweb sales. Itisweb sales×\n20.3\n3. The web sales table scales linearly with the scale fac-\nto\nr, the size for scale factor 1 is 720K, thus the number of\nentries for the web log at scale factor 1 is 14,600K. Given to\nthe log format, the raw ﬁle size is 3 gigabyte.\nFor the review sizing, a similar approach is chosen. Three\nsources for reviews are considered: anonymous reviews, ran-\ndom item reviews by registered users (customers), and re-\nviews based on sales. The number of anonymous reviews is\nrelated to the number of items, an average of 5 anonymous\nTable 1: Representation of a web log entry\nField Name Value\nwcsclicksk 996146\nwcsclickdatesk2452814\nwcsclicktimesk21563\nwcsitemsk 28\nwcswebpagesk 32\nwcsusersk 95789\n1200\n\n[Página 5]\nreviews per item is assumed. The number of reviews by\nre\ngistered users is dependent on the number of users in the\nsystem. Because not all users are actually writing reviews,\nan average of one review per 5 users is assumed. Finally,\na certain amount of the sales will directly lead to a review.\nThis amount is set to 15%. The number of reviews can be\ncomputed by the following formula:\n|reviews|=|items|×5+|customers |×0.2+|websales|×0.1 5\n3.1.3 Velocity\nVelocity, i.e. a periodic data refresh process, is an inte-\ngral part of the life cycle of a big data system. A production\ndata refresh process consists of three steps: (i) data extract\n(ii)datatransformation, and(iii)dataload. Inaproduction\nsystem environment, the data extraction step may consist of\nnumerousseparateextractoperations, executedagainst mul-\ntiple operational systems and ancillary data sources. As it\nis unlikely that the full list of these operational data sources\nresides on the system running the big data application, it\nis doubtful the measurement of the data extraction perfor-\nmance would result in a metric appropriate or meaningful\nto the scope of this benchmark. In light of this, the data\nextract step is assumed and represented in the benchmark\nin the form of generated ﬁles.\nThere are two aspects to discuss in a periodic refresh\nmodel for the tables in BigBench: (i) amount of data to\ninclude in the refresh process and (ii) the time interval at\nwhich the refresh occurs. Both aspects apply to the struc-\ntured(websales channeland itemmarketprice ta bles), semi-\nstructured ( clickstream) and un-structured data ( p roduct\nreview).\nW\ne implement BigBench’s periodic refresh process based\non the well studied methodology for data maintenance of\nTPC-DS. It deﬁnes the insertion of new data and the dele-\ntion of old data from all fact tables as well as insert and\nupdated data of dimensions. Dimensions are divided into\nthree sets, history keeping, non-history keeping and static\ndimensions. Static dimensions, such as date and time are\nnot updated. History keeping dimensions never overwrite\nany data, but they keep a history of all former changes.\nNon-History keeping dimensions resemble almost a one-to-\none copy of the table in the operational system of the busi-\nness, i.e. they update existing data. Both, history keeping\nand non-history keeping dimensions, accept new data and\nnever delete any old data. According to the above deﬁni-\ntions,clickstreamandp roductrevieware fact tables and\ni\ntemmarketprice is a history keeping table. Pseudo code\nfor the insertion, deletion of fact table data as well as insert\nand update operations for the dimension tables can be found\nin [23] and the oﬃcial TPC-DS speciﬁcation3.\nOne of the fundamental aspects of the above methodol-\nogy is the concurrent execution of the refresh process with\nthe query workload. Queries must be interspersed with in-\nsert, delete and update operations. In BigBench we run\nN concurrent query streams containing queries against the\nstructured, semi-structuredandunstructuredportionsofthe\nschema. The numberof refresh processes executedis a linear\nfunction of the number of query streams, S. In real systems,\ndata against the diﬀerent data portions is updated with dif-\nferent frequencies. Hence we deﬁne a vector V with the\nfollowing three separate data refresh velocities for each of\n3TPC –http://www.tpc.org/g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g1005/g1007/g910\n/g94/g3/g395/g437/g286/g396/g349/g286/g400 /g89/g282/g258/g410/g258/g410/g455/g393/g286/g400/g882/g1010/g3/g395/g437/g286/g396/g349/g286/g400 /g89/g282/g258/g410/g258/g410/g455/g393/g286/g400/g882/g1010/g3/g395/g437/g286/g396/g349/g286/g400\n/g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g1006\n/g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g94/g882/g1005\n/g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g94/g857\n/g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g94\n/g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g1005 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g1006 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g1007 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g94/g876/g1006\nFigure 3: Scheduling of refresh processes based on\nex\necuted queries per data type\nthe diﬀerent data portions, V= (Vstructured ,Vsemistructured\nandVunstructured ). We suggest the following values for V,\nwhich are subject to change as we run more experiments.\nThe structured data being the least frequently updated por-\ntion of the schema has a velocity of Vstructured = 1, i.e. S\nrefresh process. The unstructured data gets a velocity of\nVunstructured = 2∗Vstructured , i.e. 2∗Srefresh process, and\nthe semi-structured data being the most frequently updated\nportion gets a velocity of Vsemistructured = 2∗Vunstructured ,\ni.e. 4∗Srefresh process. The total number of refresh pro-\ncesses is 7 ∗S.\nDuring a BigBench run the following two requirements\nguarantee that the queries are interspersed with the queries\n(Sis the total number of query streams and Qdatatypeis\nthe total number of queries against the three portions of the\nschema ):\n1. The Nthrefresh set canonly startafter [((3 ∗S)+((N−\n1)∗2∗Qdatatype)] queries have completed (aggregated\nover all streams), and\n2. The [(3 ∗S)+(N∗(Qdatatype−6))+1]th query (ag-\ngregated over all streams) can only start after the Nth\nrefresh set has completed.\nThis means that at least (3 ∗S) queries must complete be-\nfore the ﬁrst refresh set can start and at least Qdatatype−6\nadditional queries must complete before the second refresh\nsetcan start. Ingeneral atleast (3 ∗S)+((N−1)∗Qdatatype−\n6)) queries must complete before the Nth refresh set can\nstart. Figure 3 shows how the refresh processes are sched-\nuled depending on the number of executed queries.\nAll three type of data tables follow the well-understood\nscale factors of TPC-DS as outlined in the previous section.\nThat is the amount of data to be inserted in each ETL op-\neration is a percentage of the initial load, e.g. 0.1%.\n3.2 DataGeneration\nOur data generation design is based on an existing tech-\nnology called Parallel Data Generation Framework (PDGF).\nPDGF was designed to address structured data. Part of the\nwork presented in this paper is to extend the framework\nto produce the semi-structured and unstructured data. The\nsemi-structured data is generated in form of weblogs and the\nunstructured data in form of item reviews. In the following\nsection, we give an overview of PDGF and then elaborate\non its extensions for semi-structured and unstructured data.\n1201\n\n[Página 6]\n3.2.1 PDGF\nPD\nGF is a generic, parallel data generator which was de-\nveloped at the Universityof Passau [18, 29]. PDGF is imple-\nmented in Java and fully platform independent. Currently,\nPDGF is used to implement the default data generator for\nthe TPC’s new ETL benchmark TPC-DI [33]. PDGF’s gen-\neration approachexploits theinherentparallelism of xorshift\nrandom number generators by using a novel seeding strat-\negy. The seeding strategy hierarchically assigns seeds to the\ntables, columns and rows of a database schema and thus\nmakes it possible to generate data completely in parallel as\nwell as re-calculate any value in the database without ac-\ncessing the original data.\nOriginally, PDGF is designed to generate relational data.\nThe data is speciﬁed in two XML documents, the schema\nconﬁgurationandthegenerationconﬁguration. Asthename\nsuggests, the schema conﬁguration speciﬁes the data simi-\nlar to the deﬁnition of a relational schema. The generation\nconﬁguration makes it possible to specify additional post-\nprocessing of the generation. The post-processing includes\nformatting data, merging and splitting tables, as well as ad-\nvanced procedures by providing a script like programming\ninterface using the Javassist4library.\nPDGF can be used as isto generate the structured parts\nof the data model. As discussed above, the current Big-\nBench schema comprises three additional entities on top\nof the TPC-DS schema: the Item marketprice table, an\nap\nache-style web server log, and the online reviews. The\nItemmarketprice table is a regular table and can easily be\nge\nnerated using PDGF. In Listing 1, an excerpt of the spec-\niﬁcation of Item marketprice can be seen. The table is de-\nﬁn\ned in a way similar to the SQL deﬁnition language, with\nan additional speciﬁcation of the generation rules. The sur-\nrogate key (imp sk) is, for example, generated with a ID\nge\nnerator. PDGF supports more complex generation spec-\niﬁcations as can be seen in the case of the imp competitor\nﬁe\nld, this ﬁeld is generated as a random string that is null\nwith a probability of 0.025%.\n<property name= \"Item_marketprice\" type=\"double\" >\n${\nitem}*${avg_competitors_per_item}\n</property>\n<table name= \"Item_marketprice\" >\n<s\nize>${Item_marketprice}</size>\n<field name= \"imp_sk\" size=\"\"type=\"NUMERIC\" >\n<g\nen_IdGenerator/>\n</field>\n[..]\n<field name= \"imp_competitor\" size=\"20\"\ntype=\"VARCHAR\" >\n<g\nen_NullGenerator>\n<probability>0.00025</probability>\n<gen_RandomAString>\n<size>20</size>\n</gen_RandomAString>\n</gen_NullGenerator>\n</field>\n[..]\n</table>\nListing 1: Excerpt of the Schema Deﬁnition for\nIt\nemmarketprice\nTh\ne web server log has a special formatting, an example\n4Javassist project homepage - http://www.csg.is.titech.\nac.jp/~chiba/javassist/is shown in Figure 2. To generate a realistic web log, we\nspeciﬁed a table in PDGF that has all required columns for\na web log entry and formated it using PDGF’s scripting ca-\npabilities. Below in Listing 2 an excerpt of the deﬁnition of\nthe web server log table can be seen. The excerpt shows the\ndeﬁnition of the size of the web log, and the table deﬁni-\ntion with two attributes. The sizing is computed according\nto the formula in Section 3.1.2, the speciﬁcation of the pa-\nrameters of the formula is omitted. For the table itself only\ntwo attributes are shown: a surrogate key wcsclickskand\nt\nhe reference to the web page wcswebpagesk. This ref-\ne\nrence is null with a probability of 0 .00025. In Listing 3,\nthe formatting code for the web log can be seen. As shown\nin the listing, some of the values in the log are static. For\nexample the request IP address is always “127.0.0.1” while\nother values such as the time and date are extracted from\nthe table.\n<property name= \"Web_clickstreams\" type=\"double\" >\n($\n{sales} * (${pages_per_item} + (${pages_to_buy}\n/ ${items_per_cart})))\n+ (${sales} * ${buy_ratio} * ${pages_per_item})\n</property>\n<table name= \"Web_clickstreams\" >\n<s\nize>${Web_clickstreams}</size>\n<field name= \"wcs_click_sk\" size=\"\"type=\"NUMERIC\" >\n<g\nen_IdGenerator/>\n</field>\n[..]\n<field name= \"wcs_web_page_sk\" size=\"\"\ntype=\"NUMERIC\" >\n<g\nen_NullGenerator>\n<probability >0.00025</probability>\n<gen_LongGenerator>\n<min>1</min>\n<max>${web_page}</max>\n</gen_LongGenerator>\n</gen_NullGenerator>\n</field>\n[..]\n</table>\nListing 2: Excerpt of the web log speciﬁcation\n<output name= \"CompiledTemplateOutput\" >\n<t\nemplate><!--\nString nl =\npdgf.util.Constants.DEFAULT_LINESEPARATOR;\nbuffer.append( \"127.0.0.1 - - [\" + fields[4] + \":\"+\nfi\nelds[5] + \" +0200] \" );\nbu\nffer.append( \"\\\"GET /page\" +fields[7]+ \".html?\" );\n[.\n.]\nbuffer.append( \" HTTP/1.1\\\" 200 0 - \\\"\" +fields[1]);\nbu\nffer.append( \"\\\" \\\"Mozilla/5.0 \\\"\" + nl);\n--\n></template>\n</output>\nListing 3: Excerpt of the formatting instructions for\nth\ne web log\nThe review generator was built as a standalone program,\nit is conﬁguredusingan XMLdocumentthatspeciﬁes all pa-\nrameters for each review. In order to generate reviews that\ncorrelate with the structured data, e.g. the items that are\nreviewed exist in the database and the registered reviewers\nare actual customers, PDGF is used to generate the XML\nconﬁguration for the review generator. This is also done\nusing the scripting interface. Again, a table is speciﬁed in\nPDGF that contains all required information and the rows\n1202\n\n[Página 7]\n/g75/g296/g296/g367/g349/g374/g286/g3/g87/g396/g286/g393/g396/g381/g272/g286/g400/g400/g349/g374/g336/g18/g258/g410/g286/g336/g381/g396/g349/g460/g258/g410/g349/g381/g374/g90/g286/g258/g367/g3\n/g90/g286\n/g448/g349/g286/g449/g400\n/g100/g381/g364/g286/g374/g349/g460/g258/g410/g349/g381/g374\n/g39/g286/g374/g286/g396/g258/g367/g349/g460/g258/g410/g349/g381/g374/g68/g258/g396/g364/g381/g448/g3\n/g18/g346\n/g258/g349/g374/g3/g47/g374/g393/g437/g410/g87/g396/g381/g282/g437/g272/g410\n/g18/g437/g400/g410/g381/g373/g349/g460/g258/g410/g349/g381/g374\n/g100/g286/g454/g410/g3/g39/g286/g374/g286/g396/g258/g410/g349/g381/g374\n/g87/g258/g396/g258/g373/g286/g410/g286/g396/g3\n/g39/g286/g374/g286/g396/g258/g410/g349/g381/g374\n/g894/g87/g24/g39/g38/g895\n/g75/g374/g367/g349/g374/g286/g3/g24/g258/g410/g258/g3/g39/g286/g374/g286/g396/g258/g410/g349/g381/g374/g39/g286/g374/g286/g396/g258/g410/g286/g282/g3\n/g90/g286\n/g448/g349/g286/g449/g400\n/g87/g396/g381/g282/g437/g272/g410\n/g18/g437/g400/g410/g381/g373/g349/g460/g258/g410/g349/g381/g374\n/g100/g286/g454/g410/g3/g39/g286/g374/g286/g396/g258/g410/g349/g381/g374\n/g87/g258/g396/g258/g373/g286/g410/g286/g396/g3\n/g39/g286/g374/g286/g396/g258/g410/g349/g381/g374\n/g894/g87/g24/g39/g38/g895\nFigure 4: Review Generation Process\nar\ne output as XML document fragments. Details on the\nreview generation are given in the section below.\n3.2.2 ReviewGeneration\nReviews buildthe unstructuredpart of our data set. They\nare an integral part of the data model and have to be pro-\ncessed. Thus they need to contain realistic and useful in-\nformation. As discussed below in the workload section, the\nbenchmark contains queries that require sentiment analysis\nand similar text analysis on the reviews. We have developed\na novel approach for generating the reviews that is based on\ntext generation using Markov chains [11].\nIn Figure 4 an overview of the review generation process\ncan be seen. The process can be separated in two phases.\nAn oﬄine phase, that processes real reviews and generates\na knowledge base for the review generation and an online\nphase that generates reviews based on the knowledge base.\nThe oﬄine process starts with collecting real reviews from\nonline resources. For our proof of concept, we collected a set\nof 150 reviews per category from an online retailer. In the\nﬁrst processing step the reviews are categorized by prod-\nuct type. For the categorization, we use an intersection of\nproduct categories from the online retailer and the class and\ncategory hierarchy in the item dimension in the TPC-DS\nschema. The online reviews have a rating which is used\nto create an orthogonal categorization for the review senti-\nment. The crawler also collects statistical information about\nthe number of reviews per item, the length of reviews and\nthe distribution of ratings. Since reviews are tailored to a\nspeciﬁc product, they are tokenized and the review subject\nis generalized. For now this process only includes ﬁltering\nout product names and replacing them with generic iden-\ntiﬁers. Although this approach removes the product name\nfrom reviews, they are still highly domain speciﬁc. Since\nthe generalization is an oﬄine process that has to be done\nonly once, the computation can be more involved. In future\nversions of the generator more sophisticated approaches will\nbe implemented.\nUsing the tokenized and generalized reviews, the transi-\ntion probabilities between words in the text are analyzed\nand stored. These probabilities are know as Markov chains.\nAnorder-1 chain will only store the frequency of a word\nappearing after another one. So for each word all possible\nsuccessors and the frequency in which they appear is stored.\nTo get more realistic text, more than one predecessor can be\ntaken into account for generating the text. In practice, weuse order-2 to order-4 text to achieve high quality reviews.\nAn excerpt of an order-2 generated text can be seen below.\nMy review title says it all. I wanted to like\nit, because it’s a good subject. Didn’t ﬂow well,\nsome times confusing. This book is not a self\nhelp book, this may be worth reading for that\nalone.\nThe review generator was implemented as a standalone\nprogram that is conﬁgured by an XML document. The con-\nﬁguration contains one <review> element for each review\nthat should be generated. For each review the item ID,\ncategory, user name, transaction ID, date, time, rating and\nword count are speciﬁed. This information is generated by\nPDGF and later fed to the review generator. This way, it\nis assured that the review data is consistent with the data\ngenerated by the other generators. In future revisions of the\nbenchmark, all parts of the data generation will be imple-\nmented within PDGF.\n3.3 Workload\nIn this section, we present the proposed workload for Big-\nBench. In addition to the queries described below, we con-\nsidertheinitial databasepopulationaspartoftheworkload.\nWe refer to this initial phase as transformation ingest (TI).\nTI covers the ETL process, including any steps needed to\nprepare the data before querying (e.g., indexing or statistics\ncollection).\nThe main part of the workload is the set of queries to be\nexecutedagainst thedata model. These queries are designed\nalong one business dimension and three technical dimen-\nsions, aiming to cover diﬀerent business cases and technical\nperspectives. Our business cases are based on Mckinsey’s re-\nport on big data [22]. From a technical perspective, we focus\non data sources, processing types and analytical techniques.\nFollowing the approach used for most TPC benchmarks,\nThe BigBench queries are deﬁned in terms of business ques-\ntions and expressed in plain English. We created a total\nof 30 business questions for the BigBench workload. Note\nthat, dueto the limited space, we do not presentall of the 30\nqueries in this paper. The complete set of BigBench queries\ncan be found in an extended version of this paper. In addi-\ntion to the English deﬁnition of the queries, we also present\nthem using Teradata Aster’s SQL-MR syntax [19, 32].\nThe remainder of this section is organized as follows: ﬁrst,\nwe discuss the business cases with query examples. We then\npresent the three technical dimensions and show the distri-\nbution of queries along each of the dimensions.\n3.3.1 BusinessCases\nThe McKinsey report gives a comprehensive view of big\ndata’s transformative potentials for retail business. From\nthe report, we identiﬁed nine big data retail levers that ﬁt in\ntheBigBenchworkload. Furthermore, weaddedreturnanal-\nysis under the category Operations which makes a total of\nten levers. (Returns are often connected with frauds, which\nmakes it important from a business perspective.) These ten\nlevers fall into the following ﬁve main categories: Marketing,\nMerchandising, Operations, SupplyChainandNewBusiness\nModels. The organization of the ten levers into these ﬁve\ncategories is shown in Table 2.\nIn the following, we present the ten retail levers and we\nillustrate each lever with a sample query.\n1203\n\n[Página 8]\nTable 2: Levers Within Business Categories\nBusiness category Big data lever\nMarketing -Cross-selling\n-Customer micro-segmentation\n-Sentiment analysis\n-Enhancing multichannel\nconsumer experience\nMerchandising -Assortment optimization\n-Pricing optimization\nOperations -Performance transparency\n-Return analysis\nSupply chain -Inventory management\nNew business models -Price comparison\n1.Cross-selling: I n this lever, we include queries in-\nvolving market basket analysis and collaborative ﬁl-\ntering based recommendations. For example, Query\n1 computes the probability of browsing products from\na category after customers viewed items from another\ncategory.\nQuery 1: Perform category aﬃnity analysis for prod-\nucts purchased online together.\n2.Customer micro-segmentation: Queriesinthislever\nranges from grouping users using one dimension to\nclusteringusersusingmoresophisticatedfeatures. Query\n2 tries to cluster users into eight groups based on their\npurchase history.\nQuery 2: Customers are separated along the follow-\ning key shopping dimensions: recency of last visit, fre-\nquency of visits and monetary amount. Use the in-\nstore and online purchase data over a calendar year to\ncompute.\n3.Sentiment analysis: These queries involve an enor-\nmous amount of text and natural language processing,\nincluding detecting sentiment words or phrases from\nreviews, determining sentiment polarity, etc., as shown\nin Query 3.\nQuery 3: For a given product, extract sentences from\nits product reviews that contain sentiments and dis-\nplay their sentiment polarity.\n4.Enhancing multi-channel consumer experience:\nQueries in this lever are targeted at understanding\nusers shopping behaviors through both online and in-\nstore channels. Query 4 checks if online browsing af-\nfects customers’ in-store purchase behaviors by mea-\nsuring the number of days between the two activities.\nQuery 4: Find all customers who viewed items of a\ngiven category on the web site in a given month and\nyear and subsequently made an in-store purchase in\nthe same category within the following three months.\n5.Assortment optimization: In this lever we focus on\nqueries that identifyproducts, categories or stores that\ncan be targeted for improvements. Query 5 ﬁnds the\nproducts with decreasing sales.\nQuery 5: Find the categories with ﬂat or declining\nsales for in-store purchases during a given year for a\ngiven store.6.Pricing optimization: Queries in this lever are fo-\ncused on measuring the impact of price changes on\nsales, as shown in Query 6.\nQuery 6: Compute the impact on sales of an item\nprice change by computing the total sales for items\nin a 30-day period before and after the price change.\nGroup the total sales by items and location of ware-\nhouse where they were delivered from.\n7.Performance transparency: Our queries for this\nlever are about ﬁnding stores with downward or up-\nwardperformance. Query7identiﬁesstoreswithdown-\nward sales and ﬁnds possible reasons through available\nreviews.\nQuery 7: Identify stores with ﬂat or declining sales in\n3 consecutive months, check if there are any negative\nonline reviews regarding these stores.\n8.Return analysis: These queries target two areas;\nidentifying problematic products and detecting refund\nfraud. Query 8 ﬁrst ﬁnds products with high return\nrate and then identiﬁes if there are any issues from\nproduct reviews.\nQuery 8: Retrieve the items with the highest number\nof returns where the number of returns was approxi-\nmately equivalent across all stores and web channels\n(within a tolerance of +/- 10%), within a week end-\ning a given date. Analyze the online reviews for these\nitems to see if there are any major negative reviews.\n9.Inventory management: Queriesfor thisleverfocus\non statistical analysis on product inventory. Query 9\ncomputes the mean and variation of item inventories\nand identiﬁes those with large variations.\nQuery 9: This query contains multiple, related itera-\ntions. Iteration 1 calculates the coeﬃcient of variation\nand mean of inventory by item and warehouse for two\nconsecutive months. Iteration 2 ﬁnds items that had\na coeﬃcient of variation in the ﬁrst months of 1.5 or\nlarger.\n10.Price comparison: In this lever, we have one query\nthat measures the correlations between competitor’s\nprices and item sales, as shown in Query 10.\nQuery 10: For a given product, measure the eﬀect of\ncompetitor’s prices on products’ in-store and online\nsales.\nThe business cases were the main driver for the deﬁnition\nof the BigBench queries. The bulk of the queries are within\nthe Marketing and Merchandising categories since these two\nare the most commonly used and can be further divided in\nsub-categories, as discussed in [22]. The overall breakdown\nof queries over the ﬁve business categories is shown in Table\n3.\n3.3.2 TechnicalDimensions\nIn the following, we elaborate on the three technical di-\nmensions with examples based on the ten queries above.\nData source dimension: It measures the type of in-\nput data the query is targeting. We have three types of\ninput data in BigBench: structured, semi-structured and\nun-structured. For example, Query 1 uses semi-structured\n1204\n\n[Página 9]\nTable 3: Business Categories Query Breakdown\nBusiness category TotalPercentage(%)\nMarketing 18 60.0\nMerchandising 5 16.7\nOperations 4 13.3\nSupply chain 2 6.7\nNew business models 1 3.3\nweb click streams as data source, while Query 3 does sen-\nti\nment words extraction on un-structured product reviews\ndata. In addition to using single data source, data source\ncombinations are covered in the queries as well. For exam-\nple, user click analysis (semi-structured) before store pur-\nchasing (structured) will join the two largest data sources,\nas is the case in Query 4.\nProcessing type dimension: It measures the type of\nprocessing appropriate for the query. This dimension covers\nthe two common paradigms of declarative and procedural\nlanguages. In other words, some of our queries can be an-\nswered by declarative languages, others by procedural lan-\nguages and others by a mix of both. In the scope of our\nbenchmark, examples of declarative languages are SQL and\nsimilar constructs like Hive-QL. Map-Reduce is an example\nof a procedural language and Pig Latin has a mix of declara-\ntive and procedural constructs. Note that while some of the\nqueries can be expressed in either declarative or procedu-\nral languages, there are queries that can only be expressed\nthrough procedural programming. In the former case, if\nthe query is written through complex SQL constructs (e.g.,\nwindow functions or user deﬁned functions) we consider it a\nprocedural query. However, queries that involve text analy-\nsis or sentiment analysis, like Query 3 and 7 ﬁt in the later\ncase as they have to be written using procedural program-\nming. In the 10 queries above, Query 5, 6 and 9 can be\nwritten using SQL and thus are in the declarative category,\nwhile the other seven queries need procedural programming\nor a mix of procedural and declarative constructs.\nAnalytic technique dimension: It measures diﬀerent\ntechniques for answering business analytics questions. In\ngeneral, we identiﬁedthreemajor categories ofanalytic tech-\nniques: statistical analysis, data mining and simple report-\ning. Statistical analysis involves correlation analysis, time\nseries, regression, etc. Statistical analysis is exempliﬁed in\nQuery 5, 9 and 10. For the data mining categories we use\nclassiﬁcation, clustering, association mining, pattern analy-\nsis and text analysis in our BigBench workload. Examples\nof data mining queries include Query 1, 2, 3, 4, 7 and 8. The\nreporting category is included in the BigBench as we believe\nthat these queries represents a small but signiﬁcant part of\nbusiness analytics.This category covers the ad hoc queries\nand those that do not belong to statistical analysis or data\nmining. Most reporting queries are simple tasks that can be\nexpressed in simple SQL. Note that most of our queries in\nthe reporting category come from TPC-DS. Query 6 is an\nexample of a reporting query.\nWhile the query deﬁnition was driven by the business case\nrepresented by BigBench, their distribution over the three\ntechnical dimensions is believed to be reasonable and repre-\nsentative of the workload portrayed by the benchmark. We\nconclude this section by summarizing in Table 4 the query\ndistribution along the three technical dimensions.Table 4: Technical Dimensions Breakdown\nQuery processing type TotalPercentage(%)\nDeclarative 10 33.3\nProcedural 7 23.3\nMix of Declarative and Pro-\nce\ndural13 43.3\nData sources TotalPercentage(%)\nStructured 18 60.0\nSemi-structured 7 23.3\nUn-structured 5 16.7\nAnalytic techniques TotalPercentage(%)\nStatistics analysis 6 20.0\nData mining 17 56.7\nReporting 8 26.7\n3.4 Metrics\nPr\nevious TPCbenchmarkslike TPC-HandrecentlyTPC-\nDS have metrics based mostly on individual query execution\ntimes. The metric for BigBench could simply be the same\nor similar to either TPC-H or TPC-DS since from a high\nlevel it has similar phases, such as initial load, data refresh\nand query execution.\nWedefer theﬁnaldesign for theBigBench metricto future\nwork. However, we believe that data loading and the type\nof processing dimension described in Section 3.3 is a nec-\nessary factor in BigBench’s metric. Our rationale is that,\non the one hand, DBMS and MR engines have diﬀerent\nstrengths in terms of loading, declarative and procedural\nprocessing. For example, Hadoop related systems are very\neﬃcient at loading and are generally optimized for MR pro-\ncessing. On the other hand, DBMS engines are optimized\nto process SQL, but MR/UDF processing and data load-\ning may be less optimized. In addition, there is a recent\neﬀort for DBMS engines to process MR more eﬃciently, ei-\nther natively or through an eﬃcient co-existence with an\nMR engine (e.g., Hadoop, HIVE or Pig). One option to\nreﬂect the importance of the processing type dimension is\nto use the diﬀerent processing types in the metric compu-\ntation instead of using individual queries. Let TLbe the\nl\noading time, TDthe total time for queries in declarative\np\nrocessing, TPthe time for procedural processing queries\na\nndTBthe time for the remaining queries that have both\nd\neclarative and procedural. A meaningful way of combin-\ning these four values in a composite metric is by computing\ntheir geometric mean as4√\nTL∗TD∗TP∗TB. If the\nw\norkload queries are used, the geometric mean could be cal-\nculated as30/radicalbig/producttext30\ni=\n1Pi(wherePidenotes the execution time\nforQuery i).\n4. EVALUATION\nBigBench is targeted at DBMS and MR systems that\nclaim to provide big data solutions. Therefore, any of those\nsystems can be used to establish the feasibility of this bench-\nmark. Standard DBMSes most likely will capture all data\nas relational tables by parsing the semi-structured data and\nestablishing a schema. The un-structured data can also be\ncaptured as a table where the review text can be stored\nas VARCHAR or a blob column. Such DBMSes can imple-\nment our queries using SQL and some procedural constructs\nlike UDF or even built in MR processing within the DBMS.\n1205\n\n[Página 10]\nAster nCluster Database \nLoader/Exporter Server Group \nWorker Server Group \nQueen Server Group \nReports, Analytics, Applications \n(SQL / ODBC \n/ JDBC) \nQueries / Answers \nQueries \nData \nFigure 5: nCluster Architecture\nHa\ndoop and its ecosystem with HIVE and Pig can also run\nBigBench. The data can be captured in HDFS or similar\nstructures. The main strength of these systems is MR but\nthey also have some relational operators like those in H-QL\nor Pig [2, 24]. Such relational operators can do joins, group-\ning and aggregations. BigBench can also be run on systems\nthat have both DBMS and MR engines like Hadoop or any\nof its ecosystem products. Such systems consists most likely\nof a DBMS that connects or co-exists with an MR engine.\nWe chose to initially run BigBench on the Teradata Aster\nDBMS. TAD has all features needed to store and process big\ndata. Data can be stored as tables and queries can be exe-\ncuted using the SQL-MR interface that extends declarative\nSQL with MR processing.\n4.1 TeradataAsterDBMS\nTAD is based on the nCluster technology. nCluster is a\nshared-nothing parallel database, optimized for data ware-\nhousing and analytic workloads [19]. nCluster manages a\ncluster of commodity server nodes, and is designed to scale\nout to hundreds of nodes and scale up to petabytes of active\ndata.\nFigure 4.1 depicts the nCluster architecture. Query pro-\ncessing is managed by one or more Queennodes. These\nnodes analyze client requests and distribute partial process-\ning among the Worker nodes. Each relation in nCluster\nis hash-partitioned (fact tables) or duplicated (dimension\ntables) across the Worker nodes to enable intra-query par-\nallelism. Loading is done by special Worker nodes shown at\nthe bottom of Figure 4.1.\nInadditiontodatabasequeryprocessing, automatedman-\nageability functionality in nCluster allows adding new ma-\nchines and redistributing data. The system performs auto-\nmatic fail-over, retry of queries, and restoration of replica-\ntion levels after a node failure. These features are essential\nin a large cluster of machines, where failures of various kinds\noccur regularly.\nThe SQL-MR supports a mix of SQL and polymorphicUDFs that process MR logic. The MR functions are paral-\nlelizable, self-describinganddynamicallypolymorphicwhere\nthefunctioninputschemasaredeterminedimplicitlyatquery\nexecution time. Output schemas are determined program-\nmatically by the function itself at query execution time as\nwell. They are also equivalent to subqueries, making them\nsubject to query optimization along with the other relations\nin a query. nCluster allows MR UDFs to be written using\nJava, C/C++, and scripting languages like Python.\n4.2 End-to-EndExecution\nThe test was executed on a 8 node Teradata Aster appli-\nance. Each node is a Dell server with two quad-core Xeon\n5500 at 3.07Ghz and hardware RAID 1 with 8 2.5”drives.\nDue to time limitation, DSDGEN is used to produce the\noriginal TPC-DS tables in the structured part of our model.\nWe used PDGF to generate the new parts of the data and\nthe XML conﬁguration for the review generator. The new\nparts produced by PDGF include the new Item marketprice\nta\nble, an apache-styleweb server log, and theonline reviews.\nPDGF is also conﬁgured to match the references (PK-FK\nrelationships) in the new data with the TPC-DS data. In\nthe future, we plan on extending PDGF to handle the whole\ndata generation aspects without the need for DSDGEN.\nThe data was loaded into TAD as tables. The web logs\nwere parsed and converted to a table similar to the structure\nshown in Section 3.1. Product reviews are also interpreted\nas a table assuming the review text as a VARCHAR(5000).\nAs a proof of concept, we executed the workload as a\nsingle stream without velocity. Since we adapt the velocity\nmethodology from TPC-DS adding it will not be diﬃcult\nand can be implemented with a simple driver that adds data\nto the system periodically and re-submits a new stream of\nqueries. Concurrent streams can also be handled similar to\nprevious benchmarks like TPC-H.\nThe queries are written using TAD SQL-MR interface\nbasedonthedescriptioninSection3.3. Thereportingqueries\nwere written using SQL only and the rest were done through\neither an MR call or a mix of both SQL and MR. Below,\nwe show the SQL-MR version of a sample of the 30 queries.\nThe full list of the 30 queries written in SQL-MR can be\nfound on our technical report that will be published with\nthis paper. Note that all TAD MR functions used in the\nevaluation are part of a library TAD provides and packaged\nwith the nCluster DBMS.\nThe queryin Listing 4 is the SQL-MRequivalentof Query\n3 in Section 3.3 which extracts sentiments and their polarity.\nThe query retrieves from a reducer function called Extract-\nSentimentthat takes inputthe source table product reviews.\nThe call to the functions also speciﬁes the column that has\nthe text, the model for the sentiment analysis and the level\nof the search (sentence or word). The WHERE clause at the\nend picks positive or negative polarity.\nThe second example is for Query 1 as described in Sec-\ntion 3.3. The query is shown in Listing 5. It is the SQL-\nMR version equivalent for Query 1. It consists of 3 blocks.\nThe most inner block is a SQL fragment that joins the\nwebsales and item tables and projects out category id and\ncu\nstomerid. The inner block is fed as input to an MR func-\nti\non called basket generator which ﬁnds the categories of\npa\nirwise items purchased together by customers. The in-\nput is partitioned by customer id as speciﬁed by the PAR-\nTI\nTION clause. The call to market basket also speciﬁes\n1206\n\n[Página 11]\nSELECT pr_item_sk, out_content, out_polarity,\nou\nt_sentiment_words\nFROMExtractSentiment\n(\nONproduct_reviews\nTE\nXT_COLUMN ( ’pr_review_content’ )\nMO\nDEL (’dictionary’ )\nLEVEL(’sentence’ )\nAC\nCUMLATE ( ’pr_item_sk’ )\n)\nWHEREout_polarity = ’NEG’orout_polarity = ’POS’;\nListing 4: Query 3\nwh\nich ﬁeld should the basket analysis be done on using\nthe BASKET ITEM clause. The last clause for the call to\nba\nsketgenerator is ITEM SETMAX(500) which limits the\nan\nalysis to 500 pairs of items for each customer. The output\nof basket generator is the input to the main query which ba-\nsi\ncally ﬁnds the degree of aﬃnity for each pair of categories.\nSELECT\ncategory_cd1 AScategory1_cd,\nca\ntegory_cd2 AScategory2_cd,\nCOUNT(*)AScnt\nFROM\nbasket_generator( ON\n(SELECT i.i_category_id AScategory_cd,\ns.\nws_bill_customer_sk AScustomer_id\nFROMweb_sales s\nINNER JOIN item i\nONs.ws_item_sk = i_item_sk\nWHEREi.i_category_id is not NULL )\nPA\nRTITION BYcustomer_id\nBA\nSKET_ITEM( ’category_cd’ )\nIT\nEM_SET_MAX(500)\n)\nGROUP BY 1,2\norder by 1,3,2;\nListing 5: Query 1\nTh\ne last example of our evaluation queries is Query 6 de-\nscribed in Section 3.3. The query is SQL only and adapted\nfrom the TPC-DS benchmark, it can be seen in Listing 6.\nAs described before, Query 6 ﬁnds the impact of pricing\nchange done on March 16, 1998. The query joins the fol-\nlowing tables: web sales used to capture sales done online,\nwe\nbreturns for returns of web sales, warehouse which cap-\ntu\nres information about warehouses, item table that cap-\ntures the products sold and date dim which is a date lookup\nta\nble. The join with web returns is done as an outer join\nsi\nncenotallordershavereturns. Thequerycomputestheto-\ntalsalesbeforeandafterMarch16, 1998 aliasedassales before\nan\nd salesafter in Listing 6. The query group on state loca-\nti\non of the warehouse and ID of the items.\nThe run time of each of the 30 queries can be found at\nour technical report that will be published with this paper.\nFigure 7 lists the run time of the 10 queries used in the\nworkload section. We also show the values of TL,TD,\nTPandTBas discussed in Figure 6. Note that we did\nn\not try any hardware or software optimizations to run the\nabove 30 queries since our goal is to just make sure these\nqueries run and produce meaningful results. The run time\nof the 30 queries varies from seconds to a little bit over an\nhour. This illustrates that we do not have a runway query\nsituation and we also have a range of query complexities.SELECT\nw_state,i_item_id\n,sum(case when (cast(d_date as date ) <\ncast(’1998-03-16’ as date ))\nthenws_sales_price -\ncoalesce (wr_refunded_cash ,0)\nelse0end)\nassales_before\n,sum(case when (cast(d_date as date ) >=\ncast(’1998-03-16’ as date ))\nthenws_sales_price -\ncoalesce (wr_refunded_cash ,0)\nelse0end)assales_after\nFROM\nweb_sales left outer join web_returns on\n(ws_order_number = wr_order_number\nandws_item_sk = wr_item_sk)\n,w\narehouse, item, date_dim\nWHERE\ni_item_sk = ws_item_sk\nandws_warehouse_sk = w_warehouse_sk\nandws_sold_date_sk = d_date_sk\nandd_date between\n(cast(’1998-03-16’ as date ) -interval ’30\nda\ny’)\nand(cast(’1998-03-16’ as date ) +interval\n’30 day’ )\nGROUP by w_state,i_item_id\nORDER by w_state,i_item_id;\nListing 6: Query 6\n0 20\n00 4000 6000 8000 10000 12000 \nLoading Declarative Procedural Both Run Time [s] \nComponents \nFigure 6: Runtime of Metric Components\n5.\nCONCLUSION\nIn this paper we presented BigBench, a proposal for an\nend-to-end big data benchmark. The proposal covers a data\nmodel addressing the velocity, variety and volume common\nin big data. Velocity is accomplished bycontinuous feed into\nthe data store while variety is addressed by including struc-\ntured, semi-structured and unstructured in the data model.\nThe data model also can scale to large volumes based on\nas scale factor. We used PDGF as a starting point for our\ndata generator that covers the structured part. PDGF is\nenhanced to produce the semi-structured and unstructured\ndata. The unstructured component is based on a novel\ntechnique we developed leveraging the Markov chain model.\nThe proposal also provides a comprehensive list of workload\nqueries and sets directions for a novel metric that focuses\non the diﬀerent types of processing in big data. Finally, we\n1207\n\n[Página 12]\n0 10\n0 200 300 400 500 600 700 \nQ1 \nQ2 \nQ3 \nQ4 \nQ5 \nQ6 \nQ7 \nQ8 \nQ9 \nQ10 Run Time [s] \nQueries \nFigure 7: Runtime of Sample Queries\nve\nriﬁed the feasibility and applicability of our proposal by\nimplementing and running it on Teradata Aster DBMS.\nFor future work, we are planning to extend this work in\nthree main areas. First, we would like to enhance the pro-\nposal to be a concrete speciﬁcation that can lead to an in-\ndustry standard benchmark. This work include ﬁnalizing\nand detailing the data, workload and metric speciﬁcations.\nWe also think system availability during failure should be\naddressed in the ﬁnal speciﬁcation. Second, we think it will\nbe useful to provide a downloadable kit that can be used\nto setup and run the benchmark. This work include ﬁnal-\nizing the implementation of our data and query generators.\nFinally, we are planning to extend the benchmark proof of\nconcept to include velocity and multi-user test. We also\nwould like to run the benchmark on one the Hadoop eco-\nsystem like HIVE.\n6. REFERENCES\n[1] Apache Hadoop Project. http://hadoop.apache.org .\n[2] Apache Hive Project. http://hadoop.apache.org/hive .\n[3] Cloudera Distribution Including Apache Hadoop (CDH).\nhttp://www.cloudera.com .\n[4] Greenplum Database. http://www.greenplum.com .\n[5] GridMix Benchmark. http://hadoop.apache.org/docs/\nmapreduce/current/gridmix.html .\n[6] Oracle Database - Oracle. http://www.oracle.com .\n[7] PigMix Benchmark. https:\n//cwiki.apache.org/confluence/display/PIG/PigMix .\n[8] Teradata Database - Teradata Inc.\nhttp://www.teradata.com .\n[9] TwinFin - Netezza, Inc. http://www.netezza.com/ .\n[10] TPC Benchmark DS, 2012.\n[11] J. Bentley. Programming Pearls . Addison-Wesley, 2000.\n[12] M. J. Carey, D. J. DeWitt, and J. F. Naughton. The oo7\nBenchmark. In P. Buneman and S. Jajodia, editors,\nSIGMOD’93 , pages 12–21. ACM Press, 1993.\n[13] M. J. Carey, D. J. DeWitt, J. F. Naughton, M. Asgarian,\nP. Brown, J. Gehrke, and D. Shah. The BUCKY\nObject-Relational Benchmark (Experience Paper). In\nSIGMOD , pages 135–146, 1997.[14] M. J. Carey, L. Ling, M. Nicola, and L. Shao. EXRT:\nTowards a Simple Benchmark for XML Readiness Testing.\nInTPCTC, pages 93–109, 2010.\n[15] C. Chambers, A. Raniwala, F. Perry, S. Adams, R. R.\nHenry, R. Bradshaw, and N. Weizenbaum. FlumeJava:\nEasy, Eﬃcient Data-Parallel Pipelines. In PLDI, pages\n363–375, 2010.\n[16] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan,\nand R. Sears. Benchmarking Cloud Serving Systems with\nYCSB. In SoCC, pages 143–154, 2010.\n[17] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data\nProcessing on Large Clusters. Communications of the\nACM, 51(1):107–113, 2008.\n[18] M. Frank, M. Poess, and T. Rabl. Eﬃcient Update Data\nGeneration for DBMS Benchmark. In ICPE, 2012.\n[19] E. Friedman, P. Pawlowski, and J. Cieslewicz.\nSQL/MapReduce: A Practical Approach to Self-Describing,\nPolymorphic, and Parallelizable User-Deﬁned Functions.\nPVLDB , 2(2):1402–1413, 2009.\n[20] J. Gray. GraySort Benchmark. Sort Benchmark Home Page\n–http://sortbenchmark.org .\n[21] D. Laney. 3D Data Management: Controlling Data Volume,\nVelocity and Variety. Technical report, Meta Group, 2001.\n[22] J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs,\nC. Roxburgh, and A. H. Byers. Big data: The Next\nFrontier for Innovation, Competition, and Productivity.\nTechnical report, McKinsey Global Institute, 2011.\nhttp://www.mckinsey.com/insights/mgi/research/\ntechnology_and_innovation/big_data_the_next_\nfrontier_for_innovation .\n[23] R. O. Nambiar and M. Poess. The Making of TPC-DS. In\nVLDB, pages 1049–1058, 2006.\n[24] C. Olston, B. Reed, U. Srivastava, R. Kumar, and\nA. Tomkins. Pig Latin: A Not-So-Foreign Language for\nData Processing. In SIGMOD , 2008.\n[25] S. Patil, M. Polte, K. Ren, W. Tantisiriroj, L. Xiao,\nJ. Lopez, G. Gibson, A. Fuchs, and B. Rinaldi. YCSB++:\nbenchmarking and performance debugging advanced\nfeatures in scalable table stores. In SoCC, pages 9:1–9:14,\n2011.\n[26] A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J. DeWitt,\nS. Madden, and M. Stonebraker. A Comparison of\nApproaches to Large-Scale Data Analysis. In SIGMOD ,\npages 165–178, 2009.\n[27] R. Pike, S. Dorward, R. Griesemer, and S. Quinlan.\nInterpreting the Data: Parallel Analysis with Sawzall.\nScientiﬁc Programming , 13(4):277–298, 2005.\n[28] M. P ¨oss, R. O. Nambiar, and D. Walrath. Why You Should\nRun TPC-DS: A Workload Analysis. In VLDB, pages\n1138–1149, 2007.\n[29] T. Rabl, M. Frank, H. M. Sergieh, and H. Kosch. A Data\nGenerator for Cloud-Scale Benchmarking. In TPCTC,\npages 41–56, 2010.\n[30] T. Rabl, M. Sadoghi, H.-A. Jacobsen, S. G´ omez-Villamor,\nV. Munt´ es-Mulero, and S. Mankowskii. Solving Big Data\nChallenges for Enterprise Application Performance\nManagement. PVLDB , 5(12):1724–1735, 2012.\n[31] A. Schmidt, F. Waas, M. L. Kersten, M. J. Carey,\nI. Manolescu, and R. Busse. XMark: A Benchmark for\nXML Data Management. In VLDB, pages 974–985, 2002.\n[32] Teradata Aster. Teradata Aster Big Analytics Appliance\n3H - Analytics Foundation User Guide , release 5.0.1\nedition, 2012. http://www.info.teradata.com/edownload.\ncfm?itemid=123060004 .\n[33] L. Wyatt, B. Cauﬁeld, and D. Pol. Principles for an ETL\nBenchmark. In TPCTC, pages 183–198, 2009.\n1208",
      "metadata": {
        "filename": "ghazal2013.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\ghazal2013.pdf",
        "file_size": 951741,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:37.777022",
        "content_length": 68603
      }
    },
    "01fbb0dc-75e5-4de9-be8a-026bed817d8f": {
      "id": "01fbb0dc-75e5-4de9-be8a-026bed817d8f",
      "content": "[Página 1]\nBigSift: Automated Debugging of Big Data Analytics in\nData-Intensive Scalable Computing\nMuhammad Ali Gulzar\nUniversity of California, Los Angeles\nUSA\ngulzar@cs.ucla.eduSiman Wang\nHunan University\nChina\nsimanw@ucla.eduMiryung Kim\nUniversity of California, Los Angeles\nUSA\nmiryung@cs.ucla.edu\nABSTRACT\nDeveloping Big Data Analytics often involves trial and error debug-\nging, due to the unclean nature of datasets or wrong assumptions\nmade about data. When errors ( e.g. program crash, outlier results,\netc.) arise, developers are often interested in pinpointing the root\ncause of errors. To address this problem, BigSift takes an Apache\nSpark program, a user-defined test oracle function, and a dataset as\ninput and outputs a minimum set of input records that reproduces\nthe same test failure by combining the insights from delta debugging\nwith data provenance . The technical contribution of BigSift is the\ndesign of systems optimizations that bring automated debugging\ncloser to a reality for data intensive scalable computing.\nBigSift exposes an interactive web interface where a user can\nmonitor a big data analytics job running remotely on the cloud,\nwrite a user-defined test oracle function, and then trigger the auto-\nmated debugging process. BigSift also provides a set of predefined\ntest oracle functions, which can be used for explaining common\ntypes of anomalies in big data analytics—for example, finding the\norigin of the output value that is more than kstandard deviations\naway from the median. The demonstration video is available at\nhttps://youtu.be/jdBsCd61a1Q.\nCCS CONCEPTS\n•Software and its engineering →Software testing and de-\nbugging ;•Information systems →Data provenance ;\nKEYWORDS\nAutomated debugging, fault localization, data provenance, data-\nintensive scalable computing (DISC), big data, and data cleaning\nACM Reference Format:\nMuhammad Ali Gulzar, Siman Wang, and Miryung Kim. 2018. BigSift: Au-\ntomated Debugging of Big Data Analytics in Data-Intensive Scalable Com-\nputing. In Proceedings of the 26th ACM Joint European Software Engineering\nConference and Symposium on the Foundations of Software Engineering (ES-\nEC/FSE ’18), November 4–9, 2018, Lake Buena Vista, FL, USA. ACM, New\nYork, NY, USA, 4 pages. https://doi.org/10.1145/3236024.3264586\n0Work done by Siman Wang as an intern at University of California, Los Angeles.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA\n©2018 Association for Computing Machinery.\nACM ISBN 978-1-4503-5573-5/18/11. . . $15.00\nhttps://doi.org/10.1145/3236024.3264586\nTest\tPredicate\tPushdownPrioritizing\tBackward\tTracesBitmap\tbased\tTest\tMemoizationInput:\tA\tSpark\tProgram,\tATest\tFunction,\tand\tInput\tdata\tOutput:\tMinimum\tFault-Inducing\tInput\tRecords\nData\tProvenance\t+\tDelta\tDebuggingFigure 1: BigSift Overall Architecture\n1 INTRODUCTION\nData-Intensive Scalable Computing (DISC) systems such as Google’s\nMapReduce, Apache Spark, and Apache Hadoop enable processing\nmassive data sets. Similar to other software development platforms,\ndevelopers often deal with unclean data or make wrong (or in-\ncomplete) assumptions about the data. It is therefore crucial to\nequip these developers with toolkits that can better pinpoint the\nroot cause of an error. Unfortunately, debugging big data analytics\nis currently an ad-hoc, time-consuming process. Data scientists\ntypically write code that implements a data processing pipeline\nand test it on their local development workstation with a small\nsample data, downloaded from a TB-scale data warehouse. They\ncross fingers and hope that the program works in the expensive\nproduction cloud. When a job fails or they get results that end up\nbeing suspicious, data scientists must identify the source of the\nerror, often by digging through post-mortem logs.\nIn such cases, the programmer ( e.g. data scientist) may want\nto pinpoint the root cause of errors by investigating a subset of\ncorresponding input records. One possible approach is to track data\nprovenance (input output record mappings created in individual dis-\ntributed worker nodes). However, according to our prior study [ 1],\nbackward tracing based on data provenance finds an input subset in\nthe order of millions, which is still too large for a developer to man-\nually sift through. Delta Debugging (DD) is a well-known algorithm\nthat re-executes the same program with different subsets of input\nrecords [ 10]. Applying the DD algorithm naively on big data analyt-\nics is not scalable because DD is a generic, black box procedure that\ndoes not consider the key-value mapping generated from individual\ndataflow operators. Therefore, DD cannot prune irrelevant input\nrecords easily by considering the semantics of dataflow operators.\nThe technical contribution of BigSift is two folds. First, it com-\nbines delta debugging with data provenance . Second, it implements\nthree systems-level optimizations—(1) test predicate pushdown,\n(2) backward trace prioritization, and (3) bitmap-based memoiza-\ntion to be discussed in Section 2 in details—to improve debugging\nperformance. Figure 1 shows the overall architecture of BigSift .\n863\n\n[Página 2]\nESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA Muhammad Ali Gulzar, Siman Wang, and Miryung Kim\n(a) Job completion time and output\n (b) User and pre defined test function selection\n (c) Area plot reports the real time information of the debugging\nprocess➊\n➋\n➌➍\n➎\n➏➐\n➑\nFigure 2: BigSift ’s Web-based User Interface\nOur evaluations show that BigSift improves the accuracy of\nfault localizability by several orders-of-magnitude ( ∼103to107×)\ncompared to Titian’s [ 4] data provenance only. BigSift improves\nperformance by up to 66×compared to using Delta Debugging\nalone [ 1]. For each faulty output, BigSift is able to localize fault-\ninducing data in less than 62% of the original job running time.\nThis tool demonstration paper builds on our prior work [ 1] and\nfocuses on the tool features and corresponding implementation\ndetails of BigSift .BigSift is fully integrated with the current\nApache Spark’s web-based UI. A user can directly inspect raw out-\nput records, and write a test-oracle function on the fly or select\nfrom pre-defined test oracle functions. BigSift streams real time\ndebugging progress information from the remote cluster to the user\nthrough an interactive area plot and presents the current set of\nfault-inducing input records in a table format. Our current imple-\nmentation targets Apache Spark 2.1.1 with programs written in\nScala and Java [9].\n2 TECHNICAL APPROACH\nThe contribution of BigSift is to adapt delta debugging for big data\nanalytics by designing new systems optimizations and by lever-\naging data provenance in tandem, which provides backward and\nforward tracing capabilities for Apache Spark [ 4]. The overview\nof our approach is described in Figure 1. Without such systems\noptimizations, delta debugging could take hours if not days. This\nis because the input dataset size is huge and thus an exhaustive,\nbinary-search like algorithm such as delta debugging could take sig-\nnificant amount of time. In our evaluation, BigSift is up to 66 times\nfaster than DD. Below we summarize three systems optimizations\nat a high level, and further details are described elsewhere [1].\n2.1 Test Function Push Down\nIn the map-reduce programming paradigm, a combiner performs\npartial aggregation for operators such as reduceByKey on the map\nside before sending data to reducers in order to minimize network\ncommunication. Since delta debugging uses a user-defined test\nfunction to check if each final record is faulty, our insight is that,\nduring backward tracing, we should isolate the exact partitions withfault-inducing intermediate inputs to further reduce the backward\ntracing search scope.\nIn Apache Spark, certain aggregation operators ( e.g.reduceByKey )\nrequire a user to provide an associative andcommutative function\nas an argument. BigSift implements a new optimization by push-\ning down a user-defined test function to partitions in the previ-\nous stage to test intermediate results. This optimization is enabled\nwhen (1) the program ends with an aggregation operator (such as\nreduceByKey ) that requires an associative function f1; (2) f1◦f2\nis associative, when f2is a test function; and (3) f1◦f2is failure-\nmonotone. If this monotonicity property is not satisfied (which can\nbe verified by testing final output), or none of the partitions fail the\ntest function, BigSift rolls back to the default case of backward\ntracing the final faulty record.\n2.2 Overlapping Backward Traces\nMultiple faulty output records may be caused by the same input\nrecords due to operators such as flatMap orjoin , where a single\ndata record can produce multiple intermediate records, leading to\nmultiple faulty outputs. Therefore, BigSift prioritizes the common\ninput records leading to multiple outputs before applying DD. To\ncheck the eligibility for this optimization, BigSift explores a pro-\ngram DAG to find at least one 1-to-many or many-to-many operator\nsuch as flatMap andjoin .\nIn order to explore all the possible overlapping traces, BigSift\noverlaps the two smallest backward traces (let’s say t1and t2), to\nfind the intersection, t1∩t2. If the test function evaluated on t1∩t2\nfinds any fault, then DD is applied to t1∩t2and the remaining\n(potential) failure-inducing inputs t1−t2and t2−t1. Otherwise, DD\nis executed over both initial traces t1and t2. If any fault-inducing\ninputs are found in the overlap, there could be potential time saving\nfrom not processing the overlapped trace twice.\n2.3 Bitmap Based Memoization of Test Results\nDD is not capable of detecting redundant trials of the same input\nconfiguration and therefore may test the same input configuration\nmultiple times. To avoid waste of computational resources, BigSift\nuses a test results memoization optimization. A naive memoization\n864\n\n[Página 3]\nAutomated Debugging of Big Data Analytics in Data-Intensive ... ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA\n1class BigSift(sc:SparkContext, logFile:String){\n2 def runWithBigSift[T](\n3 sparkProgram : (RDD[String],Lineage[String]) =>\nRDD[T] , test : T => Boolean ) : Unit\n4 ... }\nFigure 3: BigSift ’s API\nstrategy would require scanning of the content of an input configu-\nration to check whether it was tested already; such content-based\nmemoization would be time consuming and not scalable. BigSift\ninstead leverages bitmaps to compactly encode the offsets of the\ninput dataset to refer to a sub-configuration.\nThe universal splitting function for DD is thus instrumented to\ngenerate sub-configurations along with their related bitmap descrip-\ntions. BigSift maintains the list of already executed bitmaps, each\nof which points to the test result of running a program on the input\nsub-configuration. Before processing an input sub-configuration,\nBigSift uses its bitmap description to perform a look-up in the list\nof bitmaps. If the result is positive, the test result for the target sub-\nconfiguration is directly reused by the look-up. Otherwise, BigSift\ntests the sub-configuration and enrolls its bitmap and the corre-\nsponding test result in the list. This technique avoids redundant\ntesting of the same input sub-configuration and reduces the total\ndebugging time. BigSift uses the compressed Roaring Bitmaps\nrepresentation to describe large scale datasets [5].\n2.4 Implementation\nTo enable automated debugging of big data analytics applications,\na user can instantiate BigSift class with SparkContext and in-\nput file path as input arguments, as shown in Figure 3. Internally,\nthis class instantiates LineageContext that enables Titian’s instru-\nmentation for data provenance support. More details on the usage\nof Titian is described in our prior VLDB 2016 paper [ 4]. A user\ncan then call runWithBigSift method with a test oracle function,\nand a sparkProgram —a directly acyclic graph (DAG) workflow\nthat takes in an input Resilient Distributed Dataset (RDD–i.e., an\nabstraction of distributed collection) and returns the final RDD.\nBigSift is designed as an external Java library (jar) and can be\ndeployed by importing the jar file in a Spark application running\non a data-provenance enabled Spark distribution such as Titian [ 4].\nBigSift ’s interactive UI is available on port 8989 on the Spark driver\nnode. Figure 2 shows the web-based user interface. Once the job is\ncompleted, a user can examine the job execution time, raw output,\netc. She can write her own custom test-oracle function or select\nfrom pre-defined test functions. BigSift also displays a set of input\nrecords that reproduce the same test failure. The area chart reports\nthe real time debugging progress information. A user can click on\nthe graph to see the size and samples of failure-inducing inputs.\n3 DEMONSTRATION SCENARIO\nSuppose Alice is a data scientist and she writes a big data application\nin Apache Spark to analyze a large scale dataset that contains\npassenger transit information in the US. Since the data is in the\nscale of terabytes, she takes a small sample of the dataset (say 10\nMB) and builds a data processing pipeline using Spark in a local\nmachine. Alice wants to find the total transit time for all passengers\nspending less than 45 minutes while in transit for each airport in\n(a) Key-value output visualization\n (b) Data prove-\nnance visualization\nFigure 4: BigSift ’s histogram visualization of key-value\nbased output records\n1val countoftranist = sc.textFile(dataset).map{ s =>\n2 val tokens = s.split(\",\")\n3 val arrival_hr = tokens(2).split(\":\")(0)\n4 val diff = getDiff(tokens(2) , tokens(3))\n5 val airport = tokens(4)\n6 ((airport, arrival_hr), diff)}\n7 .filter{ v => v._2 < 45}\n8 .reduceByKey(_+_)\n9 .collect()\nFigure 5: A Spark program written in Scala that finds the\ntotal layover time of all passengers spending less than 45\nminutes per airport at each hour.\nthe US for each hour. A row in the dataset represents a passenger’s\ntransit information in the following format.\n[date, passenger, arrival, departure, airport code]\n9/4/17 , 161413 , 6:52 , 8:22 , MNN\nThe program in Figure 5 first loads the dataset (line 1) and scans\neach row to retrieve a key-value pair. A key consists of the airport\ncode and arrival hour of a passenger and the value is the transit\ntime spent in minutes (departure time -arrival time) at the airport\n(line 2-6). Line 7 filters passengers with the transit time less than\n45 minutes. Finally, the program sums up the transit times of all\npassengers per airport at each arrival hour (line 8).\nAfter writing this application, Alice submits the job to the pro-\nduction cloud which results in the following output:\n((SEA,7) , 175080)\n((LAX,11) , 173460)\n((MNN,23) , -27804120)\n.....\nShe then realizes that some output records look suspicious. For\nexample, the total transit time of MNN is-27804120 , when she\nexpects the total transit time to be a positive value. Alice wants\nto investigate what are the exact input records responsible for\nproducing a negative value. This task is challenging because the\nlarge scale dataset is infeasible to inspect manually and there is no\none-to-one mapping between input records and output records due\nto an aggregation step that applies user-defined functions.\nAlice decides to use BigSift that takes her program, input data\nset, and a test oracle function as input and, eventually, returns\nthe following culprit input record responsible for the suspicious\nnegative output value.\n11/9/12 , 141011 , 22:53 , 0:23 , MNN\nThe following describes BigSift demonstration step by step.\n865\n\n[Página 4]\nESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA Muhammad Ali Gulzar, Siman Wang, and Miryung Kim\nStep 1: Program Output Inspection. Figure 2 shows the landing\npage of BigSift . It shows the size of input dataset as the number of\nrecords, the job processing time, final output records in a text box.\nSee➊,➋, and➌in Figure 2 respectively. To better visualize output\nrecords, BigSift provides interactive and dynamic visualization\nof key-value pairs using a histogram to make it easier for a user\nto identify anomalous records visually (Figure 4(a)). For example,\nAlice can mark any negative value as incorrect using a histogram\nand note down this threshold to construct a test function.\nStep 2: Classifying Suspicious or Wrong Output Records by Defin-\ning a Test-Oracle Function. BigSift enables a user to write a test\nfunction—a predicate to be applied to each final output record to\ndistinguish correct outputs from incorrect or anomalous outputs.\nBigSift also enables user to choose from a list of pre-defined test\npredicate functions (Figure 2(b)- ➍) to help explain the common\ntypes of anomalies in big data analytics: for example, (1) explain\nhow a minimum output value is created, (2) explain how a maximum\noutput value is created, (3) explain how the output value greater\nthan kstandard deviations from the median is created, etc. Once\nthe selection is made from the radio buttons, a user can press the\nRunBigSift button (Figure 2(b)- ➏). Internally, BigSift selects the\ncorresponding pre-defined test function to initiate debugging.\nStep 3: Visualization of Data Provenance. To help understand the\npropagation of fault-inducing intermediate input records across\ntransformation steps, BigSift provides a pie chart based DAG vi-\nsualization of the workflow (Figure 4(b)). Each node in this graph\nis represented as a pie chart where a red segment shows the ratio\nof fault-inducing intermediate records against the total number of\nrecords processed by that transformation. By viewing data ratio at\neach transformation, a user may get deeper insight.\nStep 4: Automated DISC Debugging. When BigSift is invoked by\nthe user, a realtime area chart appears on the UI. In Figure 2(c), Y-\naxis represents the number of fault-inducing input records isolated\nbyBigSift in log scale and X-axis represents debugging time. As\nthe time passes, BigSift streams debugging progress information\nfrom the cloud. A user can click on any part of the chart to view\nsample fault-inducing input records at the selected time. A mouse\nhover-over will show the number of fault-inducing input records.\nAs soon as BigSift finds the minimum set of fault-inducing input\nrecords, BigSift reports the total debugging time through a push\nnotification (green container in Figure 2(c)- ➐).\n4 RELATED WORK\nDelta debugging (DD) is a well known technique for finding the min-\nimal failure-inducing input [ 10] that requires multiple tests of the\nprogram, which alone, is not tractable for DISC system workloads.\nHDD tries to minimize DD tests by assuming that the input is in a\nwell defined hierarchical structure which rarely holds [ 7]. RAMP\n[3] and Newt [ 6] add data provenance support to DISC systems.\nBigSift differs from these by leveraging DD and data provenance in\ntandem and by implementing unique systems optimizations to im-\nprove performance for DISC workloads. BigDebug is an interactive\ndebugger for Spark [ 2] and it leaves to the developer to manually\nidentify the root cause of errors. Data X-ray [ 8] extracts a set of\nfeatures representing input data properties and summarizes the\nerrors in a SQL table, but does not support automated debugging.5 EVALUATION AND SUMMARY\nWe are in the early days of debugging big data analytics. This tool\ndemonstration paper showcases BigSift , an automated debugging\ntoolkit in the context of data-intensive scalable computing (DISC).\nFinding failure-inducing inputs is just the beginning. We see further\nopportunities for automated debugging of DISC applications, such\nas automated data cleaning and faulty code localization.\nIn our prior work [ 1], we evaluated BigSift on a 16-node cluster\nwith 8 subject program where faults were injected in both input\ndatasets or code. The datasets used in the evaluation ranges from\nfew GB to 80GB. In comparison to using DD alone, BigSift re-\nduced the fault localization time (as much as 66×) by pruning out\ninput records that are not relevant to faulty outputs. Further, our\ntrace overlapping heuristic decreases the total debugging time by\n14%, and our test memoization optimization provides up to 26%\ndecrease in debugging time. Indeed, the total debugging time taken\nbyBigSift is on average 62% less than the original job running\ntime per single faulty output.\nACKNOWLEDGMENT\nWe would like to thank Tyson Condie and Matteo Interlandi with\ntheir insights in the design of BigSift optimizations. Participants\nin this project are in part supported through AFRL grant FA8750-\n15-2-0075, NSF grants CCF-1764077, CCF-1527923, CCF-1460325,\nCCF-1723773, ONR grant N00014-18-1-2037, and gifts from Google\nand Huawei.\nREFERENCES\n[1]Muhammad Ali Gulzar, Matteo Interlandi, Xueyuan Han, Mingda Li, Tyson\nCondie, and Miryung Kim. 2017. Automated Debugging in Data-intensive Scal-\nable Computing. In Proceedings of the 2017 Symposium on Cloud Computing (SoCC\n’17). ACM, New York, NY, USA, 520–534. https://doi.org/10.1145/3127479.3131624\n[2]Muhammad Ali Gulzar, Matteo Interlandi, Seunghyun Yoo, Sai Deep Tetali, Tyson\nCondie, Todd Millstein, and Miryung Kim. 2016. BigDebug: Debugging Prim-\nitives for Interactive Big Data Processing in Spark. In Proceedings of the 38th\nInternational Conference on Software Engineering (ICSE ’16) . ACM, New York, NY,\nUSA, 784–795. https://doi.org/10.1145/2884781.2884813\n[3]Robert Ikeda, Hyunjung Park, and Jennifer Widom. 2011. Provenance for gen-\neralized map and reduce workflows. In In Proc. Conference on Innovative Data\nSystems Research (CIDR) .\n[4]Matteo Interlandi, Kshitij Shah, Sai Deep Tetali, Muhammad Ali Gulzar, Se-\nunghyun Yoo, Miryung Kim, Todd Millstein, and Tyson Condie. 2015. Titian:\nData Provenance Support in Spark. Proc. VLDB Endow. 9, 3 (Nov. 2015), 216–227.\nhttps://doi.org/10.14778/2850583.2850595\n[5]Daniel Lemire, Gregory Ssi-Yan-Kai, and Owen Kaser. 2016. Consistently Faster\nand Smaller Compressed Bitmaps with Roaring. Softw. Pract. Exper. 46, 11 (Nov.\n2016), 1547–1569. https://doi.org/10.1002/spe.2402\n[6]Dionysios Logothetis, Soumyarupa De, and Kenneth Yocum. 2013. Scalable\nlineage capture for debugging DISC analytics. In Proceedings of the 4th annual\nSymposium on Cloud Computing . ACM, 17.\n[7]Ghassan Misherghi and Zhendong Su. 2006. HDD: Hierarchical Delta Debugging.\nInProceedings of the 28th International Conference on Software Engineering (ICSE\n’06). ACM, New York, NY, USA, 142–151. https://doi.org/10.1145/1134285.1134307\n[8]Xiaolan Wang, Xin Luna Dong, and Alexandra Meliou. 2015. Data X-Ray: A Diag-\nnostic Tool for Data Errors. In Proceedings of the 2015 ACM SIGMOD International\nConference on Management of Data (SIGMOD ’15) . ACM, New York, NY, USA,\n1231–1245. https://doi.org/10.1145/2723372.2750549\n[9]Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma,\nMurphy McCauley, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2012. Re-\nsilient Distributed Datasets: A Fault-tolerant Abstraction for In-memory Cluster\nComputing. In Proceedings of the 9th USENIX Conference on Networked Systems\nDesign and Implementation (NSDI’12) . USENIX Association, Berkeley, CA, USA,\n2–2. http://dl.acm.org/citation.cfm?id=2228298.2228301\n[10] Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and isolating failure-\ninducing input. Software Engineering, IEEE Transactions on 28, 2 (2002), 183–200.\n866",
      "metadata": {
        "filename": "gulzar2018.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\gulzar2018.pdf",
        "file_size": 868300,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:37.905152",
        "content_length": 23910
      }
    },
    "ac59e517-f93a-4192-9fe3-efa1ff711315": {
      "id": "ac59e517-f93a-4192-9fe3-efa1ff711315",
      "content": "[Página 1]\nImplementation of Big Data Analytics for Machine \nLe\narning Model Using Hadoop and Spark \nEnvironment on Resizing Iris Dataset \n1st Tresna Maulana Fahrudin  \nDepartment of Data Science  \nUniversitas Pembangunan Nasional \n“Veteran” Jawa Timur \nSurabaya, Indonesia \ntresna.maulana.ds@upnjatim.ac.id2nd Prismahardi Aji Riyantoko \nDepartment of Data Science  \nUniversitas Pembangunan Nasional \n“Veteran” Jawa Timur \nSurabaya, Indonesia \nprismahardi.aji.ds@upnjatim.ac.id3rd Kartika Maulida Hindrayani \nDepartment of Data Science  \nUniversitas Pembangunan Nasional \n“Veteran” Jawa Timur \nSurabaya, Indonesia \nkartika.maulida.ds@upnjatim.ac.id\nAbstract — The concept of Big Data to refer to huge volumes \nof data and attributes, but data samples through the use of a \ndiverse set of features gathered from various sources. A \nsignificant amount of time is spent constructing a pre-processing \nworkflow and an analysis process that make possible impactful \nfor machine learning. Big Data analytics is being driven by the \nneed to process Machine Learning data, actual real-time \nprocessing, and graphics processing. Hadoop and Spark, both \naccessible data warehousing frameworks that allow for the \ndistribution and computation of massive datasets across several \nclusters of computer nodes, are the most efficient prospects for \nBig Data analysis in a distributed setting. To test the ability of \nthese Big Data Tools, this research use Iris dataset as \nexperimental data which is resized to a larger file. Multinomial \nNaive Bayes algorithm was employed to create a classification \nmodel for Iris flowers using Spark Machine Learning Library. \nThe experimental result reported that there is a difference in \naccuracy and execution time during testing machine learning \nperformance in Hadoop. The experiment given the best \nperformance used Iris dataset is resized to 148 MB consisting of \n5,184,000 samples, the model accuracy reached 95.32% with an \nexecution time of 1 minute 4 seconds. The increase in the \nnumber of samples in the dataset is also positively correlated \nwith increasing execution time. However, execution time is \nrelatively cheap in the Hadoop Environment.  \nKeywords—big data analytics, machine learning, hadoop, \nspark, iris dataset \nI. INTRODUCTION  \nBig Data is directly related to an increase in the peak in \nvarious data streams as new technologies are gradually \ndeployed. Knowledge is increasingly widely obtainable than \nit ever has been, thanks to the rise of the internet, and the \nconsumption of social platforms, phone app, connected, and \ndemodulated things is increasing at an alarming rate [1]. Big \nData Analytics are methods for analyzing and developing Big \nData in the context of strategic planning. Data mining is a \nsubset of big data analysis that seeks to discover the \nrelationship between previously unknown aspects of a dataset \nby employing a variety of field approaches such as machine \nlearning algorithm, database, statistical method, and \nmathematics formulations [2]. Data analytics approach \nprovides both granted and interpreted technology in a variety \nof domains for future predictions. [3][4]. One of the most \ncritical data volumes is the ability to handle a large amount of \ncomplex content from an increasing number of different and \nautonomous sources. Quite a companies were using the \nconcept \"Big Data\" to refer to huge volumes of data and \nattributes, but data samples through the use of a diverse set of \nfeatures gathered from various sources have also been referred \nto as Big data [5][6]. A significant amount of time is spent constructing a pre-\nprocessing workflow and an analysis process that make \npossible impactful for machine learning. Data pre-processing \nidentifies an issues such as data redundancy, instability, noise, \nvariability, difficulty, unsupervised machine learning, and \ntransformation. Human knowledge is widely used especially \nfor data pre-processing and planning, in addition to the \navailability of a variety of alternative methods. Sophisticated \ndata interpretations do not apply to large-scale datasets, \nrendering processing ineffective. As a result, massive amounts \nof data identify the opportunity to reduce reliance on human \ninsight by drawing from larger, more complicated and \ndifficult, and occasionally improved data sets [7].  \nThe potential for heterogeneous data to be used to amount \nof coverage machine learning techniques to unique varieties \nof market opportunities and behaviors also seems to be greater \nthan ever before, but their legitimacy is mostly questioned \n[8][9]. According to the source, huge data provides an \nexceptional amount of informative depth, but traditional \nmachine learning is hindered by the enormous number of \nvariables. All are getting bigger and more intricate, \nnecessitating extensive research and advances in machine \nlearning [10]. Because learning algorithms are incredibly \nstrong and can conduct continuous learning, which reduces \nthe need for human contact, machine learning will swiftly \ndisplace multiple people's employment in the future [11]. \nBig Data analytics is being driven by the need to process \nMachine Learning data, actual real-time processing, and \ngraphics processing. Hadoop and Spark, both accessible data \nwarehousing frameworks that allow for the distribution and \ncomputation of massive datasets across several clusters of \ncomputer nodes, are the most efficient prospects for Big Data \nanalysis in a distributed setting [12]. Hadoop, the main \nsoftware that forms the basis of an ecosystem consisting of \nsoftware that works together. Primarily, as a system for \nprocessing very large volumes of data. Besides Hadoop, there \nare Hadoop Distributed File System (HDFS) that provide high \nthroughput access to application data. Apache Spark is useful \nas an open-source unified analytics engine for large-scale data \nprocessing. \nTherefore, the research presents the performance of \nHadoop and Spark as Big Data analytic environment. We use \nHadoop and Spark to solve big data analytics and employee \nthe machine learning model using Multinomial Naïve Bayes \nto solve classification task on resizing Iris dataset. The \nstandard size of benchmark Iris dataset is in kilobytes. \nHowever, the dataset is resized to up to megabytes in the \nexperiment. The purpose is to test the performance of the big \ndata environment and to evaluate the accuracy, execution time \nof different file sizes and validation sampling. 2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS)\n978-1-6654-7327-9/22/$31.00 ©2022 IEEE 4292022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS) | 978-1-6654-7327-9/22/$31.00 ©2022 IEEE | DOI: 10.1109/ICIMCIS56303.2022.10017465\nAuthorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 2]\nII. RELATED WO RKS \nDespite the fact that several applications are attempting to \nrun Big data including a variety of existing approaches that \nmanaged datasets, in this section two, the applications that \nwere chosen to be reviewed and determined the conclusions \nby authors in recent years, more or less every research in \nbelow for one implementation from big data, machine \nlearning, Hadoop, and Spark, which also implementing \nseveral algorithms that performed inside applications to \nrepresent the obvious impact of these conceptual methods. \nIlham Kusuma, et al. focused on the effectiveness of smart \nK-means throughout the environment of Hadoop using Spark \n[13]. The idea behind using Spark to develop smart K-means \nis that many types of data, such as genomes, are relatively \nlarge and continue to grow in size, making it relatively simple \nto expand the Hadoop environment. The open-source design \nthat aids various computations in both Map-reduce and \nHadoop. In the Big Data mining process, the spark design is \nscalable. In place of the standard Resilient Distributed \nDataset, the design included data batching (RDD). Using the \nfirst data RDD, compare its specification to the \nimplementation. According to experience, data batch \nimplementation is faster than first RDD implementation. \nAnjuman Prabhat and Vikas Khullar employed the method \nof machine learning classifiers using Naïve Bayes and \nlogistics-type were utilized in this work to cope with mission \nchallenges that included Twitter comments [14]. They also \nincluded Hadoop and Mahout in the classifier. To improve the \nefficiency of the experiment, an extra module for instance the \nobservation controller is inserted. A further examination of \nlogistics regression analysis yields 10.1% and delivers 4.34% \nmore accuracy for the same dataset scale (sample size). This \narticle contains some supplementary language that describes \ntweets as a potential future job. This may boost categorization \nperformance by combining text and graphics. Bi-gram, \ntrigram, and other formalized forms may be more accurate. \nTABLE I.  THE COMPARISON BETWEEN RELATED WORKS AND  \nPROPOSED WORK  IN BIG DATA RESEARCH  \nNo\n. Author(s)  Dataset  Method  and \nTools Evaluation  \n1. I. Kusuma, \net al. [13]  First dataset \nhas \nch\naracteristi\nc of 5 \nfeatures and \neach feature \nhas 5 peaks \n The second \ndataset has. \ncharacteristi\nc of 10 \nfeatures and \nit is created \nfrom 10 \ndifferent \ncentroids  K-Means \nBased on \nSp\nark for \nBig Data \nClustering \n Cluster has \n4 slave node \nand one \nmaster node  \n Every node \nutilizes with \nIntel Core i7 \nand RAM \n32 GB  • Speed up \nc\nomputational \ntime in big \ndata problem \nreach 58.4-\n3075.2 \nseconds \n• Higher \nsilhouette \nvalue than \noriginal k-\nmeans using \nsynthetic data \nreach 0.628 - \n0.7476 \n2. A. Prabhat,  \nV. Khullar. \n[1\n4]  Real t ime \ntwitter with \ntw\no \ncategories: \npositive and \nnegative \nreviews (6 \nMB) \n  Naïve \nBayes and \nLo\ngistic \nRegression \n Hadoop \n2.7.1 and \nMahout 0.9 \n Single node \nwith Intel • Accuracy of \nN\naïve Bayes \nreach 66.67% \nand Logistic \nRegression \nreach 76.76% \n• Computationa\nl time of Naïve \nBayes reach No\n. Author(s)  Dataset  Method  and \nTools Evaluation  \nCore  i3 and \nRAM 4 GB  \n 15732 mile -\nseconds and \nLo\ngistic \nRegression \nreach 3689 \nmile-seconds \n3. I. R. \nPrabaswar\na,\n R. \nSaputra. \n[15]  Mapping of \ndengue fever \nin\ncidence \nbased on \ntwitter data \nin Southeast \nAsia \n(4.056.690 \ntweets) \n  Visualizatio\nn of dengue \nfe\nver \n Hadoop \n3.1.2 and \nSpark 2.4.0 \n Cluster has \none slave \nnode and \none master \nnode \n Every node \nwith Intel \nCore i7, \nRAM  16 \nGB in \nmaster-\nnode and 8 \nGB in slave-\nnode • The minimum \ne\nxecution time \nreach 5,3 \nminutes \n• The optimal \nallocation of \nmemory is 3 \nGB and \nmaximum \nmemory \nscheduler is 4 \nGB \n4. V. Suriya \nNarayanan\n, \net al. [16]  Protein \ninteraction \npr\noblem \nusing graphs \nand its \nsemantic \nrepresentatio\nn  Large scale \ndistributed \ngr\naphs \nusing \nApache \nSpark \n Word2Vec \nlanguage \nfor model \nvocabulary • Achieved \na\npproximatel\ny 97% \naccuracy \nusing a 128-\ndimensional \nembedding \nas compared \nto around \n95% using a \n2-\ndimensional \nembedding \n5. T. M. \nFahrudin, \net\n al. \n(Proposed \nResearch) • Resizing \nI\nris Dataset \nin 5 KB - \n148 MB  Apache \nHadoop \n3.\n2.1 \n Apache \nSpark 3.0.0 \n Machine \nLearning \nModel \nusing \nMultinomia\nl Naïve \nBayes \n Single node \nwith Intel \nCore i3 and \nRAM 4 GB  \n • Accuracy \na\nnd \nexecution \ntime of \nbuilding \nclassification \nmodel based \non resizing \nIris dataset (5 \nKB – 148 \nMB) \n \n \nIr\nwan Rizqi Prabaswara and Ragil Saputra carried out \nresearch about trend mapping on the fever data Dengue \nHemorrhagic Fever (DHF) from social media twitter. This \nresearch desires to construct a visualization of data obtained \nfrom twitter with using Hadoop and spark in tracking the \ngrowth of dengue in the Southeast Asia area [15]. The findings \nof trend mapping reveal that there is a substantial association \nbetween twitter data and the original data on dengue incidence \ncollected from WHO. This research also investigated the \nperformance of Hadoop and Spark. The larger the memory 2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS)\n430Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 3]\nallocation of the executor that is applied and the larger and \nsimilar the maximum allocation of the memory scheduler \napplied to each node, the shorter the time required to complete \nthe task. However, at some point Hadoop and Spark \nconfigurations hit a breaking point, so if the allocation is \nincreased it produces the same result. \nIn the other works, V. Suriya Narayanan, et al. \ndemonstrated how and when to use Apache Spark to \nimplement a distributed learning algorithm that operates \nthrough large graphs [16]. It is technologically applicable \nmaking it easy and applicable for large-scale deployments, \nand it has a fairly clear set of powerful predictive capabilities. \nIt is advantageous for graph-based developers to use \ncomprehensible word embedding as physical world focuses. \nThe use of the Spark design and optimization for the efficient \nloading of distributed edge records and in-graph frameworks. \nThe application Word2Vec then used Spark (MLlib) fetch \nnode interconnection with a random walk according to each \ngraph node. Excellent results have been achieved. Typically, \n70% of the data could be used to visualize and reinforce \nlearning. However, this approach uses only 1% of the dataset \nto training the models. By implementing this model, they help \nin achieving a prediction precision of close to 95%. \nTable I show the comparison between related works and \nproposed works in Big Data research. Big Data tools and \nmethods used in the research commonly are Hadoop, Spark, \nand Mahout. However, what makes the difference is the \ndataset tested using the Iris dataset which has been resized to \na larger size. We test the performance of big data when faced \nwith exponentially growing data sizes. Then, we evaluate the \naccuracy and execution time. \nIII. SYSTEM DESIGN  \nI\nn this chapter, the system design implemented in the \nproposed research will be explained. It will be discussed \nregarding Iris dataset, Hadoop, Hadoop Distributed File \nSystem (HDFS), Spark and MLlib, Multinomial Naïve Bayes, \nand the evaluation model. Fig.1 show the system design of \nproposed research. \n \nFig. 1.  S ytem design of proposed research \nA. I ris Dataset \nThe Iris dataset is a popular dataset used in the learning \nand experiment about data science. The dataset stores tabular \ndata about flower such as sepal length and width, petal length \nand width. Types of flowers are classified into three categories \nnamely Setosa, Versicolour, and Virginica. The dataset was \nchosen because it is benchmark dataset that has been widely \nused and popular. The type of the features is continuous, while \nthe categories are discrete. The number of instances on Iris \ndataset is 150 which each category is proportional to 30 \ninstances. Almost all studies using iris datasets by researcher \nachieve good accuracy above 90%. B. Hadoop \n Hadoop is a big data technology product of the open-\nsource Apache software [17]. The function of Hadoop is to \nsolve the problem of large amounts of data and computing \nwith a set of computer networks. Hadoop has an architecture \nwith three components, namely HDFS (Hadoop Distributed \nFile System), MapReduce, and YARN. The characteristics of \nHadoop include: \n• Hadoop is optimally used to handle large amounts of \nstructured, semi-structured, and unstructured dataset.  \n• Hadoop replicates data across multiple computers \n(clustering). If one computer has a problem, the data \ncan be processed from one of the other computers that \nare still alive.  \n• The Hadoop process is a batch operation handling a \nvery large amount of data, so the response time is not \nreal time. \nC. Hadoop Distributed File System \nHadoop Distributed File System (HDFS) is part of Hadoop \nthat functions as a consistent data storage place [18]. An \nimportant process in HDFS is data replication to different \npartitions by massive and parallel. The replication is balanced \nin different blocks. Distributed file systems designed for fault-\ntolerant file systems can run on multiple servers with low-cost \nspecifications. HDFS is designed to support applications with \nlarge data sets, even terabytes of files. When a file is processed \nvia HDFS, it is split into smaller parts and then the smaller \npart of the file is distributed across multiple nodes in the \ncluster system thus enabling parallel processing. \nD. Spark and MLlib \nSpark is an open-source framework that is suitable for use \nin iterative algorithmic processes [19]. Spark allows \nconnecting analytics engines with high-scale data processing. \nMLlib is part of Spark with Machine Learning libraries. The \ngoal is to make machine learning easier and more scalable. \nMLlib has many uses including regression, classification, \nclustering, can perform linear and statistical algebraic \ncalculations and handle pipelines. \nE. Multinomial Naïve Bayes  \nMultinomial Naïve Bayes event in the model is a \nmultinomial vector \u0001\u0002\u0003, … . , \u0002 \u0007\b where \u0002\u0003 is the probability \nt\nhat event i will occur. Vector \u0001\t\n, … . , \t \u0007\b  in the form of a \nhistogram. \t\u0003 is the total number of events occurring within a \nc\nertain range. The Multinomial Naïve Bayes formula follows \nequation 1. \n \np \u0001x | \u000f\u0010 \b \u0011\u0012∑ \u0014\u0015\u0016\n\u0017 \u0018\u0019\u001a!\n∏ \u0014\u0015!\u0016\n\u0017\n\u001d\u0019 ∏ \u0002\u001e\u001f\u0014\u0017 \u0007\n\u0003\n \n    (1) \nF\n. Evaluation Model \nThe evaluation of the model that will be used is accuracy \nand execution time. Accuracy is how accurate the predictions \nmade by the model are with the actual predictions. The \naccuracy formula follows equation 2. \n \nCR \u0011#\n$    (2) \nWhere: \nCR : The correct rate \nC : The number of samples recognized correctly \n2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS)\n431Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 4]\nA : The number of all samples. \n \nWhile the execution time is recorded from start time to end \ntime during the running code script program. The execution \ntime formula follows equation 3. \n \nExecution time \u0011 ./0 1\u001f23 4 51671 1\u001f23    (3) \nIV. EXPER IMENTAL RESULT AND DISCUSSION  \nI\nn this section will discussed about dataset preparation, \nconfiguration Hadoop and Spark environment, building model \nof Multinomial Naïve Bayes, evaluation model and execution \ntime. \nA. Dataset Preparation \nIris dataset consists of 4 features including Sepal Length \n(cm), Sepal Width (cm), Petal Length (cm), and Petal Width \n(cm), while 3 species as class label including Iris-setosa, Iris-\nversicolor, and Iris-virginica which the total number of \nsamples for each class is 50 proportionally. If the experiment \nuses an iris dataset of 150 samples (5 kb) to test machine \nlearning models into a big data environment, this will certainly \nnot have much impact. Therefore, the iris dataset in this \nexperiment was resized to 72 MB of 2,592,000 samples and \n145 MB of 5,184,000 samples. Fig. 2 show the resizing \nsamples of Iris dataset. \n \nFig. 2.  R esizing samples of Iris dataset \nB. C onfiguration Hadoop and Spark Environment \nBefore implementing big data analytics, there are several \ntools and software that need to be prepared. The following are \nthe specifications of the device used in the experiment: \n• Processor : Intel core i3 \n• RAM: 4.00 GB \n• System type: 64-bit Operating System, x64-based \nprocessor \n• OS: Windows 10 Home Single Language \nWhile the specifications of the software used in the \nexperiment as follow: \n• Oracle VM VirtualBox Manager • Ubuntu 64-bit: Virtual hard disk 15 GB \n• Open Java Development Kit (OpenJDK) \n• Apache Hadoop 3.2.1 (stable version) \n• Apache Spark 3.0.0 (stable version) \nTo install and configure Hadoop on Ubuntu, first step is \ninstall Java via Software Development Kit Manager \n(SDKMAN), check the path $JAVA_HOME, and then use \nJDK version 8.0.242.hs-adpt. Unzipped Apache Hadoop 3.2.1 \n(file extension *.tar.gz) and will extract several files such as \nstart-all.sh, stop-yarn.sh, workers.sh, start-dfs.sh, httpfs.sh, \nstop-balancer.sh, and etc. Hadoop directory needs to have \npermissions set to change the ownership of its username by \nusing the command “chown username:username -R \nname_directory”. Bash shell script (~/.bashrc) also set Hadoop \nhome directory with a variable named HADOOP_HOME and \nset the binary files in Hadoop home directory which is located \nin HADOOP_HOME/bin. To check Hadoop configuration is \nrunning well or not through the command \"hadoop version\". \nMaster files such as hadoop-env.sh also need to be set the Java \nhome directory in path so that Yarn, HDFS, MapReduce, and \nothers can run correctly. \nTo install and configure Spark on Ubuntu, unzipped \nApache Spark 3.0.0 (file extension *.tar.gz) and will extract \nseveral libraries such as Machine Learning Library (MLlib), \nR, Kubernetes, and etc. Spark directory needs to have \npermissions set to change the ownership of its username. Bash \nshell script (~/.bashrc) also set Spark home directory with a \nvariable named SPARK_HOME and set the binary files in \nSpark home directory which is located in \nSPARK_HOME/bin. To check Spark configuration is running \nwell or not through the command \"spark-shell --version\". \nAfter installing Hadoop and Spark, the next step is Hadoop \nDistributed File System (HDFS) configuration. There are \nseveral files that must be configured in the \n/opt/hadoop/etc/hadoop/ directory, including: \n• core-site.xml : set the default file system, localhost \naddress and port \n• hdfs-site.xml : set directory locations of name node \nand data node  \n• mapred-site.xml : set the MapReduce framework \nname \n• yarn-site.xml : set manage node manager and handling \nthe shuffle on MapReduce \nThe final step is formatting HDFS, make sure there is no \nimportant data in HDFS because the data will be deleted. \nFormatting HDFS with command \"hdfs namenode -format -\nforce\". Then boot HDFS with the commands “start-dfs.sh && \nstart-yarn.sh”. To check HDFS is running correctly or not use \nthe command “jps”, the terminal shows the information the \ncurrently active services such as the Java Virtual Machine \nProcess Status Tool (JPS), ResourceManager, NodeManager, \nDataNode, NameNode, and SecondaryNameNode. Hadoop \nprovides monitoring dashboards for cluster node metrics, \nscheduler metrics, nodes, datanodes, startup progress and \nmore as shown in Fig. 3. \n2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS)\n432Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 5]\nFig. 3.  C luster metrics dashboard monitoring on Hadoop \nWhile to start Spark by using the command \"spark-shell\" \na\ns shown in Fig. 4. \n \nFig. 4.  S park-shell on Spark 3.0.0 \nAfter Hadoop and Spark are properly configured, machine \nl\nearning can be implemented. \nC. Building Model of Multinomial Naïve Bayes  \nSpark provides alternative programming languages to \nimplement Machine Learning such as Scala, Java, and Python \nThe experiment used Scala programming language and \nimporting the classification model (supervised learning) from \nMLlib such as Multinomial Naive Bayes algorithm. The Iris \ndataset is loaded into Scala, then splitting the dataset is 60% \nfor training data and 40% for testing data. The performance \nevaluation of classification model based on accuracy matrix \nand execution time. Fig. 5 show the running Multinomial \nNaïve Bayes model using Scala on Spark, the program \nrunning the code script execution until the stage completely. \n \n \nFig. 5.  R unning Multinomial Naïve Bayes Model using Scala on Spark D. E valuation Model and Execution Time \nTABLE II.  THE EXPERIMENTAL RESULT OF BIG DATA ANALYTICS \nFO\nR MACHINE LEARNING MODEL USING MULTINOMIAL NAÏVE BAYES IN \nHADOOP AND SPARK ENVIRONMENT ON RESIZING IRIS DATASET  \nN\no. Size Number \nof \nSamples Validation \nSampling Accuracy Execution \nTime \n1. 5 KB 150 Pecentage \nSplit \n(60:40) 65% 1.27 \nseconds \n2. 8 MB 199,729 Pecentage \nSplit \n(60:40) 96.04% 2.52 \nseconds \n3. 14 MB 399,458 Pecentage \nSplit \n(60:40) 95.31% 5.29 \nseconds \n4. 27 MB 798,916 Pecentage \nSplit \n(60:40) 95.27% 10.95 \nseconds \n5. 87 MB 2,591,848  Pecentage \nSplit \n(60:40) 95.31% 38.16 \nseconds \n6. 148 MB 5,184,000 Pecentage \nSplit \n(60:40) 95.32% 1 minute 4 \nseconds \n \nTable II show when the number of samples in Iris dataset \nwas resized from KB to MB, there is a difference in accuracy \nand execution time during testing machine learning \nperformance in Hadoop. If the test used the original iris dataset \nof 5 KB consisting of 150 samples, the accuracy of the \nMultinomial Naïve Bayes model only reached 65% with an \nexecution time of 3.66 seconds. On other hand, the test used \nIris dataset that has been resized to 87 MB consisting of \n2,592,848 samples, the accuracy of the model reached 95.31% \nwith an execution time of 38,16 seconds. If the Iris dataset is \nresized to 148 MB consisting of 5,184,000 samples, the model \naccuracy reached 95.32% with an execution time of 1 minute \n4 seconds. Table III show the performance of Multinomial \nLogistic Regression with the same size file, the number of \nsamples, and validation sampling. \nTABLE III.  THE EXPERIMENTAL RESULT OF BIG DATA ANALYTICS \nFO\nR MACHINE LEARNING MODEL USING MULTINOMIAL LOGISTIC \nREGRESSION  IN HADOOP AND SPARK ENVIRONMENT ON RESIZING IRIS \nDATASET  \nN\no. Size Number \nof \nSamples Validation \nSampling Accuracy Execution \nTime \n1. 5 KB 150 Pecentage \nSplit \n(60:40) 90% 3.66 \nseconds \n2. 8 MB 199,729 Pecentage \nSplit \n(60:40) 98.68% 12.29 \nseconds \n3. 14 MB 399,458 Pecentage \nSplit \n(60:40) 98.66% 25,73 \nseconds \n4. 27 MB 798,916 Pecentage \nSplit \n(60:40) 98.65% 55.78 \nseconds \n5. 87 MB 2,591,848  Pecentage \nSplit \n(60:40) 98.66% 2 minutes \n9  seconds \n6. 148 MB 5,184,000 Pecentage \nSplit \n(60:40) 98,65% 4 minutes \n11 \nseconds \n \nFigure 6 and Figure 7 show that the Multinomial Logistic \nRegression has the highest accuracy, but is also followed by \nhigh execution time consumption, while the Naïve Bayes \n2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS)\n433Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 6]\nMultinomial has the lowest execution time consumption, but \nreached a fairly good accuracy. \n \n \nFig. 6.  T he Comparison of Execution Time Between Multinomial Naïve \nBayes and Multinomial Logistic Regression on Big Data Environment \n \nFig. 7.  T he Comparison of Accuracy Between Multinomial Naïve Bayes \nand Multinomial Logistic Regression on Big Data Environment \nThe experimental results indicates that with the increase in \nt\nhe number of samples in the dataset is also positively \ncorrelated with increasing execution time. However, \nexecution time is relatively cheap in the Hadoop Environment. \nThe evaluation regarding the increase in accuracy needs to be \ninvestigated further because the Iris dataset is a benchmark \ndataset consisting of only 150 samples, while the iris dataset \nis resized in this experiment which allows the sample to be \nduplicated.  \nV. CONCLUSION  \nT\nhe implementations of big data analytics using Hadoop \nand Spark are configured and running well on resizing Iris \ndataset. Iris dataset load in Hadoop environment through \nHadoop Distributed File System, while Spark provide several \nalgorithms for classification, clustering, and regression. The \nexperiment employed Multinomial Naïve Bayes to classify \nthe Iris dataset. The experimental result reported that there is \na difference in accuracy and execution time during testing \nmachine learning performance in Hadoop. The experiment \ngiven the best performance used Iris dataset is resized to 148 \nMB consisting of 5,184,000 samples, the model accuracy \nreached 95.32% with an execution time of 1 minute 4 seconds. \nThe increase in the number of samples in the dataset is also \npositively correlated with increasing execution time. \nHowever, execution time is relatively cheap in the Hadoop \nEnvironment. Further research needs to upgrade the device to \nexperiment with a larger processor and RAM and the number \nof datasets that reach GB. REFERENCES  \n[1] B . Zerhari, A. A. Lahcen, and S. Mouline, \"Big Data Clustering: \nAlgorithms and Challenges,\" in Proceedings of the International \nConference on Big Data, Cloud, and Applications (BDCA’15), pp. 1-\n6, 2015.  \n[2] S. W. Kareem, \"Secure Cloud Approach Based on Okamoto-Uchiyama \nCryptosystem,\" Journal of Applied Computer Science and \nMathematics, vol. 14, no. 29, pp. 9-13,  2020.  \n[3] H. B. Patel and S. Gandhi, \"A Review on Big Data Analytics in \nHealthcare using Machine Learning Approaches,\" in 2018 2nd \nInternational Conference on Trends in Electronics and Informatics \n(ICOEI), pp. 84-90, 2018. \n[4] S. Suthaharan, \"Machine Learning Models and Algorithms for Big \nData Classification,\" Integrated Series in Information Systems, vol. 36, \npp. 1-12, 2016.  \n[5] V. Ajin and L. D. Kumar, \"Big Data and Clustering Algorithms,\" in \n2016 International Conference on Research Advances in Integrated \nNavigation Systems (RAINS), pp. 1-5, 2016. \n[6] S. W. Kareem, R. Z. Yousif, S. M. J. Abdalwahid, and C. Science, \"An \nApproach for Enhancing Data Confidentiality in Hadoop,\" Indonesian \nJournal of Electrical Engineering and Computer Science, vol. 20, no. \n3, pp. 1547-1555, 2020.  \n[7] A. L’heureux, K. Grolinger, H. F. Elyamany, and M. A. Capretz, \n\"Machine Learning with Big Data: Challenges and Approaches,\" Ieee \nAccess, vol. 5, pp. 7776-7797, 2017.  \n[8] E. Hossain, I. Khan, F. Un-Noor, S. S. Sikander, and M. S. H. Sunny, \n\"Application of Big Data and Machine Learning in Smart Grid, and \nAssociated Security Concerns: A Review,\" IEEE Access, vol. 7, pp. \n13960- 13988, 2019.  \n[9] M. Mohammadi, A. Al-Fuqaha, S. Sorour, M. Guizani, \"Deep Learning \nfor IoT Big Data and Streaming Analytics: A Survey,\" IEEE \nCommunications Surveys and Tutorials, vol. 20, no. 4, pp. 2923-2960, \n2018.  \n[10] L. Zhou, S. Pan, J. Wang, and A. V. Vasilakos, \"Machine learning on \nBig Data: Opportunities and Challenges,\" Neurocomputing, vol. 237, \npp. 350- 361, 2017.  \n[11] H. K. Tripathy, B. R. Acharya, R. Kumar, and J. M. Chatterjee, \n\"Machine Learning on Big Data: A Developmental Approach on \nSocietal Applications,\" in Big Data Processing Using Spark in Cloud: \nSpringer, pp. 143- 165, 2019. \n[12] Benlachmi, Y., Yazidi, A.E., Hasnaoui, M.L.“A Comparative Analysis \nof Hadoop and Spark Frameworks Using Word Count Algorithm”. \nInternational Journal of Advanced Computer Science and Applications. \nVol. 12, No. 4, pp. 778-788, 2021.  \n[13] I. Kusuma, M. A. Ma'Sum, N. Habibie, W. Jatmiko, and H. Suhartanto, \n\"Design of Intelligent K-Means based on Spark for Big Data \nClustering,\" in 2016 International Workshop on Big Data and \nInformation Security (IWBIS), pp. 89-96, 2016. \n[14] A. Prabhat and V. Khullar, \"Sentiment Classification on Big Data using \nNaïve Bayes and Logistic Regression,\" in 2017 International \nConference on Computer Communication and Informatics (ICCCI), \npp. 1-5, 2017.  \n[15] Prabaswara, I.R., Saputra, R. “Implementation of Hadoop and Spark \nfor Analysis of The Spread of Dengue Hemorrhagic Fever based on \nTwitter Data,” in IT Journal Research and Development (ITJRD), vol. \n4, no. 2, pp. 164-171, 2020. \n[16] V. S. Narayanan, V. B. Vijayakumar, S. R. Venkatraman, and P. K. \nBaruah, \"Semantic Node Embeddings of Distributed Graphs using \nApache Spark,\" in 2016 Fourth International Conference on Parallel, \nDistributed and Grid Computing (PDGC), pp. 709-713, 2016. \n[17] O. Azeroual and R. Fabre, “Processing Big Data with Apache Hadoop \nin The Current Challenging Era of COVID-19,” Big Data Cognitive \nand Computing: MDPI. , vol. 5, no. 1, pp.1-18, 2021. \n[18] D. Veeraiah and J. N. Rao, “An Efficient Data Duplication System \nbased on Hadoop Distributed File System,” in 2020 International \nConference on Inventive Computation Technologies (ICICT) , pp. 197–\n200, 2020. \n[19] A. Mostafaeipour, A. Jahangard Rafsanjani, M. Ahmadi, and J. \nArockia Dhanraj, “Investigating The Performance of Hadoop and \nSpark Platforms on Machine Learning Algorithms,” The Journal of \nSupercomputing. , vol. 77, no. 2, pp. 1273–1300, 2021.\n \n2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS)\n434Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore.  Restrictions apply.",
      "metadata": {
        "filename": "Implementation_of_Big_Data_Analytics_for_Machine_Learning_Model_Using_Hadoop_and_Spark_Environment_on_Resizing_Iris_Dataset.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\Implementation_of_Big_Data_Analytics_for_Machine_Learning_Model_Using_Hadoop_and_Spark_Environment_on_Resizing_Iris_Dataset.pdf",
        "file_size": 544902,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:38.203037",
        "content_length": 32953
      }
    },
    "4d12ebf2-2b41-4f6f-b49e-9558d6004dc6": {
      "id": "4d12ebf2-2b41-4f6f-b49e-9558d6004dc6",
      "content": "[Página 1]\nInvestigating the adoption of big data \nanalytics in healthcare: the moderating role \nof resistance to change\nMuhammad  Shahbaz* , Changyuan  Gao*, LiLi  Zhai, Fakhar  Shahzad and  Yanling  Hu\nIntroduction\nBig data analytics (BDA) is a course of action to examine large and complex data sets \n(i.e., big data) and select veiled information that can help organizations with efficient \ndecision making [ 1]. The volume of data related to healthcare organizations has grown \ndramatically in past years and is expected to increase in coming years due to the use of \ninnovative technologies [ 2]. Meanwhile, healthcare reimbursement methods are chang -\ning, and pay for performance is an emerging factor in the current healthcare environ -\nment. Recently, healthcare organizations have only focused on profit and have neglected \nto acquire the essential tools, infrastructure, and technologies for effective control of big \ndata to ensure citizens’ health care [ 3, 4]. Big data incorporates features such as variety, \nvelocity, and veracity. BDA techniques can be applied to the massive amount of prevail -\ning patient-related medical information to analyze outcomes for improvement of the \nhealthcare sector [ 5, 6]. Using BDA in the healthcare sector will help inform each physi -\ncian of the medical histories of individuals and the population and enable appropriate Abstract  \nBig data analytics is gaining substantial attention due to its innovative contribution to \ndecision making and strategic development across the healthcare field. Therefore, this \nstudy explored the adoption mechanism of big data analytics in healthcare organiza-\ntions to inspect elements correlated to behavioral intention using the technology \nacceptance model and task-technology fit paradigm. Using a survey questionnaire, we \nanalyzed 224 valid responses in AMOS v21 to test the hypotheses. Our results posit that \nthe credentials of the technology acceptance model together with task-technology fit \ncontribute substantially to the enhancement of behavioral intentions to use the big \ndata analytics system in healthcare, ultimately leading towards actual use. Meanwhile, \ntrust in and security of the information system also positively influenced the behavioral \nintention for use. Employee resistance to change is a key factor underlying failure of \nthe innovative system in organizations and has been proven in this study to nega-\ntively moderate the relationship between intention to use and actual use of big data \nanalytics in healthcare. Our results can be implemented by healthcare organizations to \ndevelop an understanding of the implementation of big data analytics and to promote \npsychological empowerment of employees to accept this innovative system.\nKeywords:  Big data analytics, Healthcare, TrustOpen Access\n© The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License \n(http://creat  iveco  mmons  .org/licen  ses/by/4.0/ ), which permits unrestricted use, distribution, and reproduction in any medium, \nprovided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and \nindicate if changes were made.RESEARCHShahbaz  et al. J Big Data             (2019) 6:6  \nhttps://doi.org/10.1186/s40537-019-0170-y\n*Correspondence:   \nshahbaz755@yahoo.com; \ngaocy2002@126.com \nSchool of  Economics \nand Management, Harbin \nUniversity of  Science \nand Technology, Harbin, \nChina\n\n[Página 2]\nPage 2 of 20 Shahbaz et al. J Big Data             (2019) 6:6 \ndecision-making regarding treatment of a particular patient [4]. However, compared \nwith the banking and retailing industries, healthcare organizations have lagged behind \nin the sophisticated use of BDA [7]. The healthcare industry also strives to make inter -\nnal developments in BDA implementation based on their quality and data performance, which provides timely feedback to interested parties [8]. Therefore, describing the cru\n-\ncial factors that are required for understanding is important prior to creating a strategy \nfor the acceptance of BDA in the healthcare industry, particularly in developing coun -\ntries such as Pakistan, where the industry requires filling the gap of BDA adoption. Fur -\nthermore, data (i.e., big data) related to healthcare are generated at a very high pace [9], \nand existing systems are unable to store and analyze the huge volume, velocity and vari -\nety of data [10]. Therefore, a need exists for a system with the ability to store and analyze \ndata with high volumes, velocities, and variety, all of which are provided by BDA sys -\ntems [9]. BDA is in the initial adoption phase, and many healthcare organizations want \nto implement BDA to obtain its benefits [11]. Thus, a comprehensive adoption model \nrelated to BDA is needed to fulfill the existing gap in the literature and help healthcare \norganizations replace traditional systems incapable of competing with BDA systems.\nFew studies have described the importance of BDA in healthcare [4, 12, 13], although \nstudies have investigated the technological aspects and required qualifications for big \ndata in healthcare [14–18]. Previous studies focused on technological and policy issues \nand not on adoption factors, such as security, trust, and fitness of technology for the tasks required to manage BDA in healthcare [13]. According to Dishaw [19], the tech\n-\nnology acceptance model (TAM) and task-technology fit (TTF) provide better outputs than either TAM or TTF alone in the adoption of information technology systems. The prior literature tries to explain BDA adoption through perceptions of technology, such \nas perceived ease of use and perceived usefulness [20–24]. However, emphasizing only \nthe end user’s perception of technology may not be sufficient. According to Goodhue and Thompson [25], the TTF model claims that the user will adopt the system when the characteristics of the technology fit the task requirements. Adoption will also occur \nwhen the user perceives the technology as useful, easy and advanced, but the technol\n-\nogy may not be adopted if a mismatch exists with his required tasks and the technology \ncannot enhance his job performance [26–29]. Therefore, not only should the user have \nthe perception that the technology is useful and easy but also the technology charac -\nteristics should match with the required job tasks. Furthermore, the previous literature \nshowed that perceived security of information [30–32] and perceived trust [33–35] were \nthe biggest hurdles for users adopting innovative information systems. Security of infor -\nmation is the main reason for the slow pace of BDA adoption [36, 37]. Perceived trust \nis a major concern in the BDA acceptance procedure, and thus organizations should \ngenerate more trust in BDA adoption [38]. Prior studies by Malaka, Shin, and Sivarajah \n[23, 39, 40] also highlighted that perceived security and perceived trust were the biggest \nchallenges and hurdles for BDA acceptance. Resistance to change (RTC) from employees \nis also a key factor that affects the adoption of different innovative systems, especially in \ndeveloping countries [41–43]. In previous literature concerning electronic health record \nsystem adoption, RTC from physicians was repeatedly reported as a key barrier for sys -\ntem adoption [44, 45], and RTC of employees mitigated the willingness of those who \nwanted to adopt the system [46]. RTC also resists or slow down the pace of information\n\n[Página 3]\nPage 3 of 20\n Shahbaz et al. J Big Data             (2019) 6:6 \nsystem acceptance in the health sector [47, 48]. The study considers RTC a key factor \nin the adoption of BDA in the healthcare sector, which has never been discussed in this \nscenario.\nDespite the fame of BDA, insufficient empirical research has investigated factors \nthat can influence BDA adoption in healthcare [21, 49]. Empirical evidence from Paki -\nstan’s healthcare organizations represents a big gap in the literature from both dimen -\nsions (i.e., knowledge about BDA and adoption of BDA) [50]. This study summarizes real facts from Pakistan for the healthcare BDA literature. The gap between the poten\n-\ntial pros of BDA and the slow and low geared adoption represents a superior opportu -\nnity for scholars to realize how BDA can be adopted in the healthcare industry. BDA is in the initial adoption phase in Pakistan, and the government should develop a clear \npolicy and mechanism for the acceptance of BDA in government and the private sec\n-\ntor [50]. Therefore, to bridge this gap in the literature, the major focus of this paper is \nto provide comprehensive research insights into the adoption of BDA in healthcare. To \nfulfill said gap, the study has two main objectives. The first objective is to help govern -\nment and private healthcare organizations determine the important factors that play key \nroles in the adoption of BDA in healthcare in developing countries, such as Pakistan. The second objective is to cover the on-hand gap in the literature concerning the influ\n-\nence of RTC from employees for BDA adoption. To achieve the above-mentioned objec -\ntives, this study incorporated both TAM and TTF models to explain BDA adoption in the healthcare sector from both viewpoints (the user’s perception of technology and the \ntask-technology fitness) with the most important and substantial factors involved in the \nadoption of information systems (i.e., perceived security and perceived trust). The study also considers RTC as a moderator in the proposed model to address the most impor\n-\ntant hurdle for developing countries, such as Pakistan [33, 41]. The results justified the \nuse of a composite of both TAM and TTF with security and trust as significant predic -\ntors of behavioral intentions (BIs) to adopt BDA, whereas RTC negatively moderated the relationship between BIs and actual use of BDA.\nIn the next section, we describe the theoretical background and develop a research \nmodel for this study to analyze the predictors linked to BDA adoption. The research methods are discussed in section three, and section four provides results from our data analysis using structural equation modeling and discussions. “ Conclusion ” section con\n-\ncludes the overall findings. In addition to the research limitations, our study also has theoretical and practical implications, as discussed in “ Conclusion ” section. References \nare given in “Reference” section.\nRelevant work and hypotheses\nDuring this phase, we underpin the relevant theories and work based on the prior lit -\nerature regarding the acceptance of a BDA system across various sectors (see Table  1) \nand produce the research hypotheses for analysis based on the research framework (see Fig. 1).\nPerceived trust\nTrust is described as a belief that a person or a particular thing will respond in a helpful \nway without manipulating the results [60]. Perceived trust is a state of mind in which\n\n[Página 4]\nPage 4 of 20 Shahbaz et al. J Big Data             (2019) 6:6 \nan individual has acquired assurance and confidence in the information provided by \nthe system [61]. Individual expectations towards technology build trust in its use. Typi -\ncally, trust in use of technology is used to mitigate the uncertainty of mind when a per -\nson lacks experience and knowledge with using innovated technology and information Table 1 Relevant work\nAuthors Year Important aspects Limitations\nEsteves and Curto [21] 2013 Predicted behavioral intention to use \nbig data technology by using the \ntheory of planed behavior based on risk and benefits point of viewSmall sample size was used to test the \nproposed model and insufficient theoretical base provided\nMahmood and Afzal [51] 2013 Provided survey on description, tech-\nnology, trend, and tools of cyber -\ncrime security in Pakistan by using big data analyticsBig data analytics adoption model not \nprovided\nTsai et al. [52] 2015 Provided a brief introduction of big \ndata analytics to help in developing \nhigh performance platform and min-\ning algorithm for big data analyticsDid Not predict the behavior of a user \nregarding use of big data analytics\nMalaka and Brown [40] 2015 Investigated the adoption of big data \nanalytics in organization prospective \nby using technology organization environment modelUser centric approach was ignored by \nthe study\nArchenaa and Anita [53] 2015 Conducted a survey to explore the \nimportance, benefits, and need of big data analytics in healthcare and governmentEmpirical evidence regarding adoption \nfrom citizen prospective and security of information was ignored in the study\nSoon et al. [54] 2016 Demonstrated the big data analytics \nadoption by using the technology acceptance model and diffusion of innovation model and explored the moderating effects of training in MalaysiaThe scope of the study was restricted \nto only private organizations which inferred the generalization of the study\nLaBrie et al. [55] 2017 Provided a comparative study of china \nand USA to understand the technol-ogy change and big data analytics adoption from a societal perspectiveStudy missed the fit between technol-\nogy and cultural dimensions of people\nSivarajah et al. [39] 2017 The systematic literature view was \nperformed to identify the challenges \nin big data analyticsTo develop the link between theories \nand practice the empirical analysis \nwas not performed\nMemon et al. [56] 2017 Apache Hadoop open source technol-\nogy was used to check the big data \nanalytics application in the healthcare sector of PakistanBig data analytics application from a \nuser’s perspective in the healthcare sector of Pakistan was not provided\nBrock and Khan [24] 2017 Combined technology acceptance \nmodel and organization learning capabilities to explore the factors linked with big data analytics usagePre-implementation assessment for \npractitioners was not performed considering the user’s perspective in the adoption of big data analytics\nWeerakkody et al. [57] 2017 To investigate the user’s behavioral \nintentions of big open data. The study applied extended technology acceptance modelThe study only focused on intention to \nuse only instead of focusing also on actual use of big open data\nArunachalam et al. [58] 2018 Provided comprehensive literature view \non capabilities of big data analytics to demonstrate the challenges which help to develop a big data analytics maturity modelA phenomenon of restriction to \nchange in the user perspective was not discussed\nGupta et al. [59] 2018 Reviewed big data analytics and pro -\nvide future research directions of big data analyticsTrust, privacy, and information security \ncan be further explained by utilizing the characteristics of big data and cognitive computing\n\n[Página 5]\nPage 5 of 20\n Shahbaz et al. J Big Data             (2019) 6:6 \nsystems [35]. Perceived trust is particularly important in the context of BDA, because \nadoption of a BDA system is a risk. Many studies have proved that perceived trust is a \nfundamental reason for the success or failure of information system adoption, including E-payment adoption [62], online purchasing [63], adoption of crypto currencies [64] and internet banking [65]. Trusting BDA is not the only issue, because mistrusting the capa\n-\nbilities of technology to deliver valuable services without interruption and data loss also \nreduce its adoption intentions [66]. Thus, based on the above cited literature, we assume \nthat perceived trust will also affect BDA adoption.\nH1 Perceived trust has a significant relationship with BIs to use BDA.\nPerceived security\nPerceived security refers to the degree to which a person believes that use of a specific \nsystem is safe and sound for transmitting and recording sensitive information [31, 61]. \nPerceived security of information is an important concern for healthcare, which con -\ntains sensitive patient information [39]. Perceived security is the factor that restricts \nuser thinking about the benefits of a system and convinces him to use a system that is \nnegatively perceived [40]. In the previous literature, many studies described the value \nof perceived security for the adoption of different analytical information systems (i.e., cloud computing [67], B2C electronic commerce [68], online markets [69], and elec\n-\ntronic health record systems [70]). The use of BDA systems has some reservations in the minds of users regarding information security in the form of informational fraud, misuse \nof sensitive information and use by various unconcerned departments [71, 72]. Prior lit\n-\nerature on BDA and its adoption has not emphasized security of information sufficiently \n[72, 73]. Based on the prior literature, one key feature that can affect BDA adoption by \nhealthcare organizations is the security of the analytical tools used to generate useful \ninformation. Employees’ perceptions of the security level of technology possibly affect \nits adoption rate. Organizations that have high-level capabilities for dealing with infor -\nmation security will possibly develop the intention to use BDA.\nH2 Perceived security has a significant relationship with BIs to use BDA.\nTask‑technology fit\nThe effective adoption of an information system relies on identification of the task for \nwhich the technology is used and whether a suitable match exists between the task and technology. As described by Goodhue [25], decomposition of TTF investigates the user’s \nrequirements for the information system, which ultimately impact the individual perfor\n-\nmance. The study of Lin and Huang [74] defines the task as activities performed to create \nvalued outputs that satisfy human wants. Technology refers to the combination of vari -\nous supportive activities to perform such tasks (i.e., computers, software, and others).\nPrior studies by Benford, D’Ambra and Khan et al. [50, 75] described the various dimen -\nsions of tasks (e.g., nonroutines, interdependence, data access, and quantitative data analy -\nsis), which were related to several technical aspects that fulfilled individuals’ needs. Task and technology both have significant impacts on the ability of TTF to estimate users’\n\n[Página 6]\nPage 6 of 20 Shahbaz et al. J Big Data             (2019) 6:6 \nperformances from several perspectives [76]. Innovative information technology and the \nsystem will be useless if they fail to satisfy individual requirements for performance of a \nspecific task [27]. Therefore, recognizing the task-technology fit of a BDA system is signifi -\ncant, because it leads towards adoption of the system by healthcare organizations.\nPrior studies discussed TTF in several aspects to determine the user task fitness for \ntechnology and group decision making, which led towards the successful adoption of an \ninformation system in various organizational settings (i.e., massive open online courses \n[MOOCs] [77], E-commerce [78], electronic health records [79], group decision support systems [80], high-speed data services [81], and mobile banking [27]). Therefore, based on the abovementioned literature, we believe that successful adoption of BDA by health\n-\ncare organizations significantly depends on matching of technology with the user task \nrequirements, which has not been studied by prior researchers. The study of Zhou et al. \n[27] designed a TTF measure that could evaluate the fit between task and technology [79], with no need to evaluate the impact of the task and technology characteristics on TTF. Therefore, we use only TTF as used by Klopping and Qiwei [78, 79], and we use the \ntask and technology characteristics separately.\nH3 TTF has a significant relationship with BIs to use BDA.\nTechnology acceptance model\nThis paper applied the TAM to observe the adoption of BDA because it satisfactorily \ndetermined users’ perceptions of the ease of use and usefulness of a new system [82]. \nTAM model determinants are optimal for determining user perceptions about its adop -\ntion [54]. TAM has been adopted by many researchers to successfully check and man -\nage new system adoptions [54, 57, 83, 84]. Since the invention of TAM, an abundance \nof studies have produced different research models to effectively predict user attitudes and behavioral intentions towards technology adoption. Interestingly, all of these studies \nused approximately similar attributes to evaluate technology adoption [85]. Many stud\n-\nies have concluded that TAM is one of the best models in different contexts (e.g., fore -\ncasting general buyer behavioral intentions [86], telemedicine adoption [87] and radio frequency identification (RFID) integrated system adoption [85]). Previous literature has \nwitnessed the adoption of innovations with massive implementation of TAM in the eval\n-\nuation of user intentions regarding new technology diffusion [88]. The literature widely \nbacks the use of TAM constructs [i.e., perceived usefulness (PU) and perceived ease of use (PEOU)] in measuring adoption solutions for new technology [24, 39, 89, 90].\nThe study proposes that TAM will provide superior understanding of BDA adoption, \nbecause BDA is a heavily technology-driven research area that is also user-oriented and inno\n-\nvation-focused [23]. Previous studies by Esteves, Rahman, and Shin [21, 23, 91] also concluded \nthat TAM constructs (i.e., PEOU and PU) were significant predictors of BDA adoption/usage.\nPerceived ease of use\nThe study of Davis [82] defined PEOU as the degree of ease involved when using an information system. Subsequently, Soon [54] clarified that ease in using the information \nsystem and technology would help enhance its acceptability among users. The use of an\n\n[Página 7]\nPage 7 of 20\n Shahbaz et al. J Big Data             (2019) 6:6 \neffortless system will help enhance individual and organizational performances [24, 92]. \nBDA potentially generates benefits for organizations, including cutting costs, control -\nling risk factors and helping with efficient decision making. Adoption of BDA depends \non user considerations in terms of its convenience of use, which employs processing of \nlarge-scale and heterogeneous data [23]. However, the intensity of difficulty and ease of using BDA vary from person to person.\nThe prior literature has proven the existence of a significant direct relationship \nbetween the PEOU and a user’s intentional behavior to adopt the system in various fields \nof study [54, 93, 94]; this relationship was also discussed by Esteves, Shin, and Weerak\n-\nkody [21, 23, 39] when studying user intentional behaviors regarding the adoption of big \ndata, although these studies were less focused on this study context. For example, meas -\nuring the relationship of PEOU with the intention to use BDA is essential for healthcare \norganizations. The idea of BDA is not much older and can create much confusion in the user’s mind about its ease of use, which may decrease user intentions towards its use in \nhealthcare organizations.\nBased on the aforesaid opinion, we propose the following hypothesis.\nH4 PEOU has a significant relationship with BIs to use BDA.\nPerceived usefulness\nPU can be defined as whether an individual considers that their job performance will be \nimproved by using the system [24, 82]. PU is the most commonly used variable and the \nprimary driver of technology adoption [92, 95]; PU is also expected to be the primary \ndriver of intention to use BDA in healthcare in this study. PU is an essential variable for studies of the adoption intentions of innovation from the user’s perspective [54]. The previous literature has provided evidence of a positive relationship with intention to use \nmany times, which has been successfully measured in various fields [26, 54, 94], includ\n-\ning the field of big data [21, 23, 57], but BDA not has not been a focus in healthcare.\nThe user cannot shape a positive perception of usefulness until he or she feels any practi -\ncal worth of BDA in healthcare organizations. According to the theoretical principle of the \nTAM model, the study hypothesized that PU had a positive relationship with BIs.\nH5 PU has a significant relationship with BIs to use BDA.\nBehavioral intention to use\nA BI is an intention to achieve some specified future behavior [21] and is a key predictor \nof an individual’s actual use of technology [96, 97]. BI is an essential first step towards \nactual adoption of any system [24]. According to Venkatesh and Davis [92], BI was an important mediator in the relationship between predictors and specific system adoption. The prior literature has proven that a person is more likely to adopt a technology if he has an intention to use it [98]. The social sciences literature has provided evidence that BIs have a direct impact on actual use [77, 82], and various studies have concluded that \nBIs have a significant influence on adoption of BDA [21, 23]. This study suggests that \nindividuals with the intention to use BDA will lead to the actual use of BDA.\n\n[Página 8]\nPage 8 of 20 Shahbaz et al. J Big Data             (2019) 6:6 \nH6 BIs have a significant relationship with actual use of BDA.\nThe moderating role of resistance to change\nAccording to French [99], change is a state in which differences exist between new and \nold ways of thinking. The behavior of individuals who protects them from the conse -\nquences of either real or imagined change is called RTC [100]. Oliver [101] defines RTC as protection of the status quo via individuals creating resistance against the innovative \nsystem. Every innovative system is commonly a source of panic and bears RTC due to \nits perception as a possible threat to the solidity of old habits [43, 101]. To avoid RTC \nin organizations when implementing new systems or ways of working, Oreg [102] pro\n-\nposed that the organization should encourage employees to learn new skills, tasks, and programs. One aspect of personality is that RTC is very important for technology adop\n-\ntion [103]. The previous literature has shown that RTC is a demotivator and negatively influences the acceptance of information technology [104–106].\nThe study of Alomari [107] concluded that RTC was one factor that caused non-adop\n-\ntion and failure of new information systems. Lallmahomed et  al. [33] investigated the adoption behavior of an e-government system by focusing on RTC and established a significant negative relationship between RTC and adoption of the system. Many other \nresearchers also proved the importance and negative associations of RTC with accept\n-\nance of information and communication technology systems [42, 43, 108]. A study of \ngreen supply chain management (GSCM) adoption [41] investigated RTC as a moderator between BIs and adoption of GSCM and concluded that higher RTC among employees \nwould lead to non-implementation of GSCM. Similarly, Bral III et al. [109] investigated the moderating role of RTC between psychological capital and organizational citizenship behavior. Regarding BDA in healthcare organizations, RTC is likely to moderate between \nintentional behaviors and actual usage of the BDA system by employees. The previous \nliterature also reported that RTC either did not significantly affect or had a minor direct impact on the actual use of technology [41, 109]. Therefore, this study also focuses on \nthe moderating role of RTC rather than on the direct influence of RTC on actual use.\nThe American Health and Human Services Department faced huge resistance to change \nfrom physicians when they were adopting an electronic health records system, especially during the initial phase [110]. The study of Bhattacherjee [111] empirically investigated and concluded that RTC was a key barrier to actual adoption of information technology in the healthcare sector in both the initial and post-adoption phases but was more of an obsta\n-\ncle during the initial phase. Other researchers also found that RTC was often regarded as the cause of failure of the actual use of information technology in healthcare organiza\n-\ntions [112, 113]. Here, we focus on RTC because we also investigate the initial adoption of \nBDA. However, although RTC has a key impact on the adoption of information systems in healthcare, previous research has not paid attention to RTC during this process. Therefore, to bridge this big gap in the literature, we considered the following hypothesis:\nH7 RTC moderates the relationship between BIs to use and actual use of BDA.\n\n[Página 9]\nPage 9 of 20\n Shahbaz et al. J Big Data             (2019) 6:6 \nMethodology\nHere, the researchers describe the methodology followed by this study. Figure  2 rep-\nresents the flow diagram of the research progress.\nDevelopment of measures\nIn this study, we adopted all of the measures from previous studies with the same con -\ntext to preserve the content validity. All constructs were measured on a 7-point Likert \nscale in which the agreement of the participants to a given statement was assigned a \nscore ranging from 1 (strongly disagree) to 7 (strongly agree). The 3-item perceived trust \nscale was adapted from [114], the four-item perceived security scale was adapted from [31], and the three-item task-technology fit scale was adapted from [27, 74]. Similarly, \nthe three-item scales for both PU and PEOU were adapted from [39]. The three-item \nscale for BI to use was adapted from [98], and the three-item scale for actual use was \nadapted from [98]. The four-item scale for RTC was adapted from [104].\nSampling and data collection\nThe structured questionnaire-based survey method was used in this empirical study to measure the proposed model. The survey method is helpful for measurement of behav\n-\niors and the relationships among constructs [115]. Typically, the survey method has been used in previous studies in which researchers assess adoption or user intention-behav\n-\nior [116]. First, a pilot study was conducted with 20 prospective users of the BDA sys -\ntem. The results of the pilot study were discussed with four senior professors with great command of construct building, and then the measurement items were refined based \non the discussions, which also confirmed the face validity of the measurement scale. All \nFig. 1 Proposed research framework. It is graphical presentation of all variables and relationship among \nthem in the proposed research model\n\n[Página 10]\nPage 10 of 20 Shahbaz et al. J Big Data             (2019) 6:6 \nprospective BDA system end users from 25 hospitals in four big cities of Punjab Paki -\nstan (Lahore, Faisalabad, Rawalpindi, and Multan) and the Islamabad capital territory \nof Pakistan were selected for data collection using a convenience sampling technique. A convenience sampling technique is a suitable sampling technique by virtue of the \naccessibility of the researcher to the participants [117]. A refined questionnaire with a \ncover letter that assured the respondents that the data would be used only for academic research purposes and kept confidential was distributed to all participants through an online survey. The online survey ensures research consistency with data collection [118]. \nFig. 2 Flow diagram of research progress. In this figure, we graphically present the flow of progress in the \nresearch\n\n[Página 11]\nPage 11 of 20\n Shahbaz et al. J Big Data             (2019) 6:6 \nA total of 400 questionnaires were distributed among the participants, and 260 filled \nquestionnaires were received, of which 36 questionnaires that contained missing values \nor biased responses were not included in the study. The remaining 224 responses were selected for the analysis. Furthermore, the study considered age and gender as control variables to ensure that the results of the model were not influenced adversely by covari\n-\nance, because the previous literature suggested that age and gender might influence the \nintention to use [63, 118].\nResults and discussion\nWe employed structural equation modeling (SEM) using the IBM-AMOS (v21) software \nto evaluate the proposed research model. It is evident from the previous literature that the AMOS software is a powerful tool for performing confirmatory factor analysis (CFA) \nand SEM [119]. AMOS is also a complete package for evaluation of formative measures \nand moderating relationships [120]. Because AMOS is user-friendly and provides a graphical interface that enables easier handling, we used this software for the CFA and SEM and SPSS (v 21) to measure the validity and reliability and conduct an exploratory \nfactor analysis (EFA) prior to the CFA and SEM. We also used SPSS for the demographic \nstatistics. Table  2 presents the demographic information for the respondents, of whom \n128 out of 224 were male, 82% were in the age bracket of 25–45 years, and 98% had a bachelor’s or higher degree; the remaining 2% had a high school certificate or relevant diploma as well as knowledge and experience with using technological systems.\nMeasurement model\nKaiser–Meyer–Olkin (KMO) and Bartlett’s test of sphericity were used to measure sam -\nple adequacy [121]. The KMO value was 0.857, which was within the cutoff range of 0.8–1 that showed sample adequacy. Data were collected without regard to differences \nin time; we checked the common method bias (CMB) in the data using Harman’s single-\nfactor test [122]. After categorizing the items into eight subgroups, the results indicated that the first factor explained 31.1% of the variance, which was below the 40% cutoff rate. Thus, CMB is not an issue in the study.\nPrior to the path analysis, the reliability and validity were measured. The value of \nCronbach’s alpha was greater than 0.7 for all factors. The composite reliability (CR) and AVE values ranged from 0.847 to 0.962 and from 0.652 to 0.864, respectively, which were within the accepted ranges [123, 124]. Table  3 shows all of the Cronbach’s alpha, \nCR and AVE values. Table  4 proves that no issue existed with the discriminant validity \nof the constructs in the study, because the square root of AVE was higher than all inter-construct correlations [125]. The study conducted EFA using SPSS to ensure that the measures in the study were correct with respect to the concerned variables. The factor \nloading values shown in Table  3 ranged from 0.744 to 0.953, which proved that no issue \nexisted regarding cross loading of the constructs [126].\nCFA was conducted through AMOS to check the consistency and validity of the con\n-\nstructs for the proposed study framework. The CFA values are CMIN/DF = 1.545, root \nmean square error of approximation (RMSEA) = 0.049 with a PCLOSE value of 0.529, \ncomparative fit index (CFI) = 0.967 and Tucker-Lewis fit index (TLI) = 0.960. All values\n\n[Página 12]\nPage 12 of 20 Shahbaz et al. J Big Data             (2019) 6:6 \nare in accordance with the threshold values [126]. These results proved the good fitness \nof the model.\nStructural model\nThe above results proved good fitness of the model and that the data were highly reliable and valid. Therefore, we continued with the path analysis. In the path analy\n-\nsis, CMIN/DF = 1.748, RMSEA =  0.058 with a PCLOSE value of 0.278, CFI = 0.939, \nand TLI = 0.901, which proved that the model was a good fit. Then, the study meas -\nured the path coefficients and found that all coefficients were significant. The results shown in Fig.  3 demonstrate that perceived trust (β = 0.124, p < 0.05), perceived secu\n-\nrity (β = 0.209, p < 0.001), TTF (β =  0.263, p < 0.001), PEOU (β =  0.240, p < 0.001), and \nPU (β = 0.118, p < 0.05) have significant positive relationships with BIs to use BDA. \nThus, H1, H2, H3, H4, and H5 are supported. In addition, BIs to use BDA (β = 0.412, \np < 0.001) have a significant positive relationship with the actual use of BDA, and thus \nH6 is also accepted. The model also demonstrates that 45% of the variance exists in the BIs to use BDA and 25% of the variance represents the actual use of BDA. The \ncontrol variables age and gender did not have a significant relationship with the \nactual use of BDA. Therefore, we concluded that the hypothesized study model was accepted.\nModerating the effect of RTC \nThe study proved that RTC moderated the relationship between BIs to use the BDA sys -\ntem and actual use of the BDA system. The interaction term (behavioral intentions to \nuse × resistance to change) (β = − 0.201, p < 0.001) had a significant and negative effect \non the actual use of the BDA system. Thus, a higher RTC weakened the relationship \nbetween BIs to use and actual use of the BDA system, which accepted H7.Table 2 Demographical information of respondents\nCategory Frequency Percentage (%)\nGender\n Male 128 57.1\n Female 96 42.9\n Total 224 100.0\nAge\n 18–25 29 12.9\n 25–35 89 39.7\n 35-45 94 42.0\n 45 and above 12 5.4\n Total 224 100.0\nEducation\n High school/diploma 4 1.8\n Bachelor 109 48.7\n Master 101 45.1\n Doctoral 10 4.5\n Total 224 100.0\n\n[Página 13]\nPage 13 of 20\n Shahbaz et al. J Big Data             (2019) 6:6 \nBDA change the decision-making style in the healthcare sector. An interactive and \ntask-oriented system can make possible use of BDA in any sector, especially in health -\ncare organizations. With these objectives in mind, this study provided useful findings \nand a framework for practical adoption of BDA systems and future research. The study \nfocused on security and trust of information, which were observed to be big concerns of system users. Similar to the results of [23], perceived trust and perceived security had a Table 3 Results of factor loadings, validity, and reliability\nVariables Items Loadings Cronbach’s alpha CR AVE\nPerceived trust PT1 0.849 0.834 0.847 0.654\nPT2 0.879\nPT3 0.778\nPerceived security PS1 0.838 0.903 0.905 0.705\nPS2 0.849\nPS3 0.806\nPS4 0.744\nTask-technology fit TTF1 0.864 0.898 0.900 0.751\nTTF2 0.797\nTTF3 0.768\nPerceived ease of use PEOU1 0.857 0.877 0.877 0.704\nPEOU2 0.859\nPEOU3 0.805\nperceived usefulness PU1 0.816 0.838 0.847 0.652\nPU2 0.881\nPU3 0.841\nBehavioral intention to use BDA BI1 0.778 0.909 0.911 0.773\nBI2 0.796\nBI3 0.809\nResistance to change RTC1 0.939 0.962 0.962 0.864\nRTC2 0.937\nRTC3 0.953\nRTC4 0.944\nActual use of BDA AU1 0.872 0.918 0.919 0.792\nAU2 0.889\nAU3 0.879\nTable 4 Correlations matrix and square root of AVE\nInclined italic lines represent the square root of the AVE of each variable\nSignificance of correlations: *p < 0.050, **p < 0.010, ***p < 0.001RTC  PS AU PEOU TTF PT PU IB\nRTC 0.929\nPS 0.092 0.840\nAU 0.168* 0.396*** 0.890\nPEOU 0.111 0.433*** 0.403*** 0.839\nTTF − 0.002 0.619*** 0.429*** 0.448*** 0.866\nPT − 0.041 0.413*** 0.075 0.152* 0.357*** 0.809\nPU − 0.034 0.291*** 0.047 0.270*** 0.258*** 0.337*** 0.808\nIB 0.004 0.556*** 0.490*** 0.524*** 0.616*** 0.370*** 0.348*** 0.879\n\n[Página 14]\nPage 14 of 20 Shahbaz et al. J Big Data             (2019) 6:6 \npositive effect on BIs to adopt BDA. The study also investigated TTF, which had a posi -\ntive impact on the behavioral intentions to adopt BDA, indicating that features of the \nsystem must be matched to the specific task of the user for successful adoption of a BDA \nsystem. Our results are consistent with the results of Afshan and Brock [127]. Similar \nto previous studies by Shin and Soon et  al. [23] concerning BDA adoption, this study also demonstrated the positive effects of PEOU and PU on BIs to adopt BDA, which suggested that the system should be easy to use and have attractive features that make \nit look useful to the user. Furthermore, the study investigated RTC in employees, which \nplayed a key role in the convergence of BIs to put the system into actual use. The results of this study illustrate that as RTC from employees increases, actual use of BDA systems will decrease. The RTC result is consistent with that of Beal III and Nejati [41, 109].\nConclusion\nThe adoption of BDA is in the initial stage, in which many healthcare organizations are thinking about adopting BDA systems. The present is an optimal time to adopt/implement \nBDA systems, especially in healthcare organizations, with an aim of providing better health\n-\ncare facilities by maintaining patients’ health records and formulating better strategies. This \nstudy contributes to the literature by showing the main factors that are important when adopting the BDA system. This study results are also imperative for strategy makers who \nwant to implement a BDA system by demonstrating factors that are important initially. In \ncontrast to existing studies, this study also expressed the huge positive combined effect of the \nTTF and TAM theories on behavioral intentions to adopt BDA. Combining TAM and TTF \ngives more effective results than use of TAM or TTF individually [19, 26]. The prior literature \ndemonstrates use of the TAM model alone in the adoption of a BDA system. The current study also incorporated important concerns from users regarding adoption of any innovative system, such as perceived security and perceived trust. These factors provide an additional \nsignificant aspect to the literature regarding BDA. Our sampling territory is Pakistan, which \nFig. 3 SEM results for hypotheses testing. In the figure, we graphically present the SEM results (i.e. path \ncoefficient and significance level) of proposed model\n\n[Página 15]\nPage 15 of 20\n Shahbaz et al. J Big Data             (2019) 6:6 \nis a developing country. RTC is the largest barrier in the adoption of innovative systems, \nparticularly in developing countries but also in developed countries. According to our best \ninformation, this study is the first to enrich the literature by linking resistance to change of employees with BDA adoption as a moderator. This moderation result will help implement\n-\ners control this factor at the time of adopting the BDA system.\nTheoretical contribution\nThis study contributes noteworthy research insights into BDA system implementation. The \nstudy fills the main gap in the literature concerning the empirical evidence for BDA in Paki -\nstani healthcare organizations for the first time. Second, the majority of previous studies only highlighted the importance, challenges, and opportunities of BDA, because BDA was in the \ninitial stage of adoption and was a comparatively new topic. Second, few researchers have investigated the adoption of BDA, and the existing studies have focused on a specific per\n-\nspective (i.e., an economic or financial perspective) or have simply emphasized TAM the -\nory. This study is probably the first on BDA adoption to propose a model that combines the TAM and TTF theories as predictors of behavioral intentions to use BDA. Thus, integration \nand implementation of the TAM theory with the TTF theory for BDA adoption is a new \nperspective that enhances the literature. Second, to switch from the previous healthcare sys\n-\ntem to a BDA system, the literature needs a strong theoretical basis for further research and a broad and general research model that is not specific to one aspect of the business. This study model will be helpful and will advance a theory for future BDA research. Furthermore, \nthe study included security and trust aspects of information in the model to elucidate their \nimpacts on BI. Our results will contribute to the security and trust perspectives in the tech\n-\nnology acceptance literature and provide security and trust grounds for further research. In addition, the results obtained for resistance to change represent an immense theoretical con\n-\ntribution for researchers, because the current study has highlighted this important barrier in the implementation of BDA. This investigation can be used as a reference for future research \nand to increase understanding of the adoption of BDA research.\nPractical contribution\nThe study also contributed practically in several ways similar to its theoretical contribu -\ntions. The findings of the study propose salient guidelines and important implications for \npractitioners and implementers of BDA systems that can assist with successful adoption \nof BDA systems. First, connecting system functions with the required tasks of the organi -\nzation as well as the PU and PEOU of the system are important. This approach will pro -\nvide results that are more fruitful for practitioners when implementing BDA systems. \nSecond, the findings of the study also indicate that perceived security and perceived trust are the key predictors of intentions regarding acceptance of a BDA system. Third, this \nstudy explores the moderating effect of RTC, which reduces the adoption of BDA systems \nin developing countries. This study will provide broad insights for implementers of BDA systems in developing countries and allow the design of strategies to ultimately reduce employees’ resistance levels. Finally, this study provides an initial platform for practitioners for adoption and promotion of BDA practices within the organization to obtain maximum \nadvantages of innovative technology, especially in developing countries.\n\n[Página 16]\nPage 16 of 20 Shahbaz et al. J Big Data             (2019) 6:6 \nLimitations and future directions\nThe authors acknowledge some limitations of the current study. First, the focus of this \nstudy is on healthcare organizations in Pakistan regarding adoption of a BDA system. \nThe impact of organizational culture was ignored by this study, which might have an \neffect on the level of adoption of this system. Future researchers may test the same research model in other organizations considering different cultural setups, because the organizational setup and culture vary from industry to industry; therefore, the findings of this study may vary when applied to different sector organizations. This study pro\n-\nvided understanding of BDA system adoption in developing countries in particular and developed countries in general. Thus, future researchers can test this model in devel\n-\noped countries to increase the generalization of the study, because the severity of resist -\nance to change from employees is greater in developing than in developed countries [41]. The research model can be tested in different cultural settings with a focus on adoption of BDA systems. This study was aimed to investigate the user adoption factors of BDA, \nwhich neglect the other side of system implementation. Therefore, future researchers \ncan identify the developers/architects intentions for development and implementation of BDA system. Finally, this study was based on cross-sectional settings, which restricted measurement of the consistency in respondent behavior; this gap should be tested in a \nlongitudinal setup to improve the significant contribution to knowledge.\nAbbreviations\nBDA: big data analytics; TTF: task-technology fit; TAM: technology acceptance model; PEOU: perceived ease of use; PU: perceived usefulness; RTC : resistance to change; BI: behavioral intentions.\nAuthors’ contributionsMS conceptualized the idea, prepared the literature and build theory, designed research framework and collect data, analyzed results, drafted and proof-read the manuscript, GCY and ZL provided supervision and guide throughout the process. FS and YH contribute in analysis and results writing. All authors read and approved the final manuscript.\nAcknowledgements\nThis study has been supported by “National Natural Science Foundation of China (NSFC)” Grant Numbers: 71774044, 71672050, and 71272191.\nCompeting interests\nThe authors declare that they have no competing interests.\nAvailability of data and materials\nData can be available on demand.\nConsent for publication\nWe, the authors, consent to the publication of this manuscript in the Journal of Big Data.\nEthics approval and consent to participate\nNot applicable.\nFunding\nNot applicable.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\nReceived: 22 October 2018   Accepted: 8 January 2019\nReferences\n 1. Müller O, Junglas I, Vom Brocke J, Debortoli S. Utilizing big data analytics for information systems research: chal-lenges, promises and guidelines. Eur J Inf Syst. 2016;25:289–302.\n\n[Página 17]\nPage 17 of 20\n Shahbaz et al. J Big Data             (2019) 6:6 \n 2. Cottle M, Hoover W. Transforming health care through big data. Washington DC: Institute for Health Technology \nTransformation; 2013. p. 6–19.\n 3. Lavalle S, Lesser E, Shockley R, Hopkins MS, Kruschwitz N. Big data, analytics and the path from insights to value. MIT Sloan Manag Rev. 2011;52:21–32.\n 4. Raghupathi W, Raghupathi V. Big data analytics in healthcare: promise and potential. Health Inf Sci Syst. 2014;2:3. https ://doi.org/10.1186/2047-2501-2-3.\n 5. Nannetti P . The deciding factor : big data & decision making. Capgemini Consulting Technology outsourcing. 2012;1–5. https ://www.capge mini.com/resou rces/the-decid ing-facto r-big-data-decis ion-makin g/.\n 6. Connolly S, Wooledge S. Harnessing the value of big data analytics. Big DataAnalytics. 2012; p. 1–14.\n 7. Wang Y, Hajli N. Exploring the path to big data analytics success in healthcare. J Bus Res. 2017;70:287–99.\n 8. Maria VMF. Big data services based on mobile data and their strategic importance. In: 7th International Confer -\nence on Computers Communications and Control. 2018;276–81. http://ieeex plore  .ieee.org/lpdoc s/epic0 3/wrapp \ner.htm?arnum ber=67580 26.\n 9. Sandhya Kumari S, Sandhya Rani K. Big data analytics for healthcare system. In: 2018 IADS international conference on computing, communications & data engineering (CCODE), 7–8 February 2018.\n 10. Gharajeh MS. Biological big data analytics. Adv Comput. 2018;109:321–55.\n 11. Wang Y, Kung LA, Byrd TA. Big data analytics: understanding its capabilities and potential benefits for healthcare organizations. Technol Forecast Soc Change. 2018;126:3–13.\n 12. Groves P , Knott D. The ‘big data’ revolution in healthcare. New York: McKinsey & Company; 2013.\n 13. Kim MK, Park JH. Identifying and prioritizing critical factors for promoting the implementation and usage of big data in healthcare. Inf Dev. 2017;33:257–69.\n 14. Braunstein ML. Practitioner’s guide to health informatics. Switzerland: Springer International Publishing; 2015. p. 133–49. https ://doi.org/10.1007/978-3-319-17662 -8.\n 15. Huang T, Lan L, Fang X, An P , Min J, Wang F. Promises and challenges of big data computing in health sciences. Big Data Res. 2015;2:2–11.\n 16. Kim MK, Cho YW, Park JH. The prospects and development directions for healthcare big data industry. Electronics and Telecommunications Research Institute. 2013.\n 17. Heitmueller A, Henderson S, Warburton W, Elmagarmid A, Pentland AS, Darzi A. Developing public policy to advance the use of big data in health care. Health Aff (Millwood). 2014;33:1523–30.\n 18. Bughin J. Reaping the benefits of big data in telecom. J Big Data. 2016;3:14.\n 19. Dishaw MT, Strong DM. Extending the technology acceptance model with task-technology fit constructs. Inf Manag. 1999;36:9–21.\n 20. Petersen Glen S. High impact sales force automation. Boca Raton: FL St Lucie Press; 1997.\n 21. Esteves J, Curto J. A risk and benefits behavioral model to assess intentions to adopt big data. J Intell Stud Bus. 2013;3:37–46. https ://www.scopu s.com/inwar  d/recor  d.uri?eid=2-s2.0-84905 68971 7&partn erID=40&md5=53271 \n4767e 19a33 35cb1 ef08d e04d6 6c .\n 22. Rahman N. Factors affecting big data technology adoption. Student Res Symp 2016. 2016;0–29. http://pdxsc holar .libra ry.pdx.edu/stude ntsym posiu m%5Cn, http://pdxsc holar  .libra ry.pdx.edu/stude ntsym posiu m/2016/Prese ntati \nons/10.\n 23. Shin DH. Demystifying big data: anatomy of big data developmental process. Telecomm Policy. 2016;40:837–54.\n 24. Brock V, Khan HU. Big data analytics: does organizational factor matters impact technology acceptance? J Big Data. 2017;4:21.\n 25. Goodhue DL, Thompson RL. Task-technology fit and individual performance. MIS Q. 1995;19:213. http://www.jstor .org/stabl e/24968 9?origi n=cross ref .\n 26. Wu B, Chen X. Continuance intention to use MOOCs: integrating the technology acceptance model (TAM) and task technology fit (TTF) model. Comput Human Behav. 2017;67:221–32.\n 27. Zhou T, Lu Y, Wang B. Integrating TTF and UTAUT to explain mobile banking user adoption. Comput Human Behav. 2010;26:760–7.\n 28. Junglas IA, Watson RT. Location-based services. Commun ACM. 2008;51:65–9. http://porta l.acm.org/citat ion.cfm?doid=13255 55.13255 68.\n 29. Lee C-C, Cheng HK, Cheng H-H, Cheng HK. An empirical study of mobile commerce in insurance industry: task–technology fit and individual differences. Decis Support Syst. 2007;43:95–110. www.elsev ier.com/locat  e/dss .\n 30. Damghanian H, Zarei A, Siahsarani Kojuri MA. Impact of perceived security on trust, perceived risk, and accept -\nance of online banking in Iran. J Internet Commer. 2016;15:214–38.\n 31. Arpaci I, Yardimci Cetin Y, Turetken O. Impact of perceived security on organizational adoption of smartphones. Cyberpsychol Behav Soc Netw. 2015;18:602–8. https ://doi.org/10.1089/cyber  .2015.0243.\n 32. Fife E, Orjuela J. The privacy calculus: mobile apps and user perceptions of privacy and security. Int J Eng Bus Manag. 2012;4:1–10.\n 33. Lallmahomed MZI, Lallmahomed N, Lallmahomed GM. Factors influencing the adoption of e-government ser -\nvices in Mauritius. Telemat Inform. 2017;34:57–72.\n 34. Wang EST, Lin RL. Perceived quality factors of location-based apps on trust, perceived privacy risk, and continuous usage intention. Behav Inf Technol. 2017;36:2–10.\n 35. Liao C, Liu C-C, Chen K. Examining the impact of privacy, trust and risk perceptions beyond monetary transac-tions: An integrated model. Electron Commer Res Appl. 2011;10:702–15. http://linki nghub  .elsev ier.com/retri eve/\npii/S1567 42231 10004 08.\n 36. Nguyen T, Zhou L, Spiegler V, Ieromonachou P , Lin Y. Big data analytics in supply chain management: a state-of-the-art literature review. Comput Oper Res. 2018;98:254–64. https ://doi.org/10.1016/j.cor.2017.07.004.\n 37. Jain P , Gyanchandani M, Khare N. Big data privacy: a technological perspective and review. J Big Data. 2016;3:25.\n 38. Heart T. Who is out there? Exploring the effects of trust and perceived risk on SaaS adoption intentions. DATA BASE Adv Inf Syst. 2010;41:49–67.\n\n[Página 18]\nPage 18 of 20 Shahbaz et al. J Big Data             (2019) 6:6 \n 39. Sivarajah U, Kamal MM, Irani Z, Weerakkody V. Critical analysis of big data challenges and analytical methods. J Bus \nRes. 2017;70:263–86.\n 40. Malaka I, Brown I. Challenges to the organisational adoption of big data analytics : a case study in the South African telecommunications industry. In: Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists. 2015; p. 27.\n 41. Nejati M, Rabiei S, Chiappetta Jabbour CJ. Envisioning the invisible: understanding the synergy between green human resource management and green supply chain management in manufacturing firms in Iran in light of the moderating effect of employees’ resistance to change. J Clean Prod. 2017;168:163–72.\n 42. Sharma M, Gupta R, Acharya P . Prioritizing the critical factors of cloud computing adoption using multi-criteria decision-making techniques. Glob Bus Rev. 2017. https ://doi.org/10.1177/09721 50917 74118 7.\n 43. Reginato E, Fadda I, Paglietti P . The influence of resistance to change on public-sector reform implementation: the case of Italian municipalities’ internal control system. Int J Public Adm. 2016;39:989–99.\n 44. Lorenzi NM, Kouroubali A, Detmer DE, Bloomrosen M. How to successfully select and implement electronic health records. BMC Med Inform Decis Mak. 2009;9:15. http://uvic.summo n.seria lssol ution s.com/2.0.0/link/0/eLvHC XMwY2 BQsEg 1TgWd _JSaZG GWbJ6 anGhg  nmRqa p5qam JmYpg E3iCG 2DqGV  Jq7CT Ewpea JMsi6 uYY4e  -iCSsj 4AsiZ \nC_GgU1y AlY2F  oYEYA 28iaP F3Xgl 4k1gK  AK4dG 90.\n 45. Bates DW. Physicians and ambulatory electronic health records. Health Aff. 2005;24:1180–9.\n 46. Bartos CE, Butler BS, Crowley RS. Ranked levels of influence model: selecting influence techniques to minimize IT resistance. J Biomed Inform. 2011;44:497–504.\n 47. Guo X, Sun Y, Wang N, Peng Z, Yan Z. The dark side of elderly acceptance of preventive mobile health services in China. Electron Mark. 2013;23:49–61.\n 48. Groves P , Kayyali B, Knott D, Van Kuiken S. The “big data” revolution in healthcare: accelerating value and innova-tion. McKinsey Glob Inst. 2013;1–22. http://www.image s-et-resea ux.com/sites /defau lt/files /media s/blog/2013/12/mckin sey_13120 4_-_the_big_data_revol ution _in_healt hcare  .pdf.\n 49. Yan X, Song T, Wu Q. An improved cultural algorithm and its application in image matching. Multimed Tools Appl. 2017;76:14951–68.\n 50. Latif Z, Tunio MZ, Pathan ZH, Jianqiu Z, Ximei L, Sadozai SK. A review of policies concerning development of big data industry in Pakistan: Subtitle: Development of big data industry in Pakistan. In: 2018 international conference on computing, mathematics and engineering technologies (iCoMET), 2018; 2018; p. 1–5.\n 51. Mahmood T, Afzal U. Security analytics: big data analytics for cybersecurity. In: 2013 2nd national conference on Information assurance (ncia). 2013;129–34. http://ieeex plore  .ieee.org/stamp /stamp  .jsp?arnum ber=67253 37.\n 52. Tsai CW, Lai CF, Chao HC, Vasilakos AV. Big data analytics: a survey. J Big Data. 2015;2:21.\n 53. Archenaa J, Anita EAM. A survey of big data analytics in healthcare and government. Procedia Comput Sci. 2015;50:408–13.\n 54. Soon KWK, Lee CA, Boursier P . A study of the determinants affecting adoption of big data using integrated technology acceptance model (TAM) and diffusion of innovation (DOI) in Malaysia. Int J Appl Bus Econ Res. 2016;14:17–47.\n 55. LaBrie RC, Steinke GH, Li X, Cazier JA. Big data analytics sentiment: US-China reaction to data collection by busi-ness and government. Technol Forecast Soc Change. 2018;130:45–55.\n 56. Memon MA, Soomro S, Jumani AK, Kartio MA. Big data analytics and its applications. Ann Emerg Technol Comput. 2017;1. www.aetic  .theia er.org .\n 57. Weerakkody V, Kapoor K, Balta ME, Irani Z, Dwivedi YK. Factors influencing user acceptance of public sector big open data. Prod Plan Control. 2017;28:891–905.\n 58. Arunachalam D, Kumar N, Kawalek JP . Understanding big data analytics capabilities in supply chain manage -\nment: unravelling the issues, challenges and implications for practice. Transp Res Part E Logist Transp Rev. 2018;114:416–36.\n 59. Gupta S, Kar AK, Baabdullah A, Al-Khowaiter WAA. Big data with cognitive computing: a review for the future. Int J Inf Manag. 2018;42:78–89.\n 60. Pavlou PA, Fygenson M. Understanding and prediction electronic commerce adoption: an extension of the theory of planned behavior. MIS Q. 2006;30:115–43. http://searc h.ebsco host.com/login .aspx?direc \nt=true&db=buh&AN=19754 863&site=ehost  -live%0A, http://www.jstor  .org/stabl e/25148 720.\n 61. Cui F, Lin D, Qu H. The impact of perceived security and consumer innovativeness on e-loyalty in online travel shopping. J Travel Tour Mark. 2018;35:819–34. https ://doi.org/10.1080/10548 408.2017.14224 52.\n 62. Nguyen TD, Huynh PA. The roles of perceived risk and trust on e–payment adoption. In: International Econometric Conference of Vietnam. 2018; p. 926–40.\n 63. Fang Y, Qureshi I, Sun H, McCole P , Ramsey E, Lim KH. Trust, satisfaction, and online repurchase intention: the moderating role of perceived effectiveness of e-commerce institutional mechanisms. MIS Q. 2014;38:407–27. https ://misq.org/trust  -satis facti on-and-onlin e-repur  chase  -inten tion-the-moder ating -role-of-perce ived-effec  tiven \ness-of-e-comme rce-insti tutio nal-mecha nisms  .html .\n 64. Shahzad F, Xiu GY, Wang J, Shahbaz M. An empirical investigation on the adoption of cryptocurrencies among the people of mainland China. Technol Soc. 2018. http://www.scien cedir  ect.com/scien ce/artic le/pii/S0160 791X1 83002 04.\n 65. Kim KK, Prabhakar B, Park SK. Trust, perceived risk, and trusting behavior in internet banking. Asia Pacific J Inf Syst. 2009;19:1–23.\n 66. Alkhater N, Walters R, Wills G. An empirical study of factors influencing cloud adoption among private sector organisations. Telemat Inform. 2018;35:38–54. http://linki nghub  .elsev ier.com/retri eve/pii/S0736 58531 73030 88.\n 67. Ackermann T, Widjaja T, Benlian A, Buxmann P . Perceived IT security risks of cloud computing: conceptualization and scale development. ICIS. 2012;1–20. http://aisel  .aisne t.org/icis2 012/proce eding s/ISSec urity /3/.\n 68. Hartono E, Holsapple CW, Kim KY, Na KS, Simpson JT. Measuring perceived security in B2C electronic commerce website usage: a respecification and validation. Decis Support Syst. 2014;62:11–21.\n 69. Mekovec R, Hutinski Ž. The role of perceived privacy and perceived security in online market. In: MIPRO, 2012 Proceedings of the 35th International Convention. 2012; p. 1883–8.\n\n[Página 19]\nPage 19 of 20\n Shahbaz et al. J Big Data             (2019) 6:6 \n 70. Zandieh SO, Yoon-Flannery K, Kuperman GJ, Langsam DJ, Hyman D, Kaushal R. Challenges to EHR implementation \nin electronic- versus paper-based office practices. J Gen Intern Med. 2008;23:755–61.\n 71. Ferguson AG. Policing predictive policing. Wash Univ Law Rev. 2017;211–68. https ://paper s.ssrn.com/sol3/paper s.cfm?abstr act_id=27655 25.\n 72. Broeders D, Schrijvers E, van der Sloot B, van Brakel R, de Hoog J, Hirsch Ballin E. Big Data and security poli-cies: towards a framework for regulating the phases of analytics and use of big data. Comput Law Secur Rev. 2017;33:309–23.\n 73. Abouelmehdi K, Beni-Hessane A, Khaloufi H. Big healthcare data: preserving security and privacy. J Big Data. 2018;5:1.\n 74. Lin TC, Huang CC. Understanding knowledge management system usage antecedents: an integration of social cognitive theory and task technology fit. Inf Manag. 2008;45:410–7.\n 75. D’Ambra J, Wilson CS, Akter S. Application of the task-technology fit model to structure and evaluate the adoption of e-books by academics. J Am Soc Inf Sci Technol. 2013;64:48–64.\n 76. Aljukhadar M, Senecal S, Nantel J. Is more always better? Investigating the task-technology fit theory in an online user context. Inf Manag. 2014;51:391–7.\n 77. Khan IU, Hameed Z, Yu Y, Islam T, Sheikh Z, Khan SU. Predicting the acceptance of MOOCs in a developing country: application of task-technology fit model, social motivation, and self-determination theory. Telemat Informat. 2018;35:964–78.\n 78. Klopping IM, Mckinney E. Extending the technology acceptance model and the task-technology fit model to consumer e-commerce. Inf Technol Learn Perform J. 2004;22:35–48.\n 79. Gan Q, Cao Q. Adoption of electronic health record system: multiple theoretical perspectives. In: 2014 47th Hawaii International Conference on System Sciences (HICSS). 2014;2716–24. http://ieeex plore  .ieee.org/docum ent/67589 42/.\n 80. Dennis AR, Wixom BH, Vandenberg RJ. Understanding fit and appropriation effects in group support systems via meta-analysis understanding fit and appropriation effects in group support systems via meta-analysis. Source MIS Q MIS Q. 2001;25:167–93. http://www.jstor  .org/stabl e/32509 28%5Cn, http://about .jstor  .org/terms  .\n 81. Pagani M. Determinants of adoption of high speed data services in the business market: evidence for a combined technology acceptance model with task technology fit model. Inf Manag. 2006;43:847–60.\n 82. Davis FD. Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Q. 1989;13:319. http://www.jstor  .org/stabl e/24900 8?origi n=cross ref .\n 83. Dillon A, Morris MG. User acceptance of new information technology: theories and models. Annu Rev Inf Sci Technol. 1996;31:3–32. http://arizo na.openr  eposi tory.com/arizo na/handl e/10150 /10558 4.\n 84. Shahzad F, Xiu GY, Khan I, Wang J. m-Government security response system: predicting citizens’ adoption behavior. Int J Hum Comput Interact. 2018. https ://doi.org/10.1080/10447 318.2018.15168 44.\n 85. Kapoor K, Dwivedi Y, Piercy CN, Lal B, Weerakkody V. RFID integrated systems in libraries: extending TAM model for empirically examining the use. J Enterp Inf Manag. 2014;27:731–58. https ://doi.org/10.1108/JEIM-10-2013-0079.\n 86. Gentry L, Calantone R. A comparison of three models to explain shop-bot use on the web. Psychol Mark. 2002;19:945–56.\n 87. Chau PYK, Hu PJ-H. Information technology acceptance by individual professionals: a model comparison approach. Decis Sci. 2001;32:699–719. https ://doi.org/10.1111/j.1540-5915.2001.tb009 78.x.\n 88. Park SY, Nam M-W, Cha S-B. University students’ behavioral intention to use mobile learning: evaluating the tech-nology acceptance model. Br J Educ Technol. 2012;43:592–605. https ://doi.org/10.1111/j.1467-8535.2011.01229 .x.\n 89. Pei Y, Xue W, Su Y, Li D. Discussion on influence factors and evaluation of customer experience for B2C E-commerce enterprises. In: 2015 International Conference on Logistics, Informatics and Service Sciences (LISS), 2015. 2015.\n 90. Prieto JCS, Migueláñez SO, García-Peñalvo FJ. ICTs integration in education: mobile learning and the technology acceptance model (TAM). In: Proceedings of the second international conference on technological ecosystems for enhancing multiculturality. 2014;683–7. https ://doi.org/10.1145/26697 11.26699 74.\n 91. Rahman N. Factors affecting big data technology adoption Nayem Rahman department of engineering and technology. In: Student Res Symp Pap 10. 2016;0–29. http://pdxsc holar  .libra ry.pdx.edu/stude ntsym posiu m/2016/\nPrese ntati ons/10.\n 92. Venkatesh V, Davis FD. A theoretical extension of the technology acceptance model: four longitudinal field stud-ies. Manag Sci. 2000;46:186–204. https ://doi.org/10.1287/mnsc.46.2.186.11926 .\n 93. Ibrahim R, Leng NS, Yusoff RCM, Samy GN, Masrom S, Rizman ZI. E-learning acceptance based on technology accept -\nance model (TAM). J Fundam Appl Sci. 2017;9:871. https ://www.ajol.info/index .php/jfas/artic le/view/16545 1.\n 94. Ambak K, Harun NE, Rosli N, Daniel BD, Prasetijo J, Abdullah ME, et al. Driver intention to use electric cars using technology acceptance model. ARPN J Eng Appl Sci. 2016;11:1–4.\n 95. Claes V, Devriendt E, Tournoy J, Milisen K. Attitudes and perceptions of adults of 60 years and older towards in-home monitoring of the activities of daily living with contactless sensors: an explorative study. Int J Nurs Stud. 2015;52:134–48.\n 96. Castañeda JA, Muñoz-Leiva F, Luque T. Web acceptance model (WAM): moderating effects of user experience. Inf Manag. 2007;44:384–96.\n 97. Brock C, Blut M, Linzmajer M, Zimmer B. F-commerce and the crucial role of trust. In: Thirty Second International Conference on Information Systems. 2011; p. 1–11.\n 98. Venkatesh V, Morris MG, Davis GB, Davis FD. User acceptance of information technology: toward a unified view. MIS Q. 2003;27:425–78.\n 99. French WL. CHB. Organization development: behavioral science interventions for organization improvement. 6th ed. Upper Saddle River: Prentice Hall; 1999. http://ezpro  xy.yorks j.ac.uk/login ?url=, http://searc h.ebsco host.com/\nlogin .aspx?direc  t=true&db=edb&AN=62665 11&site=eds-live&scope =site .\n 100. Zander A. Resistance to change—its analysis and prevention. Adv Manag J. 1950;15:9–11. http://psycn et.apa.org/psyci nfo/1950-06096 -001.\n 101. Oliver C. Strategic responses to institutional processes. Acad Manag Rev. 1991;16:145–79. https ://doi.org/10.5465/AMR.1991.42790 02.\n\n[Página 20]\nPage 20 of 20 Shahbaz et al. J Big Data             (2019) 6:6 \n 102. Oreg S. Resistance to change: developing an individual differences measure. J Appl Psychol. 2003;88:680–93.\n 103. Venkatesh V, Morris M, Ackerman P . A longitudinal field investigation of gender differences in individual technol-\nogy adoption decision-making processes. Organ Behav Hum Decis Process. 2000;83:33–60.\n 104. Huang RT. Exploring the moderating role of self-management of learning in mobile english learning. Educ Tech-nol Soc. 2014;17:255–67.\n 105. Kim H-W, Kankanhalli A. Investigating user resistance to information systems implementation: a status quo bias perspective. MIS Q. 2009;33:567–82. http://www.jstor  .org/stabl e/20650 309.\n 106. Nov O, Ye C. Users’ personality and perceived ease of use of digital libraries: the case for resistance to change. J Am Soc Inf Sci Technol. 2008;59:845–51.\n 107. Alomari MK, Sandhu K, Woods P . Exploring citizen perceptions of barriers to e-government adoption in a develop -\ning country. Transform Gov People Process Policy. 2014;8:131–50. https ://doi.org/10.1108/TG-05-2013-0013.\n 108. Nov O, Ye C. Resistance to change and the adoption of digital libraries: an integrative model. J Am Soc Inf Sci Technol. 2009;60:1702–8.\n 109. Beal III L, Stavros JM, Cole ML. Effect of psychological capital and resistance to change on organisational citizen-ship behaviour. SA J Ind Psychol. 2013;39. http://sajip  .co.za/index .php/sajip /artic le/view/1136.\n 110. Ford EW, Menachemi N, Peterson LT, Huerta TR. Resistance is futile: but it is slowing the pace of EHR adoption nonetheless. J Am Med Informat Assoc. 2009;16:274–81.\n 111. Bhattacherjee A, Hikmet N. Physicians’ resistance toward healthcare information technology: a theoretical model and empirical test. Eur J Inf Syst. 2007;16:725–37.\n 112. Di Fabio A, Bernaud J-L, Loarer E. Emotional intelligence or personality in resistance to change? Empirical results in an Italian health care context. J Employ Couns. 2014;51:146–57. https ://doi.org/10.1002/j.2161-1920.2014.00048 .x.\n 113. Alkraiji A, Jackson T, Murray I. Barriers to the widespread adoption of health data standards: an exploratory qualita-tive study in tertiary healthcare organizations in Saudi Arabia. J Med Syst. 2013;37:9895.\n 114. Jang SH, Kim RH, Lee CW. Effect of u-healthcare service quality on usage intention in a healthcare service. Technol Forecast Soc Change. 2016;113:396–403.\n 115. Newsted PR, Huff SL, Munro MC. Survey instruments in information systems. MIS Q. 1998;22:553.\n 116. Cheung CMK, Lee MKO. Understanding the sustainability of a virtual community: model development and empirical test. J Inf Sci. 2009;35:279–98. https ://doi.org/10.2307/30036 540.\n 117. Saunders M, Lewis P , Thornhill A. Research methods for business students. Res methods Bus students. 2016;649. http://lib.myili brary  .com/Open.aspx?id=81948 7#.\n 118. Liu H, Chu H, Huang Q, Chen X. Enhancing the flow experience of consumers in China through interpersonal interaction in social commerce. Comput Human Behav. 2016;58:306–14.\n 119. Byrne BM. Structural equation modeling with AMOS: basics concepts, applications, and programming. Struct Equ Model. 2016. http://www.uta.fi/aktkk /lectu res/sem_en/pdf/sem_exerc ise_v2.4.pdf .\n 120. Von Der Heidt T, Scott D. Rethinking the role of external collaboration in product innovation. Int J Entrep Innov Manag. 2012;15:59–90. http://www.scopu s.com/inwar  d/recor  d.url?eid=2-s2.0-84857 30684 7&partn \nerID=40&md5=66b5e 12dd2 8c6cc f46ea 9bca9 8bfbe 54.\n 121. Bartlett MS. A note on the multiplying factors for various X2 approximations. J R Stat Soc. 1954;16:296–8. http://www.jstor  .org/stabl e/29840 57.\n 122. Podsakoff PM, MacKenzie SB, Lee JY, Podsakoff NP . Common method biases in behavioral research: a critical review of the literature and recommended remedies. J Appl Psychol. 2003;88:879–903.\n 123. Flynn B. Empirical research methods in operations management. J Oper Manag. 1990;9:250–84.\n 124. Hair JF, Anderson RE, Tatham RL, Black WC. Multivariate data analysis with readings. 5th ed. Prentice-Hill: Up. Sad-dle River; 1998.\n 125. Fornell C, Larcker D. Evaluating structural equation models with unobservable variables and measurement error. J Mark Res. 1981;18:39–50. https ://doi.org/10.2307/31513 12.\n 126. Hair JF, Black WC, Babin BJ, Anderson RE. Multivariate data analysis. Vectors. Upper Saddle River: Prentice Hall; 2010. p. 816.\n 127. Afshan S, Sharif A. Acceptance of mobile banking framework in Pakistan. Telemat Inform. 2016;33:370–87.",
      "metadata": {
        "filename": "Investigating the adoption of big data 2019.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\Investigating the adoption of big data 2019.pdf",
        "file_size": 1297337,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:38.573814",
        "content_length": 71945
      }
    },
    "23c7b594-d82f-4664-8dfc-c9cf484578c3": {
      "id": "23c7b594-d82f-4664-8dfc-c9cf484578c3",
      "content": "[Página 1]\nRESEARCH Open Access\nBig data security access control algorithm\nbased on memory index acceleration in\nWSNs\nJianhua Peng1,2*, Hui Zhou1, Qingjie Meng1and Jingli Yang1\n* Correspondence: pengjh@niit.edu.\ncn\n1College of Computer and Software,\nNanjing Institute of Industry\nTechnology, Nanjing 210046,\nPeople ’s Republic of China\n2Nanjing Shendi Intelligent\nConstruction Technology Research\nInstitute, Nanjing 210019, People ’s\nRepublic of ChinaAbstract\nThe access control is used to ensure these data security when WSN (wireless sensor\nnetwork) with a large number of base stations transmits huge amount of data to a\ndata center server. Meanwhile big data systems are used to efficiently store, manage,\nand use data from large-scale WSNs. In big data systems for WSNs, the traditional\naccess control technology will greatly affect the system performance. This paper first\nanalyzes the data processing flow of the traditional access control strategy in big\ndata systems, analyzes its time complexity, and explores how it affects system\nperformance. Then, we propose the big data security access control algorithm based\non memory index acceleration in WSNs which has better performance over the\ntraditional ones. In our experiments, under the same test environment and security\nstrategy, the performance has been greatly improved with the proposed algorithm.\nKeywords: Wireless sensor networks, Security, Big data, Access control, Memory\nindex\n1 Introduction\nA wireless sensor network (WSN) is an autonomous wireless communication system\ncomposed of a large number of micro-sensor nodes with limited computing capacity,\nstorage capacity, and communication capability [ 1]. The advancement of mass data\ncollection technology in wireless sensor networks has led to the emergence of a large\nnumber of wireless sensor applications. It is becoming increasingly important to ensure\nthe data security of big data systems for WSNs [ 2]. The security is the cornerstone of\nthe big data systems. A data breach could result in serious harm to any of the individ-\nuals to whom the information relates. Access control is an effective method to ensure\ndata security. It is based on authentication and authorization and is the most widely\nused strategy for data security prevention and protection in big data systems. It can re-\nstrict access to key resources and prevent intrusion of illegal users or inadvertent oper-\nation of legitimate users [ 3]. The focus of access control is on authorization. In a\ndistributed system, nodes need to be coordinated and the access rights are synchro-\nnized between nodes. After various security technologies applied to distributed\n© The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which\npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the\noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit\nline to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by\nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a\ncopy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .Peng et al. EURASIP Journal on Wireless Communications and Networking\n        (2020) 2020:90 \nhttps://doi.org/10.1186/s13638-020-01725-1\n\n[Página 2]\nsystems, each technology faces its own security challenges. The distributed systems\nmust not only implement access control policies for data leaving each collaboration\nsystem, but must also control access to local resources. Depending on the sensitivity of\nthe data, it needs to ensure that distributed applications on other coordination systems\nhave access to the data they are processing. With all these factors considered, the imple-\nmentation of the access control functions for distributed systems is very complicated [ 4].\nThe access control models can be divided into 5 categories: the discretionary access\ncontrol (DAC) [ 5], the mandatory access control (MAC) [ 6], the role-based access con-\ntrol (RBAC) [ 7–9], the attribute-based access control (ABAC) [ 10], the policy-based ac-\ncess control (PBAC) [ 11,12]. Various data security algorithms are used to improve the\nsecurity of the access control model. These algorithms severely degrade the perform-\nance of big data systems. There are three ways to improve the performance of access\ncontrol in big data systems. One is to improve the performance of the algorithm itself,\nthe other is to improve the big data security model, and the third is to study the\nmethods that affect the performance of big data systems after the algorithm is applied\nto the model.\nIt is the focus of access control research from rights rules and control policy to get\nrights. Due to the large amount of data and the large number of access users, the wire-\nless sensor big data system still has major shortcomings in the way of getting and con-\ntrolling access control rights. Separation of duty (SOD) is used for enforcing least\nprivilege concept in access control model [ 13]. In the RBAC model, rights are assigned\nto roles, and roles are associated with user to form rights relationships in the access\ncontrol model [ 14]. In the ABAC model, each attribute has a set of attribute and value\ndefinitions. The defined relationship is combined with the user to form the rights rela-\ntionship of the access control model [ 15]. In the access control system, these rights re-\nlationships are stored in metadata files. The system gets the user ’s rights to access\nresources by parsing the metadata. The parsing process will affect the performance of\naccess control. Binary tree method [ 16,17] can reduce the parsing time. Moreover, we\ncan also improve the performance of getting rights by including flexible authentication\nbased on user context information in the attributes [ 18]. Big data systems for wireless\nsensor networks have lot of metadata because of the large amount of data and users.\nTherefore, it takes long time for access control to get rights from the metadata and\njudge rights. To improve its performance, the current main solution is to parameterize\nuser function attributes to reduce the number of rights policies [ 19]. The method will\nrestrict the design and application scope of access control policies. The Hadoop ecosys-\ntem provides a complete solution for big data. Therefore, it can provide all round func-\ntions such as data storage, data processing, data analysis, and data security for WSNs in\nthe context of big data [ 20]. Apache Ranger is a data security management framework\nfor the Hadoop ecosystem. It performs unified data authorization, management, and\nauditing for the Hadoop ecosystem. Various algorithms and methods for big data se-\ncurity can be easily applied to the Hadoop ecosystem through it [ 21]. During the devel-\nopment of the Apache Ranger project, we found that after enabling access control, the\nperformance of big data systems will be greatly reduced.\nThe binary tree is the main method to improve the performance of access control.\nThe binary search tree has no performance advantage when it is a single branch tree. It\nalso takes a relatively long time when the binary tree is used to obtain permissions fromPeng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 2 of 17\n\n[Página 3]\na large amount of metadata in big data systems. It can improve the performance of per-\nmission acquisition in the big data access control process by parameterizing user func-\ntion attributes to reduce the number of permission policies [ 22]. However, this method\nwill limit the design and application scope of access control policies. In top-level big\ndata security engineering applications such as Apache Ranger, access control storage\nstructured policy data. The system first loads the access control policy into memory\nduring the access control process. The system queries the policies from the memory,\nobtains permissions, and judges permissions in a conditional loop when user accesses\ndata. So this method has serious performance problems. The method which uses L2\ncache to build indexes can solve current problems and significantly improve the per-\nformance of access control for big data systems.\nThere are three main contributions of this paper. The first contribution is to build a\nsecond-level cache to reduce the number of cycles during policy extraction. The second\ncontribution is to build a memory index to shorten the access time of the security pol-\nicy. The third contribution is to update the second-level cache and index content to en-\nsure the effectiveness of policy changes. In this way, the access control authority\nefficiency will be greatly improved without affecting the security of access control.\nThe rest of the papers are organized as follows. Section 2 discusses the related work\nof the access control methods in big data system. Section 3 theoretically analyses the\ncause of performance degradation and proposes the big data security access control al-\ngorithm based on memory index acceleration in WSNs. Section 4 performs experimen-\ntal verification and results analysis. And section 5 concludes the paper with summary.\n2 Related work\n2.1 Data security for WSNs\nData fusion can effectively reduce the data transmission volume and network energy\nconsumption in wireless sensor networks (WSNs). At the same time, it can be used to\nimprove the security of wireless sensor network data. An intelligent data fusion algo-\nrithm was proposed in [ 23] for wireless sensor network based on hybrid delayed per-\nception clustering. This algorithm combines the advantages of single-layer cluster\nstructure and multi-layer cluster structure. It effectively improves the security of data\ntransmission while reduces network delay and network energy consumption. Haomeng\nXie [ 24] classifies various wireless sensor networks attack detection methods based on\nthe protocol stack layer, explains the advantages and disadvantages of those methods,\nand measures the security of wireless sensor networks. A game theory-based DSA algo-\nrithm was proposed in [ 25] to implement spectrum leasing and interference mitigation\nbetween SUs in network channels which reduces data loss in wireless sensor network\ndata transmission and improves data security confidence. In order to improve the se-\ncurity of data transmission in wireless sensor networks, a decision transmission scheme\nwas proposed in [ 26] to enhance collaborative spectrum sensing in industrial IoT and\nestablished a cooperative spectrum sensing mathematical model based on decision\ntransmission, which was added packet error and packet loss factors.\nWith the further development of wireless communication and sensor technology,\nlarge-scale wireless sensor networks are being applied in more and more industries.\nThe Internet of Vehicles is a typical large-scale wireless sensor network. It has thePeng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 3 of 17\n\n[Página 4]\ncharacteristics of large data volume and high data security requirements. In order to\nimprove the data security of large-scale wireless sensor networks, Jiliang Li [ 27] de-\nsigned the CL-CPPA protocol which can effectively protect the data security of large-\nscale wireless sensor networks. Wearable devices are also a typical large-scale wireless\nsensor network. Hong Liu [ 4] designed a collaborative privacy protection scheme for\nwearable devices. This solution has authentication and data access control consider-\nations in the context of space awareness and time awareness. The experimental results\nprove that this scheme can better protect the security of the sensor ’s big data.\n2.2 Data security for big data system\nIn big data systems, storing cipher text in the cloud is one of the safest ways to store\nand access big data. However, verifying the user ’s access legitimacy and securely updat-\ning the cipher text in the cloud based on the access policy specified by the data owner\nare two key challenges for making cloud-based big data storage effective. Traditional\napproaches either completely ignore the issue of access policy updates or delegate up-\ndates to third-party agencies. Access policies update is vital to enhance security and\nhandle the dynamics caused by user joins and leaves. Based on this, Chunqiang Hu [ 28]\nproposed a secure and verifiable access control scheme based on NTRU (Number The-\nory Research Unit) cryptosystem for big data storage in the cloud. This solution allows\nthe cloud server to effectively update cipher text when the data owner specifies a new\naccess policy. Data owner can verify the update to resist the spoofing behavior of the\ncloud. The test results show that the big data system using the solution can effectively\nprevent users from cheating and has ability to resist various attacks.\nIn big data systems, unstructured and semi-structured data account for the vast major-\nity. The complicated data types make the traditional authorization mode difficult to meet\nthe minimum authorization principle. In big data systems, fine-grained access control can\nmeet the minimum authorization principle. With the rapid development of big data sys-\ntems, the fine-grained access control model has opened up a new wave of access control\nresearch in the field of big data. After a large amount of research on the canonical system,\nJulian A. Padget [ 29] proposed a first-order logic based on deontic to represent and infer\ndata access strategies. Shangping Wang [ 17]u s e db i n a r yt r e et e c h n o l o g yt od e a lw i t ha t t r i -\nbute revocation and grant, proposed an effective RABE (revocable and grantable\nattribute-based encryption) scheme. Under the assumption of error hypothesis, the secur-\nity of the scheme has selective security features in the standard model.\nThe value of the access control can only be maximized when it is applied to big data\nsystems. Yuqing Mo [ 30] discussed the security requirements of big data and extracted\nkey technologies for big data security from authentication, authorization, access con-\ntrol, data hiding and encryption, network security, and system security. Based on the\nkey technology of extraction, he designed a security management system for the\nHadoop platform. Apache Hadoop is an important framework for fault-tolerant distrib-\nuted big data storage and processing. The Hadoop core platform and other open source\ntools such as Apache Hive, Storm, and HBase provide an ecosystem that enables users\nto take full advantage of the potential of big data. Apache Ranger and Apache Sentry\nprovide centralized policy management and implementation through plugins, providing\nfine-grained access control for components of this ecosystem [ 31,32].Peng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 4 of 17\n\n[Página 5]\nThe access control involves various algorithms and models. In a traditional system,\nthe impact of access control technology on system performance is not obvious. How-\never, in big data systems, the impact is very serious because massive data is stored and\nprocessed in different nodes. In the application of access control to ensure the security\nof big data, especially the fine-grained access control, the specific implementation\nmethod of access control technology has a decisive impact on the performance.\nThrough the verification of HBase, the Apache big data top-level project, the fine-\ngrained big data access control method implemented by Apache Ranger seriously af-\nfects the performance of HBase. Based on the analysis of the impact of access control\non the performance of HBase, this paper improves the implementation algorithm of\nApache Ranger. The improvement algorithm significantly improved the performance of\nbig data system.\n3 Proposed method\n3.1 Access control analysis in WSNs big data system\nThe purpose of access control is to ensure the security of data. In the WSNs big data\nsystem, the access control will perform the access control operation according to the\naccess control policy, access control information, access control decision, and access\ncontrol implementation information to obtain the access right to the access data when\nthe visitor accesses the data. As shown in Fig. 1, the execution of access control is a\ncomplex process. It is executed in the data storage, access, and processing stages in the\nWSNs big data system.\nThe data storage structure is the basic concern of access control. It is determined by\nthe characteristics of the data of WSNs. The resources in big data system can be deter-\nmined by:\nr¼dmr datadesðÞ ð 1Þ\nwhere ris the resource and datades is the characteristics of the data of WSNs.\nGet strategyAccessor OperateSecurity view\n(Filter rule)DataNode\nDataNode\nDataNode.\n.\n.FilterAccess request\n(Accessor,Operation,Data)DataData\nAccess\ncontrol\ndecision\nAccess control policyAccess control InformationAccess control\nimplementationAccess control decision\nAcc\nesscontr\nolimp\nleme\nnta\ntion\nJudgement result\nFeedbackGetInformat\nion\nMetaData\nCollectionStorage\nReal-time\ndata analysis\nand processingData analysis\nand processingApplication\nSensing\nNetworking\nLayerData\nCollection\nLayerData\nProcessing\nLayerApplicatio n\nLayer\nData\nStorage\nLayer\nFig. 1 Big data access control framework in WSNsPeng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 5 of 17\n\n[Página 6]\nBig data system gets rights policies by loading rights resources (metadata) into mem-\nory. Its formula is as follows:\narp¼frðÞ ð 2Þ\nwhere ris the resource and arpis the rights policy for access control.\nThe system can get the rights policy of the user through the user and policy, the for-\nmula is as follows:\nuap¼getUserPolicy u ;arpðÞ ð 3Þ\nwhere uap is the all rights policy of user for access control.\nThe accessed object, user, and the rights policy to which the user belongs determine\nthe access control policy when a user accesses the WSNs big data system. The formula\nis as follows:\nucp¼getCurAP ao ;u;uap ðÞ ð 4Þ\nwhere aois the accessed object, uis the access user, uap is the all rights policy of user\nfor access control, and ucpis the access related rights policy.\nWhen a user accesses the WSNs big data system, he will be granted to access if the\naccessed object is in the access rights policy. The logic can be expressed by:\nup¼getAP ao ;ucpðÞ ð 5Þ\nwhere upis the rights.\nThe process for users to get rights is as follows:\nup¼getAP ao ;ucpðÞ\n¼>upower ¼getAP ao ;getCurAP ao ;u;uap ðÞ ðÞ\n¼>upower ¼getAP ao ;getCurAP ao ;u;getUserPolicy u ;arpðÞ ðÞ ðÞ\n¼>upower ¼getAP ao ;getCurAP ao ;user;getUserPolicy u ;frðÞ ðÞ ðÞ ðÞ\n¼>upower ¼getAP ao ;getCurAP ao ;u;getUserPolicy u ;f dmp data pro ðÞ ðÞ ðÞ ðÞ ðÞ\nð6Þ\n3.2 Problem analysis\nAs shown in Fig. 1, the access control for big data systems is very complex. So a unified\nbig data security framework is needed to ensure the realization of big data access con-\ntrol. Apache Ranger is a data security management framework for the Hadoop ecosys-\ntem. It performs unified data authorization, management, and auditing for the Hadoop\necosystem. Various algorithms and methods for big data security can be easily applied\nto the Hadoop ecosystem through it [ 18–21]. During the development of the Apache\nRanger project, we found that after enabling big data security functions in big data sys-\ntems, the performance of big data systems will be greatly reduced, which will have a\ngreat impact on the application of big data systems.\nApache Ranger uses a policy-based model to express the “User-Resource-Rights ”\nlogic. It is also a fine-grained security control model that effectively manages data se-\ncurity on the Hadoop platform. The “User-Resource-Rights ”logic has the following\ncharacteristics:\n/C15A user is an object that accesses a resource and belongs to a group or role.Peng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 6 of 17\n\n[Página 7]\n/C15Different components correspond to different business resources.\n/C15AllowACL andDenyACL expressed rights. The AllowACL is an access control list\nthat describes the conditions that allowed to access. The DenyACL is a negative\naccess control list that describes the case of denying access.\nDefinition 1 : Access control item (ACI): A collection of visitors (users, user groups,\nroles) and access types, represented by AccessItem.\nAccessItem ¼List<User=Group =Role>þList<AccessType > ð7Þ\nwhere User/Group/Role is the object which accesses the big data resource; AccessType\nis the type of access object requires, i.e. access rights.\nDefinition 2 : Allow access control item (ACCI): A control that allows access to big\ndata resources, represented by allow.\nDefinition 3 : Exceptions allow access control items (EAACI): Allow access to excep-\ntion access items in control items, represented by allowException .\nDefinition 4 : Allow access control list (AACL): Allows the data set of the access con-\ntrol item set plus the exception allowed access control set, represented by AllowACL .\nAllowACL ¼List<AccessItem >allow þList<AccssItem >allowException ð8Þ\nDefinition 5 : Negative controls item (NCI): Control that do not allow access to big\ndata resources, expressed in deny.\nDefinition 6 : Abnormal negative control item (ANCI): Exception accesses in access\ncontrol entries did not allow, expressed as denyException .\nDefinition 7 : Negative control list (NCL): A data set in the set of negative access\ncontrol items that excludes the set of abnormal negative control items, represented by\nDenyACL .\nDenyACL ¼List<AccessItem >deny þList<AccssItem >denyException ð9Þ\nDefinition 8 : Access control policy (ACP): A logical rule for accessing resources by a\nlarge data resource access object, consisting of a resource, an allowable control list, and\na negative control list, represented by a Policy .\nPolicy ¼List<Resource >þAllowACL þDenyACL ð10Þ\nDefinition 9 : Access control service (ACS): A collection of access control policies,\nrepresented by a service.\nFigure 2shows the current policy-based access control process.\nAs can be seen from Fig. 2, when a user AU requests access to resources Ares, Apa-\nche Ranger obtains all the rights policies of the user related to requested resources.\nThe access right of AU will be determined by the following logic:\nIf the ARes is in the NACI and not in ANAC, AU cannot access it.\nIf the ARes is not in ACCI, AU cannot access it when it is in NACI and ANAC.\nIf the ARes is not in EAACI, AU cannot access it when it is in NACI, ANAC, and\nACCI.\nIf the ARes is in EAACI, AU can access it when it is in NACI, ANAC, and ACCI.\nIf the ARes is not in ACCI, AU cannot access it when it is not in NACI.Peng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 7 of 17\n\n[Página 8]\nIf the ARes is not in EAACI, AU cannot access it when it is not in NACI but in\nACCI.\nIf the ARes is in EAACI, AU can access it when it is not in NACI but in ACCI.\nFigure 2contains the query of the strategy, the analysis of the strategy, and the logical\njudgment based on the parsed strategy. Its logic can be decomposed to formulas\n(7)–(10) in the following order:\nAccessItem ¼List<User=Group =Role>þList<Access Type >\nDenyACL ¼List<AccessItem >deny þList<AccssItem >denyException\nAllowACL ¼List<AccessItem >allow þList<AccssItem >allowException\nPolicy ¼List<Resource >þAllowACL þDenyACL2\n664\nð11Þ\nIt can be seen from formula ( 11) that the user can get rights only after a quadruple\nloop. Since the time complexity of the nested loop is equal to the number of times the\ninnermost statement of the line is executed, the time complexity of the algorithm is\nT(n)=O(n4). In big data systems, due to the large amount of data and resources being\ndistributed in many different cluster nodes, the time to judge whether an access object\nhas the right to access a certain resource right will be very long. Therefore, the current\nimplementation of access control based on the “user-resource-priv~ ilege ”policy will\nseriously affect the performance of big data systems.\n3.3 Security access control algorithm based on memory index acceleration\nDefinition 10 : Security access control algorithm based on memory index acceleration\n(SACABMIA): Using the principle of second-level cache to build keys, establish in-\ndexes, and place frequently accessed resources and rights on the memory acceleratorUser access\nMatch deny\nAccessItem\nMatch\ndenyException\nAccessItemYes\nRefuse to access\nresourcesNoMatch allow\nAccessItemNo\nYesMatch\nAllowException\nAccessItemYes\nNoNoAllow access to\nresources\nYesGet associated\npermissions\nFig. 2 The current policy-based access control processPeng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 8 of 17\n\n[Página 9]\nthrough the index. When a user requests access to a resource, system first checks the\nindex. If there are no objects in the index, the index is then extracted and updated from\nthe configuration resource. When the rights resource is changed, the algorithm updates\nthe indexes in the secondary cache synchronously. Figure 3shows the execution flow\nof the algorithm.\nThe key is constructed based on the user, the accessed object, and the type of access.\nThe formula is as follows:\npk¼generateKey ao ;u;at ðÞ ð 12Þ\nwhere aois an accessed object, uis the access user, atis the access type, and pkis the\nconstructed key.\nThe key can be parsed to obtain the accessed object, access user, and access type.\nThe formula is as follows:\nao;u;at hi ¼analyseKey pk ðÞ ð 13Þ\nA memory accelerated index is built using the key and the rights obtained when a\nuser accesses a WSN big data system.\nindexMap ¼indexMap pk ;upðÞ ð 14Þ\nwhere pkis the constructed key, upis the rights and indexMap is the memory acceler-\nated index.\nThe memory accelerated index can be parsed to a list of keys. The formula is as\nfollows:\nList<pk>¼getAllKeyInMemoryIndex indexMap ðÞ ð 15Þ\nwhere pkis the constructed key and indexMap is the memory accelerated index.\nThe system updates the policies corresponding to all keys in the memory acceleration\nindex when the system policy is changed. The pseudo code for the update algorithm is\nas follows:\nInput: void\nOutput: void\n1. funcation updateIndexMap()\n2. arp←f(r) /*Get new rights policies*/\n3. uap←getUserPolicy (u,arp) /* Assign policy to users*/\n4. listPowerkey ←getAllKeyInMemoryIndex (indexMap ) /* Obtain a list of keys */\n5. for k to listPowerkey.size\n6. pk←listPowerkey.get(k) /*Get ao, user from Powerkey*/\n7. <ao, u, at> ←analyseKey (pk) /*Get ao, user from Powerkey*/\n8. ucp←getCurAP (ao,u,uap) /*Get user's access control policy*/\n9. up←getAP (ao,ucp) /* Get access control rights */\n10. indexMap ←indexMap (pk,up)/* Update the memory acceleration index */\n11. end for\n12. return\n13. end funcation\nThe process of getting permissions is as follows:Peng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 9 of 17\n\n[Página 10]\nup¼indexMap pk ðÞ\n¼power ;indexMap pk ðÞ≠null\ngetAP ðao;getAP ao ;getCurAP ao ;u;getUserPolicy u ;f dmp data pro ðÞ ðÞ ðÞ ðÞ ðÞ ;indexMap pk ðÞ ¼ null/C26\nð16Þ\nIn a WSNs big data system, user rights policy rarely changes once it is successfully\nconfigured [ 33,34]. According to formula ( 16), the index relationship is relatively stable\nafter the memory index is established, which greatly accelerates the efficiency and speed\nof access control in WSNs big data systems. It can be seen from formula ( 16) that the\nsystem can get rights when it only reads once from the memory index. So, the time\ncomplexity of the algorithm is O(n).User access\nresources\nBuild Key\n(user-resource-\nrights)\nMatch negative access\ncontrol items?\nMatching abnormal\nnegative access control\nitems?Yes\nNoMatch allowed access\ncontrol itemsNo\nYesMatch\nabnormal permission\naCcess control?Yes\nNoNo\nYesMemory index\naccelerator\nHave\npermission\nobject?FalseGet access to\nresourcesYes\nHave access?Allow access to\nresourcesYesRefuse to access\nresources\nEnd\nBuild memory\nindex accelerator\nRefuse to\noperate\nresourcesAllow operation\nresources\nEnd\nPolicy changeUpdate memory\nindex acceleratorParsing policy resources as\nuser-resource-policy\nobjectsRead\ncompleted?\nYesNoGet users, resources,\npermissionsGet strategic\nresources\nParsing a policy resource as a\npolicy resource object\nRead policy\nresource fileRead the memory\nindex accelerator to\nget the Key\nFig. 3 The execution flow based on the memory index accelerator algorithmPeng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 10 of 17\n\n[Página 11]\nAs can be seen from Fig. 3, the system builds a key based on User-Resource-Rights\nand accesses the memory index accelerator when the user accesses resources. If there is\na User- Resource-Rights object corresponding to the key in the accelerator, the user\nwill obtain the requested resources. Otherwise, the system obtains rights according to\nthe logic of Fig. 1. The system re-reads the policy resource file, which is in json format,\nand converts it into a policy resource object when access control strategy is changed.\nThe system also updates the memory index accelerator.\nThe specific method of building a key is as follows:\nAfter receiving the user ’s access request, the system parses the request to obtain the\nuser who visited this time, the resources to be accessed this time, and the rights of this\naccess. Adds the user of this visit, the resources to be accessed this time, and the rights\nof this visit to get the key.\nThe main features of the “value ”object of the in-memory index accelerator include\nservice information, service definition list, policy details related to the user, resource,\nand rights status of the current access. The first layer is service information, service\ndefinition list, and policy detail list. The service definition includes the name of the ser-\nvice, the basic configuration related to the service, the resource details, the access type,\nand the policy conditions. The policy details mainly include information about policy-\nrelated resources, policy visitor objects, negative policy details, access policies, and the\nException Access Policy details.\n4 Experimental results and analysis\nThe experiment was carried out on a four-node HBase cluster with Apache Ranger for\ntest strategy.\nThe hardware configuration of the test cluster node is listed in Table 1.\nA test table was created in HBase as listed in Table 2.\nThe YCSB (Cloud Serving Benchmark) tool was used for testing in this experiment.\nIt is a tool developed by Yahoo to test the performance of cloud services. We generatedTable 1 Test cluster node hardware configuration details\nNode Configuration\nNode1 CPU: 32 core, Intel(R) Xeon(R) CPU E5-2650 v2 2.60GHz\nNode2 Memory: 128GB\nNode3 CPU: 48 core, Intel(R) Xeon(R) CPU E5-2670 v3 2.30GHz\nNode4 Memory: 128GB\nTable 2 Test table\nTest\nInfo Score\nName Age Math Physical Political\n00001 John 15 90 93 95\n00002 Paul 15 91 92 97\n00003 Carly 16 90 94 91\n00004 Scott 14 92 93 96Peng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 11 of 17\n\n[Página 12]\n102.4 GB of data using this tool and tested the current access control function based\non the “User-Resource-Rights ”policy.\nWe applied a security policy in HBase and tested performance degradation. The test\nresults are listed in Table 3.\nAs shown in Table 3, the performance is reduced by 10.10% with a security policy ap-\nplied for access control in HBase system. This result shows that the traditional access\ncontrol based security technology greatly affects the performance of big data systems\nfor WSN. Our previous analysis was proved by this result.\nWe also applied a different number of security policies in HBase to analyze their im-\npact on system performance. The test results are listed in Table 4.\nFigure 4shows the correlation between execution time and number of strategies. The\nnumber of policies is set from 1 to 25 with an interval of 5. The execution time in-\ncreases from 642.01 to 732.46 s using the old algorithm with the increase of the number\nof strategies. This indicates that the impact of access control security on system per-\nformance is high and the increase in the policy has also big effect on the performance\nwhen the old method is used.\nFigure 5shows the correlation between performance degradation and number of\nstrategies. The number of policies is set from 1 to 25 with an interval of 5. The per-\nformance degradation rate decreased from 0 to 14.09% using the old algorithm with\nthe increase of the number of strategies. This indicates that the impact of access con-\ntrol security on system performance is high and the increase in the policy has also big\neffect on the performance degradation rage when the old method is used.\nAs shown in Table 4, the execution time of HBase system gradually increased and its\nperformance gradually degraded as the number of strategies increases. When the num-\nber of strategies reaches 25, the system performance drops by 14.09%. These results\nprove that the impact of access control algorithms on the performance of big data sys-\ntems is gradually increasing with the number of policies increases. The results alsoTable 3 Current access control function test results based on the User-Resource-Rights policy\nUser Policy\nnumberNodes The amount of\ndataExecution\ntime (s)Data processing\nspeed (ops/s)Performance\ndegradation rate\nHbase_test 0 4 100 million(102.4GB) 642.01 155687 base\nHbase_test 1 4 100 million(102.4GB) 706.26 141520 10.01% ↓\nTable 4 Test results of different number of security policies based on the User-Resource-Rights\npolicy\nUser Policy\nnumberNodes The amount\nof dataExecution\ntime (s)Data processing\nspeed (ops/s)Performance\ndegradation rate\nHbase_test 0 4 100 million(102.4 GB) 642.01 155687 base\nHbase_test 5 4 100 million(102.4 GB) 712.81 140221 11.03% ↓\nHbase_test 10 4 100 million(102.4 GB) 719.1 138994 12.01% ↓\nHbase_test 15 4 100 million(102.4 GB) 725.52 137764 13.01% ↓\nHbase_test 20 4 100 million(102.4 GB) 731.88 136578 14.00% ↓\nHbase_test 25 4 100 million(102.4 GB) 732.46 136460 14.09% ↓Peng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 12 of 17\n\n[Página 13]\nverified that our analysis and judgment that the access control technology seriously af-\nfects the performance of big data systems is correct.\nAs a comparison, we performed experiments on the proposed new algorithm on\nHBase. Test results are listed in Table 5.\nFrom the results in Table 5, we can see that with the new algorithm the performance\nof HBase reduced only by 2.74% when a security policy is applied for access control.\nA different number of security policies in HBase is applied to test the performance of\nthe new algorithm. The test results are listed in Table 6.\nFigure 6presents the correlation between execution time and number of strategies.\nThe graph shows that the time for users to access 102.4 GB data is 652.40 s in the case\nof one security policy, the time for users to access 102.4 GB data is 652.60 s in the case\nof five security policies, the time for the user to access the 102.4 GB data is 652.78 s in\nthe case of ten security policies, and the time for the user to access the 102.4 GB data is\n653.29 s in the case of twenty-five security policies. It shows that the increase in the\npolicy has little effect on the time spent in parsing the data.\nFig. 4 Execution time trend\nFig. 5 Performance degradation rate trendPeng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 13 of 17\n\n[Página 14]\nAs shown in Table 6, when the number of strategies increases, execution time of\nHBase with new algorithm increases much slower than same system with traditional al-\ngorithm. System performance degradation is also much slower with new algorithm.\nWhen the number of strategies reaches 25, the system performance drops only 2.88%.\nThe system performance is much less affected.\nFigure 7reveals the correlation between performance degradation and number of\nstrategies. The graph shows that the performance degradation rage for users to access\n102.4 GB data is 2.74% in the case of two security policy, the performance degradation\nrage for users to access 102.4 GB data is 2.77% in the case of three security policies, the\nperformance degradation rage for the user to access the 102.4 GB data is 2.80% in the\ncase of four security policies, and the performance degradation rage for the user to ac-\ncess the 102.4 GB data is 2.88% in the case of seven security policies. This indicates that\nthe impact of access control security on system performance is low and the increase in\nthe policy has little effect on the performance degradation rage.\nFigure 8shows the performance degradation trend of the new algorithm compared with\nthe traditional algorithm. The number of policies is set from 1 to 25 with an interval of 5.\nThe performance degradation rate decreased from 0 to 14.09% using the old algorithm,\nand the performance degradation rate only decreased from 0 to 2.88% using the new algo-\nrithm with the increase of the number of strategies. This indicates that the impact of ac-\ncess control security on system performance is low, and the increase in the policy has also\nlittle effect on the performance degradation rage when the method is used.\nFrom the experimental results, we can get the conclusions:\nThe new algorithm can significantly improve the performance of big data systems\nwhere the fine-grained security policy-based access control model is applied.\nBig data security technologies mainly include data asset grooming, data encryption,\ndata security operation and maintenance, data desensitization, and data leakage scan-\nning. These security technologies can only exert their value if applied to big data sys-\ntems. The performance impact of big data systems using big data security includes twoTable 5 Access control function test results using the new algorithm\nUser Policy\nnumberNodes The amount of\ndataExecution\ntime (s)Data processing speed\n(ops/s)Performance\ndegradation rate\nHbase_\ntest0 4 100\nmillion102.4GB)635 157484 base\nHbase_\ntest1 4 100\nmillion102.4GB)652 153284 2.74% ↓\nTable 6 Test results of different number of security policies using the new algorithm\nUser Policy\nnumberNodes The amount\nof dataExecution\ntime (s)Data processing\nspeed (ops/s)Performance\ndegradation rate\nHbase_test 0 4 100 million(102.4 GB) 635.02 157484 base\nHbase_test 5 4 100 million(102.4 GB) 652.60 153239 2.77% ↓\nHbase_test 10 4 100 million(102.4 GB) 652.78 153195 2.80% ↓\nHbase_test 15 4 100 million(102.4 GB) 652.91 153165 2.82% ↓\nHbase_test 20 4 100 million(102.4 GB) 653.10 153120 2.85% ↓\nHbase_test 25 4 100 million(102.4 GB) 653.29 153075 2.88% ↓Peng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 14 of 17\n\n[Página 15]\naspects, one is the impact of the security algorithm itself on the system, and the other\nis the impact of the method of applying the security algorithm on the system perform-\nance. This paper mainly studies from the second perspective to improve the perform-\nance of big data systems. The method proposed in the paper will not improve the\nperformance of big data security algorithms.\nBy analyzing the results of RANGER-1729 and this experiment, we can draw the con-\nclusion that the new algorithm can significantly reduce the impact of access control\ntechnology on the performance of big data systems. RANGER-1729 is an issue of Apa-\nche Ranger project, and its link address is https://issues.apache.org/jira/browse/RAN-\nGER-1729 .\n5 Conclusions\nPerformance is the life of big data systems and security is the cornerstone of big data\nsystems. When big data systems apply access control technology to ensure the security\nof data, existing methods will seriously affect the system performance. This paper first\nFig. 6 The execution time trend\nFig. 7 The performance degradation rate trendPeng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 15 of 17\n\n[Página 16]\nanalyses the data processing flow of the existing access control technology in the big\ndata system and its time complexity. Then it points out that the system performance\nwill be greatly affected by this existing technology. We proposed a big data security ac-\ncess control algorithm based on memory index acceleration for WSNs in this article.\nWe walked through its data processing flow and analyzed its time complexity. We also\ntheoretically proved that the new algorithm has better performance. Through experi-\nments, we further proved that compared with the traditional access control technology,\nthe new algorithm has less impact on the performance of big data systems.\nAbbreviations\nYCSB: Cloud Serving Benchmark; NTRU: Number Theory Research Unit; DAC: Discretionary access control;\nMAC: Mandatory access control; RBAC: Role-based access control; ABAC: Attribute-based access control; PBAC: Policy-\nbased access control; RABE: Revocable and grantable attribute-based encryption\nAcknowledgements\nThe authors acknowledged the anonymous reviewers and editors for their efforts in valuable comments and\nsuggestions.\nAuthors ’contributions\nJ. Peng proposes the innovation ideas and theoretical analysis, and H. Zhou carries out experiments and data analysis.\nQ. Meng and J. Yang conceived of the study, and participated in its design and coordination and helped to draft the\nmanuscript. All authors read and approved the final manuscript.\nFunding\nThe author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of\nthis article: This work was supported by the school research fund of Nanjing Institute of Industry Technology (Grant\nNo. YK18-05-03).\nAvailability of data and materials\nData sharing is not applicable to this article as no datasets were generated or analyzed during the current study.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 14 January 2020 Accepted: 23 April 2020\nReferences\n1. Z. Huang, X. Xu, J. Ni, H. Zhu, C. Wang, Multimodal representation learning for recommendation in Internet of Things.\nIEEE Internet Things J. 6(6), 10675 –10685 (2019)\n2. B. Wu, T.L. Yip, X. Yan, C. Guedes Soares, Fuzzy logic based approach for ship-bridge collision alert system. Ocean Eng.\n187, 106152 (2019)\n3. P. Alexander, L. Pike, P. Loscocco, G. Coker, Model checking distributed mandatory access control policies. ACM\nTransactions on Information and System Security (TISSEC) 18(2), 1 –25 (2015)\nFig. 8 Comparison of performance degradation trendsPeng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 16 of 17\n\n[Página 17]\n4. H. Liu, X. Yao, T. Yang, H. Ning, Cooperative privacy preservation for wearable devices in hybrid computing-based smart\nhealth. IEEE Internet Things J. 6(2), 1352 –1362 (2018)\n5. Terzis, S., Wagealla, W., English, C., & Nixon, P. Trust lifecycle management in a global computing environment. In\nInternational Workshop on Global Computing (pp. 291-313). Springer, Berlin, Heidelberg (2004).\n6. Chang, R., Jiang, L., Chen, W., He, H., Yang, S., Jiang, H., & Liu, Y. (2018). Towards a multilayered permission-based access\ncontrol for extending Android security. Concurrency and Computation: Practice and Experience, 30(5), e4180 (2018).\n7. N. Mundbrod, M. Reichert, Object-specific role-based access control. International Journal of Cooperative Information\nSystems 28(01), 1950003 (2019)\n8. M.U. Aftab, Z. Qin, N.W. Hundera, O. Ariyo, N.T. Son, T.V. Dinh, Permission-based separation of duty in dynamic role-\nbased access Control Model. Symmetry 11(5), 669 (2019)\n9. C.M. Subramanian, A.K. Cherukuri, C. Chelliah, Role based access control design using three-way formal concept analysis.\nInt. J. Mach. Learn. Cybern. 9(11), 1807 –1837 (2018)\n10. V. Hu, C. Kuhn, D. Richard, D.F. Ferraiolo, Attribute-based access control. Computer 48(2), 85 –88 (2015)\n11. M. Uriarte, J. Astorga, E. Jacob, M. Huarte, M. Carnerero, Expressive policy-based access control for resource-constrained\ndevices. IEEE Access 6,1 5 –46 (2017)\n12. J.P. Cruz, Y. Kaji, N. Yanai, RBAC-SC: role-based access control using smart contract. Ieee Access 6, 12240 –12251 (2018)\n13. S. Aditham, N. Ranganathan, A system architecture for the detection of insider attacks in big data systems. IEEE\nTransactions on Dependable and Secure Computing 15(6), 974 –987 (2017)\n14. Q. Xia, E.B. Sifah, K.O.B.O. Agyekum, H. Xia, K.N. Acheampong, A. Smahi, M. Guizani, Secured fine-grained selective access\nto outsourced cloud data in IoT environments. IEEE Internet Things J. 6(6), 10749 –10762 (2019)\n15. Y. Zhu, D. Huang, C.J. Hu, X. Wang, From RBAC to ABAC: constructing flexible data access control for cloud storage\nservices. IEEE Trans. Serv. Comput. 8(4), 601 –616 (2014)\n16. S. Wang, X. Zhang, Y. Zhang, Efficient revocable and grantable attribute-based encryption from lattices with fine-grained\naccess control. IET Inf. Secur. 12(2), 141 –149 (2018)\n17. M. Zhang, D. Zhang, F. Goerlandt, X. Yan, P. Kujala, Use of HFACS and fault tree model for collision risk factors analysis\nof icebreaker assistance in ice-covered waters. Saf. Sci. 111, 128 –143 (2019)\n18. M. Alam, N. Emmanuel, T. Khan, Y. Xiang, H. Hassan, Garbled role-based access control in the cloud. J. Ambient. Intell.\nHumaniz. Comput. 9(4), 1153 –1166 (2018)\n19. S. Pal, M. Hitchens, V. Varadharajan, T. Rabehaja, Policy-based access control for constrained healthcare resources in the\ncontext of the Internet of Things. J. Netw. Comput. Appl. 139,5 7 –74 (2019)\n20. M. Babar, F. Khan, W. Iqbal, A. Yahya, F. Arif, Z. Tan, J.M. Chuma, A secured data management scheme for smart societies\nin industrial internet of things environment. IEEE Access 6, 43088 –43099 (2018)\n21. D. Chattaraj, M. Sarma, A.K. Das, N. Kumar, J.J. Rodrigues, Y. Park, HEAP: an efficient and fault-tolerant authentication and\nkey exchange protocol for Hadoop-assisted big data platform. IEEE Access 6, 75342 –75382 (2018)\n22. Huang Z., Tang J., G. Shan, Ni J., Chen Y., & Wang C. An efficient passenger-hunting recommendation framework with\nmulti-task deep learning. IEEE Internet of Things Journal . DOI: https://doi.org/10.1109/JIOT.2019.2901759(2019) .\n23. X. Liu, R. Zhu, A. Anjum, J. Wang, H. Zhang, M. Ma, Intelligent data fusion algorithm based on hybrid delay-aware\nadaptive clustering in wireless sensor networks. Futur. Gener. Comput. Syst. 104,1–14 (2020)\n24. H. Xie, Z. Yan, Z. Yao, M. Atiquzzaman, Data collection for security measurement in wireless sensor networks: a survey.\nIEEE Internet Things J. 6(2), 2205 –2224 (2018)\n25. X. Liu, R. Zhu, B. Jalaian, Y. Sun, Dynamic spectrum access algorithm based on game theory in cognitive radio networks.\nMobile Networks and Applications 20(6), 817 –827 (2015)\n26. R. Zhu, X. Zhang, X. Liu, W. Shu, T. Mao, B. Jalaian, ERDT: Energy-efficient reliable decision transmission for intelligent\ncooperative spectrum sensing in industrial IoT. IEEE Access 3, 2366 –2378 (2015)\n27. J. Li, Y. Ji, K.K.R. Choo, D. Hogrefe, CL-CPPA: certificate-less conditional privacy-preserving authentication protocol for the\nInternet of Vehicles. IEEE Internet Things J. 6(6), 10332 –10343 (2019)\n28. C. Hu, W. Li, X. Cheng, J. Yu, S. Wang, R. Bie, A secure and verifiable access control scheme for big data storage in\nclouds. IEEE Transactions on Big data 4(3), 341 –355 (2017)\n29. J.A. Padget, W.W. Vasconcelos, Fine-grained access control via policy-carrying data. ACM Transactions on Internet\nTechnology (TOIT) 18(3), 1 –24 (2018)\n30. Y. Mo, A data security storage method for IoT under Hadoop cloud computing platform. Int. J. Wireless Inf. Networks\n26(3), 152 –157 (2019)\n31. X. Fu, Y. Gao, B. Luo, X. Du, M. Guizani, Security threats to Hadoop: data leakage attacks and investigation. IEEE Netw.\n31(2), 67 –71 (2017)\n32. X. Min, Q. Yong, W. Kui, Z. Jizhong, L. Mo, Using potential to guide mobile nodes in wireless sensor networks. Ad Hoc &\nSensor Wireless Networks 12(3-4), 229 –251 (2011)\n33. Y. Yang, X. Zheng, W. Guo, X. Liu, V. Chang, Privacy-preserving smart IoT-based healthcare big data storage and self-\nadaptive access control system. Inf. Sci. 479, 567 –592 (2019)\n34. K. Yang, Q. Han, H. Li, K. Zheng, Z. Su, X. Shen, An efficient and fine-grained big data access control scheme with\nprivacy-preserving policy. IEEE Internet Things J. 4(2), 563 –571 (2016)\nPublisher ’sN o t e\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Peng et al. EURASIP Journal on Wireless Communications and Networking         (2020) 2020:90 Page 17 of 17",
      "metadata": {
        "filename": "peng2020.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\peng2020.pdf",
        "file_size": 983296,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:38.807520",
        "content_length": 49091
      }
    },
    "ce755c85-2d8c-46b5-821f-4f3090e19a0c": {
      "id": "ce755c85-2d8c-46b5-821f-4f3090e19a0c",
      "content": "[Página 1]\n(IJACSA) International Journal of  Advanced Computer Science and Applications,  \nVol. 1 4, No. 9, 202 3 \n990 | P a g e  \nwww.ijacsa.thesai.org  Next -Generation Intrusion Detection and Prevention \nSystem Performance in Distributed Big Data Network \nSecurity Architectures  \nMichael Hart1, Rushit Dave2, Eric Richardson3 \nCollege of Science, Engineering, & Technology , Minnesota State Uni versity, Mankato, United States1, 2  \nCollege of Health and Human Services , University of North Carolina Wilmington , United States3 \n \n \nAbstract—Big data systems are expanding to support the \nrapidly growing needs of massive scale data analytics. To \nsafeguard user data, the design and placement of cybersecurity \nsystems is also evolving as organizations  to increase their big data \nportfolios. One of several challenges presented by these changes \nis benchmarking real -time big data systems that use different  \nnetwork security architectures. This work introduces  an eight -\nstep benchmark  process to evaluate  big data systems in varying  \narchitectural environments. The  benchmark is tested on real -\ntime big data systems running in perimeter -based and perimeter -\nless network environments. Findings show that marginal I/O \ndifferences exist on distributed file systems between network \narchitectures. However, during various types of cyber incidents \nsuch as distributed denial of service (DDoS) attacks, certain \nsecurity ar chitectures like zero trust require more system \nresources  than perimeter -based architectures . Results illustrate \nthe need to broaden research on optimal benchmarking and \nsecurity approaches for massive scale distributed computing \nsystems.  \nKeywords —Big data  systems; zero trust architecture; \nbenchmarking; distributed denial of service attacks  \nI. INTRODUCTION   \nBig data systems are u nified environments designed for \nmassive -scale data analytics . Systems capable of handling \nlarge  amounts of data are becoming  more important  as the \nvolume of data created and communicated over the Internet \nincreases  [1]. Cybersecurity  systems play an important role in \nensuring the large quantities of data on the Internet remains \nsafe. One dimension of several necessary to accomp lish the \nlatter are next -generation security devices. I ntrusion detection  \nand prevention  systems (ID PSs) properly manage data \naccessibility, privacy, and safety.  IDPS algorithms are able to \nidentify cyber threats  using several mechanisms. This includes \nusing prior information from previous attacks, anomalies in \nnetwork packets  [1], and machine learning [ 2]. \nAs big data systems become more common, their roles will \ncontinue to  expand. This includes the capability to analyze and \ndetect information security vul nerabilities at scale. For \nexample, several big data frameworks exist that discover \ndistributed denial of service (DDoS) attacks [3].  This \nexpansion of roles offers many exciting opportunities for \norganizations . However, as the use of big data systems grow s, \nthe capability of attackers to leverage associated parallel \ncomputing power for nefarious reasons  also increases  [3]. A systematic review of 32 papers pertaining to securing  big data \nfound that a critical need  in future research  is building more \nsecure big data infrastructure [4]. Contributing to the latter \nobjective , the researchers demonstrate how varying network \narchitectures impact the security and p erformance of big data \nsystems.  \nOrganization of the paper  is as follows. Section II reviews  \nliterature on intrusion detection and prevention methods for big \ndata systems. Section III outlines the research design and \nmethodologies used to test perimeter -based security and \nperimeter -less security applied to a big data system \nenvironment. Section IV describes the research results. Section \nV concludes the study by discussing the limitations and future \noutlook.  \nII. LITERATURE REVIEW  \nWork is necessary to optimize both the information security \nand performance of distributed  systems. Today , several open -\nsource big data frameworks provide remarkable  potential for \nsolving challenging data science and related problems  by \nleveraging powerful parallel and distributed data processing . \nHowever, securing these systems often carries performance \npenalties.  The review  of literature  that follows explores \nresearch on the impact of various IT infrastructure  security  \nstrategies and their influence on big data environments.  It \nbegins by reviewing  comprehensive surveys most closely \nrelated to  information security and big data systems . \nA. Surveys of Big Data  and Intrustion Detection  \nPrevious s ystematic reviews of literature  focused on \ninformation security and big data  provide a vast array of \nobjectives. A prominent theme is using  deep learning  [1] and \nmachine learning  [2] to assist  in detecting or preventing  \ncybersecurity attacks. This line of research often utilizes deep \nlearning or machine learning algorithms for near real -time data \nprotection . \nA recent and well cited comprehensive survey in [1] \nevaluates how deep learning is used  for intrusion detection \nsystems in the cybersecurity domain.  It found notable contrast s \nbetween machine learning approaches in cybersecurity and \ndeep learning. Conventional machine learning approaches \nutilized in  cybersecurity were classified by approach es such as \nartificial neural networks (ANNs), Bayesian networks , decision \ntrees,  fuzzy logic , k-means clustering , k-nearest neighbor \n(kNN) algorithm,  and support vector machines (SVMs).  The\n\n[Página 2]\n(IJACSA) International Journal of  Advanced Computer Science and Applications,  \nVol. 1 4, No. 9, 202 3 \n991 | P a g e  \nwww.ijacsa.thesai.org  survey centered on deep learning focal intrusion detection \nmethods  that included autoencoders (AEs), convolutional \nneural networks (CNNs), deep belief networks (DBNs), \ngenerative adversarial networks (GANs), and long short -term \nmemory (LSTM) recurrent neural networks  [1]. \nAEs, DBNs , and GANs were highlighted  in [1]  for their \nunsupervised learning strengths.  In the absence of gradient \nestimation, AEs can use gradient descent to train data.  A \nstrength of LSTM is its capabilities in analyzing time -series \ndata.  CNNs do not need as much data processing prior to \nevaluation as c ertain algorithms and is able to classify cyber -\nattacks  using multiple characteristics well. Combined, the \nsurvey of literature finds that AEs, CNNs, DBNs, GANs, and \nLSTM networks each have potential to improve intrusion \ndetection methods. Furthermore, the  survey [1] outlined the \nimportance of dataset reliability when evaluating deep learning \nintrusion detection effectiveness. Variance in cybersecurity \nattack datasets can introduce  model bias when comparing \nmultiple deep learning methods. Thus, any biases i n attack \ndatasets or data from live systems could increase  spurious \nresults [1].  \nA subsequent theme in  the literature concentrate s on \ncybersecurity and privacy prevention in big data applications.  \nWhile this research again  employs  various data science \nmeth ods to detect or prevent data breaches, it also illustrate s \nhow big data techniques can prevent information privacy \nissues . Research in  [4] led to a proposed model for enhancing \ninformation privacy. The model highlights people, \norganizations, society, and government  roles . It leverages  IDS, \nIPS, and encryption as its primary techniques to prevent data \nbreaches  [4]. \nB. Big Data Architectures and Information Security  \nAs big data evolves, the supporting infrastructures will \nrequire proper encryption, intrusion detection , and intrusion \nprevention . Changing architectures within computer networks, \nmessaging techniques, and undefined communication metho ds \nintroduce numerous challenges . In a 2014 study Mitchel and \nChen [5] recognized this paradigm. Their emphasis on cyber -\nphysical systems (CPS) ranging from smart grids to unmanned \naircraft systems led to the classification of four primary \nintrusion detect ion categories. These include legacy \ntechnologies, attack sophistication, closed control loops, and \nphysical process monitoring. Each of the latter is narrow \nconcepts as they relate to the broader field of intrusion \ndetection, underlying the unique customi zation of IDSs for \ncyber -physical systems [5] . \nThree years later Zarpelo et al. [6] outlined a similar but \ndistinct paradigm; intrusion detection focal to  the Internet of \nthings (IoT). The researchers stated that IoT has similar \ninformation security matter s as the Internet, cloud services, and \nwireless sensor networks (WSNs). Despite similarities, IoT \ninformation security approaches are distinct , according to the \nauthors due to concepts such as data sharing between users, the \nvolume of interconnected object s, and the amount of \ncomputational power of the associated devices. Like cyber -\nphysical systems, IoT presents diverse challenges to the design \nof instruction detection systems [6].  Designing secure cloud computing environments poses \nseveral novel problems at multiple infrastructure layers. As an \nexample, cloud resources can be leased by numerous vendors \nfocused on varying as -a-service models such as infrastructure \nas a service (Iaas), platform as a service (PaaS), and/or \nsoftware as a service (SaaS). Multi -cloud applications rely \nupon the seamless integration of cloud resources from \nproviders focused on one or many as -a-service types, which \ncontinue to expand. In Casola et al. [7] a model is outlined for \ndesigning, creating, and implementing  multi -cloud \napplications. The flexible approach accounts for varying as -a-\nservice components. Security -by-design is a primary objective \nof the process lifecycle between the functional design of multi -\ncloud applications and the security design. The functional \ndesign phase defines the application logic, interconnections of \nservices, and resource requirements. In the security design \nphase, each cloud element is assessed in terms of security risks \nand security needs. Security policies and controls are designed \nbased on  the lat ter requirements. Similar to CPS  [5] and IoT \n[6], the multi -cloud application model is a subsequent example \nof how information security solutions play a prominent role \ndue to  the systems’  distinct architectural and infrastructure \nlayers.  \nSecuring big data environments or leveraging associated \ntechniques like machine learning  to enhance information \nsecurity intertwines  numerous fields include  but not limited to \nCPS, IoT, and cloud computing.  Like big data systems, CPS \nrequires cybersecurity protection [8] of private data [9]. Big \ndata, IoT, and CPS often overlap through the ad hoc interfaces \nof systems such as smart vehicles, buildings, factories, \ntransportation systems, and grids [10]. As a vulnerable attack \nsurface, IoT advances the need for in telligent information \nsecurity.  \nMachine learning [11], including ensemble intrusion \ndetection [12], and IDS design [13] are proposed techniques to \nmitigate malicious cyber security attacks. Due in part to porous \nattack surfaces in cloud centric big data, IDSs may require \ncollaborative frameworks [14]. In [15], fuzzy c means cluster \n(FCM) and support vector machine (SVM) were proposed as a \ncollaborative technique for IDS detection rates. Compared to \nother  mechanisms, the proposed hybrid FCM -SVM showed \nlower false alarm ratios and higher detection accuracy [15].  \nFurthermore, [16] illuminates the need for scaling IDS \ndetection algorithms using the resources of parallel computing \nin the cloud.  \nIn [17] the res earchers propose the BigCloud security -by-\ndesign framework. The framework draws from the need to \nintegrate big data security into the system development \nlifecycle. Its primary cloud application domain is focal to \ninfrastructure as a service. It notes IaaS as one of the faster \ngrowing as -a-service options for big data. The model helps \ndesign and enforce secure authentication, authorization,  data \nauditability, availability, confidentiality, integrity, and privacy. \nHowever, its IaaS concentration could provid e greater benefits \nto as -a-service components specific to host operating systems, \nhypervisors, networking, and hardware [17].  Similar to IaaS, \nthe evolution of serverless platforms and Function -as-a-service \n(FaaS) applications requires careful security des ign to \novercome security threats that new services often suffer [18].\n\n[Página 3]\n(IJACSA) International Journal of  Advanced Computer Science and Applications,  \nVol. 1 4, No. 9, 202 3 \n992 | P a g e  \nwww.ijacsa.thesai.org  While distinct, CPS, IoT, cloud computing, and big data are \nmerely a few examples of why designing intrusion detection \nand prevention systems remains highly elastic  in modern \ncomputation al architectures . As the information technology \nlandscape changes, information security bends to meet the \nevolving needs of the complete environment.  To conclude the \nliterature  review , the authors will outline several relevant \nstudies  introduc ing potential  solutions to design stronger  \ninformation security controls  for big data systems . \nC. Encryption  \nAn ongoing challenge in distributed big data systems is \nsecuring communication between multiple systems operating \nacross various  computer networks . Apache Hadoop a nd \nApache Spark are examples of big data frameworks that \npresent several opportunities for attackers to access the data \nthey facilitate. Central to big data frameworks is the ability to \nuse parallel processing to analyze massive amounts of data. \nMapReduce is one of many programming paradigms that \nleverages Hadoop to extract valuable knowledge from large \nvolumes of data . However, like most application or service \nmodules within big data frameworks, MapReduce highlights \nthe vast attack vectors that exist in di stributed big data systems.  \nMapReduce examples in literature include side channel attacks \n[19], job composition attacks [20], and malicious  worker \ncompromises  in the form of distributed denial -of-service \n(DDoS) or replay attacks  [21], Eaves dropping and data \ntampering [22].  Encryption is a primary countermeasure to \nsecure transmissions and prevent data leaks between big data \nservers [19].  \nA primary objective in addressing cybersecurity attacks on \nparallel processing services is identifying and preventing leaks \nthat often occur during data transmission between distributed \nworker nodes , also referred to as DataNodes in Apache \nHadoop . These unique yet integrated servers work in parallel to \ncomplete MapReduce jobs . Often in Hadoop, data is stored and \nretrieved  from the Hadoop Distributed File System (HDFS) . In \n[19] side -channel attacks are addressed that can occur between \nMapReduce workers that utilize HDFS for data storage.  These \ntypes of cybersecurity attacks can target worker nodes to \nextract valuable inform ation pertaining to MapReduce jobs \nsuch as the amount of packet bandwidth. This further \ncontributes to successful pattern attacks. The authors proposed \na solution to this vulnerability labeled Strong Shuffle that \nenforces strong data hiding between workers  [19]. In contrast \nto alternative countermeasures such as correlation hiding in \n[20], Strong Shuffle avoids leaking the number of records \naccepted by each reducer during MapReduce runtime . Secure \nplaintext communications is a function of semantically secur e \nencryption in the Strong Shuffle solution  [19].  \nIn [19] data communicated between Hadoop DataNodes \nand stored in HDFS is encrypted with semantically secure \nAES -128-GCM encryption. Although the latter helps prevent \nclear text  leakage between MapReduce job s in Hadoop, \nencryption in big data environments has limitations. For \nexample, encrypted databases can still reveal  certain \ninformation during operations that include table queries. \nDeterministic encryption and order -preserving encryption can \nleak the equa lity relationship and the order between records. \nOne proposed solution is semantically secure encryption. In [23] the authors propose a semantically secure database system  \nnamed Arx . Alternative to order -preserving encryption, \nsemantic security within Arx only allows an attacker to extract \norder relationships and frequency of the direct database query \nin use in contrast to the entire database. The authors note that \nworst -case attackers would gain as much information from a \ndata leak as deterministic or orde r-preserving encryption  over \ntime [23]. While methods such as encryption  and \nauthentication  help with cross -node data leaks, they do not \nprevent other attacks , such as DDoS  and passive network \neavesdropping [21] . A subsequent countermeasure is the \neffective design and implementation of intrusion detection and \nprevention systems  [14]. \nD. Next-Generation Security and Big Data Systems  \nNext -generation security at a high level can detect  and \nprevent  malicious cybersecurity attacks.  Much of the  literature \nfocuses on identifying malicious network packets in real -time. \nThe comprehensive survey in [24] reviews how modern data \nmining techniques are evolving to meet real -time detection \nneeds. The review classifies intrusion detection systems by \narchitecture, imple mentation, and detection method s. Detection \nmethods are categorized as anomaly -based, signature based, \nand hybrids. Signature based methods or misuse often rely \nupon a database that defines patterns or existing malicious \nattack signatures. Anomaly detectio n can detect  non-normal \nnetwork traffic behavior that has yet to be defined in a \nsignature database. Data mining methods including supervised, \nunsupervised, and hybrid learning are being used to improve \nanomaly -based intrusion detection systems [24].  \nWhile  supervised , unsupervised , and hybrid learning  IDS \nresearch continues to progress [24], the ongoing need to \nimprove existing big data implementations remains. In several \nsystematic literature reviews [1, 2, 3, 24], IDSs are known to \nhave limitations that c ontradict the performance benefits of \nparallel processing and distributed computing. For example, \nlarge signature based systems drain CPU and memory \nresources  [24]. While researchers continue to advance areas of \nintrusion detection such as packet anomalies  and encryption , \nonly a few studies are advancing security by design and its \neffects on varying big data architectures [1]. To address this \nneed, the authors of this study designed a distributed big data \nsystem over a wide area network  to explore the perfo rmance of \ndistributed nodes under different network traffic loads . \nIII. METHODS  \nThis research  methodology follows the design science \napproach in [25 ] and  [26]. Design science is based on a \nscientific framework for IT research. As March and Smith [25] \noutline, IT research should  consider natural and design science \nas a method  to build and evaluate tangible objects . Within this \nphilosophy, objects  often have output s in the form of models or \ninstantiations . Instantiations associate with new artifacts in the \ndesign science methodology and the understanding of the \nartifact in its environment [25]. IT artifacts can be realized in \nmany forms such as through the design of  an object  that helps \nsolve business problems [26].\n\n[Página 4]\n(IJACSA) International Journal of  Advanced Computer Science and Applications,  \nVol. 1 4, No. 9, 202 3 \n993 | P a g e  \nwww.ijacsa.thesai.org  A. Organizational Problem  \nCentral to the organizational  problem in this study is the \nneed to architect a  real-world or simulated big data \nenvironment that generate s important inputs and outputs. In the \ncase of this study, several architectural layers require design, \nconfiguration, benchmarking, and evaluation  that accurately \nrepresent industry big data system implementations . These \nresearch activities could establish  a more mature  model for \nIDPS placement in evolving network architectures . Design \nscience methods guide the latter activities [26].  \nBig data clusters can have thousands of nodes. Attempting \nto secure individual servers poses several issues ranging from \nsignificant costs to lost computational resources. Important to \nthe artifact design  process  is the creation of  an IDS and IPS \ntesting environment  that result s in minimal disruption to \nexisting big data infrastructures. Additionally , the authors \nconstructed an  experimental setup similar to several local  small \nbusiness environment s that are readily  available , relatively \ninexpensive,  and relevant to a broad audience. Therefore, the \ntesting environment is limited to several small commodity \nvirtual machines (VM s) operating in physically distanced data \ncenters . The authors will briefly outline the network \narchitecture,  hardware , software used in the experimental \nenvironment.  \nB. Network Architecture  \nFig. 1 depicts the baseline network architecture used in this \nstudy. The experimental network emulates a small to medium -\nsized business with a 200 Mbps dedicated lease line between \nfour distinct physical locations. Connections are 1 Gbps copper \nfrom the demarc ation point to the LAN nodes. Each  server  is \nconnected to layer 2 switches followed by a layer 3 Cisco \nSystems enterprise class router.  \n \nFig. 1.  Perimeter -based security network architecture.  The cybersecurity servers labeled “CyberOne” to \n“CyberFour” illustrate the systems used to attack the big data \ncluster. The big data cluster includes  four servers labeled \n“SparkOne” to “SparkFour.” One streaming server is depicted \nas the data strea m located in the same local area network \n(LAN) as SparkOne. Four intrusion dete ction and prevention \nsystems are situated  between each big data server and its \nextrinsic networks.  \nC. Hardware  \nThe big data servers r un on parallel  Dell hardware [2 7]. \nThe hardware is manufactured on the same date and shipped in \nthe same container. The testin g server used the same single \nIntel CPU with 16 logical cores and 32 GBs of physical \nrandom -access memory. The baseline Intel CPU benchmark \naverage results from the Pass Mark version 10 performance test \n[29] are 2,799 M Ops per s econd for a single thread and 5,443 \nmega bytes  per second for data encryption.  \nCisco RV series routers with integrated firewalls exist \nbetween each Apache Spark node and the external network. \nCisco Firmware 1.0.3.55 is in use with the default firewall \nruleset . The authors added customized rules that allow the \ninternal LAN IP addresses  to communicate on the necessary \nApache HDFS and Spark ports. Subsequent ports are blocked  \n[28].  \nD. Big Data Systems  \nEach big data server  and streaming server  used equivalent  \nsoftwar e and versions. Systems ran on the Ubuntu  server  \n20.04.3 LTS operating system. Installed software included Java \n11, Python 3.8, Apache Hadoop 3. 2, and Apache Spark 3. 2. \nThe big data environment is comprised of five servers . This \nincludes one primary cluste r manager labeled SparkOne  and \nthree secondary work nodes labeled SparkTwo , SparkThree , \nand SparkFour . Apache Spark is tuned using optimal \nparameters such as those specified in [30] and [31]. HDFS \ndisks are balanced between nodes with DFS replicating three  \nblocks. The data stream denotes the indepe ndent Spark \nstreaming instance.  \nSparkOne is the primary node  in the testing environment \nused in this study . It is comprised of the  driver program. The \ndriver program executes the big data application’s main() clas s \nand generates the SparkContext [3 2]. SparkContext  is capable \nof using various big data resource managers. Tests in this study \nuse Yet Another Resource Negotiator (YARN) as the \ndistributed cluster manager  [33]. \nSparkContext helps communicate application jobs \ncontaining code in various forms such as Python and JAR files \nto the executors on the worker or secondary nodes in the \ncluster. YARN has two primary high -level components labeled \nthe NodeManager and ResourceM anager. Secondary nodes in a \nbig data cluster managed by YARN  each have a NodeManager. \nIts function is to manage containers on each server. Containers  \nencompass resources such as network, disk, CPU , and \nmemory. These are allocated properly to facilitate ta sk \nexecution . The YARN ResourceManager consists  of the \nApplicationsManager and the Scheduler . While the Scheduler \ndetermines the necessary resources for each application the\n\n[Página 5]\n(IJACSA) International Journal of  Advanced Computer Science and Applications,  \nVol. 1 4, No. 9, 202 3 \n994 | P a g e  \nwww.ijacsa.thesai.org  ApplicationsManager identifies which container the application \nwill use and subseq uently monitors their task execution  [33]. \nApache Spark and HDFS replicate between three secondary \nbig data servers. The secondary or worker nodes labeled  \nSparkTwo, SparkThree, and SparkFour  contain executor \nprocesses. An executor  process remains throughou t the runtime \nof tasks that each worker is allocated by the cluster manager. \nEvery application receives it s own executor process and/or \nprocesses as necessary. The driver program  on SparkOne is \nconfigured to listen  for executor process communications from \nthe secondary nodes until the job is completed . Per Apache \nSpark documentation in  [32], when possible, the driver \nprogram should be on the same local area network as the \nworker nodes due to the latter communication . In the \nexperimental network design, the worker nodes are physically \ndistanced. Therefore, Spark is optimized to open local remote \nprocedure  calls on the worker LANs [3 2]. \nE. Attack Systems  \nAlthough the cybersecurity servers ran on the same \nhardware as the big data servers, they used different softw are. \nCyberOne, Cyber Two, CyberThree, and CyberFour each \ndelineate a server used to carry out cyber -attacks  on the big \ndata cluster. The s oftware includes the Kali Linux operating \nsystem running the 5.14 kernel. Kali Linux is an open -source \noperating system  based on Debian Linux. It is designed for \nnumerous information security objectives such as reverse \nengineering, forensics, pen testing, and research [3 4]. \nF. Intrustion Detection and Prevention Systems  \nConsistent with Fig . 1, the baseline IDS and IPS systems  \nare located between the cyber -attack  and big data systems. \nRegardless, the authors manipulate the placement of these \nsystems throughout  each experimentation. As a simulated \nconstruct in the research methodology, the authors propose that \nIDS and IPS archit ecture placement predicts data streaming \nperformance between worker nodes. Performance evaluation of \nthis potential construct is an important step toward advancing a \nfuture ID PS placement framework for physically distanced big \ndata systems.  \nThe authors imp lemented Snort and Suricata, two popular \nopen -source IDS and IPS systems. Snort is developed by Cisco \nSystems. It serves as a leading intrusion detection engine and \nrule set for Cisco next -generation firewalls and IPSs . Its \nmechanisms for detecting and pre venting security threats \ncontinue to evolve . However, a fundamental capability during  \nthis writing is the formation of rules. In contrast to traditional \nmethods such as signature -based detection, rules focus on \nvulnerability detection [35]. Suricata is dev eloped by the Open \nInformation Security Foundation  (OISF). Similar to Snort, \nSuricata can use rules to detect and block cyber -attacks  [36].  \nVersion 2.9.7 of Snort ran with libpcap version 1.9.1 and \nversion 8.39 of the payload detection rules. Suricata test ing \nuses version 6.0.6  with the emerging threats open ruleset. The \nauthors customized the  latter default  Snort and Suricata rulesets \nto secure the distributed nodes. The rulesets are parallel in \ncount and type (e.g. alert, drop)  to control  significant vari ations \nin resource contention.  Suricata and Snort use the same rules in \nthe tests , except for minor incompatibilities. Where incompatible, the rules are adjusted to perform the same action \nin both IDSs  at parallel throughput rates.  \nSnort and Suricata  run o n the same server hardware and \noperating system s as the big data servers. A second NIC allows  \nthe servers to act as gateways between trusted and untrusted \nnetworks. The servers communicate between the local area \nnetworks using Transport Layer Security (TLS ) and Secure \nShell (SSH) Protocols . Ubuntu server 20.04.3 LTS is \nconfigured using OpenSSH  version  8.2 and OpenSSL  version  \n1.1.1.  \nG. Benchmarks  \nThe authors developed custom benchmarks  to identify how \nbig data clusters perform under various IDS physically \ndistanced network architectures . The benchmarks perform two \nsignificant network load functions, 1) stream ing unstructured \ndata to the Spark big data cluster  and 2) flooding the Spark \nnodes via DDoS attacks.  Network and system benchmarking \nuses version 16m of the nmon source code  to measure network \nperformance . Originally developed by IBM, nmon  is an open -\nsource Linux project that monitors system resource utilization . \nPerformance metrics include CPU, disk, memory, and \nnetworking  [37]. \nThe authors follow the design science methodology [25] to \ndesign and implement an IDS placement experiment for \nphysically distanced big data systems. Next, the authors \nconstruct a series of tests to determine how IDS locations \ninfluence real -world distributed worker  nodes . \nIV. RESULTS  \nEach of the tests followed a n eight -step process, 1) network \narchitecture is determined and implemented, 2) IDPS  locations \nare identified and configured, 3) IDPS  customized rulesets  are \nimplemented, 4) the big data system  cluster  is started and \ntested as operational, 5) data stream s to the cluster are invoked, \n6) DDoS attacks are executed, 7) the benchmarks are run, and \n8) the researchers maintain and monitor the testing \nenvironment for anomalies. Each of the t ests was repeated \nthree times  to ensure  saturation existed in the results . \nA. Test 1 Perimeter -Based Security Results  \nFig. 1 illustrates the IDPS  placement location for the first \ntest. The cloud represents the leased line between the \ngeographical sites. Below the cloud icon is the selected IDPS  \nsolution followed by the Apache Spark cluster.  Network \narchitecture in the first test follows Cisco Systems ’ best \npractices for a collapsed data center and LAN core [38]. Within \nthis design, a hardware -based IDPS  is situated between the \npublic untrusted  and private trusted network s. Test one \nincludes a traditional perimeter Cisco Systems IDPS . \nIndividual Spark nodes are networked in a single VLAN \nconnected through the collapsed core.  \nIn contrast to the network architecture in Fig. 1, CyberOne \nthrough CyberFou r servers are not deployed for tests 1 -3. In \neach of these tests, typical network traffic is present void of \nany DDoS attacks.  \nBenchmark metrics are specific to the big data systems \nunless otherwise specified. During the data stream, HDFS is\n\n[Página 6]\n(IJACSA) International Journal of  Advanced Computer Science and Applications,  \nVol. 1 4, No. 9, 202 3 \n995 | P a g e  \nwww.ijacsa.thesai.org  writing 128 MB  block s to disk on all three Spark worker nodes \nat a constant rate. Inconsequential wait time exists on disk \nreads and writes. Average CPU utilization per thread  or \n“CPU%”  on the big data worker nodes is 4.3% during the first \ntest. The average time a proce ss waits  for an input -output (I/O) \nto complete or “wait%” is 0.3. The a verage number of \nprocessor context switches per second is 1,728 , identified as \n“PWps ” hereafter . \nThe authors measured network performance between each \nof the Spark nodes  using four metrics. Metrics are captured on \nthe worker node network interface cards. The first performance \nvariable measure s the average number of all network packet \nreads per second  (APRps) . The second  variable  captures  the \naverage number of  all network packet writ es per second  \n(APWps). The measure “APIORk Bs” refers to the amount of \nnetwork I/O read traffic in kB per second  sent between the \nservers. The fourth metric, “APIOW kBs,” indicates the amount \nof network I/O write traffic in kB per second sent between the \nservers.  \nFig. 3 illustrates the average n etwork I/O (KB/s) on each \nApache Spark node in tests 1 -3 while Fig . 4 demonstrate the \naverage n etwork I/O (KB/s) on each Apache Spark node in \ntests 3 -6. \nIn the perimeter -based network architecture , the average  \nAPRps re ads per second are 637 across all Spark worker nodes. \nThe a verage APWps writes per second are 620. The a verage \nAPIORkBs read traffic between all Spark  worker nodes is 80 \nwhile APIOWkBs is 78. The authors reconfigured the network \narchitecture  in the subsequ ent test  to provide further insight \ninto IDPS  placement impact on distributed big data systems . \n \nFig. 2.  Perimeter -less security network architecture.  \nB. Tests 2 -3 Perim eter-less Security Results  \n Fig. 2 demonstrates the big data network designed for  \ntests two and three. Network architecture uses a modified \nperimeter -less design proposed by Kotantoulas  [39]. In contrast \nto the traditional perimeter IDPS  location in Fig. 1, every big \ndata worker node is in a zero  trust network . The authors \ndesigned an SD-WAN trust boundary  to secure each big data \nnode . The boundary consists of Snort and Suricata intrusion detection and prevention  security gateways . Similar to the \nvirtual software defined perimeter (vEPC) proposed by Bello et \nal. [40] , this study’s  zero trust software -based system acts as a \nsecurity gateway for all distributed servers. Sparkone through \nSparkfour are designed to operate securely in most cloud  \narchitectures in this model  by integrating an SDN security \nstack on each physically distanced serv er. The integrated  IDPS  \ngateways  control and authorize incoming and outgoing \nnetwork communication . The design emulates the trust \nboundary surrounding the cloud edge in [39] using  the SSH \nand TLS protocol s. Gateways authenticate and connect the \ndistributed  systems using  a 3072 -bit key generated by the \nRivest –Shamir –Adleman (RSA) algorithm . \nBenchmark results for test 2 with Snort SDN gateways \nshow the wait% is 0.413% and CPU% is 12.54%. Results from \nthis study show that CPU resource consumption is over two \ntimes greater in the zero trust architecture than the perimeter \nnetwork design. Test 3 with Suricata  SDN gateways results in \n11.05% CPU% and 0.342% wait%. Similar to the perimeter -\nless design in test 2, test 3 used considerably more CPU \nresources than test 1. Despite similar rulesets, Suricata  SDN \ngateways used slightly less CPU than Snort.  \nIn the test 2 perimeter -less network architecture the average \nAPRps reads per second are 2,198 across all Spark worker \nnodes. The a verage APWps writes per second are 653. The \naverage APIORkBs read traffic between all Spark worker \nnodes is 298 in test 2 , APIOWkBs is 82. \n \nFig. 3.  Tests 1 -3 spark per node network I/O in KB/s . \nThe t est 3 network architecture had similar results  to test 2 . \nThe average APRps reads per second are 2,120 across the \ndistributed Spark systems. The a verage APWps is 611. \nAPIORkBs between the big data servers is 289  and \nAPIOWkBs is 77. Fig. 3 illustrates the average network I/O \n(KB/s) on each Apache Spark node in tests 1 -3. These results \nindicate that network traffic and network I/O are nominal when \nwriting to HDFS in all network architectures within this study. \nIn contrast, the number of packets the systems have to read is \nhigher in the perimeter -less network architectures. APRps is \nover three times higher in tests 2 and 3 than in  test 1.  \n020040060080010001200\n1234567891011121314151617181920Kilobits  \nSeconds\n\n[Página 7]\n(IJACSA) International Journal of  Advanced Computer Science and Applications,  \nVol. 1 4, No. 9, 202 3 \n996 | P a g e  \nwww.ijacsa.thesai.org  C. Test 4 Perimeter -Based DDoS Attack Results  \nTest 4 uses the network architecture  (Fig. 1), parallel to test \n1. Perimeter -based intrusion detection and prevent ion systems \nprotect the internal LANs of the Spark nodes. CyberOne \nthrough CyberFour are active in test 4. The cyber servers are \nconfigured to flood the big data cluster with  unlimited TCP \nSYN handshakes.  \nBenchmark results for the big data servers during the DDoS \nattacks parallel test 1 i n test 4. In test 4, the IDPS s prevented \nadditional CPU load and network load on the big data servers.  \nIn the test case, the hardware IPSs successfully blocked the \nDDoS attacks.  \nD. Tests 5 -6 Perimeter -less DDoS Attack Results  \nTests 5 and 6 are similar to test s 3 and 4 . However, DDoS \nattacks are administered  on the big data cluster. Tests 5 -6 use \nthe (Fig. 2) perimeter -less security network architecture. Test 5 \nuses the Snort -based SDN security boundary , while test 6 uses \nSuricata. CyberOne through CyberFour ar e active in tests 5 and \n6. The cyber servers execute DDoS attacks on  the big data \ncluster by flooding the servers with  unlimited TCP SYN \nhandshakes.  \nSnort and Suricata security gateways successfully protect \nthe big data systems  from DDoS attacks  in a zero trust network \nin tests 5 and 6 ; however, at the expense of local computational \nresource increases.  Results for test 5 with Snort SDN gateways \nshow the wait% is 0.308% and CPU% is 13.8%. CPU resource \nconsumption increases on average over 1% on the big data \nservers during the DDoS attacks. Test 6 with Suricata SDN \ngateways results in 11.95% CPU% and 0.337% wait%. DDoS \nattacks increased average CPU% by 0.9% across big data \nsystems. Suricata SDN gateways used slightly less CPU than \nSnort SDN gateways during the  DDoS attacks.  \nWithin the test 5 perimeter -less network architecture the \naverage APRps reads per second are 4,762 across all \ndistributed by data secondary nodes. The average APWps \nwrites per second are 626. The average APIORkBs traffic \nbetween the distributed systems is 425. APIOWkBs is 79.  \n \nFig. 4.  Tests 4 -6 spark per node network I/O in KB/s . The Suricata gateways in test 6 have  average APRps reads \nper second of 4,311  across the distributed Spark systems. \nAverage APWps is 6 61. APIORkBs between the big data \nservers is 416 and APIOWkBs is 81. Fig. 4 demonstrates the \naverage network I/O (KB/s) on each Apache Spark node in \ntests 3 -6. \nE. Test 7 Perimeter -Based DDoS Attack Results  \nTest 7 shares the same network architecture as test 1 and \ntest 4, illustrated in Fig. 1. To decipher how the DDoS attacks \naffect  the big data servers in the perimeter -based network \narchitecture without IDPS  protection, test 7 repeats test 4 but \nallow all network traffic from CyberOne through CyberFour to \nthe big data cluster. When the DDoS attacks are allowed \nthrough the perimeter IPSs in the Fig. 1 network architecture, \nresults show an average CPU% of 17.9% across all distributed \nbig data systems. Predictably, network packets increase in test \n7 compared  to tests 1 and 4. APRps is 2,895 while APIORkBs \nis 518. Test 7 has the highest APIORkBs of all network \nbenchmarks performed in this study.  \nF. Discussion  of the Results  \nThe results illustrate  that network traffic and network I/O \nhave  marginal  differences when writi ng to HDFS in the \nnetwork architectures  studied . CPU resources and network \ntraffic read by the operating systems increased in zero trust \nnetwork architectures. The most substantial differences were \nbetween tests 4 and 5. During the DDoS attacks, the big da ta \nservers required more  CPU resources in the perimeter -less \nsecurity network architecture. In test 5, APIORkBs are \nconsiderably higher at 425 than test 4 at 80. This additional \ntraffic is partly due  to the SDN security boundaries necessary \nto protect the systems in a zero trust network environment.  \nShifting compute resources closer to individual devices \nmay be  necessary as network security perimeters dissipate . \nHowever, zero trust architectures in the experimental \nenvironment reduced cluster performance. T herefore, \nadditional research is beneficial to optimize the design of  \nperimeter -less network environments . \nG. Limitations  \nSeveral environmental factors limit the results. Site -to-site \nnetworks were on leased 200 Mbps connections. Future studies \nmight consider  leased lines capable of establishing more robust \ndata streams to the distributed nodes. A subsequent restriction \nis the number of architectures and communication technologies \ntested. Similar to the architecture in [40], gateways allow for IP \nSecurity ( IPsec) or Transport Layer Security (TLS) protocols. \nFuture IDPS  SDN gateways could add this layer of encryption \nin a software -defined security boundary between geo-\ndistributed big data systems. The outlined limitations \nemphasize the need for future research t o investigate more \nextensive network architectures and IDPS  technologies for big \ndata system security.  \nV. CONCLUSION  \nAs the volume of data expand s, organizations require big \ndata systems to perform large -scale data analytics. One of \nseveral needs for these sy stems is effective intrusion detection \nand prevention strategies. This paper builds  a review of the 020040060080010001200140016001800\n1234567891011121314151617181920Kilobits  \nSeconds\n\n[Página 8]\n(IJACSA) International Journal of  Advanced Computer Science and Applications,  \nVol. 1 4, No. 9, 202 3 \n997 | P a g e  \nwww.ijacsa.thesai.org  literature on methods used to reduce cybersecurity threats in a \nrange of network architectures that big data systems operate.  \nFindings from literature sugge st intrusion detection and \nprevention systems can respond  to certain  security attacks . \nHowever, a potential disadvantage  of capable security systems  \nis the impact on big data system cluster performance. Using a \ndesign science approach, the authors develop an eight -step \nprocess to benchmark big data systems in varying network \narchitectural environments.  The new benchmark process is \ntested on real -time big data systems running in perimeter -based \nand perimet er-less network environments. During DDoS \ncyber -attacks, perimeter -based network architectures \noutperformed perimeter -less network  architectures.  This \nunderlines the importance of optimizing the design of zero trust \narchitectures for distributed big data s ystems.  \nREFERENCES  \n[1] D. Gümüşbaş, T. Yıldırım, A. Genovese, and F. Scotti, “A \ncomprehensive survey of databases and deep learning methods for \ncybersecurity and intrusion detection systems ,” IEEE Systems Journal , \nvol. 15, no. 2, pp. 1717 –1731, Jun. 2021, doi:  \n10.1109/JSYST.2020.2992966 . \n[2] I. D. Aiyanyo, S. Hamman, and H. Lim, “A systematic review of \ndefensive and offensive cybersecurity with machine learning,” Applied \nSciences, vol. 10, no. 17, p. 5811, 2020, doi: 10.3390/app10175811.  \n[3] N. V. Patil, C. Rama Krishna, and K. Kumar, “Distributed frameworks \nfor detecting distributed denial of service attacks: A comprehensive \nreview, challenges and future directions,” Concurrency and \nComputation: Practice and Ex perience , vol. 33, no. 10, pp. 1 -21, May \n2021, doi: 10.1002/cpe.6197 . \n[4] R. Rafiq, M. J. Awan, A. Yasin, H . Nobanee , A. M. Zain, and S. A. \nBahaj, “Privacy prevention of big data applications: A systematic \nliterature review ,” Sage Open , vol. 12, no. 2, Apr. 2022, doi: \n10.1177/21582440221096445 . \n[5] R. Mitchell and I.  R. Chen, “A survey of intrusion detection techniques \nfor cyber -physical systems,” ACM Com put. Surv. , vol. 46, no. 4, Mar. \n2014, doi: 10.1145/2542049 . \n[6] B. B. Zarpelão, R. S. Miani, C. T. Kawakani, and S. C. de Alvarenga, \n“A survey of intrusion detection in Internet of Things,” Journal of \nNetwork an d Computer Applications , vol. 84, pp. 25 –37, Apr. 2017, doi: \n10.1016/j.jnca.2017.02.009 . \n[7] V. Casola, A. De Benedictis, M. Rak, and U. Villano, “Security -by-\ndesign in multi -cloud applications: An opt imization approach,” \nInformation Sciences , vol. 454 –455, pp. 344 –362, Jul. 2018, doi: \n10.1016/j.ins.2018.04.081 . \n[8] R. Atat, L. Liu, J. Wu, G. Li, C. Ye, and Y. Yang, “Big data meet cyber -\nphysical systems: a panoramic survey,” IEEE Access , vol. 6, pp. 73603 –\n73636, 2018, doi: 10.1109/ACCESS.2018.2878681 . \n[9] R. Gift y, R. Bharathi, and P. Krishnakumar, “Privacy and security of big \ndata in cyber physical systems using Weibull distribution -based \nintrusion detection,” Neural Computing and Applications , vol. 31, no. 1, \npp. 23 –34, Jan. 2019, doi: 10.1007/s00521 -018-3635 -6. \n[10] S. F. Ochoa, G. Fortino, and G. Di Fatta, “Cyber -physical systems, \ninternet of things and big data,” Future Generation Computer Systems , \nvol. 75, pp. 82 –84, Oct. 2017, doi: 10.1016/j.future.2017.05.040 . \n[11] K. A. P. da Costa, J. P. Papa, C. O. Lisboa, R. Munoz, and V. H. C. de \nAlbuquerque, “Internet of Things: A survey on machine learning -based \nintrusion detection approaches,” Computer Network s, vol. 151, pp. 147 –\n157, Mar. 2019, doi: 10.1016/j.comnet.2019.01.023 . \n[12] N. Moustafa, B. Turnbull, and K. R. Choo, “An ensemble intrusion \ndetection technique based on proposed statistical flow features for \nprotecting network traffic of Internet of Things,” IEEE Internet of \nThings Journal , vol. 6, no. 3, pp. 4815 –4830, Jun. 2019, doi: \n10.1109/JIOT.2018.2871719 . \n[13] A. Yang, Y. Zhuansun, C. Liu, J. Li, and C. Zhang, “Design of intrusion \ndetection system for Internet of Things based on im proved BP neural network,” IEEE Access , vol. 7, pp. 106043 –106052, 2019, doi: \n10.1109/ACCESS.2019.2929919 . \n[14] Z. Tan et al. , “Enhancing big data security with collaborative intrusion \ndetection,” IEEE  Cloud Computing , vol. 1, no. 3, pp. 27 –33, Sep. 2014, \ndoi: 10.1109/MCC.2014.53 . \n[15] A. N. Jaber and S. U. Rehman, “FCM –SVM based intrusion detection \nsystem for cloud computing environment,” Cluster Computing , vol. 23, \nno. 4, pp. 3221 –3231, Dec. 2020, doi: 10.1007/s10586 -020-03082 -6. \n[16] M. Hafsa and F. Jemili, “Comparative study between big data analysis \ntechniques in intrusion detection,” Big Data and Co gnitive Computing , \nvol. 3, no. 1, pp. 1 -13, Dec. 2018, doi: 10.3390/bdcc3010001 . \n[17] F. M. Awaysheh, M. N. Aladwan, M. Alazab, S. Alawadi, J. C. \nCabaleiro, and T. F. Pena, “Security by design for big data frameworks \nover cloud computing,” IEEE Transactions on Engineering \nManagement , pp. 1 –18, Feb. 2021, doi: 10.1109/TEM.2020.3045661 . \n[18] A. Bocci, S. Forti, G. L. Ferrari, and A. Brogi, “Secure FaaS \norchestration in the fog: How far are we?” Computing, vol. 103, no. 5, \npp. 1025 –1056, May 2021, doi: 10.1007/s00607 -021-00924 -y. \n[19] Y. Wang, X. Z hang, Y. Wu, and Y. Shen, “Enhancing leakage \nprevention for mapreduce,” IEEE Transactions on Information \nForensics and Security , vol. 17, pp. 1558 –1572, 2022, doi: \n10.1109/TIFS.2022.3166641 . \n[20] O. Ohri menko, M. Costa, C. Fournet, C. Gkantsidis, M. Kohlweiss, and \nD. Sharma, “Observing and preventing leakage in MapReduce,” in \nProceedings of the 22nd ACM SIGSAC Conference on Computer and \nCommunications Security , New York, NY, USA, 2015, pp. 1570 –1581. \ndoi: 10.1145/2810103.2813695 . \n[21] A. M. Sauber, A. Awad, A. F. Shawish, and P. M. El -Kafrawy, “A novel \nhadoop security model for addressing malicious collusive wo rkers,” \nComputational Intelligence and Neuroscience , vol. 2021, pp. 1 -10, \n2021, doi: 10.1155/2021/5753948 . \n[22] P. Derbeko, S. Dolev, E. Gudes, and S. Sharma, “Security and privacy \naspects in MapReduce on clo uds: A survey,” Computer Science Review , \nvol. 20, pp. 1 –28, May 2016, doi: 10.1016/j.cosrev.2016.05.001 . \n[23] R. Poddar, T. Boelter, and R. Popa, “Arx: An encrypted database using \nsemantically secure encryption,” Proceedings of the VLDB Endowment , \nvol. 12, pp. 1664 –1678, Jul. 2019, doi: 10.14778/3342263.3342641 . \n[24] A. Nisioti, A. Mylonas, P. D. Yoo, and V. Katos, “From intrusion \ndetection to attacker attribution: A comprehensive survey of \nunsupervised methods,” IEEE Communications Surveys & Tutorials , \nvol. 20, no. 4, pp. 3369 –3388, Fourthquarter 2018, doi: \n10.1109/COMST.2018.2854724 . \n[25] S. T. March and G. F. Smith, “Design and natural science research on \ninformation technology,” Decision Support Systems , vol. 15, no. 4, pp. \n251–266, Dec. 1995, doi: 10.1016/0167 -9236(94)00041 -2. \n[26] A. R. Hevner, S. T. March, J. Park, and S. Ram, “Design science in \ninformation systems research,” MIS Quarterly , vol. 28, no. 1, pp. 75 –\n105, 2004, doi: 10.2307/25148625 . \n[27] “Dell technology,” Dell Inc , June, 2022 . [Online]. Available: \nhttps://www.dell.com.  \n[28] “Cisco routers and SD -WAN,” Cisco Systems , June, 2022. [Online]. \nAvailable: \nhttps://www.cisco.com/site/us/en/products/networking/sdwan -\nrouters/index.html.  \n[29] “Benchmarking & Diagnostic Software,” Passmark Software , June, \n2022. [Online]. Available: https://www.passmark.com.  \n[30] “Spark tuning guide on 3rd generation Intel® Xeon® scalable \nprocessors based platform,” Intel Corporation , August, 2021, [Online]. \nAvailable:  \nhttps://www.intel.cn/content/www/cn/zh/developer/articles/guide/spark -\ntuning -guide -on-xeon -based -systems.html.  \n[31] “Tuning Spark,”  The Apache Software Foundation , July, 2022. [On line]. \nAvailable: https://spark.apache.org/docs/3.2.2/.  \n[32] “Cluster Mode Overview,” The Apache Software Foundation , June, \n2022. [Online]. Available: https://spark.apache.org/docs/latest/cluster -\noverview.html.  \n[33] “Apache Hadoop YARN,” The Apache  Software Foundati on, June, \n2022. [Online]. Available:\n\n[Página 9]\n(IJACSA) International Journal of  Advanced Computer Science and Applications,  \nVol. 1 4, No. 9, 202 3 \n998 | P a g e  \nwww.ijacsa.thesai.org  https://hadoop.apache.org/docs/stable/hadoop -yarn/hadoop -yarn-\nsite/YARN.html.  \n[34] “Kali linux features,” OffSec Services Limited , June, 2022. [Online]. \nAvailable: https://www.kali.org/features.  \n[35] “Snort FAQ/Wiki,” Cisco Systems , July, 2022. [Online]. Available: \nhttps://www.snort.org/faq . \n[36] “Suricata user guide,” Open Information Security Foundation , July, \n2022. [Online]. Available:  https://suricata.readthedocs.io/en/suricata -\n6.0.6 . \n[37] “nmon for Linux ,” IBM, June, 2022. [Online]. Available: \nhttp://nmon.sourceforge.net.  [38] “Collapsed data center and campus core deployment guide,” Cisco \nSystems , June, 2022. [Online]. Available: \nhttps://www.cisco.com/c/dam/global/en_ca/solutions/strategy/docs/sbaG\nov_nexus7000Dguide_new.pdf.  \n[39] J. Kotantoulas, “Zero trust for government networks ,” Cisco Systems , \nJune, 2022. [Online]. Available: \nhttps://blogs.cisco.com/government/zero -trust-for-government -\nnetworks -6-steps -you-need -to-know.  \n[40] Y. Bello, A. R. Hussein, M. Ulema, and J. Koilpillai, “On sustained zero \ntrust conceptualization security for mobile core ne tworks in 5G and \nbeyond,” IEEE Transactions on Network and Service Management , vol. \n19, no. 2, pp. 1876 –1889, Jun. 2022, doi: 10.1109/TNSM.2022.3157248 .\n\n[Página 10]\n© 2023. This work is licensed under\nhttp://creativecommons.org/licenses/by/4.0/ (the “License”).  Notwithstanding\nthe ProQuest Terms and Conditions, you may use this content in accordance\nwith the terms of the License.",
      "metadata": {
        "filename": "Performance in Distributed Big Data.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\Performance in Distributed Big Data.pdf",
        "file_size": 898722,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:39.175497",
        "content_length": 51518
      }
    },
    "e473384f-6f46-40a1-8e91-8b965a349b2e": {
      "id": "e473384f-6f46-40a1-8e91-8b965a349b2e",
      "content": "[Página 1]\nDOM: A big data analytics framework  for mining Thai public opinions  Santitham Prom-on, Sirapop Na Ranong, Patcharaporn Jenviriyakul,  Thepparit Wongkaew, Nareerat Saetiew and Tiranee Achalakul Department of Computer Engineering, Faculty of Engineering King Mongkut’s University of Technology Thonburi Bangkok, Thailand   Abstract—This paper presents the development of DOM, a mobile big data analytics engine for mining Thai public opinions. The engine takes in data from multiple well-known social network sources, and then processes them using MapReduce, a keyword-based sentiment analysis technique, and an influencer analysis algorithm to determine public opinions and sentiments of certain topics. The system was evaluated its sentiment prediction accuracy by matching the predicted result with the human sentiment and tested on various case studies. The effectiveness of the approach demonstrates the practical applications of the engine. Keywords—opinion mining; big data analytics; MapReduce; public sentiment  I.  INTRODUCTION We, human being, have never been more connected through the emergence of social networks. Social networks, in terms of both data and users, have been exponentially growing and connect our lives together in various dimensions. We can connect with people across the planet with a touch of a finger. In every second, hundred thousands of messages are shared through social media such as Facebook, Twitter, Foursquare, Pantip, etc. They are about our life, feeling, experience and opinion. This practically represents the 21st century of our civilization, “The era of social network”.  Social media networks generated huge volumes of data. They have been use in various types of applications including public health [1], emergency coordination [2], news recommendation [3], and stock market prediction [4]. The data from social media networks gathered under the catch-all term, “big data”. However, as much as 90% of the data stored is \"unstructured,\" meaning that it is spontaneously generated and not easily captured and classified. Big data is only valuable if it tells a story.  The fuller the story your data tells, the better you’ll be able to take advantage of that data. While recognizing a trend can help you make better decisions, understanding the cause behind that trend is even more valuable. The organizations that can use stories to make sense of big data are going to excel. In this paper, we presents the developments of DOM (Data and Opinion Mining), a big data analytics engine that is capable of mining Thai public opinions regarding specific issues discussed on the social network sites, and its corresponding mobile solution for answering public opinions about events and locations. Software features and design will be discussed in Section II. Section III explains how the software was implemented using cloud-based technology. Section IV shows the evaluation of the DOM effectiveness in predicting the sentiment score of public opinions. Usages of DOM for different tasks are presented in Section V. Comparisons of DOM with respect to others and the future steps in the development are discussed in Section VI.  II. DOM A. Data Sources We collected data from four different data sources; Twitter, Facebook, Foursquare and Pantip, as described in Table 1. These social network data, if the locations can be specified, were collected in scope of Bangkok area. For Twitter data, we used Search API [5] provided from Twitter Inc. to collect tweets without any keywords. We collected approximated 15 million tweets or about 12GB uncompressed data each month. Each tweet contains multiple data fields, including time, username, user followers, retweet, count, location, and the textual comment. For Facebook, we used Graph API [6] developed by Facebook Inc. Unlike Twitter, we can only request and collect data from Facebook fanpage which consists of posts and comments of specific topics. We collected Facebook data about 5,000 messages, which is approximately \nTable 1. Sources of Social Network Data. Source Data Description Twitter Twitter messages, also known as tweets, are short 140-character text messages. Tweets are all public. Facebook Facebook data can only be retrieved if the privacy is set to public. They are in forms of status posts and Facebook Page posts. Foursquare Foursquare provides both text comments and review score of a number of places. Pantip Pantip data are in forms of webboard threads. It is one of the prominent Thailand online social communities.   2014 International Conference on Computer, Control, Informatics and Its Applications\n978-1-4799-4575-7/14/$31.00 c/circlecopyrt2014 IEEE 1\n\n[Página 2]\n4MB per fanpage each month. Graph API provides attributes including time, username, number of Like, location as well as textual comment for each message. For Foursquare, the sitation is like Graph API. Foursquare provides their API for developers to gather data named Venues and Tip search API [7]. Foursquare provides comments of places. In each month we collected approximated 500 messages or 0.4 MB per place. Foursquare data includes time, username, like count, location and the textual comment. Our last data source is Pantip.com, one of the prominent Thailand online social communities. We developed a web crawler to gather the data on this website, since they do not provide an API to gather data. The web crawler was designed to have features like Search API. First we simulated the browser by set user-agent to be Mozilla, and then assigned the keywords to the search form of web and submitted the request. We found that approximately 300 messages or about 0.2MB were collected for each topic. Each Pantip thread contains time, username, like count and the text comment. B. System Architecture  We categorized components into two sides: server-side and client-side. The architecture design of our whole framework is illustrated in Figure 2. The components of DOM engine are classified into server-side which is cloud-based cluster. DOM engine is responsible for collecting, analyzing data and distributing the analyzed data to client-side. AskDOM components are client-side. The client-side requests the analyzed data, queries and displays them to end-users. Workflow of our framework is as follows.  Public messages are collected from social networks, blogs and forums using DOM’s crawler module. All collected messages are stored in MongoDB, an unstructured database. After that each message is then processed using basic Natural Language Processing (NLP) technique to parse the text data, categorize its topic, compute its sentimental score and analyze its influences. DOM also uses MapReduce technique based on Apache Hadoop framework to reduce the processing time. DOM periodically processed the data to compute their sentimental score. Finally AskDOM, the mobile application, gets the analyzed data, queries and displays the information to users according to the inquired topics.  In this paper, we focus the usage of DOM as a Thai public opinion mining framework to track social issues and provide sentiment rating and information of point of interest (POI) based on public opinions. However, the core functions of DOM engine was designed to support dynamic data. There are several features that could be added or further developed to provide additional functionality (e.g. adding more data sources, supporting other languages). Since DOM is cloud-based engine, scalability is also available. Furthermore DOM can be easily applied in various types of usage, either community side or commercial side.  There are case studies in section IV that shows some potential usage of DOM.  The current version of DOM consists of the following: modules: C. MapReduce Framework Since huge data are involved in this project, MapReduce [8], the high performance computing technique, is used. This is because if the data is to be processed sequentially, the processing time would be too large for the practical application. MapReduce technique on Apache Hadoop framework is therefore the best way to accelerate the analysis speed. In this paper, the MapReduce technique separates the mining process into two main steps; Map and Reduce. Map function takes the entire text input, breaks it into subsets to be evaluated for their sentiment scores and distributes them to worker nodes.  Reduce function combines the resulting sentiment scores from each small worker nodes by grouping keywords of specific topics of interest and summarizing the sentiment scores into final results.  D. Sentiment Analysis In this work we targeted words in which opinions are expressed in each sentence. A simple observation was that these sentences always contain sentiment words (e.g. great, good, bad, worst). To simplify the process, if the sentences do not contain any sentiment words, their sentiment values will be neutral (non-opinion). So we designed our framework to classify the sentiment of each sentence based on its sentiment words and the combination of them. Furthermore we designed the system to be able to process Thai conditional sentences, which are sentences that describe \n Fig. 1 Twitter activity heatmap in Bangkok area.  \n Fig. 2 Conceptual framework of DOM and its corresponding mobile application, AskDOM.  2\n\n[Página 3]\nimplications or hypothetical situations and their consequences. For example, in the sentence like ‘I like the location of this company but I do not like their staffs.’ The sentiment of ‘location’ is positive but negative on ‘staffs’. We found that most conditional sentences contain modifiers and conjunctions (e.g. but, and, or).  To classify each message as positive, neutral or negative, we employed a lexicon-based algorithm to measure sentiment score of each message. We defined five corpora including positive words, negative words, modifier, conjunction as well as name of point of interest. Each word in two sentiment corpora, positive words and negative words, contains sentiment rating ranging from -5 to 5. The examples of our corpuses are shown in Table 2. DOM detects and matches words and its sentiment polarity by using these corpora. Since the nature of Thai sentence structure is continuous without any whitespace breaks between words, we need to tokenize each sentence into group of words In this process we used ‘LexTo’ [9], the opensource Thai word tokenize tool, to tokenize words in each sentence and then store them as arrays using the longest word matching algorithm [10]. The example of this procedure is shown in Figure 3. DOM generates small jobs to detect words of each sentence in parallel. First of all, DOM filters the non-related sentences out by matching words with name of POI corpus. After that only sentences that relate to specific topics of interest (in this case is point of interest) would remain. DOM then iteratively matches sentiment keywords with remaining corpuses. If there are sentiment words in array, DOM collect its sentiment score and summarize at the end of each sentence. DOM then automatically classifies each sentence into sentiment group; positive, neutral and negative, depending on its score band (the range of distributed sentiment score).  DOM not only determines keyword from sentences, but also determines context of each sentence. The positions of words, modifiers, conjunctions as well as emoticons are also determined in our framework. In some cases these words can be important clues to emphasize the mood of the sentences. Especially for the modifier keywords, they can invert the sentiment score if their positions are adjacent to the sentiment words as illustrated in Figure 4.  \nTable 2. The examples of sentences in the corpora. # Type of Corpus Word Value 1 Positive words เท ห  (smart) 3 ด  (good) 3 เย  ยม (best) 4 2 Negative Words เส  อมโทรม (decadent) -3 แย  (bad) -3 ห วยแตก (worst) -4 3 Modifiers ไม  (not) -1 ค อนข าง (likely) 0.5 ท  ส ด (best) 1.5 4 Conjunctions แต  (but) 2 และ (and) 1 รวมไปถ ง (including) 1 5 Name of places สวนล มพ น  (Lumphini Park) - สยาม (Siam) - จต จ กร (Chatuchak market) -  \n Fig. 3 Example of Thai word tokenization  \n Fig. 4 Example of Thai Sentiment Analysis   3\n\n[Página 4]\nE. Influencer Analysis The rise of social media platforms such as Twitter, with their focus on user-generated content and social networks, has brought about the study of authority and influence over social networks to the forefront of current research. For companies and other public entities, identifying and engaging with influential authors in social media is critical, since any opinions they express can rapidly spread far and wide. For users, when presented with a vast amount of content relevant to a topic of interest, sorting content by the source’s authority or influence can also assist in information retrieval. In the social network community, a variety of measures were designed for the measurement of importance or prominence of nodes in a network [11, 12]. In the following, we will briefly summarize the centrality measure that we have used to describe possible candidate indicators for the power of influential in message diffusion. For DOM engine, we have used “Degree centrality” to identify influential users in the Twitter’s networks.   Degree centrality is the simplest centrality measure, as illustrated in Figure 5. The degree of a node i denoted by ki, is the number of edges that are incident with it, or the number of nodes adjacent to it. For networks where the edges between nodes are directional, we have to distinguish between in-degree and out-degree. The out-degree centrality is defined as 𝐶!!𝑖=  𝑎!\"!!!!                (1) where aij is 1 in the binary adjacency matrix A if an edge from node i to j exists, otherwise it is 0. Similarly, the in-degree centrality is defined as 𝐶!!𝑖=  𝑎!\"!!!!                                 (2) where i describes the node i and aji is 1 if an edge from node j to i exists, otherwise it is 0. F. AskDOM: Mobile Application To utilize DOM to its fullest extent, we developed AskDOM, a mobile solution designed to use DOM to provide a means for general publics to help improving their own communities by providing reviews, feedbacks, and rating of service providers automatically analyzed from public opinions on social networks (Twitter, Facebook, Pantip and Foursquare). AskDOM comprises two important modules: (a) front-end interface with features designed to connect users to service providers such as I-Share (direct feedback), Map (traffic and incident map), Anomaly (ab-normal situations reports), and (b) DOM Engine, the back-end system that periodi-cally gathers and processes social network data, performs public sentiment analysis, determines relationship influencers, and conducts natural language processing for both Thai and English. The integration of both modules will increase the transparency of the service businesses, make the agencies more accountable for their service quality, and provide a means for general citizens to involve with the improvement of the public services in terms of both information availability and improvement. Such an involvement will improve not only the quality of service, but also create a sense of community to the general citizens that they have to be part of the social function. •  III. IMPLEMENTATION Figure 7 shows the overall implementation architecture of DOM engine. The structure has three main components which are Server, Core Service and I/O.  A. Server Server section consists of three components; Ubuntu server, MongoDB as well as Apache Hadoop. We implemented DOM engine based on Apache Hadoop MapReduce which on Ubuntu server. MongoDB, the famous unstructured database was also used in this framework. The unstructured database is often highly optimized key–value stores intended for simple retrieval and appending operations to improve the performance in terms of latency and throughput. B. Core Service Core Service, the main part of our framework, consists of three components.  1) Data Crawler: This module provides automatically raw data feed from social network and stores them in the database, MongoDB. Each crawler code is specific for each social network or websites.  2) Data Preprocessing: This component prepares raw data to be ready for the analysis part by tokenizing Thai and \n Fig. 5 Simulation of Influencer network graph in the Twitter’s networks  \n Fig. 6 AskDOM Application  \n Fig. 7 DOM engine architecture  4\n\n[Página 5]\nEnglish words from sentences and removing outliers and reformatting data. Then the cleaned data will be sent to Data Analysis part. 3) Data Analysis: There are two main analyses in this component:  a) Sentiment Analysis evaluates sentiment in twitter text and find people’s mood in particular topic. For example, how people think about traffic in Bangkok. b) Influencer Analysis determines people’s positions in network, which indicate how influential they are. The influential people are more likely to acquire connections and have more connections. C. I/O I/O, the web-service implemented using PHP, receives the result from Core Service and then sent them to client-side to display in JSON format. Since the number of data in social network is increasing every second, using the static resources (e.g. static server) may not be practical. So we designed to run DOM on the cloud. Cloud provides ability to add a blob storage depending on the size of data. Furthermore DOM has ability to scale the number of processer. In other word, DOM can increase or decrease the number of mapper and reducer for running job.  IV. VALIDATION To validate the effectiveness of DOM, we conducted a subjective experiment to assess the sentiment prediction accuracy. In the following, we will describe the validation procedure and discuss on validation results. A. Validation Parameter • 184,184 messages from Facebook, Twitter and Foursquare (both positive and negative messages) divided into short and long messages, including 172,717 short messages (≤\t\r 150 characters) and 11,467 long messages (> 150 characters). • 12 subjects (6 males and 6 females) were participated in the experiment. They were students at the Computer Engineering Department, King Mongkut’s University of Technology Thonburi, Thailand. B. Validation method 1. For the human end, 184,184 messages were divided into 12 parts, each of which was assigned to each subject. They classified the messages in to positive and negative classes. 2. For DOM engine, 184,184 messages were classified by the engine into positive and negative classes. 3. The results of both human and DOM were compared and analyzed together to assess the system prediction accuracy. C. Validation results Table 3 and 4 shows the comparision results of 12 students and DOM engine. We found that DOM engine can classified messages and do sentiment analysis with accuracy over 75%. The accuracy of DOM engine is in the standard of text classification [13], so DOM engine is practical to use in social network analysis and can be applied to many dimensions in the real word.  V. CASE STUDIES In addition to the evaluation of the system effectiveness, we tested DOM engine further on various case studies that were of interest of Thai public during the time periods. Each case study aims to explore either specific social or political issue that people were discussed widely on the Internet, thus offers a summary of Internet public opinions of that issue. A. Political opinion: #prayforthailand Around the end of 2013, citizen of Bangkok faced with multiple rounds of political protests, and violent acts toward both protesters and officers. Hashtag “#prayforthailand” is one that was frequently used in social media to express the concerns over the situation. Different opinions were expressed regarding this political issue. We used DOM to mine the general public opinions that were expressed in the social network to determine the political climate at that time. We collected tweets around Bangkok area that contain the hashtag “#PrayForThailand.” There were over 100K tweets collected from 29 November to 7 December 2013. We implemented Naïve Bayes and Support Vector Machine (SVM) to DOM engine to classify political opinions into six predefined categories as shown in Table 5. DOM can accurately put tweets into categories with more than 85% accuracy. \nTable 3. Summary of prediction accuracy. Message Type Positive Comment Accuracy (%) Negative Comment Accuracy (%) Total  Short  79.75 56.33 75.99 Long  86.53 38.95 81.29 Total  80.19 55.57 76.32 Table 4. Detail analysis of the system effectiveness. Msg. type TP FP TN FN Precision Accuracy (%) Short  115,643 12,103 15,613 29,358 0.905 75.99 Long  8,830 771 492 1,374 0.919 81.29 Total  124,473 12,874 16,105 30,732 0.906 76.32  5\n\n[Página 6]\nB. Bangkok traffic congestion ranking Bangkok’s traffic problem is one of the most serious problem that urban citizen have been facing in their daily life. Knowing such information on which streets the traffic jams often occur would allow citizens to prepare to encounter the problem and allows the government to find a way to solve it. We used DOM engine to track traffic jams keywords, name of streets, intersections as well as famous places in Bangkok, Thailand that contained in public tweets, and then rank the streets that were mostly mentioned about traffic jam based on 22K tweets collected from 17 February to 8 March 2014.  The results as shown in Table 6 are consistent with what Thailand’s Department of Highways hotline gathered the statistics from phone calls. However using DOM engine is much faster and cheaper. VI. DISCUSSION AND CONCLUSION This paper presents the development, evaluation, and case studies of DOM, a big data analytics framework for assessing public sentiments of specific social issues. DOM, which is an opinion mining and sentiment analysis engine, is encapsulated as a mobile application known as AskDOM, that allows users to interact and find information of places suggested by the sentiment ratings. We have demonstrated both accuracy, as discussed in Section IV, and generalizability, as shown in Section V, of the engine in the analysis of various topics that are relevant to public interests. Further improvements are still needed to make DOM engine more adaptive and robust. First, the sentiment score associated with each keyword is currently context independent and come mainly from the manual adjustment by the administrator. A context-dependent keyword-score association study is needed for each of the task required. After obtaining these related associations from different contexts, rules can be derived so that the system can work effectively on different tasks. Second, public opinions usually contain a lot of personal messages that are irrelevant to the places under discussion. A filter that is capable of detecting the context of the message is required. ACKNOWLEDGEMENT We would like to thank for the financial support the Faculty of Engineering, King Mongkut’s University of Technology Thonburi through the research grant (to SP) and the Office of Higher Education Commission through the National Research University (NRU) grant, fiscal year 2011-2013 (to TA). REFERENCES [1] M. J. Paul and M Dredze, A Model for Mining Public Health Topics from Twitter. Technical Report. Johns Hopkins University. 2011.  [2] H. Purohit, A. Hampton, V. L. Shalin, A. P. Sheth, J. Flach, and S. Bhatt, “What Kind of #Conversation is Twitter? Mining #Psycholinguistic Cues for Emergency Coordination,” Computers in Human Behavior, vol. 29, pp. 2438-2447, November 2013. [3] O. Phelan, K. McCarthy, and B. Smyth, “Using Twitter to recommend real-time topical news” Proceedings of the third ACM conference on Recommender systems, New York City, NY, USA, 22-25 October 2009. [4] J. Bollen, H. Mao, and X.-J. Zeng, “Twitter mood predicts the stock market,” Journal of Computer Science, vol. 2, 1-8, March 2011.  [5] Twitter Search API, https://dev.twitter.com/rest/public/search [6] Facebook Graph API, https://developers.facebook.com/docs/graph-api [7] Foursqaure API, https://developer.foursquare.com [8] Sathya, S., Jose, M.V. Application of Hadoop MapReduce technique to Virtual Database system design. Emerging Trends in Electrical and Computer Technology (ICETECT), 2011 International Conference: IEEE, 2011 [9] Lexto, www.sansarn.com/lexto/ [10] Haruechaiyasak, C., Kongthon, A. LexToPlus: A Thai Lexeme Tokenization and Normalization Tool, The 4th Workshop on South and Southeast Asian NLP (WSSANLP) International Joint Conference on Natural Language Processing, Nagoya, Japan, 14-18 October 2013. [11] L. C. Freemann, Centrality in social networks: I. conceptual clarification, Social Networks 1 (215-239). [12] C. Kiss and M. Bichler, “Identification of Influencers - Measuring Influence in Customer Networks,” Decision Support Systems, vol. 46, pp. 233–253, December 2008. [13] Si, J., Mukherjee, A., Liu, B., Li, Q., Li, H., Deng, X. (2013) “Exploiting topic based twitter sentiment for stock prediction” The 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August 4-9, 2013.       \nTable 5. Summary of opinions with “#prayforthailand.” Opinions Percentage Oppose to the government 29.45 Loyal to the king 20.91 Feeling depressed about the situation 15.61 Oppose to both government and protests 0.82 Oppose to protesters 0.01 Others 33.2  Table 6. . Bangkok traffic congestion ranking Rank Streets / Intersections Percentage 1 Ladprao - Paholyothin 19.47 2 Vibhavadi - Rangsit 11.62 3 Petchaburi 7.76 4 Sukhumvit 4.71 5 Ramkumhaeng 4.13 6 Others 52.31   6",
      "metadata": {
        "filename": "prom-on2014.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\prom-on2014.pdf",
        "file_size": 2812558,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:39.460714",
        "content_length": 25646
      }
    },
    "595ef6c6-2dcb-4304-bf72-45fcd35d46e2": {
      "id": "595ef6c6-2dcb-4304-bf72-45fcd35d46e2",
      "content": "[Página 1]\nDOI referen ce number: 10.18293/SEKE2016 -166 \n Quality Assurance for Big Data  Application – Issues , Challenges, and Needs  \nChuanqi Tao                                                                                                   Jerry Gao  \n    Computer Science&Engineering  Department                                                                Computer Engineering Department  \nNanjing University of Science and Tech nology                                                         San Jose State University , San Jose, USA  \nNanjing, China                                                                                 Taiyuan University of Techno logy, Taiyuan, China                       \ntaochuanqi@njust.edu.cn                                                                             Corresponding to:  jerry.gao@sjsu.edu  \n \nAbstract —With the fast advance of big data technology and \nanalytics solutions, building high -quality big data computing \nservices in different application domains is becoming a very hot \nresearch and application top ic among academic and industry \ncommunities, and government agencies. Therefore, big data based \napplications are widely -used currently, such as recommendation, \npredication , and decision system. Nevertheless, there are \nincreasing quality problems resulting i n erroneous testing costs in \nenterprises and businesses. Current research work seldom \ndiscusses how to effectively validate big data applications to assure \nsystem quality. This paper focuses on big data system validation \nand quality assurance, and includes  informative discussions about \nessential quality parameters, primary focuses, and validation \nprocess. Moreover, the paper discusses potential testing methods \nfor big data application systems. Furthermore , the primary issues, \nchallenges, and needs in testin g big data application are presented.  \nKeywords — Quality assurance, big dataapplication quality \nassurance , big data validation . \nI.  INTRODUCTION  \nAccording to IDC [1], the Big Data technology market will \n\"grow at a 27% compound annual growth rate (CAGR) to \n$32.4 b illion through 2017 \".Today, with the fast advance of \nbig data science and analytics technologies , diverse data \nmining solutions, machine learning algorithms, open-source \nplatforms & tools, and  big data  database technologies  have \nbeen developed, and become available to be used for big data \napplications . This suggests that big data computing and \napplication services bring  large -scale  business requirements \nand dema nds in  people ’s daily life . Big data -based \napplication system is widely -used nowadays , such as \nrecommendation system, predictions, recognized patterns, \nstatistical report applications, etc.  Emergent big data \ncomputing and services can be used in many disciplines and \ndiverse applications, including business management, \nlibrary science, energy and environment, education, \nbiomedical, healthcare and life science, social media and \nnetworking, smart city and travel, and transportation, etc .[2]. \nNevertheless , due to  the huge vol ume of generated data, the \nfast velocity of arriving data, and the large variety of \nheterogeneous data, the big data based applications brings \nnew challenges and issues for QA engineer s. For instance, it \nis a hard job to validate the co rrectness of a big data -based \nprediction system due to the large scale data size and the \nfeature of timeliness. Therefore, Big data quality validation \nand big data -based application system quality assurance \nbecomes a critical concern and research subject.  Although \nthere has been a numerous of published papers [ 2-6] \naddressing data quality and data quality assurance in the \npast, seldom researches focus on validation  for big data \napplication  quality . There is an emergent need in research work to quality study  issues and quality assurance solutions \nfor big data applications.  \nThis paper is writ ten to provide our perspective  view s on \nbig data system validation for quality assurance . The paper is \norganized  as follows.  Section II discusses the typical types \nof big data systems and covers the essential quality \nparameters and their associated factors . Section III reviews \nand compares the existing testing methods for big data \nsystem validation. The major issues, challenges, and needs \nare presented in Section IV. Conclusions are in Section V.  \nII. UNDERSTANDING QUALITY ASSURANCE FOR  BIG \nDATA  APPLICATION SYSTEM  \nThis section discusses the scope and process of quality \nassurance for big data application system s. Moreover, it \ncovers th e primary quality parameters with  related  factors. \nBig data  applications  have the following unique features:  \n(a) statistical computation based on  multi -dimensional \nlarge -scale data sets, b)  machine -learning and knowledge \nbased  system evolution , c) intelligent decision making with \nuncertainty , d) non-oracle function s, and e) complicated \nvisualization.  These unique features  bring more interesting \nquality  assurance and QoS requirements, challenges, and \nneeds.  Based on the recent feedbacks from engineers at \nSilicon Valley, how to assure the quality of  big data -based \napplication system s becomes a critical concern  and research \nsubject  currently . \n \n \nFigure 1 The Typical Types of Big Data Application Systems\n\n[Página 2]\nA. Scope and Process of Big Data Application Quality \nAssurance  \nBig data applications provide services for prediction, \nrecommendations, decisions support through large -scale \ndata sets and complicated intelligent algorithms. Figure 1 \ndescribes the typical  types of  big data application s.  \nIn general, big data application quality assurance  refers \nto the study and application of various assurance processes, \nmethods, standards, criteria, and systems to ensure the \nquality of big data  system  in terms of a set of quality \nparameters. Figure 2 shows a sample  scope of v alidation for \nquality assurance of  big data applications.  \n \nFigure 2 The Scope of Validation for Big Data Application System Quality  \nCompared to conventional software testing, a  test process \nof big data based applications  primarily focuses on their \nunique  features , such as oracle problems, learning capability, \nand timeliness  testing. Figure 3 shows a  sample  test process \n(function testing) for big data application system validation .  \nThe testing process,  shown in Figure 3, includes the \nfollowing steps .  \nStep1 Big data system function testing , including rich \noracles, intelligent algorithms , learning capability , as well as \ndomain -specific functions;  \nStep 2  Big data system function testing,  includ ingsystem \nconsistency, security, robustness, and QoS;  \nStep 3  Big data system feature testing , checks usability, \nsystem evolution, visualization , and so on ;  \nStep 4  Big data system timeliness testing , targets time \nrelated  feature testing, including continuous  testing, real-\ntime testing, life-time testing, and others.  \nB. Quality factors for big data application validation  \nConventional system quality parameters  such as \nperformance, robustness, security, etc., can be applicable  \nonto big data systems. They are listed below . \n- System Performance –This parameter  indicates the \nperformance of the system, such as availability, \nresponse time, throughout, scalability, etc.  \n- System Data Security –This parameter  could be used to \nevaluate the security of big data based system in different perspectives.  Using this parameter , data \nsecurity could be evaluated in various perspectives at \nthe different levels.  \n- System Reliability –This parameter  is used to \nevaluatethe durability of  the system when perform ing a \nrequired function under stated conditions for a specified \nperiod of  time.  \n- System Robustness - This parameter  evaluates the \nability of a  system  to resist change without adapting its \ninitial stable configuration .  \n \nFigure 3 A Quality Test Process for Big Data Application System  \nIn addition, d ue to the special characteristics , such as \noracle problems, big data applications bring some \nimpacted factors contributing to  the challenges of system \nquality assuranc e. There are two  typical big data \napplications: a) recommendation system s, and b) prediction \nsystem s. We have collected a number of common quality \nparameters from survey . Figure 4 s ummarizes the typical \nquality f actors for prediction and recommendation systems  \nin a fishbone graph respectively . Those factors are  presented \nin taxonomy below . \nQuality factors for prediction system s \n- System Correctness , which is a quality  factor used to \nevaluate the correctness  of the big data  applications . \nUnlike the conventional system, big data applications \nare hard t o validate their correctness. F or instance, \nprediction –related software is mainly developed to \nmake predictions or better understand about real world \nactivities. Hence, it is difficult to determine the correct \noutput for those types of software.  Correctness is \nrelated to the prediction pattern or model. For instance, \nsome models are more likely used to predict  point of \ninflexion  values  while some other models are doing \nwell in predicting continuity. Thus, in order to verify \nthe correctness of the system effectively, engineers \nneed to evaluate the capability of prediction in the \nspecified condition s and environment s.  \n- System Accuracy , which is used to evaluate  if the \nsystem  yields true (no systematic errors) , and consistent \n(no random errors) results.  Some big data applications\n\n[Página 3]\nare developed to find previously unknown answers, \nthereby  only approximate solutions might be available.  \nThis can be  called uncontrollable prediction . Some \nprediction is used to prevent something happening in the future, and the prediction result will affect actions \nor behaviors. In turn, those actions can promote the \nprediction result.  \nFigure 4 Big Data Application System Quality Factors  \n- System Stability , which reflects the stability of the \nsystem prediction while environment change or data \nchanges. For example, if the prediction capability of a \nsystem is stable with little changes when statistical data \nare acquired  from different timeframes.  \n- System Consis tency, which is a quality indicator  useful \nto evaluate the consistency of the targeted systemin  \ndifferent perspectives.  Due to the inherent uncertainties \nin system models, some applications do not produce \nsingle  correct output for a given set on inputs. This \nleads to hardly determining the expected behaviors  of \nthe software. In such situation, domain -specific experts \ncould provide opinions to support system consistency.  \n- Duration , which  indicates the expected prediction \nperiod . It can measure how up -to-date data is, and \nwhether it is correct despite the possibility of \nmodifications or changes that impact time and date \nvalues  [6]. For instance, commonly -used prediction \nduration in enterpri se management can be divided  into \nshort term, middle term, and long term.  \n- Deviation Analysis , which  is used to analyze the \nprediction  deviation within an accepted range  or \nconfidence interval.  \n- System usability, which  is a parameter that in dicates  \nhow well the big data application service  can be used . \nThis can be very subjective due to different developers and users have diverse user experiences.  The typical \nusability factors include  intuitiveness, comfortability, \nand flexibility.  \n- System Perfo rmance , which  is a distinct quality factor \nfor big data application service . It is useful to evaluate \nhow well big data are structured, designed, collected, \ngenerated, stored, and managed to support large -scale \nprediction services.  \n \nQuality factors for recommend ation system s \n- Correctness  –This quality factor  reflects if the \nrecommended service or commodity meets the demands \nof customers. Correctness could be subjective between \ndifferent persons. Thus, how to measure correctness is \nstill a challenge for quality ass urance engineers.  \n- Correlation  – This quality factor  evaluates the d egree \nof correlation  of the recommended service. This \ninvolves various recommendation strategies, such as \nuser content -based, behavior -based, and collaboration \nfiltering -based.  \n- Multiplicity  – This quality  factor refers to the \nmeasurements for repeatability of recommended service. \nFor instance, a poor quality system probably  \nrecommends too many repeated or similar commodities  \nto users.\n\n[Página 4]\n- Category  Coverage  – This indicator is useful  to \nevaluate the coverage rate for diverse categories . This \nfactor measure s the completeness of recommendation  \nwithin a selected domain.  \n- Accountability  –This quality parameter is very \nimportant and mandatory for both big data service \napplications and user s. This could be measured in a \nquantitative way, such as user rating similarity, domain \ntrust value, domain related degree, and social intimacy \ndegree.  \n- Duration –This factor indicates the expected \nrecommendation period . For instance, commonly -used \nrecommendation duration in enterprise  management \ncan be divided  into short term, middle term, and long \nterm.  \n- Deviation Analysis –This factor is used to analyze the \nrecommendation  deviation within acce pted range  or \nconfidence interval.  - System usability –This parameter  indicates how well big \ndata application service  can be used .  This can be very \nsubjective due to different developers and users have \ndiverse user experiences.  \n- System Performance –This is a distinct quality factor for \nbig data application service, and it is useful to evaluate \nhow well big data are structured, designed, collected, \ngenerated, stored, and managed to support large -scale \nrecommendation services.  \nIn addition to the two typical applications discussed \nabove, there are more big data related applications such as \nmachine learning system, ranking system, and search system. \nDue to page limits, we do not list their quality factors here. \nA comparison of con ventional testing and big data \napplication testing  in detail  is presented in Table 1. \nTable 1  A comparison of  conventional testing and big data application testing  \nIII. VALIDATION METHODS  FOR BIG DATA APPLICATION  \nThis section discusses and reviewsthe existing research \nresults in software testing methods which have been  used to \nvalidate various types of big data applications  including intelligent systems,  data mining programs, bioinformatics \nprograms , and learning based applications.  \nProgram -based software t esting –Conventional program -\nbased testing methods have been used in big da ta analytics \napplication s. Csallner  et al.  presents a novel technique that  Conventional Testing  Big Data Application T esting  \nPrimary Objectives  - Validate the quality of  software , including functions, \nprograms, performance, etc.  - Provide on -demand testing services for big data  application \nsystems  to support software validation and quality engineering \nprocess.  \nTesting Focuses  - Diverse software errors in its structures, functions, \nbehaviors, user interfaces, and connections to the \nexternal systems.  \n- System non -functional requirements such as \nperformances, reliability, availability, vertical \nscalability, security, and etc.  - Non-oracle problem or rich oracle function problem . \n- Complicated  algorithm s. \n- Large -scale data input . \n- Complicated data models and integrations . \nTest Input  - Limited  scale  \n- Specified  data fo rmats  \n- Structured data  - Large-scale data volume with diverse formats and media  \n- Structured and non -structured data  \n- Timeliness  \nTesting Execution  - Offline testing in a test lab before product delivery . \n- Testing in a cloud -based test environment . - On-demand test execution in a cloud -based virtual test \nenvironment . \n- Continuous testing for  big data applications . \nTest Coverage  - Function -based, data flow, structure -based , state \ndiagram . - To be develop ed; lack able  currently . \nData Model  - Data partition, boundary analysis,  etc. - Training data; sampling data; classifier; image data \nclassification;  pseudo -oracles  \nTesting Environment  - A pre-configured test environment in a test labwith \npurchased hardware/software and tools . - Provide testing environment references  \n- Develop rapid reuse framework  \nTesting Process  - Enterprise -oriented test processes for each project . \n - Crowd sourcing -based process  \n- Learning -based testing  \n- Classification based testing  \nTesting Techniques  - Apply selected well -known white -box and black -box \ntesting techniques at the component level (or unit level) \nand the system level . - Required innovative continuous , timeliness , and currency  \ntesting techniques . \n- New testing solutions to deal with multi -dimensional large -\nscale data sets , uncertainty data , learning -based system \nevolution , and complicated visualization . \n \nTesting Tools  - Use limited testing solutions and tools with the \npurchased licenses . \n- Select and use diverse testing tools solutions which are \npre-configured, installed, and deployed . - Support development process . \n- Construct the whole process tool chains . \n- Data analysis tool . \n- Continuous  evaluation including crowdsourcing an d sampling . \nTool Connectivity and \nPlatform  - Traditional test tool/solution integration and \ncomposition . - Domain -specific application .  \n- On-demand selective solutions which support users to \nintegrate and composite test solutions and tools . \n- Incremental data sets. \n -  -\n\n[Página 5]\nsystematicallysearches for such bugs in MapReduce \napplications and generates corresponding test cases  [18]. The \ntechnique works by  encoding the high -level MapReduce \ncorrectness conditions  as symbolic program constraints and \nchecking them for the  program under test. Shang et al. \npropose d an approach to uncover the  different behaviors of the \nunderlying platforms for BDA  Apps  using Handoop between \nruns with small testing data and large real -life data  in a cloud \nenvironment  [19]. \nClassification -based t esting -A classification approach  to \nprogram testing  usually in volves two steps: a) training  a \nclassifier to distinguish failures from successful cases  on a \nselected subset of results, and  then b) applying the  trained \nclassifier to identify failures in the main set of  results.  A \nresembling reference model is usually used to train a classifier . \nMore specifically, there are te chniques for applying  pattern \nclassifications to alleviate the test oracle  problems. Last et al. \n[9] and Vanmali et al. [11] apply a data mining approach to \naugment the  incomplete specification of legacy systems. They \ntrain classifiers to learn the casual input -output relationships  \nof a legacy system. Podgurski et al.  classify  failure cases into \ncategories  [10]. However, they do not  study how to distinguish \ncorrect and failure behaviors  of programs. Later, t heir research \ngroup  further proposes classification tree approaches to refine  \nthe results obtained from classifiers  [8]. Bowring et al.  use a \nprogressive machine learning approach  to train a classifier on \ndifferent software behaviors  [7].They apply their technique in \nthe regression testing of  a consecutive sequence of minor \nrevisions of a program.  \nMetamorph ic testing (MT) - This is a classic approach to \ntesting programs that do not have oracles. Whenever a formal \noracle is not avail able or costly to  apply, we run into a  test \noracle  problem . A test oracle  is a mechanism against which \ntesters can check the output  of a program and decide whether \nit is correct. When  an oracle is not available, other means of \ndetermining  whether the test result is correct are known as \npseudo -oracles . MT operates by  checking whether a program \nunder test behaves according to an  expected set of properties \nknown as metamorphic relations. A metamorphic relation \nspecifies how a particular change to the  input of the program  \nshould change the output . MT was used  for testing scientific \napplications in different areas such as  machine learning \napplications [ 12, 13], bioinformatics programs  [14], programs \nsolving p artial differential equations [ 15] and  image \nprocessing applications [ 16]. When testing programs  solving \npartial diff erential equations, MT uncovered faults that  cannot \nbe uncovered by special value testing [ 15]. \nLearning -based testing – This involves  how to adopt the \nvarious learning approaches and mechanisms  to support \ntesting for big data system s. Meinke et al. developed a \ntechnique for automatic test casegeneration for numerical \nsoftware based on learning based testing  (LBT) [ 17]. The \nauthors first created a polynomial model as  an abstraction of \nthe program under test. Then the test cases are  gener ated by \napplying a satisfiability algorithm to the learned  model.   \nCrowd -sourced  testing –This testing approach uses  \nfreelance  testers  and/or contracted engineers  in a crowd sourcing community . It is a cost -effective method to validate  a \nmachine -learning based application systems, such as  a human  \nface recognition system . Currently, crowd -sourced  testing has \nbeen used in mobile app testing and mobile TaaS  (Testing as a \nService ). One good example is  uTest \n(http://www.utest.com/company ).  \nData m odel-based t esting –Since big data are the input \nvalues for big data application systems, diverse data models \ncan be used to assist test case generations. Vilkomir et al. \npresents a method to  automatically gen erate  test cases for  a \nscientific program having  many input parameters  with \ndependencies  [20]. They use a directed graph to model the \ninput  data space , including parameters and values as well as \ntheir dependencies. Valid test cases can be  automati cally \ngenerated based on the  directed graph  model. Since their  \nmodel  satisfies the p robability law of Markov chains, it can be \nused to  generate random and weighted test cases according to \nthe likelihood  of taking the parameter values.  \nRule -based s oftware t esting –This approach could be  used \nin testing ru le-based or knowledge based systems. The basic \nidea is to design test cases based on the rules specified in an \nexpert system . Deason  et al.  in [21]  proposed a rule -based test \ndata generation method for Ada programs. They demonstrate d \nthat rule -based test  data generation is feasible. The paper  \nshows a great promise  in assisting test engineers in test \ngeneration . Andrews  et al.  present ed a test pattern generation \napproach based on VHDL  specific heuristi c rules  [22]. Their  \nresults indicated  the rule-based approach leading to a better \ntest coverage . \nConventional black -box software t esting –To assure the \nsystem performance and other related QoS parameters  of big \ndata applications , engineers could  use convention black -box \napproaches to controlling their quality. Typical examples \ninclude  decision table  testing , equivalence  partitioning , \nboundary value  analysis , cause -effect graph, and use case \ntesting,  and so on . \nIV. ISSUES , CHALLENGES , AND NEEDS  \nThere are a number of major issues and challenges in big data \nquality validation and assurance. Here are typical ones.  \nIssue #1 –What are the  adequate test models and test  \ncoverage criteria for big data based service applications ? \nWith the fast advance of big data technologies and anal ytics \nmethods, more and more big data based applications and \nservice systems are developed to be used in many areas of our \ndaily life, including smart cars, smart city, business \nintelligence, environmental control, and so on. The increasing \ndeployment of big data  applications and service s raises quality \nassurance  concerns. In the past, m any existing white -box and \nblack -box software test models and adequate validation \ncriteria are developed to address validation needs of software \napplications in functions, b ehaviors, and structures. Ho wever, \nthese existing adequate test models  only focus on program \nfunctions, state -based behaviors, and program structures. In \nthe software testing and quality assurance community, there is \na lack of research work on adequate tes t modeling and\n\n[Página 6]\ncoverage analysis for big data application systems by \nconsidering their special features and needs in rich oracle \nfunctions, machine -learning  based system evolutions, \nknowledge based system intelligence , and multi -dimensional \nlarge -scale data sets.  \nHence, according to real world practitioners, there is a clear \ndemand on establish ing well-defined test coverage criteria for \nbig data application systems. Otherwise, engineers and big \ndata analy sts will  have difficult time to figure out when th ey \nshould stop quality testing for big data applications. This leads \nto the first demand described below.  \nNeed #1 –Developing w ell-defined adequate validation models \nand criteria to address the special features and needs of big \ndata applications and service s. \nIssue #2 –Where are the  well-defined big data system  quality \nassurance  programs and standards , including processes,  \nassessment metrics, regulations,  and policies ? \nAs we discussed in [25], ISO is working on updating of \nexisting data quality assurance standards and programs  for big \ndata quality assurance. Considering the popularity of big data \napplications and services, we must address the quality control \nand assurance of big data based applications.  Here, we point \nout the second emergent need  below . \nNeed #2  – Establishing quality assurance program s and \nstandards to consider the special QoS parameters and factors \nof big data applications and services  to ensure system quality . \nIssue #3 – What  are test automation solutions and tools \nsupport ing efficient and large -scale  testing  operations  for big \ndata applications and servi ces? \nIn the past three decades,  many test automation tools and \nsoluti ons have been developed for engineers in assisting test \nautomation activities and oper ations. Unfortunately, m ost of \nthese tools are only useful to validate software system  \nfunctions, behaviors , program structures , and system \nperformance and other QoS parameters . As discussed in \nsection III, there  are a few published papers addressing \nvalidation methods . \nHowever, these validation methods are not designed and \ndeveloped to address the special features and needs of big data \napplication systems. As discussed in Section III, there has \nbeen a few  of publi shed research work addressing special \nvalidation needs in big data based applications . However,  \nthere is a clear lack of research work on automatic validation \nmethods and solutions for big data application services. \nTherefore, the third emergent need for b ig data applications is \nlisted below.  \nNeed #3 –More  innovative adequate testing methods and test \nautomation tools to address the special needs and features of \nbig data application system s and services.  \nUnlike conventional software test automation tools, th ese \nexpected test automation solutions must consider big data \napplications’ special features listed below :  - Large -scale big data inputs  with diverse formats, and \nstructured and non -structured data ; \n- Learning and knowledge based system evolutions ; \n- Non-oracles problems and r ich oracle functions  with \nuncertainty ;   \n- New QoS parameters, such as accuracy, accountability, \nusability, and  \n- Data modeling  \nV. CONCLUSIONS  \nWith the fast advance of big data management technologies  \nand analytics solutions, how to build high -quality big data \napplication  service s becomes  a very hot  subjec t. Nevertheless, \nthere are increasing quality problems resulting in erroneous \ndata costs  in enterprises and businesses  [25]. Current research  \nwork  seldom discusses how to effectively  validate big data \napplications to ensure system quality . This paper provides \ninformative discussions on big data system validation and \nquality assurance, including the essential concepts , focuses, \nand validation process. Moreover, the paper identifies and  \ndiscusses  some  primary quality factors . In addition,  it presents  \na comparison  between conventional testing and big data \napplication testing . Furthermore , the primary issues, \nchall enges, and needs are pre sented . \nREFERENCES  \n[1] Editor of Hosting Journalist, IDC forecast: big Data technology \nand services to hit $32.4 billion in 2017 , December 18, 2013.   \n[2] A.O. Mohammed, S. A. Talab . Enhanced extraction clinical data \ntechnique to improve data quality in clinical data warehouse.  \nInternational Journal of Database Theory and Appl ication, 8 (3): \n333-342, 2015 .  \n[3] M. R. Wigan , R. Clake . Big data’s big unintended consequences . \nIEEE Computer, 46 (6):46-53, 2013.  \n[4] J. Alferes, P.Poirier, C. Lamaire -Chad, et al. Data quality \nassurance in monit oring of wastewater quality: Univariate on -\nline and off -line methods. In Proc. of the11th IWA conference \non instrumentation control and automation, pp. 18-20, \nSeptember,  2013.  \n[5] R. Clarke . Quality factors in big data and big data analytics . \nXamax Consultancy Pry Ltd. 2014.  \n[6] A. Immonen, P. Paakkonen, and E. Ovaska . Evaluating the \nquality of social media data in big data architecture. IEEE \nAccess, 3: 2028 - 2043 October 16, 2015.  \n[7] J.F. Bowring , J.M. Rehg,  and M.J. Harrold . Active learning \nforautomatic classific ation of software behavior. In Proc. of the \n2004 ACM SIGSOFT International Symposium on \nSoftwareTesting and Analysis (ISS TA). ACM, New York , NY, \npp.195–205, 2004 . \n[8] P. Francis, D. Leon , M. Minch,  and A. Podgurski . Tree-\nbasedmethods for classifying software failures. In  Proc. of the  \n15th International Symposium on Software Reliability \nEngineering  (ISSRE). Los Alamitos, CA,  pp. 451–462. 2004 . \n[9] M. Last, M. Friedman, and A. Kandel . The data mining \napproachto automated software testing. In  Proc. of the 9th ACM  \nSIGKDD International Conference on Knowledge Discovery \nandData  Mining (KDD). ACM, New York , NY, pp. 388 –396, \n2003 .\n\n[Página 7]\n[10] A. Podgurski, D. Leon, P. Francis, W. Masri,  M. Minch, J. Sun, \nand B. Wang . Automated support for classifying software failure  \nreports. In  Proc.  of the 25th International Conference on  \nSoftware Engineering (ICSE) , LosAlamitos, CA, pp. 465 –475, \n2003 . \n[11] M. Vanmali, M.  Last, and A. Kandel . Using a neural networkin \nthe software testing process. Internationa l Journal of Intelligent  \nSystems , 17 (1) : 45–62, 2002 . \n[12] X. Xie, J.W. Ho, C. Murphy, G. Kaiser, B. Xu, and T.Y. Chen . \nTesting and validating  machine learning classi fiers by \nmetamorphic testing . Journal of System and Software, 84 \n(4):544–558, 2011 . \n[13] C. Murphy, G. Kaiser, L. Hu, and L. Wu, Properties of machine \nlearning application sfor use in metamorphic testing . In Proc. of \nthe 20th International Conferenceon Software Engineering and \nKnowledge Engineering (SEKE), pp.867–872, 2008 . \n[14] T.Y. Chen, J.W.K. Ho , H. Liu, and X. Xie . An innovative \napproach for testing  bioinformatics programs using \nmetamorphic testing, BMC Bioinform. 10(2009).  \n[15] T. Chen, J. Feng, and T.H. Tse . Metamorphic testing of \nprograms on partial  differential equations: a case study . In Proc. \nof the 26th Annual  International Computer Software and \nApplications Conference, (COMPSA C), pp. 327 –333, 2002 . \n[16] J. Mayer, R. Guderlei . On random testing of image processing \napplications, In Proc. of the  6th International  Conference on \nQuality Software (QSIC ), pp. 85–92, 2006 . \n[17] K. Meinke, F. Niu . A learning -based approach to unit testing of \nnumerical  software, in: A. Petrenko, A. Simo, J. Maldonado \n(Eds.), Testing Software andSystems, Lecture Notes in \nComputer Science, vol. 6435, Springer, Berlin,Heidelberg, 20 10, \npp. 221–235. \n[18] C. Csallner , L. Fegaras , C. Li. New Ideas Track: Testing \nMapReduce -style programs . In Prof. o f 9th ACM SIGSOFT \nSymposium on the Foundations of Software Engineering \n(ESEC/ FSE’11), pp 1 -4, 2011.  [19] W. Shang，Z.M. Jiang，H. Hemmati，B. Adams，and A.E. \nHassan . Assisting developers of big data analytics  applications \nwhen deploying on Hadoop clouds . In Prof. of 35th International \nConference on Software Engineering (ICSE), pp 402 -411, 2013 . \n[20] S.A. Vilkomir, W.T. Swain, J.H. Poore, and K.T. Clarno . \nModeling input space fortesting scientific compu tational \nsoftware: a case study . In Prof. of the  8th International \nConference on Computational Science, Part III (ICCS),  pp. 291–\n300, 2008 . \n[21] W.H. Deason，D.B. Brown，and K.H. Chang . A rule -based \nsoftware test data generator . IEEE Transactions on Knowledge \nand Data Engineering, 3(1): 108 -117, 1991.  \n[22] A. Andrews , A.O. Fallon , and T. Chen . A Rule -Based Software \nTesting Method for VHDL Models.  VLSI -SOC  2003 : 92. \n[23] W. Afzal, R. Torkar, and R. Feldt . A systematic review of \nsearch-based testing for  non-functional system properties . \nInform ation and Softw are Technol ogy, 51 (6) :957–976, 2009 . \n[24] T. Clune, R. Rood, Software testing and verification in climate \nmodel  development, IEEE Softw are, 28 (6) :49–55, 2011 . \n[25] J. Gao, C .L. Xie, and C .Q. Tao. Quality assurance for big data– \nissuses, challenges, and needs. In Prof. of  IEEE 9th International \nSymposium on Service oriented System Engineering , OZFORD, \nUK, 2016.  \n \nACKNOWLEDGEMENT  \nThis paper is s upported by the National Natural Science \nFoundation of China under Grant No.61402229 and \nNo.61 502233; the Open Fund of the State Key Laboratory for \nNovel Software Technology (KFKT2015B10), and the \nPostdoctoral Fund  of Jiangsu Province under Grant \nNo.1401043B . \n.",
      "metadata": {
        "filename": "Quality Assurance for Big Data Application.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\Quality Assurance for Big Data Application.pdf",
        "file_size": 592825,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:39.887869",
        "content_length": 34007
      }
    },
    "3791031e-e4e6-4607-b30c-11e884efa5c1": {
      "id": "3791031e-e4e6-4607-b30c-11e884efa5c1",
      "content": "[Página 1]\nJust can’t get enough - Synthesizing Big Data\nTilmann Rabl\nMiddleware Systems\nResearch Group\nUniversity of Toronto\nCanada\ntilmann.rabl@utoronto.caManuel Danisch,\nMichael Frank,\nSebastian Schindler\nbankmark UG\nPassau, Germany\n{ﬁrst.last}@bankmark.deHans-Arno Jacobsen\nMiddleware Systems\nResearch Group\nUniversity of Toronto\nCanada\njacobsen@eecg.toronto.edu\nABSTRACT\nWith the rapidly decreasing prices for storage and storage systems\never larger data sets become economical. While only few years ago\nonly successful transactions would be recorded in sales systems,\ntoday every user interaction will be stored for ever deeper analysis\nand richer user modeling. This has led to the development of big\ndata systems, which offer high scalability and novel forms of anal-\nysis. Due to the rapid development and ever increasing variety of\nthe big data landscape, there is a pressing need for tools for testing\nand benchmarking.\nVendors have little options to showcase the performance of their\nsystems but to use trivial data sets like TeraSort or WordCount.\nSince customers’ real data is typically subject to privacy regula-\ntions and rarely can be utilized, simplistic proof-of-concepts have\nto be used, leaving both, customers and vendors, unclear of the tar-\nget use-case performance. As a solution, we present an automatic\napproach to data synthetization from existing data sources. Our\nsystem enables a fully automatic generation of large amounts of\ncomplex, realistic, synthetic data.\n1. INTRODUCTION\nData generation is a tedious part of the daily routine of researchers\ntesting new algorithms, database administrators testing new con-\nﬁgurations, and performance engineers testing new optimizations.\nTypically, the data is generated using scripting solutions, which are\nwritten and rewritten for each individual use case. While this is\nmanageable for simple and small data sets, databases are ever in-\ncreasing in size and complexity. Database testing typically is done\non simple scenarios, however, customers demand ever more real-\nistic benchmarks that match their use cases [21]. Modern enter-\nprise systems comprise hundreds to thousands of tables, which fre-\nquently have more than ﬁfty columns. System vendors that can\nshow the performance of their systems in a most realistic setup\nhave an advantage over vendors that present generic, simple bench-\nmarks.\nTo this end, multiple generic data generators have been devel-\noped, that enable a fast description of data models and correlations\n[10, 9, 1, 4]. Although all of these systems feature more efﬁcient\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor proﬁt or commercial advantage and that copies bear this notice and the full cita-\ntion on the ﬁrst page. Copyrights for components of this work owned by others than\nACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-\npublish, to post on servers or to redistribute to lists, requires prior speciﬁc permission\nand/or a fee. Request permissions from permissions@acm.org.\nSIGMOD’15, May 31–June 4, 2015, Melbourne, Victoria, Australia.\nCopyright is held by the owner/author(s). Publication rights licensed to ACM.\nACM 978-1-4503-2758-9/15/05 ...$15.00.\nhttp://dx.doi.org/10.1145/2723372.2735378.and more realistic generation of data none has reached wide adop-\ntion in the industry. While building custom data generators for each\nuse case is highly inefﬁcient, generic tools are either too limited in\ntheir features or their learning curve is too steep to be accepted by\ndevelopers. This has led to a dilemma, where it is often infeasible\nfor customers to ﬁnd the best solution for their use case since they\ncan only give metadata but not real data to vendors and vendors are\nnot able to cover the complex schemas and dependencies in their\nproof of concept systems.\nWhat is needed is a simple solution, that is able to automatically\ngenerate realistic data based only on meta information of the orig-\ninal data. Because of ever increasing data sizes and the prevalent\nincrease of cores, processors, and physical machines in deployed\nsystems, the solution has to be performant and scalable. The data\nhas to be rich in features even in single values, since big data sys-\ntems do not stop processing at the value level. With these require-\nments in mind, we have developed DBSynth, a fully automatic data\ngeneration solution that can reproduce realistic data sets based on\nschema information and sampling. Unlike any other generic data\ngenerator, it can extract features on the value level and generate\nnew relevant values.\nDBSynth is an extension to the Parallel Data Generation Frame-\nwork (PDGF), a generic data generator suite [17]. PDGF is the ba-\nsis for the data generator of the new industry standard ETL bench-\nmark TPC-DI, which was released by the Transaction Processing\nPerformance Council (TPC) in January 2014 [16] and was also used\nto implement the data generator for BigBench, the ﬁrst end-to-end\nproposal for a big data analytics [7].\nOur main contributions are the following, we demonstrate DB-\nSynth, which is the answer to one of the fundamental challenges\nin the big data space today - ﬁnding the best system for a given\nuse case. DBSynth is the ﬁrst tool, which can generate complete\ndata models from large complex databases on a variety of systems,\nsample data sets and automatically create relevant dictionaries and\nMarkov models. Furthermore, we demonstrate PDGF, a highly ad-\nvanced generic data generator, which can use models created by\nDBSynth to create big, realistic, synthetic data. PDGF is faster\nand more efﬁcient than any other generic generator and can be run\nwith perfect speedup in multi-core and multi-node environments.\nPDGF can write data in various formats (e.g., CSV , JSON, XML,\nand SQL) to ﬁles, database systems, streaming systems, and mod-\nern big data storage systems (e.g., HDFS).\nThe remainder of this paper is structured as follows. We describe\nPDGF brieﬂy in Section 2 and then show the core functionality of\nDBSynth in Section 3. After an evaluation in Section 4, we explain\nour demonstration in Section 5. In Section 6, we give an overview\nof related work, before concluding in Section 7.\n1457\n\n[Página 2]\n2. PDGF\nThe Parallel Data Generation Framework (PDGF) is a versatile,\ngeneric data generator [17]. PDGF has a novel computation-based\ngeneration strategy that enables a completely parallel generation of\ndata. PDGF’s generation strategy is based on the exploitation of de-\nterminism in pseudo random number generators (PRNG). Random\nnumber sequences generated using PRNGs are repeatable, which\nmeans that the exact same sequence can be generated over and over\nagain. PDGF uses xorshift random number generators, which be-\nhave like hash functions. Repeatable, parallel data generation is\npossible using an elaborate seeding strategy (depicted in Figure 1).\nIn the example, starting with a project seed one seed per table is\ngenerated. This seed is used to create one seed per column, which\nagain is used to generate one seed per abstract time unit and ﬁnally\nper ﬁeld. The ﬁeld seed is used to generate the random number\nsequence for the value generation. The values are generated using\nﬁeld value generators. These can be simple generators, like number\ngenerators, generators based on dictionaries, or reference genera-\ntors, but also meta generators, which can concatenate results from\nother generators or execute different generators based on certain\nconditions [18]. The concept of meta generators enables a func-\ntional deﬁnition of complex values and dependencies using simple\nbuilding blocks. Although the seeding hierarchy and meta genera-\ntor stacking seems expensive, most of the seeds can be cached and\nthe cost for generating single values is very low. We will show an\nanalysis of the exact costs in Section 4.\nCustomer\nRow # / CustKey Name Address …Table RNG\n1\n2\n3\nseed\n t_id\nColumn RNG\n seed\n c_id\nUpdate RNG\n seed\n u_id\nGenerator(rn)\n rn\n4\nID (Row) RNG\n seed\n id\nFigure 1: PDGF’s seeding strategy\nPDGF’s architecture is shown in Figure 2. The user speciﬁes\ntwo XML conﬁguration ﬁles, one for the data model and one for\nthe formatting instructions. These will be explained in more de-\ntail in Section 3. Additionally, all previously speciﬁed properties\nof a model and format (e.g., scale factors, table sizes, probabili-\nties) can be changed in the command line interface. The controller\nthen initializes the system. The meta scheduler manages multi-\nnode scheduling, while the scheduler assigns work packages to the\nworkers. A work package is a set of rows of a table that need to be\ngenerated. The workers then initialize the correct generators using\nthe seeding system and the update black box. Whenever a work\npackage is generated, it is sent to the output system, where it can\nbe formatted and sorted.\nPDGF\nXML\nXML\nMeta \nScheduler\nScheduler\nWorker\nWorker\nWorker\nUpdate \nBlack Box\nSeeding \nSystem\nWorker\nWorker\nGenerator\nOutput\nSystem\nCSV\nDB\nRNG\nP\nController\nUI\nPPData Flow\nControl Flow\nPlugin\nOptional\nFigure 2: PDGF’s architecturePDGF has been successfully used to implement a variety of bench-\nmarks, e.g., TPC-H [17], the Star Schema Benchmark [19], TPC-\nDI [6], and BigBench [7].\n3. DBSYNTH\nDBSynth is an extension to PDGF that automates the conﬁgu-\nration and enables the extraction of data model information from\nan existing database. DBSynth’s abstract architecture and mode of\noperation can be seen in Figure 3. In DBSynth, the user speciﬁes\nprojects, which integrate workﬂows, such as data generation, data\nextraction, etc. The ﬁgure shows the complete automatic workﬂow,\nfrom model extraction to data generation. Not all steps are neces-\nsary for a given project.\nDBSynth connects to a source database via JDBC, using the\nmodel creation tool, schema information and a conﬁgurable level of\nadditional information of the data model are extracted. Possible in-\nformation includes min/max constraints, histograms, NULL prob-\nabilities, as well as statistic information collected by the database\nsystem such as histograms. DBSynth also features a rule based\nsystem that searches for key words in the schema information and\nadds predeﬁned generation rules to the data model. For example,\nnumeric columns with name keyoridwill be generated with an ID\ngenerator.\nIf sampling the database is permissible, the data extraction tool\nbuilds histograms and dictionaries of text-valued data and stores the\naccording probabilities for values. Users can specify the amount of\ndata sampled and the sampling strategy. In future versions, we will\ninclude a dynamic sampling, which adapts the sample size and sam-\npling strategy according to the base data. If the text data contains\nmultiple words, DBSynth uses a Markov chain generator, which an-\nalyzes the word combination frequencies and probabilities. These\nare stored and linked to the data model.\nUsing the generated data model, PDGF can generate the data.\nThe model is translated into a SQL schema, which is loaded into\nthe target database using JDBC. The data can be loaded into the\ntarget database either using SQL statements generated by PDGF or\na bulk load option, if featured by the target database.\nListing 1 presents an excerpt of the automatically generated con-\nﬁguration for a TPC-H data set [15]. The excerpt shows the gen-\neral structure of the PDGF schema conﬁguration. It contains the\nproject’s seed, changing the seed will modify every value of the\ngenerated data set, the random number generator, PdgfDefaultRa-\nndom is a custom built, very fast xorshift PRNG, property deﬁni-\ntions, which can also be changed from the command line, and the\nschema information itself. A default property is the scaling factor\nSF, which is used to determine the size of the data set. DBSynth\nwill generate a size property for each table and assign it the prod-\nuct of the scale factor and the original table size. This way other\nscaling dependencies can be easily speciﬁed in a centralized point\nin the model. Furthermore, all boundaries for numerical values and\ndates are stored in properties.\nThe schema model is speciﬁed in form of table entries. Each ta-\nble speciﬁes its size, which DBSynth sets to be linear with the scale\nfactor as shown in the example. However, any formula can be used\nto calculate the size. Then the columns of the table are speciﬁed in\nform of ﬁeld entries. The ﬁrst ﬁeld is \"l_orderkey\", the name and\nsize are extracted from the database’s schema information. The fact\nthat it is a key is deduced from the column name, this is the reason\nwhy DBSynth chooses an ID generator for this column. The next\nﬁeld is \"l_partkey\", which is a reference to the table \"partsupp\".\nThis is speciﬁed in the schema, which is why DBSynth chooses a\nDefaultReferenceGenerator, which will generate consistent refer-\nences to this table. The ﬁnal ﬁeld that is shown is \"l_comment\",\n1458\n\n[Página 3]\nTable TableMeta DataSource DatabaseDataSynth\nModel \nCreation\nPDGFData \nModel\nMarkov \nChains\nDictsData \nExtractionJDBC JDBC\nTable TableTarget Database\nMeta DataSchema \nTranslatorFigure 3: Abstract architecture and data ﬂow in DBSynth\na text ﬁeld containing free text. DBSynth chooses the Markov-\nGenerator for this ﬁeld, thus it will sample the original database to\nbuild the Markov model. For a TPC-H data set the comment ﬁeld\nmodel contains 1500 words and 95 starting states, which can eas-\nily be ﬁt in memory. The choice of the generator type used for a\nﬁeld is based ﬁrst on referential integrity constraints, i.e., a refer-\nence will always be generated by a reference generator independent\nof its type. Then the data type determines if a number generator,\ne.g., Long, Integer, Double, or a date generator, or a text gener-\nator is used, DBSynth and PDGF support all SQL 92 datatypes.\nIf the database is not sampled, the column name is parsed to de-\ntermine whether a matching high level generator construct exists,\ne.g., names, addresses, comment. In case nothing is found a ran-\ndom string is generated. The Markov generator builds dictionaries\nfor single word text ﬁelds and Markov chains for free text, the pa-\nrameters for the Markov model are adjusted based on the original\ndata. If the original data cannot be sampled or analyzed, DBSynth\nfalls back to random values based on the database statistics as well\nas predeﬁned generators for URLs, addresses, etc.\n4. EV ALUATION\nWe evaluated the performance of PDGF and DBSynth on a 24\nnode, dual socket, dual core cluster and on a single node with two\nsockets and eight cores per socket. Where possible, generated data\nwas written to /dev/null to ensure the throughput was not I/O bound.\nIn the experiments, we used either the BigBench data set or our\ncustom implementation of the TPC-H data set.\nIn the ﬁrst experiment, we evaluate the performance of PDGF by\ngenerating a BigBench data set of scale factor 5000, which results\nin a total data size of 4392 GB on the 24 node cluster. The results of\nthis experiment can be seen in Figure 4. As is shown in the ﬁgure,\nPDGF has linear throughput scaling in the number of nodes.\nIn the second experiment, we benchmark the scale out perfor-\nmance of PDGF by increasing the number of workers and thus\nthreads used for the data generation. This experiment is conducted\non the single node. The results can be seen in Figure 5. PDGF’s\nthroughput increases linearly with the number of cores (16) and fur-\nther increases with the number of hardware threads (32), but not as\nsigniﬁcantly as for the number of cores. An interesting observation\nis that scheduling exactly the same number of workers as the num-\nber of system cores or threads is not optimal due to the additional\ninternal scheduling and I/O threads.\nIn Figure 6, a comparison of the data generator DBGen and\nPDGF is shown. As can be seen, both tools achieve a similar perfor-\nmance. In parallel mode, it is not possible to write to /dev/null using\nDBGen, which is why in this experiment the throughput of both,\nDBGen and PDGF, was disk-bound. We also show PDGF’s CPU-<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<schema name=\"tpch\">\n<seed>12456789</seed>\n<rng name=\"PdgfDefaultRandom\"></rng>\n<property name=\"SF\" type=\"double\">1</property>\n<property name=\"lineitem_size\" type=\"double\">6000000 *\n${SF}</property>\n<table name=\"lineitem\">\n<size>${lineitem_size}</size>\n<field name=\"l_orderkey\" size=\"19\" type=\"BIGINT\"\nprimary=\"true\">\n<gen_IdGenerator>\n</gen_IdGenerator>\n</field>\n<field name=\"l_partkey\" size=\"19\" type=\"BIGINT\"\nprimary=\"false\">\n<gen_DefaultReferenceGenerator>\n<reference table=\"partsupp\"\nfield=\"ps_partkey\"></reference>\n</gen_DefaultReferenceGenerator>\n</field>\n[..]\n<field name=\"l_comment\" size=\"44\" type=\"VARCHAR\"\nprimary=\"false\">\n<gen_NullGenerator> probability=\".0000d\"\n<gen_MarkovChainGenerator>\n<min>1</min>\n<max>10</max>\n<file>markov\\l_comment_markovSamples.bin</file>\n</gen_MarkovChainGenerator>\n</gen_NullGenerator>\n</field>\n</table>\n[..]\nListing 1: Excerpt of the schema deﬁnition for TPC-H\nbound performance, which is 33% higher than its disk-bound per-\nformance. DBGen’s parallelization is non-transparent. This means\nthat for each parallel stream a new instance is started, which writes\nits own ﬁles. As a result, DBGen’s parallel output will be split\nin as many ﬁles as instances were started, whereas PDGF writes\nsorted output into a single ﬁle. PDGF also supports the same par-\nallel generation strategy as DBGen does, which is starting multiple\ninstances and generating a distinct range of the data set with each\ninstance. With this approach it is possible to scale out the gener-\nation to shared nothing systems with linear speedups [17]. When\ncomparing the single process performance, i.e., starting only a sin-\ngle DBGen instance and running PDGF with a single worker, DB-\nGen achieves 48 MB/s and PDGF 30 MB/s. Thus, PDGF is has the\n1459\n\n[Página 4]\n0200400600800100012001400\n0 4 8 12 16 20Throughput MB/s\nNodes\n0100200300400500600700\n-2 2 6 10 14 18 22Duration min\nNodesFigure 4: PDGF BigBench scale-out\nperformance\n 0100200300400500\n0 10 20 30 40 50Throughput MB/s\nThreadsFigure 5: PDGF TPC-H scale-up per-\nformance\n 0510152025\n1 10 30 100 300Duration s\nScale FactorDBGen PDGF PDGF /dev/nullFigure 6: DBGen vs PDGF perfor-\nmance\n \n \n  \n \n  050100150200250\nStatic Value\n(no Cache)Null Generator\n(100% NULL)Null Generator\n(0% NULL)ns\nBase Time Generator Base Time Sub Sub Generator\n0100200300400500\n0 10 20 30 40 50Throughput MB/s\nThreads\nFigure 7: Generation latency\n \n \n 050100150200250\nStatic Value\n(no Cache)Null Generator\n(100% NULL)Null Generator\n(0% NULL)ns\nBase Time Generator Base Time Sub Sub Generator\n0100200300400500600\nDictList Long Double Date Stringns\nGenerator\n0200400600800100012001400160018002000\nDictList Null (100%) Null (0%) Date\n(formatted)Sequential\n(2 double +\nlong)Double\n(4 places)ns\nGenerator Figure 8: Basic generator latency\n \n \n 050100150200250\nStatic Value\n(no Cache)Null Generator\n(100% NULL)Null Generator\n(0% NULL)ns\nBase Time Generator Base Time Sub Sub Generator\n0100200300400500600\nDictList Long Double Date Stringns\nGenerator\n0200400600800100012001400160018002000\nDictList Null (100%) Null (0%) Date\n(formatted)Sequential\n(2 double +\nlong)Double\n(4 places)ns\nGenerator Figure 9: Complex generator latency\nsame order of performance as DBGen, although being completely\ngeneric and adaptable.\nWe conducted further experiments to determine the sources of la-\ntencies for the individual value generation. The experiments were\ndone in a single threaded setup, to get the per value overhead. These\nresults show the pure computational requirements and do not dis-\ncuss latencies added by the I/O subsystem. In Figure 7, the latency\nof independent value generation is broken down into its subparts.\nFor a static value, i.e., a column contains only one unique value\nthat is never changed, the pure system overhead can be seen. It is\nin the order of 50 Nanoseconds (ns). If a NULL value generator is\nwrapped around a static value that is NULL with 100% probabil-\nity, the overhead of the NULL generator is added to the generation\nof the (static) NULL value, this is again in the order of 50 ns. The\ndeﬁnition of the NULL value generator can be seen in Listing 1. Fi-\nnally, if the NULL probability is 0% the inner static value generator\nhas to be executed in all cases, this adds the base time for the sub-\ngenerator and the actual value generation, both of which are again\nca 50 ns. Thus the total duration for each value is in the order of 200\nns. In Figure 8, it can be seen that this is a good ballpark number\nfor simple values that are not formatted. Picking values from dictio-\nnaries, computing random numbers, and generating random strings\nare all in the range of 100 ns - 500 ns. String formatting is the\nmost expensive operation in data generation in Java, this can seen\nin Figure 9. Formatting a date value (e.g., \"11/30/2014\") increases\nthe generation cost to 1200 ns, which is similar to generating a\nvalue that consists of a formula that references 2 double values and\nconcatenates it with a long. Although the formatting is expensive,\nits cost is ﬁxed since PDGF does lazy formatting, which means\neven very complex values will only be formatted once. This anal-\nysis shows that using subgenerators incurs nearly negligible cost\n(ca. 100 ns) and it also shows that computing values rather than\nrereading them is much more efﬁcient. While generating complex\nvalues might cost up to 2000 ns, doing a single random read will\ncost ca. 10 ms on disk, which means the computational approach\nis 5000 times faster than an approach that reads previously gener-\nated data to solve dependencies. Furthermore, the computational\napproach enables a completely parallel generation of data, which is\nFigure 10: Standard screen of DBSynth\nnot possible using reading based approach without replicating data\nor extensive network communication.\nIn our ﬁnal experiment, we tested the performance of the DB-\nSynth metadata extraction. Using a TPC-H database with scale\nfactor 1 loaded in a PostgreSQL DBMS, it takes 600 ms to get\nthe schema information, 1.3 s to get the table sizes, 600 ms to get\nNULL probabilities, 10 seconds to get all min and max constraints,\nand between 800 ms (0.001% samples) and 200 s (100% samples)\nto retrieve data for the Markov chains. These results indicate an in-\nteractive response time for data model generation. Using PDGF’s\npreview generation, which shows samples of the generated data in-\nstantaneously, data models can be built and improved very fast.\n5. DEMONSTRATION\nTo show the ease of use of DBSynth, we will demonstrate typ-\nical work ﬂows for data generation. We will start by generating\nindustry standard data sets such as TPC-H. The data will be gener-\nated using PDGF, but this conﬁguration is compliant to the TPC-H\ndata set [15] and was developed in cooperation with the TPC-H\nsubcommittee. This is a default project in DBSynth. The accord-\ning selection screen can be seen in Figure 10. We will generate a\n10 GB TPC-H data set. We will show how the data can be altered\n1460\n\n[Página 5]\nFigure 11: Mission Control interface for PDGF\nby changing the output format. To this end, the data will be writ-\nten in CVS and XML format. The generation progress and system\nutilization will be monitored using Java Mission Control1. PDGF\nuses the Java Management Extensions internally for inter process\ncommunication and using these interfaces, the progress of single\ntables and the complete data set as well as general performance\nparameters can be visualized. This can be seen in Figure 11.\nAfter the data generation, we will demonstrate the conﬁgura-\ntion generation. To show a real use case, we will use the pub-\nlicly available parts of the IMDb database2. The data set is hosted\nin a MySQL database, which was loaded using the imdbpy2sql.py\nscript, which is part of the IMDbPY package3. We will ﬁrst use\na basic schema extraction, where only the schema information is\nretrieved from the database and no tables are accessed. The gen-\nerated XML ﬁle will be explained, which contains the model for\nthe data generation. Then we will do a second more elaborate\nschema extraction, where min/max constraints, NULL values, and\ndata samples for Markov chains will be read from the database. We\nwill compare the newly extracted model with the ﬁrst one and then\ngenerate the data. The according screen can be seen in Figure 12.\nWe will show excerpts of the generated data in comparison to the\noriginal data and verify the quality by running SQL queries on the\noriginal data and the generated data and compare the results. To\nthis end, the generated data will be loaded to a database system.\nFinally, we will explain how the model can be changed or adapted.\nWe will change the automatically generated conﬁguration by adding\nadditional columns to the model and reﬁning correlations that could\nnot automatically be detected. This will be done using an automat-\nically generated version of the TPC-H conﬁguration. We will then\nshow the differences between the original TPC-H conﬁguration and\nthe newly generated conﬁguration and compare the generated data\nsets.\n6. RELATED WORK\nThere is a rich body of work on data generation for database\nbenchmarking and testing. In the following, we will ﬁrst give an\noverview of related work on data generation in general and then\nshow other approaches for synthesizing existing data sets.\nEven though there are many generic data generation tools, most\ndata generators either produce very simple data or are non-reusable\nhard coded programs or collections of scripts. Examples for simple\ndata generation are the data generator used by all variations of the\n1http://www.oracle.com/technetwork/java/javaseproducts/mission-\ncontrol/java-mission-control-1998576.html\n2http://www.imdb.com/interfaces\n3https://github.com/alberanid/imdbpy\nFigure 12: Data generation screen of DBSynth\nsorting benchmark (e.g., TeraSort4) and the Yahoo Cloud Serving\nBenchmark data generator [5]. Examples for hard coded generators\nare all TPC data generators, with the notable exceptions of TPC-DS\n[14] and TPC-DI [16]. While simple data is helpful for testing basic\nfunctionality it does not represent real world use cases. However,\nhard coded data generators cannot easily be adapted for changing\nrequirements and for different systems. In many cases real data can\nbe used, but due to the impossiblity of scaling, privacy constraints\nand cost of storage and transportation it is not feasible for bench-\nmarking.\nAn important characteristic for benchmarking data is repeatablity.\nBasis for repeatable parallel data generation is the work by Gray et\nal. on synthetic data generation [8]. This work describes how to\ngenerate non uniform data in parallel on shared nothing systems.\nThese techniques along with parallel pseudo random number gen-\nerators for the basis of our Parallel Data Generation Framework.\nModern generic data generators can be divided into three sub-\nsets according to their reference and correlation generation: (1)\nno reference generation; (2) reference tracking; and (3) reference\ncomputation. Many data generators do not generate references or\ncorrelations explicitly, but rely on users providing correct statistic\ndistributions to generate correlating values. Generators that track\nreferences either compute all references at the same time that the\noriginal value is generated, or they track the original value when a\nreferences is generated. The former approach is frequently done us-\ning graph models [10, 11] or declarative description [2, 24] which\ncan lead to realistic data, however, typically is very slow and hard\nto parallelize. Tracking references is done by rereading the previ-\nously generated data. This approach was for example presented by\nBruno et al. [4], this approach is very ﬂexible but also very slow\nand does not scale well. A faster approach is generating all related\ndata at the same time. Generic suites that use this approach include\nMUDD [22] and PSDG [9]. The fastest way of generating correct\nreferences in most cases is recomputing them. This approach was\nﬁrst implemented in PDGF. A very similar data generator, Myriad,\nwas built at the Technical University of Berlin [1]. It shares the\nsame generation strategy with PDGF, however, does does not in-\nclude many of the features of PDGF, such as update generation and\ntext generation.\nAs part of the BigDataBench suite of test cases for big data\nsystems, an early version of PDGF is used to generate structured\ndata [13]. The suite comprises additional generators for graphs\n4https://hadoop.apache.org/docs/current/\napi/org/apache/hadoop/examples/terasort/\npackage-summary.html\n1461\n\n[Página 6]\nand a similar text generator as the current version of PDGF fea-\ntures. Unlike PDGF’s text generator, the different generators are\nnot connected and, therefore, cannot generate heterogeneous data\nsets with references in between different data sets, e.g., references\nfrom structured data into text.\nAlthough synthetic data is usually better suited for benchmark-\ning purposes, synthetic data should reﬂect characteristics of real\ndata. Therefore, typically real data sets are analyzed for modeling\ndata sets for benchmarking. Although this step is typically manual,\nit can be automated. Like DBSynth other tools use the metadata\nstored in database systems to get information about the distribution\nand structure of the data. RSGen reads metadata and schema in-\nformation of existing data sets and generates similar data sets by\nusing histograms of the original data [20]. Although similar to DB-\nSynth, the approach is limited to numerical data. Another tool that\nis able to scale existing data sets is UpSizeR [23]. It builds a graph\nof the original schema information and correlation information and\ngenerates data accordingly. However, the individual, non-key val-\nues are deemed application speciﬁc and thus have to be speciﬁed\nby the user. DBSynth uses sampling of the data set to generate\ndictionaries and Markov chains for non-key, non-numerical values.\nFurthermore, DBSynth uses its built in dictionaries to increase the\nvalue domain in scale out scenarios. Myriad also comes with con-\nﬁguration generation tool Oligos, which can analyze the schema\nand statistical information of a DB2 database [1]. However, it can-\nnot not sample a database and also has no tools to analyze and\nsynthesize values.\nA line of work that is orthogonal to our work is test data genera-\ntion for queries. QAGen analyzes queries to generate data that pro-\nduces desired results and intermediate results to cover all required\ntest cases [3]. A similar tool is MyBenchmark [12]. Analyzing\nqueries to ensure desired results is currently not a feature of our\ntool, but will be included in future versions. Given the determinis-\ntic approach of data generation, our tool will then also be able to\ndirectly execute the query without ever generating the data, which\ncan be used to verify results for correctness.\nAlthough many of the features of DBSynth are covered to some\nextent by other projects, none of these does include synthetic value\ngeneration. Values are always treated as atomic units. This is prob-\nlematic for big data sets, where values frequently are texts that have\nto be further analyzed using machine learning techniques. To gen-\nerate data sets, which satisfy requirements of big data use cases,\nDBSynth includes Markov chain generators and dictionaries that\ncan generate realistic, synthetic values based on sampled data.\n7. CONCLUSION\nWe demonstrate DBSynth, an extension to the Parallel Data Gen-\neration Framework, which enables a fully automatic conﬁguration\nof the data generator based on existing databases. DBSynth can\nbuild realistic data models from a deployed database extracting\nschema information, sampling, and analyzing the database. It uses\nheuristics for data type determination and builds dictionaries and\nMarkov models for data generation. The generated models and\nconﬁguration can be directly used by PDGF to generate data for\na target database. A simple, intuitive graphical user interface ties\nall parts together and features wizards to guide users through every\nstep of the process. In our demonstration, we showcase the ease of\nuse, the high performance, and the ﬂexibility of the system.\nIn future work, we will extend DBSynth to automate the com-\nplete benchmarking process. To this end, we will generate the\nqueries consistently using PDGF and build additional driver and\nanalysis modules. Furthermore, we will include query analysis togenerate data sets with predeﬁned (intermediate) results and gener-\nate veriﬁcation results for queries for given data models.\n8. REFERENCES\n[1] A. Alexandrov, K. Tzoumas, and V . Markl. Myriad: Scalable\nand Expressive Data Generation. In VLDB, 2012.\n[2] A. Arasu, R. Kaushik, and J. Li. Data Generation Using\nDeclarative Constraints. In SIGMOD, 2011.\n[3] C. Binnig, D. Kossmann, E. Lo, and M. T. Özsu. QAGen:\nGenerating Query-aware Test Databases. In SIGMOD, 2007.\n[4] N. Bruno and S. Chaudhuri. Flexible Database Generators.\nInVLDB, pages 1097–1107, 2005.\n[5] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and\nR. Sears. Benchmarking Cloud Serving Systems with YCSB.\nInSoCC, pages 143–154, 2010.\n[6] M. Frank, M. Poess, and T. Rabl. Efﬁcient Update Data\nGeneration for DBMS Benchmark. In ICPE, 2012.\n[7] A. Ghazal, T. Rabl, M. Hu, F. Raab, M. Poess, A. Crolotte,\nand H.-A. Jacobsen. BigBench: Towards an industry\nstandard benchmark for big data analytics. In SIGMOD,\n2013.\n[8] J. Gray, P. Sundaresan, S. Englert, K. Baclawski, and P. J.\nWeinberger. Quickly Generating Billion-Record Synthetic\nDatabases. In SIGMOD, pages 243–252, 1994.\n[9] J. E. Hoag and C. W. Thompson. A Parallel General-Purpose\nSynthetic Data Generator. SIGMOD Record, 36(1):19–24,\n2007.\n[10] K. Houkjær, K. Torp, and R. Wind. Simple and Realistic\nData Generation. In VLDB, pages 1243–1246, 2006.\n[11] P. J. Lin, B. Samadi, A. Cipolone, D. R. Jeske, S. Cox,\nC. Rendón, D. Holt, and R. Xiao. Development of a\nSynthetic Data Set Generator for Building and Testing\nInformation Discovery Systems. In ITNG, pages 707–712,\nWashington, DC, USA, 2006. IEEE Computer Society.\n[12] E. Lo, N. Cheng, and W.-K. Hon. Generating Databases for\nQuery Workloads. PVLDB, 3(1-2):848–859, 2010.\n[13] Z. Ming, C. Luo, W. Gao, R. Han, Q. Yang, L. Wang, and\nJ. Zhan. BDGS: A Scalable Big Data Generator Suite in Big\nData Benchmarking. In WBDB, 2013.\n[14] M. Poess and C. Floyd. New TPC Benchmarks for Decision\nSupport and Web Commerce. SIGMOD Record,\n29(4):64–71, 2000.\n[15] M. Poess, T. Rabl, M. Frank, and M. Danisch. A PDGF\nImplementation for TPC-H. In TPCTC, 2011.\n[16] M. Poess, T. Rabl, H.-A. Jacobsen, and B. Cauﬁeld. TPC-DI:\nThe First Industry Benchmark for Data Integration. PVLDB,\n13(7):1367–1378, 2014.\n[17] T. Rabl, M. Frank, H. M. Sergieh, and H. Kosch. A Data\nGenerator for Cloud-Scale Benchmarking. In TPCTC, pages\n41–56, 2010.\n[18] T. Rabl, M. Poess, M. Danisch, and H.-A. Jacobsen. Rapid\nDevelopment of Data Generators Using Meta Generators in\nPDGF. In DBTest, 2013.\n[19] T. Rabl, M. Poess, H.-A. Jacobsen, P. E. O’Neil, and\nE. O’Neil. Variations of the Star Schema Benchmark to Test\nData Skew in Database Management Systems. In ICPE,\n2013.\n[20] E. Shen and L. Antova. Reversing Statistics for Scalable Test\nDatabases Generation. In DBTest, 2013.\n[21] V . Sikka. Does the World Need a New Benchmark?\nhttp://www.saphana.com/community/blogs/\nblog/2013/09/16/\ndoes-the-world-need-a-new-benchmark, 2013.\n[22] J. M. Stephens and M. Poess. MUDD: a multi-dimensional\ndata generator. In WOSP, pages 104–109, 2004.\n[23] Y . Tay, B. T. Dai, D. T. Wang, E. Y . Sun, Y . Lin, and Y . Lin.\nUpSizeR: Synthetically Scaling an Empirical Relational\nDatabase. Information Systems, 38(8):1168–1183, 2013.\n[24] E. Torlak. Scalable Test Data Generation from\nMultidimensional Models. In FSE, 2012.\n1462",
      "metadata": {
        "filename": "rabl2015.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\rabl2015.pdf",
        "file_size": 1316417,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:40.072400",
        "content_length": 35962
      }
    },
    "64561264-69f4-43da-aaab-595a6d3b8c8c": {
      "id": "64561264-69f4-43da-aaab-595a6d3b8c8c",
      "content": "[Página 1]\nFull Terms & Conditions of access and use can be found at\nhttp://www.tandfonline.com/action/journalInformation?journalCode=teis20\nEnterprise Information Systems\nISSN: 1751-7575 (Print) 1751-7583 (Online) Journal homepage: http://www.tandfonline.com/loi/teis20\nSchema on read modeling approach as a basis of\nbig data analytics integration in EIS\nSlađana Jankovi ć, Sne žana Mladenovi ć, Dušan Mladenovi ć, Slavko Veskovi ć\n& Dra ženko Glavi ć\nTo cite this article:  Slađana Jankovi ć, Sne žana Mladenovi ć, Dušan Mladenovi ć, Slavko Veskovi ć\n& Dra ženko Glavi ć (2018): Schema on read modeling approach as a basis of big data analytics\nintegration in EIS, Enterprise Information Systems, DOI: 10.1080/17517575.2018.1462404\nTo link to this article:  https://doi.org/10.1080/17517575.2018.1462404\nPublished online: 18 Apr 2018.\nSubmit your article to this journal \nView related articles \nView Crossmark data\n\n[Página 2]\nARTICLE\nSchema on read modeling approach as a basis of big data\nanalytics integration in EIS\nSlađana Jankovi ć, Sne žana Mladenovi ć,D ušan Mladenovi ć, Slavko Veskovi ć\nand Dra ženko Glavi ć\nFaculty of Transport and Tra ﬃc Engineering, University of Belgrade, Belgrade, Serbia\nABSTRACT\nBig Data analysis is the process that can help organizations to make\nbetter business decisions. Organizations use data warehouses and busi-\nness intelligence systems, i.e. enterprise information systems (EISs), to\nsupport and improve their decision-making processes. Since the ultimate\ngoal of using EISs and Big Data analytics is the same, a logical task is to\nenable these systems to work together. In this paper we propose a\nframework of cooperation of these systems, based on the schema on\nread modeling approach and data virtualization. The goal of data virtua-\nlization process is to hide technical details related to data storage from\napplications and to display heterogeneous data sources as one inte-\ngrated data source. We have tested the proposed model in a case\nstudy in the transportation domain. The study has shown that the\nproposed integration model responds ﬂexibly and e ﬃciently to the\nrequirements related to adding new data sources, new data models\nand new data storage technologies.ARTICLE HISTORY\nReceived 14 September 2017\nAccepted 2 April 2018\nKEYWORDS\nBig data analytics; data\nvirtualization; schema on\nread; data warehouse;\nbusiness intelligence system\nIntroduction\nA large number of new approaches and technological solutions in data modeling, storage,\nprocessing and analysis, grouped together under the common term ‘Big Data ’, have the task of\nkeeping under control the massive in ﬂow of data and placing it in the service of organizations and\nindividuals. The initial successful initiatives in the application of Big Data technologies soon gave\nrise to a problem known as Big Data integration. Big Data integration means any software\nintegration involving the data characterized as Big Data, i.e. the data with at least one of the\nfollowing features: volume, variety, velocity and veracity. According to Arputhamary and Arockiam\n(2015 ), there are two categories of Big Data integration, namely integration of several Big Data\nsources in Big Data environments and integration of the results of Big Data analysis with structured\ncorporate data. This research is focused on addressing the second, above-mentioned category of\nthe Big Data integration problem.\nAn Enterprise Information System (EIS) is an integrated information system with the basic task of\nproviding the management with the necessary information. This research addresses two major\nchallenges encountered by modern EISs in the sphere of data management in order to be quali ﬁed\nas ‘integrated ’as per the above de ﬁnition. The promotion of business operation of organizations\nnearly always involves the introduction of new sources of corporate data. If new data sets fall into\nthe category of Big Data, they require the application of Big Data storage, processing and analysis\nCONTACT SlađanaJankovi ć s.jankovic@sf.bg.ac.rs Faculty of Transport and Tra ﬃc Engineering, University of Belgrade,\nBelgrade, SerbiaENTERPRISE INFORMATION SYSTEMS, 2018\nhttps://doi.org/10.1080/17517575.2018.1462404\n© 2018 Informa UK Limited, trading as Taylor & Francis Group\n\n[Página 3]\nmethods. To use new corporate Big Data sets in a business context, they have to be integrated with\nthe existing corporate data sets, after which the integrated data should be subjected to Big Data\nanalysis. The integration of the existing and new corporate data sets to create the subject of the\nfuture Big Data analysis is the ﬁrst challenge to which this research will try to respond. The second\nchallenge and the subject of this research is the integration of the results of Big Data analysis with\nEIS. This task has to be solved regardless of whether corporate or external data are the subject of\nBig Data analysis. External data, such as social media and web data, are increasingly used as the\nsubject of Big Data analyses in order to examine user satisfaction, habits and needs etc.\nZdravkovi ćand Panetto ( 2017 ) highlighted that current challenges in EISs development are\nrelated to the growing need for ﬂexibility caused by cooperation with other EISs. EISs environment\nhas become very dynamic and variable not only in terms of collaboration with other EISs, but also\nin terms of availability of data sources. The research aims to o ﬀer a solution that would e ﬃciently\nmeet the following three key requirements: frequent appearance of new Big Data sources (either\ncorporate or external), application of new data processing, analysis and visualization methods, and\nintegration of structured (i.e. relational) and semi- and non-structured data sources. To solve the\nabove problems, the schema alignment method of data integration has been selected. The\ntraditional schema alignment method of data integration has been adapted to Big Data sources\nand methods of Big Data analysis by being based on the schema on read data modeling approach\nand data virtualization concepts. Schema on read means you create the schema only when reading\nthe data. Structure is applied to the data only when it ’s read, this allows unstructured data to be\nstored in the database. Since it ’s not necessary to de ﬁne the schema before storing the data it\nmakes it easier to bring in new data sources on the ﬂy. Data virtualization is any approach to data\nmanagement that allows an application to retrieve and manipulate data without requiring techni-\ncal details about the data, such as how it is formatted at source, or where it is physically located.\nThe research also provides a technological framework for the implementation of the proposed\nintegration model. It includes the following three technological environments: NoSQL databases,\ndata virtualization servers and data integration tools.\nThe second section of this paper presents the reference literature review. In the third section, we\npropose and describe our Big Data analytics integration approach based on the ‘data integration\non demand ’approach and the ‘schema on demand ’modeling approach. In order to evaluate our\napproach, we have implemented the proposed approach in a case study in the transportation\ndomain. We have carried out the custom analysis of road tra ﬃc data on a Big Data platform and\nintegrated it with the SQL Server database, Business Intelligence (BI) tool and tra ﬃc geo-applica-\ntion, according to the proposed integration approach. Finally, we will present our conclusions\nabout the possibilities and constraints of our integration approach.\nLiterature review\nAs pointed out in the introduction of the paper, this research does not deal with the integration of\ndiﬀerent Big Data sources on Big Data platforms but with the integration of the results of Big Data\nanalysis with structured corporate data. For this reason, the literature review includes the data\nintegration approaches and solutions that can be applied to Big Data sources as well as the existing\nEIS architectures.\nFor decades, there have been two main approaches to data integration, namely batch data\nintegration and real-time data integration. Both approaches have secured a place for themselves in\nBig Data integration processes as well. From the data analytics perspective, Big Data systems\nsupport the following classes of applications: batch-oriented processing, stream processing, OLTP\n(Online Transaction Processing) and interactive ad-hoc queries and analysis (Ribeiro, Silva, and da\nSilva 2015 ). The batch data integration approach is used in batch-oriented processing applications,\nwhereas the real-time data integration approach is used in stream processing, OLTP and interactive\nad-hoc queries and analysis applications. An overview of the most important approaches and2 S. JANKOVI ĆET AL.\n\n[Página 4]\nsolutions in the ﬁeld of Big Data integration with EISs, both in the batch as well as the real-time\nmode, will be given in the text below.\nBatch data integration for big data\nWhen data exchange between two systems is performed through periodic big ﬁle transfers on a\ndaily, weekly or monthly basis, we call this batch data integration. In the era of the Internet of\nThings (IoT) and social media, i.e. the era of Big Data, this interval between two successive ﬁle\ntransfers can be much shorter and measured in hours or even minutes. The transferred ﬁles include\nrecords with an unchangeable structure, which is adapted to the requirements of the system that\nreceives them. This approach to integration is known as a ‘tightly coupled ’approach, because it\nimplies that systems are compatible in terms of ﬁle and data format and that the format can only\nbe changed if both systems simultaneously implement speci ﬁc changes (Reeve 2013 ). The standard\nbatch data integration process includes the following operations: extract, transform, and load (ETL).\nToday, there is a large number of commercial and open-source ETL tools (Alooma 2018 ). The main\npurpose of these tools is to upgrade and facilitate the warehousing, archiving, and conversion of\ndata.\nBig Data are most frequently raw data, which are ‘dirty ’and incomplete and therefore it is\nnecessary to perform the operations of extracting, cleaning and data quality processing (Macura\n2014 ; Chen and Zhang 2014 ) in order to work with them. In the Big Data context, ETL tools are\nused to extract, clean and transform raw data from Big Data platforms and NoSQL databases into a\nrelational or another required form, as well as to load the results of Big Data analytics into\nEnterprise Data Warehouses (EDWs) (Florea, Diaconita, and Bologa 2015 ). The task can only be\nperformed by ETL tools enabling the creation of interfaces according to both traditional data\nsources (relational databases, ﬂatﬁles, XML ﬁles, etc.) as well as Big Data platforms (Hortonworks\nData Platform, Cloudera Enterprise, SAP HANA Platform, etc.) and NoSQL databases (MongoDB,\nCassandra, HBase, Neo4j, etc.). Such commercial ETL tools include Informatica, Oracle Data\nIntegrator, Alooma, SAS ETL and Altova MapForce. The major open-source tools of this type include\nApache NiFi, Talend and Pentaho Data Integration.\nTransformation as an operation can vary, ranging from an extremely simple operation to an\ninexecutable operation, and it may require the use of additional data collections. In the simplest\ncase, it consists of the simple mapping of source ﬁelds to target ﬁelds, but most frequently it also\nincludes operations such as aggregation, normalization and calculation. Some ETL tools, such as\nAltova MapForce, include a revolutionary interactive debugger to assist with the data mapping\ndesign.\nApache Hadoop is an open-source distributed software platform for storing and processing\ndata. Central to the scalability of Apache Hadoop is the distributed processing framework known as\nMapReduce (Sridhar and Dharmaji 2013 ). According to the research done by Russom ( 2013 ), the\nmain reason to integrate Hadoop into Business Intelligence or Enterprise Data Warehouse is the\nexpectation from Hadoop to enable Big Data analytics. The basic advantage of Hadoop is the\npossibility to use advanced non-OLAP (Online Analytic Processing) analytic methods, such as data\nmining, statistical analysis and complex SQL. However, in addition to the fact that it can be used as\nan analytical sandbox, Apache Hadoop includes many components useful for ETL. For example,\nApache Sqoop is a tool for transferring data between Hadoop and relational databases. When data\nare located in the Hadoop File System, they can be e ﬃciently subjected to the ETL tasks of\ncleansing, normalizing, aligning, and aggregating for an EDW by employing the massive scalability\nof MapReduce (Intel Corporation 2013 ). In this way, the Apache Hadoop platform represents a\npowerful ETL tool enabling the integration of the results of Big Data analysis of structured and non-\nstructured data in an EDW.\nResearch (Wang et al. 2016 ) has shown that the most important Big Data technologies that\nsupport batch data integration include the following: MapReduce, Hadoop (HDFS, Hive, HBase),ENTERPRISE INFORMATION SYSTEMS 3\n\n[Página 5]\nFlume, Scribe, Dryad, Apache Mahout, Jaspersoft BI Suite, Pentaho, Skytree Server, Cascading,\nSpark, Tableau, Karmasphere, Pig and Sqoop.\nReal-time data integration for big data\nIn many cases of data integration, the batch mode is unacceptable so that real-time or near real-\ntime data integration has to be performed instead. Real-time data integration involves the transfer\nof much smaller quantities of data in one interaction, in the form known as a ‘message ’(Gokhe\n2016 ). The quantity of data transferred in this way is limited and each interaction means ensuring\nsecurity on all levels, the same as in batch data integration. Consequently, when it comes to larger\nquantities of data, real-time data movement is slower than batch data movement. The traditional\n‘point-to-point ’interaction model means that there are direct ‘tightly coupled ’interfaces between\neach two systems which have to share data. The data from each data source have to be\ntransformed as per the requirements of each target data format. If the number of systems which\nshould be connected by an interface is n, the number of interfaces is (n * (n –1))/2. The most\nsigniﬁcant and most important design pattern for architecting real-time data integration solutions\nis the ‘hub-and-spoke ’design for data interactions (Reeve 2013 ). The point of this interaction model\nis that data from all sources are transformed into a common, shared format, from which they are\ntransformed into the target format. The number of interfaces for the connection of n systems is n in\nthis case. From the technological point of view, the central segment of the real-time data integra-\ntion solution is the implementation of an enterprise service bus (ESB). An enterprise service bus is\nan application used to coordinate the movement of data messages across di ﬀerent servers that\nmay be running di ﬀerent technologies.\nXML (eXtensible Markup Language) has been a de facto standard for the exchange of\ninformation in the past two decades and, consequently, it also plays a major role in the ﬁeld\nof data integration. XML ﬁles are a typical example of semi-structured data (Gandomi and\nHaider 2015 ). Modern data integration software ena bles the transformation of data from XML\nﬁles into other types of data warehouses (Big Data included) and vice versa. Other self-\ndocumenting data interchange formats that a re popular include JSON (Java Script Object\nNotation).\nHadoop o ﬀers excellent performances in the processing of massive data sets, but query execu-\ntion on the Hadoop platform (e.g. Hive queries) is measured in minutes and hours. This constitutes\na great challenge in the integration of Hadoop into a real-time analytics environment. Intel and SAP\nhave joined forces to tackle this challenge (Intel Corporation 2014 ). The Intel® Distribution for\nApache Hadoop (IDH) is highly optimized for performance on Intel® architecture. Intel and SAP\nhave enabled the generation of queries that will be e ﬃciently executed on both platforms, SAP\nHANA as well as IDH.\nResearch (Wang et al. 2016 ) has shown that the most important Big Data technologies that\nsupport stream processing and real-time integration include the following: Kafka, Flume, Kestrel,\nStorm, SQLstream, Splunk, SAP Hana and Spark Streaming.\nSchema alignment in big data integration\nThe main task of data integration, regardless of whether it is traditional or Big Data integration,\nbatch or real-time data integration, is to download the required data from their current warehouse,\nto change their format in order to be compatible with the destination warehouse and to place\nthem at the target location (Loshin 2013 ). It is the challenges which data integration has to address\nthat have changed. The three main steps in data integration include schema alignment, record\nlinkage and data fusion. Schema alignment should respond to the challenge of semantic ambi-\nguity, enabling the identi ﬁcation of attributes with the same meaning as well as those without it.\nRecord linkage should ﬁnd out which records refer to the same entity and which do not. Data4 S. JANKOVI ĆET AL.\n\n[Página 6]\nfusion should enable the identi ﬁcation of accurate data in an integrated data set in cases when\ndiﬀerent sources o ﬀer con ﬂicting values.\nDong and Srivastava ( 2015 ,3 5 )u n d e r l i n et h a t , ‘schema alignment is one of the major\nbottlenecks in building a data integration system ’. They believe that in the Big Data context,\nw h e r et h en u m b e ro fd a t as o u r c e si sp e r m a n e n t l yo nt h er i s ea n dw h e r es o u r c es c h e m a sa r e\nexpected to change all the time, no up-to-date schema mappings are possible. In contrast, Gal\n(2011 ) speaks of the important role schema matching plays in the data integration life cycle. He\nbelieves that the Big Data challenges of variet y and veracity can be dealt with by using schema\nmatching, while the challenges of volume and velocity can be dealt with by using entity\nresolution (record linkage).\nBig data analytics integration framework\nThis section of the paper presents the framework for the integration of Big Data sources with\nstructured data sources, which still form the backbone of EISs. In the previous section, we have\nseen that both the batch data integration approach as well as the real-time data integration\napproach have their advantages as well as disadvantages and, consequently, our goal has been\nto propose a model capable of supporting both integration methods.\nIn view of the fact that EISs are based on s tructured data (data warehouses, prede ﬁned\nbusiness analytics and reports, etc.), we believ e that variety and veracity constitute the key\nchallenges in the integration of Big Data analysis and EISs. The integration framework we\npropose is therefore based on the upgrade of the model of application of the schema\nalignment (schema matching) method of data integration. The upgrade is expected to be the\nresult of the application of the schema on read modeling approach and data virtualization\nconcepts. In the text below, the two approaches will be ﬁrst brie ﬂy outlined and then the\nreason why they have been selected explained.\nSchema on read modeling approach in big data integration process\nSchema on write is a standard modeling approach, where we create a database schema and a\ndatabase for a speci ﬁc purpose, and then we enter data into the database. This means that the data\nmust be adequately prepared for the developed schema. The schema on read approach involves\nstoring raw data, and then, when we need it for a speci ﬁc purpose, we create a schema while\nreading data from a data storage ( Figure 1 ). Unlike schema on write, which requires you to expend\ntime before loading the data, schema on read involves very little delay and you generally store the\nFigure 1. Schema on read modeling approach.ENTERPRISE INFORMATION SYSTEMS 5\n\n[Página 7]\ndata at a raw level. In data-intensive computation problems data is the driver, not analytical human\nor machines. When the schema on read modeling approach is used, these very large data sets can\nbe used multiple times in di ﬀerent ways, for various types of analysis. However, we believe that the\nschema on read modeling approach has a big potential not only in the ﬁeld of Big Data analysis\nbut also in the ﬁeld of Big Data integration.\nAccording to (EMC Education Services, ed 2015 ), the main phases of the data analytics life cycle\ninclude data discovery, data preparation, model planning, model building, communicate results\nand operationalize. However, in our experience, Big Data integration process, too, has to include\nalmost all above phases, as shown in Figure 2 . Consequently, we shall speak of the roles the\nschema on read modeling approach plays in all mentioned activities, as the phases of Big Data\nintegration process:\n●Phase ‘discovery ’: at this stage, the schema on read modeling approach plays an important\nrole in getting to know the team with data and the selection of appropriate data preparation\nmethods.\n●Phase ‘data preparation ’: given that the possibilities of data transformation with ETL tools are\nnevertheless limited, the data in Big Data source systems have to be organized and formatted\nso as to be able to be transformed with ETL tools into the format required by EIS. The data in\nBig Data source systems can be prepared for ETL operations through adequate modeling.\nData modeling when necessary, at the point of reading, is precisely what the schema on read\nmodeling approach makes possible. In this way, ETL operations are more e ﬀectively realized\nusing the schema on read modeling approach.\n●Phase ‘model planning ’: the schema on read modeling approach allows a deeper exploration\nof data and recognition of the relationships between individual variables.\n●Phase ‘model building ’: at this stage, the schema on read modeling approach has the\nmost signi ﬁcant role, because it allows ﬂexible creation, testing and changing of the\nmodels. In data integration process, the phases of ‘model planning ’and ‘model building ’\ncan occur several times. They will de ﬁnitely occur during the ETL operations and, if there\nis a data virtualization level, they will occur also during the creation of virtual tables.\nDue to the above roles the schema on read modeling approach can play in Big Data\nintegration process, we believe that thi s modeling approach is imperative for e ﬃcient Big\nData integration.\nFigure 2. Schema on read modeling approach in Big Data integration lifecycle.6 S. JANKOVI ĆET AL.\n\n[Página 8]\nData virtualization integration approach for big data analytics\nBig Data analytics is characterized by a permanent appearance of new data sources and new\nrequirements regarding analytical models and methods, so that we have tried to adopt an\nintegration approach likely to ensure a satisfactory degree of ﬂexibility. We have recognized the\ndata virtualization concept as a suitable basis for ﬂexible ‘on-demand ’integration and multiple use\nof the same data, without copying.\nAs van der Lans ( 2012 , 9) points out, ‘Data virtualization is the technology that o ﬀers data\nconsumers a uni ﬁed, abstracted, and encapsulated view for querying and manipulating data stored\nin a heterogeneous set of data stores ’. Basically, when data virtualization is applied, the middle\nlayer that hides from an application most of the technical aspects on where and when data are\nstored is provided. Besides that, all data sources are shown as one integrated data source. Data\nvirtualization is available in various implementation processes. Some of them include the following:\na server for data virtualization, Enterprise Service Bus (ESB) architecture, placing data warehouse on\nthe cloud, a virtual in-memory database and object-relational mappers.\nWe have concluded that all above phases of Big Data integration, which include data\ndiscovery, data preparation, mo del planning, model building, communicate results and oper-\nationalize, can be performed on data virtualiz ation servers. This is not the case in other data\nvirtualization implementation processes. Consequently, our approach to data virtualization\nimplies the use of data virtualization servers. The main parts of a data virtualization server\ninclude source tables, mappings and virtual ta bles. Mappings represent the way to transform\ndata from source tables to virtual tables. What ma kes virtualization servers powerful tools is\nthe fact that source tables are not restrict ed to relational tables, but instead di ﬀerent data\nsources such as data generated by websites, the result of a web service call, a HTML page, a\nspreadsheet or a sequential ﬁl e ,c a nb eu s e d .U s e r sc a na c c e s sv i r t u a lt a b l e sb yu s i n gd i ﬀerent\nAPIs (Application Programming Interface), such as the JDBC/SQL interface, MDX\n(MultiDimensional eXpressions) and the SOAP-b ased interface. That means that same tables\nwould be seen di ﬀe r e n t l yb yd i ﬀerent users.\nA c c o r d i n gt o( v a nd e rL a n s 2012 ), a data virtualization server consists of a design module\nand a runtime module. When data consumers acc ess the virtualization layer, they use the\nruntime module of a data virtualization serv er. The design module is an environment which\ndata analysts and data model desi gners use to create concept de ﬁnitions, data models, and\nspeciﬁcations for transformation, cleansing and integration. Some data virtualization servers\nenable the creation of unbound virtual tables. That means that it is possible to create data\nmodels using them, and to join them with the real data source afterwards. The runtime\nmodule of a data virtualization server represe nts a virtual sandbox for data scientists and\nenables managed self-service re porting for business analysts.\nAt a time when new data sources appear on a daily basis, in order to ensure the understanding\nand integrity of data, it is very important to manage metadata. Metadata must be a link between\nthe existing and new data sources. As Zdravkovi ćet al. ( 2015 , 5) point out, ‘the capability to\ninteroperate will be considered as the capability to “semantically interoperate ”.’It is very important\nthat data virtualization servers allow the entering and using of data models, glossaries and\ntaxonomies.\nThe data virtualization integration approach can help in two ways in data integration processes\nenabling Big Data analytics. Firstly, data virtualization can help in the phases of data discovery and\ndata preparation according to the requirements of di ﬀerent analytical models. Big Data analyses\ncan include only external data or only internal historical data stored in an EDW, but they often\nrequire the integration of external and corporate data. Considering that we are talking about\nanalyzing a huge amount of external data coming at a high speed, it makes no sense to consider\nthe physical integration of data based on their copying into a single central data warehouse.\nInstead of that, Big Data analysis is performed on Big Data platforms and in NoSQL databases withENTERPRISE INFORMATION SYSTEMS 7\n\n[Página 9]\nappropriate storage and processing performances. In that case, the required corporate data can be\nensured on the data virtualization layer, according to the requirements of a speci ﬁc Big Data\nanalysis, and can then be exported to a Big Data platform, such as Hadoop. If data virtualization is\nconducted via a virtualization server, the required data are ensured by means of virtual tables. This\nmeans that no local copy of the selected data is made, but the data can instead be exported to\ndiﬀerent warehouses, in the form de ﬁned by a given virtual table. Data virtualization servers have\nbuilt-in functions for the export of data to di ﬀerent warehouses, Big Data platforms included.\nSecondly, the data virtualization integration approach can help in the phase of integration\nof the results of Big Data analytics and EIS. Aft er becoming familiar with the available data\nsources, the operations of model planning and model building can be performed on a data\nvirtualization server, similarly as in any database management system. Data models are\ndesigned by creating unbound vir tual tables. Regrettably, at this point, not all data virtua-\nlization servers have this option. Once a virtu al table is created, it can be linked with some\nexternal or internal data source. The design of virtual tables depends on the form of analysis\nresults which should be integrated and the data model into which they should be inte-\ngrated. We propose that the designing of virt ual tables be based on the application of the\nschema alignment method and the available dat a virtualization concepts, such as nested\nvirtual tables. Nested virtual tables are virtua l tables created on top o fo t h e rv i r t u a lt a b l e s .\nThe schema alignment method and the way it is applied on a data virtualization server will\nbe explained in detail in the next section.\nSchema alignment based on schema on read and data virtualization\nSchema alignment is used when one domain includes several di ﬀerent source schemas, which\ndescribe it in di ﬀerent ways. The results of schema alignment include the following:\n●a mediated schema, which provides a uniform view over heterogeneous data sources, cover-\ning the most important domain aspects;\n●attribute matching, which matches attributes in all source schemas with the corresponding\nattributes in a mediated schema;\n●schema mapping between each source schema and a mediated schema, specifying the\nsemantic ties between the data described by source schemas and the data described by a\nmediated schema.\nThere are two classes of schema mappings: Global-as-View (GAV) and Local-as-View (LAV). GAV\ndeﬁnes a mediated schema as a set of views over source schemas. LAV expressions describe source\nschemas as views over a mediated schema. We shall ﬁrst de ﬁne GAV and LAV schema mappings\nand then, by using these two formalisms, we shall give an example to show how the application of\nschema alignment method of data integration can be upgraded through the application of data\nvirtualization concepts and the schema on read modeling approach. To demonstrate this, we have\nselected an example from the case study conducted to verify the proposed model. The case study\nis described in detail in the next section of the paper.\nThis is followed by the de ﬁnitions of GAV and LAV schema mappings according to Doan, Halevy,\nand Ives ( 2012 ).\nDeﬁnition 1 (GAV Schema Mappings). Let G be a mediated schema, and let /C22S¼S1;... ;Sn fg be\nschemata of n data sources. A Global-as-View schema mapping /C22Mis a set of expressions of the\nform G iðXÞ/C19 QðSÞ, where\n●Giis a relation in G,\n●and appears in atmost one expression in M, and Q ðSÞis a query over the relations in S8 S. JANKOVI ĆET AL.\n\n[Página 10]\nDeﬁnition 2 (LAV Schema Mappings). Let G be a mediated schema, and let /C22S¼S1;...;Sn fg be\nschemata of n data sources. A Local-as-View schema mapping /C22Mis a set of expressions of the form\nSiðXÞ/C19 QiðGÞ, where\n●Qiis a query over the mediated schema G, and\n●Sia source relation and it appears in at most one expression in M.\nThe example: The backbone of the EIS architecture consists of an enterprise data warehouse\n(EDW), a data virtualization server and a business intelligence (BI) tool. This particular EIS is used by\na road maintenance organization. We shall extract the relations modeling the road network, EDW.\nRoad and EDW.Road_section, from the EDW schema. The problem in hand is to integrate new datasources, the Big Data analysis results and new reports to be created in the BI tool with the existing\nEIS. There are two new data sources: one stores the road traﬃ c data, the other stores the data on\nautomatic traﬃ c counters monitoring traﬃ c. The new reports should enable the visualization of Big\nData analysis results over integrated data. The traﬃ c data are stored in TXT ﬁles. In view of the fact\nthat TXT ﬁles are semi-structured and that they contain a large amount of data that is constantly\ngrowing, they are warehoused on the Big Data platform HDFS (Hadoop Distributed File System).\nThe following three tasks have been identi ﬁed:\n●Data on traﬃ c counters, which are small in volume and do not change often, should be\nintegrated with EIS on the data warehouse level.\n●Data on traﬃ cﬂow volume and structure, which will be the result of Big Data analysis, should\nbe integrated with EIS on the corporate data model level.\n●The new reports should be integrated with EIS on the corporate data model level.\nWhat we are interested in are the Road and RoadSection relations, which belong to the EDW:EDW.Road(RoadID, RoadName, RoadCategory),\nEDW.RoadSection(SectionID, SectionName, RoadID, SectionLength).\nTheﬁrst task will be solved by adding a new relation to the EDW system and by linking it to the\nRoadSection relation. The new relation is Counter:\nEDW.Counter(Location, Longitude, Latitude, SectionID, Type).The second task requires a far more complex solution. The integration of a new data source\nwith EIS on the corporate data model level will be performed through the successive multiple\napplication of the schema alignment method. The results of the application of this method will\nbe implemented on a data virtualization server , by creating virtual tables and nested virtual\ntables. We have adopted a top-down approach to this problem. This means that we ﬁrst\nanalyze the end goal to be achieved through in tegration. The end goal is a data schema as\nrequired by new reports. Since this data schema should be a common, uniform view over theEDW and the Big Data source, it will be designed as a mediated schema by using GAV schema\nmappings. Its relations will be nested virtual tables (NVT_Counter and NVT_AADT), created as\nviews over virtual tables (VT_Road , VT_Section, VT_Counter, VT_Tra ﬃc). The virtual tables\nVT_Road, VT_Section, VT_Counter and VT_Tra ﬃc will be created as unbound virtual tables.\nTheir role is very important. At this point, they will enable the application of GAV schemamappings and the creation of a virtual mediated s chema. The following expressions describe\nthe above GAV schema mappings:\nMediate.NVT_Counter(Location, Longitude, Latitude, RoadName, SectionName) ⊇\nVT_Road(RoadID, RoadName),VT_Section(SectionID, SectionName, RoadID),\nVT_Counter(Location, Longitude, Latitude, SectionID).\nMediate.NVT_AADT(Location, Year, AADT, AADT_D1, AADT_D2) ⊇ENTERPRISE INFORMATION SYSTEMS 9\n\n[Página 11]\nVT_Tra ﬃc(Location, Year, AADT, AADT_D1, AADT_D2, AADT_A0, AADT_A1, AADT_A2, AADT_B1,\nAADT_B2, AADT_B3, AADT_B4, AADT_B5, AADT_C1, AADT_C2, AADT_X).\nThe AADT ﬁeld represents Annual Average Daily Tra ﬃc, while AADT_D1 and AADT_D2 represent\nAADT by vehicle movement direction. The other ﬁelds represent AADT by vehicle categories.\nIn the next phase, by using LAV schema mappings, the unbound virtual tables VT_Road,\nVT_Section and VT_Counter are linked with the corresponding EDW relations. The EDW schema\nrepresents a mediated schema in this case. The following expressions describe the above LAV\nschema mappings:\nVT_Road(RoadID, RoadName) ⊆\nEDW.Road(RoadID, RoadName, RoadCategory)VT_Section(SectionID, SectionName, RoadID) ⊆\nEDW.RoadSection(SectionID, SectionName, RoadID, SectionLength)\nVT_Counter(Location, Longitude, Latitude, SectionID) ⊆\nEDW.Counter(Location, Longitude, Latitude, SectionID, Type)Using LAV schema mappings, source schemas are created for the Big Data source (BD) based on\nVT_Tra ﬃc. The virtual table schema VT_Tra ﬃc represents a mediated schema in this case. The\nfollowing expressions describe the above LAV schema mappings:\nBD.AADT(Location, Year, AADT) ⊆VT_Tra ﬃc(Location, Year, AADT, AADT_D1, AADT_D2,\nAADT_A0, AADT_A1, AADT_A2, AADT_B1, AADT_B2, AADT_B3, AADT_B4, AADT_B5, AADT_C1,\nAADT_C2, AADT_X)\nBD.AADTByDirections(Location, Year, AADT_D1, AADT_D2) ⊆VT_Tra ﬃc(Location, Year, AADT,\nAADT_D1, AADT_D2, AADT_A0, AADT_A1, AADT_A2, AADT_B1, AADT_B2, AADT_B3, AADT_B4,AADT_B5, AADT_C1, AADT_C2, AADT_X)\nBD.AADTByCategories(Location, Year, AADT_A0, AADT_A1, AADT_A2, AADT_B1, AADT_B2,\nAADT_B3, AADT_B4, AADT_B5, AADT_C1, AADT_C2, AADT_X) ⊆VT_Tra ﬃc(Location, Year, AADT,\nAADT_D1, AADT_D2, AADT_A0, AADT_A1, AADT_A2, AADT_B1, AADT_B2, AADT_B3, AADT_B4,\nAADT_B5, AADT_C1, AADT_C2, AADT_X)\nOnce schemas for the Big Data sources BD.AADT, BD.AADTByDirections and BD.\nAADTByCategories are designed, the designing of Big Data analysis begins so as to get the resultsdescribed in the above schemas. This is when the schema on read modeling approach comes intoplay. It is applied to a Big Data source in situations when one knows what kind of data schema is\nrequired. In other words, the data on a Big Data platform are organized according to the schema\nderived through the successive application of GAV and LAV schema mappings. Once a Big Datasource is created according to the above schemas, it should be linked with a data virtualization\nserver. After that, the unbound virtual table VT_Tra ﬃc is linked with the real Big Data source. This\nsolves the task of integrating the results of Big Data analysis with EIS on the corporate data model\nlevel.\nThe third task, integration of new reports with EIS on the corporate data model level, will be\nsimply solved by linking the BI tool with the virtual schema Mediate on a data virtualization server.\nWe can say now that the key factors of the proposed model of Big Data integration include in\nthe following:\n●a top-down approach to solving the integration problem, i.e. starting with reports and\nmoving down to data sources,\n●application of GAV schema mappings in order to create a uniform view over the domain –a\nmediated schema, using the concept of unbound nested virtual tables on a data virtualizationserver,\n●application of LAV schema mappings in order to create the required local and external data\nsource schemas, using the concept of unbound nested virtual tables on a data virtualization\nserver,10 S. JANKOVI ĆET AL.\n\n[Página 12]\n●application of the schema on read modeling approach in creating data schemas for Big Data\nsources, derived by using the above combined GLAV (Global-as-Local-as-View) schema map-\nping approach.\nAlthough some authors, such as Dong and Srivastava ( 2015 ), believe that schema alignment is\nnot an appropriate Big Data integration method, we have shown that it can be e ﬀectively\nimplemented using unbound nested virtual tables and bound virtual tables on the data virtualiza-\ntion server.\nBig data analytics integration scenarios\nBetween the enterprise information system and the Big Data analytic tool a two-way data\nexchange is necessary. In Big Data analysis for business purposes, apart from data originating\nfrom external sources, such as sensor data, data generated by various machines, social networking\ndata etc., corporative data are used, too. Corporative data that are used in Big Data analysis or are\ncrossed with Big Data analysis results frequently appear on their own as a result of some\nprede ﬁned analysis in a business intelligence system. Thus, it is necessary to enable integration\nof corporative data and other data that are the object of Big Data analysis. One corporative data\npart, which is archived and traditionally used for business reporting, is structured. However, a\nsigniﬁcant part of corporative data are semi-structured and unstructured data.\nOn the other hand, external sources generate heterogeneous data that are stored in di ﬀerent\ntypes of data storages. The amount of external data that are of interest for corporative analysis as a\nrule increases. The results of Big Data analysis should become available to business analysts and\nother business users, and sometimes even end users, such as buyers, service users, etc. This can be\nachieved through data integration or through integration on the report level. Integration of\ncorporate data, external data and Big Data is done in the phase of preparing input data for various\nadvanced Big Data analysis techniques. After Big Data analysis is completed, it is necessary to\nintegrate the results of the analysis with the corporate data. Big Data analysis scenarios can be\ndiﬀerent. Only the data analyzed on a Big Data platform can be analyzed without the use of\ncorporate data. In this case, the only remaining task is to integrate the Big Data analysis results with\nEIS. The Big Data analytics integration framework we suggest allows us to integrate Big Data\nanalysis and EIS on three levels: data warehouse level, corporate data model level and report level\n(Figure 3 ). The example described in the previous section demonstrates all three levels of integra-\ntion, as shown in Figure 3 . It has been mentioned earlier that all data integration phases can be\nconducted on the data virtualization server. Consequently, as seen in Figure 3 , integration on the\ncorporate data model level is performed directly between the Big Data platform and the data\nvirtualization server, without the mediation of ETL tools.\nIntegration on the data warehouse level means that the Big Data platform is used to design\nschema on read which is identical to the one segment of the data warehouse model. Data from the\nBig Data platform can be obtained, transformed and loaded into data warehouse tables by using\nsome ETL tool. It has been mentioned earlier that, among other things, the goal of the schema on\nread approach to modeling Big Data is to prepare Big Data so that ETL operations could be more\neﬃcient. As seen in Figure 3 , the ETL tool is linked with one of the ‘schemas on read ’on the Big\nData platform. In the case of data warehouse level of integration, the ﬁrst four phases of the Big\nData integration process from Figure 2 : discovery, data preparation, model planning and model\nbuilding are executed on the Big Data platform, or within ETL tools, and most often combined in\nboth environments ( Figure 3 ). The last two phases from Figure 2 : communicate results and\noperationalize, are executed in the data warehouse ( Figure 3 ). This kind of integration is suitable\nfor batch-oriented Big Data analysis which is repeated periodically (monthly, quarterly, yearly) or on\ndemand ( Figure 4 ).ENTERPRISE INFORMATION SYSTEMS 11\n\n[Página 13]\nIntegration on the corporate data model level can be carried out in two ways. The ﬁrst method\ninvolves the prior preparation of the organization and storage of data on the Big Data platform and\nthe creation of schemas on read according to the corporate data model. The di ﬀerence between\nthis method of integration and integration on the data warehouse level is that, in this way, the\nintegration is done on the virtual level. The data virtualization server connects virtual tables derived\nfrom internal data sources and virtual tables generated from external –Big Data sources (schemas\non read). In the case of integration on the corporate data model level, the ﬁrst two phases of the\nBig Data integration process from Figure 2 : discovery and data preparation are executed on the Big\nData platform ( Figure 3 ). The remaining four phases from Figure 2 : model planning, model\nbuilding, communicate results and operationalize, are realized on the data virtualization server\n(Figure 3 ). The second method involves the implementation of the schema on read modeling\napproach only within the design module of the data virtualization server. This means that by\ndesigning unbound virtual tables, a data model is created, which is subsequently associated with\nreal data sources.\nThe key stages of the schema on read modeling approach are Explore Data and Develop Model\n(Figure 1 ). Both of these phases, according to our integration framework, can be performed on Big\nData platforms over Big Data sources, but also on the data virtualization server, over integrated\ninternal and external data sources. This is shown by schemas on reads in the form of a puzzle\npuzzle segment in Figure 3 .\nIf we observe the three mentioned levels of integration, only integration on the corporate data\nmodel level enables all types of Big Data analysis applications: batch-oriented processing, stream\nprocessing, OLTP (Online Transaction Processing) and interactive ad-hoc queries and analysis\n(Figure 4 ).\nIntegration on the report level means creating schemas on read on Big Data platforms. These\nschemas are created with the aim of representing the data sources for the prede ﬁned reports and\nare designed so as to suit the reports ’requirements. In the case of integration on the report level,\ntheﬁrst four phases of the Big Data integration process from Figure 2 : discovery, data preparation,\nmodel planning and model building are executed on the Big Data platform ( Figure 3 ). The\nremaining two phases from Figure 2 : communicate results and operationalize, are executed on\nthe BI tool ( Figure 3 ). This kind of integration is used for the following Big Data analysis applica-\ntions: batch-oriented processing, stream processing and OLTP ( Figure 4 ).\nFigure 3. Proposed framework for Big Data analytics integration in EISs.12 S. JANKOVI ĆET AL.\n\n[Página 14]\nIn existing batch data integration solutions, which are based only on the use of ETL tools,\nphases: discovery, data preparation, model planning and model building, include copying and\ntemporary storage of large amounts of data in the data staging area. Our integration framework\ndoes not envision data staging area, because these Big Data integration phases are performed\neither on the Big Data platform, or at the virtual level on the server for data virtualization. If\nthese four phases are implemented on the Big Data platform, our integration framework does\nnot exclude the use of some existing solutions, such as Apache Hadoop components useful\nfor ETL.\nWhen it comes to real-time data integration scenarios, our integration framework does not\nexclude existing ESB-based solutions. On the contrary, our approach enables the development of a\ntraditional ESB approach, by the implementation of the ‘hub-and-spoke ’design on the data\nvirtualization server. As described in the previous section, data from all sources are transformed\ninto a uni ﬁed shared format called the mediated schema.\nIf we do not want to store permanently the data in a data warehouse, integration on the data\nwarehouse level can be replaced with integration on the corporate data model level. Additionally,\nintegration on the report level can be replaced with integration on the corporate data model level.\nThe prerequisite for that is to imply data virtualization as an integration approach.\nAs the needs of business analysts and data analysts are becoming similar, the proposed\napproach enables the integration of reporting and analytical tools with enterprise data warehouse\nand external data sources. Depending on the categories of Big Data analytics use cases and the\nspeciﬁc needs and skills of a particular user, the proposed framework enables the following\nintegration scenarios:\n(1) integration on the data warehouse level, for data analysts and developers;\n(2) integration on the corporate data model level, for business analysts (self-service analysis),\ndata analysts and developers;\n(3) integration on the report level, for end users, business analysts, data analysts and\ndevelopers;\n(4) integration on the corporate data model level and data warehouse level, for data analysts\nand developers;\nFigure 4. Levels of integration and Big Data analysis applications.ENTERPRISE INFORMATION SYSTEMS 13\n\n[Página 15]\n(5) integration on the corporate data model level and report level, for business analysts (self-\nservice analysis), data analysts and developers;\n(6) integration on the data warehouse level and report level, for data analysts and developers,\nand\n(7) integration on the corporate data model level, the data warehouse level and the report\nlevel, for data analysts and developers.\nThe integration scenarios appropriate for particular user categories are presented in Figure 5 .\nImplementation of integration framework in transportation domain\nTraﬃc data are an excellent example of heterogeneous data that are continuously coming, making\na demand for Big Data storage and analysis. Excellent tailor-made tra ﬃc data are the best basis for\nexcellent transportation models (Jankovi ćet al. 2016a ). We want to provide the tra ﬃc engineers\nand authorities with pre-attributed maps tailored to their speci ﬁc needs. For the analysis of tra ﬃc\nﬂow, the tra ﬃc engineers calculate the indicators on an annual basis. For example, Annual Average\nDaily Tra ﬃc (AADT), along with its main characteristics of composition and time distribution\n(minutes, hourly, daily, monthly, yearly), is the basic and key input to the tra ﬃc-technical dimen-\nsioning of road infrastructure and road facilities. This parameter is used in capacity analysis, level of\nservice analysis, cost bene ﬁt analysis, safety analysis, environmental assessment impact analysis of\nnoise emission and air pollution, analyses of pavement construction, as well as for the static\ncalculation of road infrastructure objects, tra ﬃc forecasting, etc.\nTo count the tra ﬃc at the speci ﬁed locations on the state roads in the Republic of Serbia, 391\ninductive loop detectors were used (Lipovac et al. 2015 ). These detectors are QLTC-10C automatic\ntraﬃc counters (ATC). The case study included the analysis of tra ﬃc data in ten locations on the\nstate roads and streets in the city of Novi Sad, Serbia, which the tra ﬃc counters generated during\n2015. In order to have sensor data, it is necessary to link them to the tra ﬃc infrastructure data. As\ntwo di ﬀerent data categories exist, namely one that is continually generated and the other that is\nchanged rarely, we recognized the need to process them di ﬀerently. The tra ﬃc data that are\ncontinually generated in our case study are analyzed on the Big Data platform, while the data\nrelated to the tra ﬃc infrastructure are stored in the local relational database. Obviously, there is a\nneed for their integration. In this study, we have integrated Big Data analytics with the existing EIS,\nﬁrst traditionally, without a data virtualization layer, then by using a data virtualization server.\nFigure 5. Levels of integration from Big Data analytics use cases point of view.14 S. JANKOVI ĆET AL.\n\n[Página 16]\nBefore developing the integration solution for our use case, we needed to go through the\nfollowing phases:\n(1) A relational data model was developed and the SQL server database STATE ROADS created.\nThese enable storing the data on the state road reference system in the Republic of Serbia\nand the data on the automatic tra ﬃc counters used on these roads. The most important\nentities of the relational model are the following: road, road section, intersection, automatic\ntraﬃc counter, etc.\n(2) Each automatic tra ﬃc counter generated 365 text ﬁles in 2015. Each ﬁle contained about\n10,000 records on average, so that the collected data amounted to 10 ⋅365⋅\n10,000 = 36,500,000 records.\n(3) For the storage and processing of tra ﬃc data, the Apache Hadoop platform was chosen.\nUsing the Apache Ambari user interface, on the Hortonworks Sandbox –a single-node\nHadoop cluster, with the help of Apache Hive data warehouse software and HiveQL query\nlanguage, a Hive database named TRAFFIC ANALYSIS was created.\n(4) An ETL application was designed to ‘clean up ’the text ﬁles of any invalid records generated\nby tra ﬃc counters. Also, for each counter, this application consolidated the content of all\n365 .txt ﬁles into a single text ﬁle which generated ten large .txt ﬁles. After that, we\nuploaded each of the ten large .txt ﬁles into the HDFS (Hadoop Distributed File System).\nWhite ( 2015 ) did useful work on HDFS. Using HiveQL query language we ‘ﬁlled ’Hive\ndatabase tables with the data from the .txt ﬁles that are stored on HDFS.\nIntegration approach without data virtualization\nThe traditional integration solution –without data virtualization –is presented in Figure 6 . This\nintegration solution was implemented in the following phases:\n(5) We carried out numerous HiveQL queries on the Hadoop TRAFFIC ANALYSIS database\nresulting in useful information on tra ﬃc volumes, tra ﬃc structure, vehicle speeds, etc.\n(Jankovi ćet al. 2016b ). HiveQL has a powerful technique known as Create Table As Select\n(CTAS). This type of HiveQL queries allow us to quickly derive Hive tables from other tables in\norder to build powerful schemas for Big Data analysis. This data modeling approach is known\nas schema on read. Schemas of Hive tables are designed so as to be joined to the relational\nmodel of the local SQL server database. This enables the integration of Big Data analytics\nwith EIS on the corporate data model level. The query results include tra ﬃc volume and\ntraﬃc safety indicators for each counting place: AADT, AADT by directions and vehicle\ncategories, Monthly Average Daily Tra ﬃc (MADT), average speed of vehicles, 85thpercentile\nof vehicle speed, percentage of vehicles that exceed the speed limit, average speeding, etc.\n(6) In the IDE Microsoft Visual Studio 2015, a Windows Forms geo-application called Tra ﬃc\nCounting was developed. It has the following features:\n●An intuitive GUI that allows the tra ﬃc engineers to de ﬁne the query parameters and start\nexecuting the queries against Hive tables on the Hadoop database TRAFFIC ANALYSIS and\ntables from the local SQL server database STATE ROADS. This enables the integration of Big\nData analytics with EIS on the report level. Access to the Hadoop database TRAFFIC\nANALYSIS from the Windows Forms geo-application Tra ﬃc Counting was enabled with\nthe help of Hortonworks ODBC Driver for Apache Hive.\n●A GUI for graphical and tabular visualization of query results and their geo-location. For the\ngeo-location of query results in the Tra ﬃc Counting application, we used Bing Maps and\nOpenStreetMaps.ENTERPRISE INFORMATION SYSTEMS 15\n\n[Página 17]\n(7) The results of the query of the Hadoop database TRAFFIC ANALYSIS were stored in the SQL\nServer database STATE ROADS with the help of Hortonworks ODBC Driver for Apache Hive\nand the Windows Forms geo-application Tra ﬃc Counting. This enabled the integration of Big\nData analytics with EIS on the data warehouse level.\nIntegration approach based on data virtualization\nThe architecture of the integration solution based on data virtualization is presented in Figure 7 .\nThis integration solution includes the ﬁrst phase of the traditional integration approach, while its\nsecond and third phase di ﬀer from the traditional approach:\n(2) A virtual data source was created on the Denodo Express 6.0 data virtualization platform, by\nvirtualizing and integrating data from the local SQL Server database STATE ROADS and the\nHadoop database TRAFFIC ANALYSIS ( Figure 8 ). In this way, the data on volume, structure\nand speed of tra ﬃcﬂow that are generated on the Big Data platform are connected with the\nlocally stored data on state roads in the Republic of Serbia. This enables the integration of Big\nFigure 6. Big Data analytics integration solution without data virtualization.16 S. JANKOVI ĆET AL.\n\n[Página 18]\nData analytics with EIS on the corporate data model level. In Figure 9 , a tree view and a\nrelationship view of data schemas created by combining (merging) ﬁelds from the local and\nBig Data sources are shown. One should notice that the local data source is a relational table,\nand a non-relational table that does not even have the primary key. The query results based\non which ﬁeld combining from the mentioned heterogeneous data sources is performed are\npresented in Figure 10 .\n(3) The Tra ﬃc Counting geo-application, which was developed during the sixth phase of the\ntraditional approach to integration, was linked to the unique virtual data source on the\nDenodo Express 6.0 data virtualization platform. In this way, the Tra ﬃc Counting geo-\napplication uses the results of Big Data analysis from the Hadoop database TRAFFIC\nANALYSIS and data from the local SQL Server database STATE ROADS, integrated on the\nFigure 7. Architecture of the proposed Big Data analytics integration solution.ENTERPRISE INFORMATION SYSTEMS 17\n\n[Página 19]\ndata virtualization platform. Figure 11 shows one window from the Tra ﬃc Counting geo-\napplication that displays average speeding for each counting place. As seen in Figure 11 ,\nvisualization is achieved on the tabular and graphical level and the maps.\nConclusions\nThe continuous emergence of new data sources, data models, database management systems and\ndata integration platforms, coupled with the pronounced need for the self-service analytics used by\nbusiness analysts, makes it increasingly necessary to integrate Big Data analytics with traditional\nEISs on demand. IFAC TC5.3 Technical Committee for Enterprise Integration and Networking of the\nInternational Federation for Automatic Control has recognized the most serious challenges that\nmust be solved in the Next Generation Enterprise Information System (NG EIS) development. The\nfollowing have been selected as its key required features: omnipresence, a model-driven architec-\nture and openness. ‘In the ideal scenario, NG EIS will become a software shell, a core execution\nFigure 8. Big Data analytics integration solution based on data virtualization.18 S. JANKOVI ĆET AL.\n\n[Página 20]\nenvironment with the integrated interoperability infrastructure. Such an environment is foreseen as\na highly ﬂexible and scalable, deployable on any and every platform, using the external models and\nservices infrastructure, exclusively or on a sharing basis. ’(Zdravkovi ćand Trajanovi ć2015 ). Our\napproach to integration of Big Data analytics with EISs is based on a ﬂexible appending of external\nmodels and their joining with the existing corporate data model on the virtual level. In this\nresearch, an approach that enables ﬂexible integration from heterogeneous external sources and\nBig Data analytics with EISs is developed. The key drivers of our integration approach include\nﬂexibility, the reuse of raw/atomic data and querying multiple data stores and types at once.\nThe proposed Big Data analytics integration framework enables seven integration scenarios,\nwhich can include integration on the corporate data model level, on the data warehouse level and\non the report level. Only integration on the corporate data model level enables all kinds of Big Data\nanalysis applications, which include batch-oriented processing, stream processing, OLTP and inter-\nactive ad-hoc queries and analysis. Integration on the data warehouse level enables integration in\nBig Data analysis applications based on batch-oriented processing. Integration on the report level\nenables integration in Big Data analysis applications based on batch-oriented processing, stream\nprocessing and OLTP. All integration scenarios start with the designing of schemas for data analysis\nat the time of reading raw Big Data sources. Schemas on read are designed so as to be integrated\ninto the existing relational corporate data models and/or the existing business reports, taking into\naccount the structure of the source data ﬁles.\nFrom the point of view of Big Data analytics use cases, integration scenarios can be divided into\nthree categories. For end users, integration scenarios that include integration on the report level\nFigure 9. Tree view and relationships view of data source on data virtualization platform.ENTERPRISE INFORMATION SYSTEMS 19\n\n[Página 21]\nare appropriate. For business analysts and business reporting, integration scenarios that include\nintegration on the corporate data model level and/or on the report level are appropriate. For data\nanalysts, data discovery and developers, integration scenarios that include integration on all three\nlevels, namely the corporate data model level, the data warehouse level and the report level, are\nappropriate.\nThe case study conducted has con ﬁrmed that the use of a data virtualization layer o ﬀers\nnumerous advantages. These can be classi ﬁed into three groups. The ﬁrst group of advantages\ncomes into play if the user accesses only one data source and it consists of the following: a data\nvirtualization layer with the capability of language translation and API supported by a language\ndata warehouse and API suitable for data users, independence from data source technologies (in\nthe era of the IoT and Big Data, the possibility of exchanging a non-SQL data warehouse with a SQL\nwarehouse is very important), and minimal negative user in ﬂuence of data warehouse perfor-\nmance. The second group of advantages is connected to metadata speci ﬁcation, such as: a table\nstructure, cleansing and transformation operations, aggregation and similar. When data virtualiza-\ntion is used metadata speci ﬁcation is implemented only once and it is not necessary to copy it for\nseveral data users. In other words, data users share and use metadata speci ﬁcations on multiple\noccasions, with which they achieve more simple table structures, centralized data transformation,\ncentralized data cleansing, simpli ﬁed application development, more consistent application beha-\nvior and more consistent results. The third group refers to data integration from multiple data\nsources and includes the following: a uni ﬁed approach to di ﬀerent types of data warehouses (SQL\nServer database, Excel worksheets, index sequential ﬁles, NoSQL databases, XML ﬁles, HTML web\nFigure 10. Query results view on data virtualization platform.20 S. JANKOVI ĆET AL.\n\n[Página 22]\npages, etc.), centralized data integration and sharing of integration programming code, consistent\nreport results and e ﬃcient distributed data access.\nIn view of the positive experiences gained while using a data virtualization platform, the\nauthors ’future research will focus on the use of the above platform in the integration of Big\nData analytics with NoSQL databases, such as column and key-value databases.\nDisclosure statement\nNo potential con ﬂict of interest was reported by the authors.\nFunding\nThis paper has been partially supported by the Ministry of Education, Science and Technological Development of the\nRepublic of Serbia project under No. 36012, and the project under the name “Novel Decision Support tool for\nEvaluating Strategic Big Data investments in Transport and Intelligent Mobility Services –NOESIS ”. NOESIS project has\nreceived funding from the European Union ’s Horizon 2020 research and innovation programme under grant agree-\nment No 769980. The data generated by automatic tra ﬃc counters have been provided by the company MHM -\nProject from Novi Sad.\nReferences\nAlooma. 2018 .“ETL Tools. ”January 4. https://www.etltools.net/\nArputhamary, B., and L. Arockiam. 2015 .“A Review on Big Data Integration. ”International Journal of Computer\nApplications Proceedings on International Conference on Advanced Computing and Communication Techniques for\nHigh Performance Applications 5: 21 –26.\nChen, C. L. P., and C. Y. Zhang. 2014 .“Data-Intensive Applications, Challenges, Techniques and Technologies: A Survey\non Big Data. ”Information Sciences 275: 314 –347. doi: 10.1016/j.ins.2014.01.015 .\nDoan, A., A. Halevy, and Z. Ives. 2012 .Principles of Data Integration . Waltham: Morgan Kaufmann.\nFigure 11. Traﬃc Counting geo-application –Average Speeding window.ENTERPRISE INFORMATION SYSTEMS 21\n\n[Página 23]\nDong, X. L., and D. Srivastava. 2015 .Big Data Integration (Synthesis Lectures on Data Management) . Williston: Morgan &\nClaypool Publishers.\nEMC Education Services, ed. 2015 .Data Science & Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting\nData . Indianapolis: John Wiley & Sons.\nFlorea, A. M. I., V. Diaconita, and R. Bologa. 2015 .“Data Integration Approaches Using ETL. ”Database Systems Journal\n(VI)3: 19 –27.\nGal, A. 2011 .Uncertain Schema Matching . (Synthesis Lectures on Data Management). Williston: Morgan & Claypool\nPublishers\nGandomi, A., and M. Haider. 2015 .“Beyond the Hype: Big Data Concepts, Methods, and Analytics. ”International\nJournal of Information Management 35: 137 –144. doi: 10.1016/j.ijinfomgt.2014.10.007 .\nGokhe, P. 2016 .“Enterprise Real-Time Integration. ”E-book. http://www.enterpriserealtimeintegration.com/enterprise-\nreal-time-integration/\nIntel Corporation. 2013 .“Extract, Transform, and Load Big Data with Apache Hadoop. ”White Paper Big Data Analytics.\nhttps://software.intelcom/sites/default/ ﬁles/article/402274/etl-big-data-with-hadoop.pdf\nIntel Corporation. 2014 .“Real-Time Big Data Analytics for the Enterprise. ”White Paper Intel® Distribution for Apache\nHadoop. https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/big-data-hadoop-real-\ntime-analytics-for-the-enterprise-paper.pdf\nJankovi ć, S., D. Mladenovi ć, S. Mladenovi ć, S. Zdravkovi ć, and A. Uzelac. 2016a .“Big Data in Tra ﬃc.”InProceedings of\nthe First International Conference Transport for Today ’s Society –TTS 2016 , edited by M. M. Todorova, 28 –37. Bitola,\nMacedonia: Faculty of Technical Science.\nJankovi ć, S., S. Zdravkovi ć,S. Mladenovi ć, D. Mladenovi ć, and A. Uzelac. 2016b .“The Use of Big Data Technology in the\nAnalysis of Speed on Roads in the Republic of Serbia. ”InProceedings of the Third International Conference on Tra ﬃc\nand Transport Engineering - ICTTE Belgrade 2016 , edited by O. Čokorilo, 219 –226. Belgrade: City Net Scienti ﬁc\nResearch Center.\nLipovac, K., M. Vujani ć, T. Ivani šević, and M. Rosi ć.2015 .“Eﬀects of Application of Automatic Tra ﬃc Counters in Control\nof Exceeding Speed Limits on State Roads of Republic of Serbia. ”InProceedings of the 10th Road Safety in Local\nCommunity International Conference , edited by Pro ﬀ. K. Lipovac and M. Ne šić, 131 –140. Belgrade: Academy of\nCriminalistic and Police Studies.\nLoshin, D. 2013 .Big Data Analytics: From Strategic Planning to Enterprise Integration with Tools, Techniques, NoSQL, and\nGraph . Waltham: Elsevier.\nMacura, M. 2014 .“Integration of Data from Heterogeneous Sources Using ETL Technology. ”Computer Science 15 (2):\n109 –132. doi: 10.7494/csci.2014.15.2.109 .\nReeve, A. 2013 .Managing Data in Motion . Waltham: Elsevier.\nRibeiro, A., A. Silva, and A. R. da Silva. 2015 .“Data Modeling and Data Analytics: A Survey from A Big Data Perspective. ”\nJournal of Software Engineering and Applications 8: 617 –634. doi: 10.4236/jsea.2015.812058 .\nRussom, P. 2013 .Integrating Hadoop into Business Intelligence and Data Warehousing . Renton, WA: Data Warehousing\nInstitute.\nSridhar, P., and N. Dharmaji. 2013 .“AComparative Study on How Big Data Is Scaling Business Intelligence and\nAnalytics. ”International Journal of Enhanced Research in Science Technology & Engineering 2 (8): 87 –96. -izbaciti.\nvan der Lans, R. F. 2012 .Data Virtualization for Business Intelligence Systems . Waltham: Elsevier.\nWang, H., Z. Xu, H. Fujita, and S. Liu. 2016 .“Towards Felicitous Decision Making: An Overview on Challenges and\nTrends of Big Data Technologies. ”Information Sciences 367 –368: 747 –765. doi: 10.1016/j.ins.2016.07.007 .\nWhite, T. 2015 .Hadoop: The De ﬁnitive Guide . Sebastopol, CA: O ’Reilly Media.\nZdravkovi ć, M., F. Luis-Ferreira, R. Jardim-Goncalves, and M. Trajanovi ć.2015 .“On the Formal De ﬁnition of the\nSystems ’Interoperability Capability: An Anthropomorphic Approach. ”Enterprise Information Systems 11 (3): 389 –\n413. doi: 10.1080/17517575.2015.1057236 .\nZdravkovi ć, M., and H. Panetto. 2017 .“The Challenges of Model-Based Systems Engineering for the Next Generation\nEnterprise Information Systems. ”Information Systems and e-Business Management 15 (2): 225 –227. doi: 10.1007/\ns10257-017-0353-z .\nZdravkovi ć, M., and M. Trajanovi ć. 2015. “On the Runtime Models for Complex, Distributed and Aware Systems ”In\nProceedings of the 5th International Conference on Information Society and Technology –ICIST 2015 , edited by M.\nZdravkovi ć, M. Trajanovi ć, and Z. Konjovi ć, 236 –240. Kopaonik, Serbia: Society for Information Systems and\nComputer Networks.22 S. JANKOVI ĆET AL.",
      "metadata": {
        "filename": "Schema on read modeling approach as a basis of.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\Schema on read modeling approach as a basis of.pdf",
        "file_size": 3131260,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:40.349884",
        "content_length": 68041
      }
    },
    "ffa90a09-f02d-4af5-baf2-e3c80f3aa6fb": {
      "id": "ffa90a09-f02d-4af5-baf2-e3c80f3aa6fb",
      "content": "[Página 1]\n1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing\n1\nCommon Pitfalls of Benchmarking\nBig Data Systems\nGwen Shapira, Y anpei Chen. Cloudera Inc.\nfgshapira,yanpeig@cloudera.com\nAbstract —It is challenging to get reliable performance benchmarking results. Benchmarking matters because one of the deﬁning\ncharacteristics of big data systems is the ability to process large datasets faster. “How large” and “how fast” drive technology choices,\npurchasing decisions, and cluster operations. Even with the best intentions, performance benchmarking is fraught with pitfalls - easy to\nget numbers, hard to tell if they are sound.\nThis paper discusses ﬁve common pitfalls drawn from engineering and customer experiences at Cloudera, a leading big data vendor.\nThese pitfalls are: “Comparing Apples to Oranges”- when too many parameters are modiﬁed and comparison is impossible, “Not\nTesting at Scale” - trying to test a big data system by extrapolating from an under-sized test system, “Believing in Miracles” - failing to\nquestion suspicious results, “Using Unrealistic Benchmarks” - using workloads far removed from what will realistically be used by\ncustomers, and “Communicating Results Poorly” - neglecting to communicate sufﬁcient information for customers to understand and\nreproduce the results.\nThese pitfalls offers a behind-the-scenes look at internal engineering and review processes that produces rigorous benchmark results.\nReaders working on big data in both the industry and in academia can draw lessons from our experience.\nIndex Terms —Big data, performance, benchmarking, case studies.\nF\n1 I NTRODUCTION\nDone poorly, performance benchmarking produces dis-\nastrous results. Here are two stories from the authors’ early\ncareers.\nAn engineer ran a benchmark on a proof-of-concept\n5-node cluster. Extrapolating the results, the engineer as-\nsumed the system will scale linearly and plans for a 50-node\ncluster to support the required production workloads. The\nproduction cluster ran for 30 minues before latency became\ncompletely unacceptable. It hit network bottlenecks not\nrevealed at the proof-of-concept scale. As a result, rollout\nof the production system had to be delayed by a week as\nthe scalability problems were being resolved.\nA graduate student ran a Hadoop benchmark without\nrealizing that he accidentally mounted the Hadoop Dis-\ntributed File System (HDFS) on the departmental network\nﬁler. The benchmark promptly took down the ﬁler for all\nprofessors, staff, and students at the department. The stu-\ndent received angry e-mails from the system administrators\nfor days following the incident.\nThese two particular stories reveal how difﬁcult it is\nto do performance benchmarking in a way that does not\ndisrupt customer-facing, production systems, in a way that\nrepresent real-life workloads running there. Despite perfor-\nmance being an increasingly visible aspect of big data sys-\ntems, there has not yet been many case studies of common\nbenchmarking pitfalls, nor ways to avoid them. In this in-\ndustry experience paper, we offer a collection of stories that\nillustrate important principles of conducting performance\nbenchmarking and assessing others’ results:\n1) Workload and hardware choices should be relevant\nto the expected use of the product.2) When modifying a standard benchmark, the modi-\nﬁcation should be documented and justiﬁed.\n3) Testing big data means testing the system along\nmultiple dimensions of large scale: Large number\nof jobs, jobs with large number of tasks, large data\nsize, large clusters, and large nodes.\n4) Tests designed to compare systems across a single\nparameter, e.g., new version of platform, must make\nsure this parameter was the only change. Changing\nadditional parameters invalidates the comparison.\n5) Having a model of expected behavior of the system\nis mandatory. Otherwise it is impossible to reason\nabout the results.\n6) Benchmark results should include enough informa-\ntion to reproduce the result - hardware, conﬁgura-\ntion, and workload.\n7) Make sure any results tables and charts are clear,\nmeaningful, and not misleading.\nThe stories in this paper come from internal engineering\nand customer experiences at Cloudera, a leading big data\nvendor. The pitfalls involve performance benchmarking of\ndifferent components in the Hadoop ecosystem. This is not\na comprehensive categorization of all possible mistakes, our\ngoal is to give readers in both the industry and in academia\ntools with which they can improve their own work.\n2 C OMPARING APPLES TO ORANGES\nWe often run two tests, expecting only one parameter to\nchange, while in fact many parameters changed and a\ncomparison is impossible - in other words, we compare\napples to oranges.\n\n[Página 2]\n1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing\n2\nLate 2013, the Hadoop community adopted MapRe-\nduce 2 (MR2) running on Yet Another Resource Negotiator\n(YARN) as the default MapReduce execution framework [1],\n[25]. This change offers functionality improvements over the\noriginal MapReduce, or MapReduce 1 (MR1) [12]. Many\ncluster operators did performance benchmarking on their\nown when they considered whether to upgrade. They ini-\ntially reported a performance regression from MR1 to MR2.\nWhat actually happened was that a straightforward com-\nparison ended up comparing two different things, in other\nwords, “comparing apples to oranges”. Two issues led to\nthis discrepancy.\nOne issue was that TeraSort, a limited but frequently\nused benchmark, changed between MR1 and MR2 [24]. To\nreﬂect rule changes in the GraySort benchmark on which it\nis based, the data generated by the TeraSort included with\nMR2 is less compressible. A valid comparison would use\nthe same version of TeraSort for both releases, because map\noutput compression is enabled by default as a performance\noptimization in Cloudera Distribution with Apache Hadoop\n(CDH). Otherwise, MR1 will have an unfair advantage by\nusing more compressible data (Figure 1).\nAnother issue was the replacement of “task slots” in\nMR1 with “containers” in MR2. YARN has several conﬁg-\nuration parameters that affected how many containers will\nbe run on each node [5]. A valid comparison would set these\nconﬁgurations such that there is the same degree of parallel\nprocessing between MR1 and MR2. Otherwise, depending\non whether hardware is over or under-committed, either\nMR1 or MR2 will have the advantage.\nWe committed these pitfalls ourselves in the early days\nof ensuring MR1 and MR2 performance parity. We regularly\ncompared MR1 and MR2 performance on our nightly CDH\nbuilds, and the “regression” was caught the very ﬁrst time\nwe did this comparison. Our MapReduce and Performance\nEngineering teams collaborated to identify the code changes\nand understand what makes a valid performance compari-\nson. This effort culminated in MR2 shipped in CDH5.0.0 at\nperformance parity with MR1.\nHere are some questions to ask regarding your own per-\nformance tests: If you are comparing hardware, are you run-\nning identical workloads? If you are comparing software,\nare you running your workload on identical hardware?\nIdentical data, with identical formats and compression? Did\nthe test procedure or test harnesses change?\n3 N OTTESTING AT SCALE\nBig data is called big for a reason. Testing small workloads\non small clusters and expecting the results to extrapolate to\nlarge scale systems simply does not work.\n”Scale” for big data systems can mean data scale, con-\ncurrency scale (number of jobs and number of tasks per\njob), cluster scale (number of nodes/racks), or node scale\n(per node hardware size). Failing to test “at scale” for any of\nthese dimensions can lead to surprising behavior for your\nproduction clusters.\nIt is illustrative to look at another aspect of our efforts\nto drive MR2 to performance parity with MR1. We wanted\nto verify that MR2 and MR1 perform at parity when a\nlarge number of jobs are running. We ran SWIM [6], which\nFig. 1. Terasort performance when the data generation in MR1 and MR2\nuse different algorithms (left) or the same algorithm (right).\nsubmits many jobs concurrently over hours or even days,\nsimulating the workload logged on actual production clus-\nters. The ﬁrst runs of SWIM on MR2 revealed a live-lock\nissue [3] where the jobs would appear as submitted, but\nnone of them would make any progress. Figure 2 shows\na web user-interface (UI) screenshot of a YARN Resource\nManager that is experiencing live-lock.\nThe cause of the live-lock is not straightfoward. Each\nMR2 job has an Application Master, which is a book-keeping\ntype task that tracks the progress of the entire job. The\nApplication Master still requires a YARN container to run.\nWithout additional conﬁgurations, YARN would give all\navailable resources to the Application Masters, leaving no\nroom for the actual tasks. The tasks are behaving normally,\nbut making no progress, i.e., live-lock.\nThis issue escaped detection in our other scale tests that\ncovered a range of data, cluster, and node scales. The live-\nlock occurs only when all the containers in a cluster are\ntaken up by Application Masters. On a cluster of non-trivial\nsize, this means hundreds or even thousands of concurrent\njobs. SWIM is speciﬁcally designed to reveal such issues by\nreplaying production workloads with their original level of\nconcurrency and load variation over time. In this case, we\nfound a critical issue.\n4 B ELIEVING IN MIRACLES\nIf something is too good to be true, it is probably not true.\nWe should always have a model of expected system behav-\nior and bottlenecks. This way, we can tell if a performance\nimprovement is reasonable, or too good to be true. Here are\nsome recent “miracles” we debunked.\n4.1 Miracle 1: 1000x SQL speedup\nA customer reported that Impala [11], a SQL-on-Hadoop\nsystem, performs more than 1000x better than their existing\nrelational database manage system (RDBMS). The customer\nwanted us to help them set up a new cluster to handle their\ngrowing production workload.\nThe 1000x difference is orders of magnitude larger than\nour own measurements [14], and immediately made us\nskeptical. Following much discussion, we realized that the\ncustomer was comparing very simple queries running on\n\n[Página 3]\n1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing\n3\nFig. 2. YARN Resource Manager screenshot of live-lock symptoms.\na proof-of-concept Impala cluster versus complex queries\nrunning on a heavily-loaded production RDBMS system.\nWe helped the customer do an apple-to-apple compar-\nisons, and turns out Impala still has an advantage (average\n2x faster and up to 4.5x faster, from [14]). We left the\ncustomer with realistic plans for how to grow their data\nmanagement systems.\n4.2 Miracle 2: Indirect writes faster than direct writes\nA customer asked us to run several conﬁgurations of\nSqoop [2], a Hadoop-to-RDBMS connector used to bulk\ntransfer data between the two types of systems. The intent\nwas to ﬁnd the conﬁguration leading to the best perfor-\nmance of exporting data from Hadoop to RDBMS. Among\nother tests, we compared the performance of loading data\nto new partitions through Oracle’s direct path writes, to\nloading the same data through normal inserts.\nWe expect direct path writes to be signiﬁcantly faster,\nsince they bypass the busy buffer-cache and redo log sub-\nsystems, writing data blocks directly to Oracle’s data ﬁles.\nIn this test, the normal inserts exercising an indirect write\npath were 3 times faster than the direct path writes. This\nsuspicious result called for additional investigation.\nThe investigation revealed that Sqoop was exporting\naround 50GB of data to an otherwise idle Oracle cluster\nwith over 300GB of memory dedicated to the buffer cache.\nLoading data into memory in a server with no contention\nis obviously faster than writing the same data to disk. We\nexplained the results to the customer and recommended\nrepeating the tests on a cluster with realistic workloads.\n4.3 Miracle 3: 100x Hadoop sort speedup\nA customer asked us for comment on a Hadoop sort bench-\nmark result in the trade press. The result was more than100x faster than what we found internally.\nIt turns out that the data size being tested was consid-\nerably smaller than the available memory in the cluster.\nIn other words, a knowledgeable operator would be able\nto conﬁgure Hadoop in a way that the sort takes place\ncompletely in memory.\nThis departed from the common practice of conﬁguring\nsort with data size much greater than total cluster memory.\nThe more-than-100x gap came from the inherent hardware\ndifference between memory and disk IO, rather than a\ndifference between two software systems.\nThe ability to identify miracles requires us having mod-\nels of expected performance beyond just a “gut-feeling”.\nThese models can come from prior results, or an under-\nstanding of where the system bottlenecks should be. Bench-\nmarking without such models would give you a lot of\nnumbers but not a lot of meaning.\n5 U SING UNREALISTIC BENCHMARKS\nUnrealistic benchmarks are benchmarks where the work-\nload, hardware, or presentation is chosen without regard of\nreal-life requirements. Rather, these choices intend to inﬂate\nthe capabilities of benchmarked system under test. Here are\nsome warning signs of a biased benchmark:\n5.1 Misleading workloads\nExamples of misleading workloads include when some-\none ran benchmarks on 100GB of data when the system\nis intended for 100TB data sets, or when a transactional\nworkload is used to test a system with mostly analyti-\ncal use-cases. Terasort, a very popular benchmark for big\n\n[Página 4]\n1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing\n4\nFig. 3. Different dimensions of a big data workload.\ndata systems, is also potentially misleading. Terasort has\nvery speciﬁc characteristics that stress very speciﬁc subsets\nof the processing subsystem. It is not necessarily a good\nbenchmark to evaluate how the system will scale for all\nworkloads, even though it is a useful ﬁrst step in comparing\ndifferent hardware conﬁgurations.\nAn example of how we avoid it at Cloudera: Terasort is\nonly one job in our MapReduce performance benchmarking\nsuite. We run a set of stand-alone, artiﬁcial jobs designed to\nstress in isolation different components of the MapReduce\nIO and compute pipeline; this suite includes open source\njobs such as Terasort, and some jobs written in-house that\nwe consider proprietary assets. We also use an open source\ntool [6] to replay full customer workloads with a large range\nof job sizes, types, and arrival patterns. We run both the\nstand-alone jobs and multi-job workloads under different\ndimensions of scale beyond just data size (See Section 3).\n5.1.1 What makes a representative workload?\nCluster operators often ﬁnd it challenging to reason about\ntheir own workload. If someone has no idea what their\nproduction workload looks like, they will have no idea\nwhether the workload captured in a benchmarking study\nwill match their own use case.\nFigure 3 is a diagram to help readers characterize their\nworkload. In broad strokes, there are three dimensions -\nthe data characteristics, the compute characteristics, and\nthe load-over-time characteristics [7]. Readers should ask\nthemselves what is the following for their workload:\nData:\n\u000fHow large is the data?\n\u000fWhat is the data schema, i.e., how do different parts\nof the data relate to each other?\n\u000fIs there any data skew, i.e., whether some data is\naccessed more frequently than others?\n\u000fHow is the data represented and stored, i.e, what is\nthe data format or data type?\nCompute:\n\u000fWhat is the hardware bottleneck for the computation\ndone? CPU, memory, disk, or network?\n\u000fIf the workload is a SQL workload, whether the\nqueries involve joins, scans, ﬁlters, group-by’s?\u000fIf the workload is MapReduce, whether the jobs need\nto do a lot of shufﬂe, sort, combiner operations, are\nthey map-heavy or reduce heavy?\n\u000fIf the workload is something else, characterize it in\nterms of the semantics of that processing paradigm.\nLoad:\n\u000fWhat is the load average?\n\u000fHow long and how high are bursts in load?\n\u000fHow do the mix of jobs or queries change over time?\n\u000fAre there diurnal patterns?\nThese questions should get readers started on charac-\nterizing their own workload. Answering these questions\ndirect the discussion to other, more complicated, case-by-\ncase characteristics that are also important to capture.\nIn a real-world example, we start by identifying the\nprimary components of a production workload. If, say,\nMapReduce, HBase, and Impala are all involved, we need\nto make sure the test workload combines all of those.\nDrilling farther in, we may see that most of the MapReduce\nworkload is map-only, with very little data being shufﬂed or\nreduced. We may also see that the HBase workload is 75%\nput and 20% get and 5% scans, and the Impala workload\nconsists of star-schema joins that include one large table and\nmany smaller tables, the results of which will be aggregated\nby day and month. We make sure our benchmark workload\nincludes this level of details.\nThe next step is to note the data sizes, and either copy\nsufﬁcient data from production, or write a small script\nthat will generate synthetic data for the benchmark. It is\nrecommended to note speciﬁc data patterns that should\nbe part of the test - for example, if the workload involves\nsales data, it is likely that some regions and dates have\nsigniﬁcantly more records than others. This type of skew\ncan impact performance and therefore benchmark results.\nThe last step is to check characteristics of the load pat-\nterns. Start with ﬁnding out how many concurrent jobs and\nqueries typically run in production. Then decide whether\nto test with average load, peak load, expected future peak\nload, or perhaps the test should increase the load to the\npoint the system breaks in order to ﬁnd theoretical limits\n(test to destruction). Since multiple workloads are involved\n(MapReduce, Impala and HBase), we need to know if\nthose workloads are typically executed together, or if they\nrun during different times. For example, if we run Impala\nqueries mostly during business hours and MapReduce dur-\ning the night, the test should combine light Impala load with\nheavy MapReduce load and vice-versa, to simulate expected\nproduction conditions.\nThis type of planning leads to more meaningful results\nand is well worth the extra effort.\n5.2 Premium hardware\nBenchmark reports often contain results that come from\nhardware not typically used in real-life - solid state drives\n(SSDs) in environments that commonly use hard disk drives\n(HDDs), or premium SSDs not available in the general\nmarket. The Transaction Processing Council - C (TPC-C) [30]\nbenchmark allows the use of hardware that is not available\nprovided that availability dates are published. It is wise to\n\n[Página 5]\n1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing\n5\ncheck if the hardware choices make results irrelevant for\nguiding purchase decisions.\nAn example of how we avoid it at Cloudera: We have\nexplored MapReduce performance for SSDs [20]. We were\nvery conscious of SSD’s prevalence in the market compared\nwith HDDs. This prompted us to suggest to our hardware\npartners to track SSD performance-per-cost, which shows\nSSDs approaching parity with HDDs, even though the gap\nin capacity-per-cost remains large.\n5.3 Cherry picking queries or jobs\nSome reports pick very speciﬁc queries out of a standard\nbenchmark, but cannot explain the choice with objective\ncriteria that is relevant to the real-life use cases (or worse,\ndoes not disclose that a choice was made).\nAn example of how we avoid it at Cloudera: Our\npast Impala performance results [13], [14] used 20 queries\nderived from the TPC - Decision Support (TPC-DS) [32]\nbenchmark. These queries were chosen based on what our\ncustomers observed for business intelligence (BI) use cases.\nThey cover interactive, reporting, and deep analytic use\ncases. At the time, it was a major improvement over a\nfrequently cited set of ﬁve queries [21] that were constructed\nwithout empirical backing from actual customer use cases.\nThe 20 queries also represent a step forward from our own\nearly efforts [9] using queries derived from TPC-H [31].\nTPC-H is a less demanding benchmark with fewer and less\ncomplex queries than TPC-DS, while both are backed by\ncustomer surveys from vendors in the TPC Consortium. We\nhave kept the set of 20 queries derived from TPC-DS to help\nourselves compare against our own prior results, and we are\nwell aware they are less than the full set of 99 queries in the\nofﬁcial TPC-DS. Look for our future reports in this space.\n5.4 Questions to ask all benchmark reports\nTo an extent all commercial and even research benchmarks\nare suspect of bias, since they are performed by a spe-\nciﬁc vendor or research group to promote their products\nor search project. Cluster operators can hold benchmark\nreports accountable by understanding their own workload\nand have a conversation about whether a product or re-\nsearch project addresses their speciﬁc use case. The follow-\ning is a list of questions to ask.\n\u000fWhat hardware did you use?\n\u000fHow was it conﬁgured?\n\u000fIs it similar to the hardware you are selling?\n\u000fWhich jobs or queries did you run?\n\u000fWhy do you think they mimic my workload?\n\u000fWere they modiﬁed from a well-known spec?\n\u000fHow did you choose these speciﬁc jobs or queries?\n\u000fWhat if the jobs or queries are different?\nWith these questions, cluster operators force benchmark\nreports to discuss the limits of their own work.\n6 (M IS)COMMUNICATING RESULTS\nPoor communication detracts from otherwise good perfor-\nmance results. Here at Cloudera, we check all external-\nfacing benchmarking communications for the following:We select a benchmark that\n\u000fIs unbiased (see Section 5),\n\u000fExercise workloads relevant to actual customers, and\n\u000fScales across data size, concurrency level, cluster\nsize, and node size.\nWe report sufﬁcient information for industry peers to assess\nthe signiﬁcance of the result, and to reproduce the tests if\nneeded. This requires reporting\n\u000fThe benchmark used and why we chose it,\n\u000fThe metrics used and how we measured them,\n\u000fThe hardware used and the software tuning applied.\nThese simple guidelines are often neglected in results\ncoming from both industry and academia.\nOne more aspect of a good benchmarking report is\nwhether the results have been independently veriﬁed or\naudited. The purpose of an independent audit is to have\nthe above checks done by someone other the organization\nthat produced study. Results that passed independent audit\nare more likely to be communicated clearly and completely.\nThere are several gold-standards for audit and veriﬁca-\ntion practices established before the rise of big data:\nDedicated auditors\nThe Transaction Processing Council (TPC) [28] uses ded-\nicated auditors. Each auditor is certiﬁed to audit a particular\nbenchmark only after passing a test designed by the work-\ning group who initially speciﬁed that benchmark [29].\nValidation kit and fair-use rules\nThe Standard Performance Evaluation Corporation\n(SPEC) [27] uses validation checks built into benchmark-\ning kits, fair-use rules governing how the results should\nbe reported, and review by the SPEC organization, which\nencompasses many industry peers of the test sponsor.\nPeer review\nThe ofﬁcial Sort Benchmark [26] has new submissions\nreviewed by past winners. The winners would “hand over\nthe torch” only if new entries are sufﬁciently rigorous.\nThere are not yet any widely accepted audit and veri-\nﬁcation processes for big data. The need for complete and\nneutral benchmarking results sometimes gets diluted by the\nneed to stand out in the trade press. However, the past\nyear has seen a phenomenal growth in the level of per-\nformance knowledge in the broader technical community.\nEvery benchmark report is now scrutinized by industry and\nacademia peers. This increases the need to be rigourous and\nopen about performance benchmarking results.\n6.1 A picture in need of 1000 words\nPerformance reports often use graphs to summarize re-\nsults. Poor graphs can unintentionally or deliberately mis-\nlead readers. We include here an example of a poorly-\ncommunicated graph and a better-communicated graph.\nFigure 4 comes from one of the author’s early work mea-\nsuring the performance of distributed databases. None of\nthe axes were labeled, the performance metrics are unclear,\n\n[Página 6]\n1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing\n6\nFig. 4. An example of a poorly-communicated graph.\nFig. 5. An example of a better-communicated graph. It still needs a lot of\nsurrounding text for a full explanation.\nand the test scenario and test settings are unclear. Even the\ngraph’s creator cannot recollect what was being displayed.\nHere is what the authors together deciphered. The graph\nis showing database throughput measured in transactions\nper minute (TPM), query latency (response time), and CPU\nutilization of the system. The horizontal axis is likely show-\ning the number of concurrent user or a similar sense of\n“load”. CPU utilization increases under higher load, and\nthe right vertical axis is of the correct numerical range for\nCPU utilization in percentages. The left vertical axes could\nbe either TPM in number of queries, or response time in\nmilliseconds. There is no way to tell without additional\ninformation. Without proper labeling and documentation,\nevery well-done performance benchmarking studies lose\ntheir meaning over time.\nFigure 5 appears in a recent Cloudera blog [9]. It is a\nbetter communicated graph. Without further text, here is\nwhat the ﬁgure communicates: The graph shows Impala\nmulti-tenant performance, with the metric being a nor-\nmalized, unitless metric of multi-tenant performance as a\nfraction of stand-alone performance. This metric has the\nproperty that “higher is better”. The graph comes from\nﬁve tests, with Impala receiving an increasing fraction of\nsystem resources ranging from 25% to 75%. There is large\nperformance variation as shown by the error bars. There isalso a model of desired system behavior, one that suggests\nImpala should show fraction xof stand-alone performance\nwhen given fraction xof system resources.\nThere is still a lot of information missing from the graph:\nWhat was the workload being tested? It was Impala running\nconcurrently with MapReduce on the same cluster, speciﬁ-\ncally one MapReduce job concurrent with one Impala query\nat a time. The cluster is conﬁgured to give fraction xof the\nresources to Impala, with MapReduc receiving the remain-\ning fraction 1\u0000x.\nWhat metric is being normalized? Impala query duration when\nthe cluster is executing only the Impala query vs. when the\ncluster is executing an Impala query with a MapReduce job.\nWhat do the error bars show and why are they so large? Each\ndata point is the arithmetic average of 56 MapReduce job\nand Impala query combinations. The 56 job-query combi-\nnations cover a large range of MapReduce job types and\nImpala query types, hence the large variation. The error bars\nthemselves represent 25th to 75th percentile range across the\njob-query combinations.\nWhat fractions of resources were assigned to Impala for the 2nd\nand 4th markers? It is not immediately clear from the ticker\nmark intervals on the horizontal axes, but the 2nd and 4th\nmarkers represent 40% and 60% of the cluster resourcess\nassigned to Impala.\nWhat about MapReduce multi-tenant performance? The com-\npanion graph for MapReduce multi-tenant performance is\nFigure 6.\nThe graph guides the discussion to more interesting\ntopics, such as why should the performance model be as\nit is, whether the test workload is realistic and useful, and\nwhether the performance is actually good.\nThe following is a list we use to check our own graphs.\n\u000fDoes the graph need a title, or is one unnecessary\nbased on surrounding text?\n\u000fIf the graph shows multiple data series, is a legend\ndisplayed or included in the graph caption?\n\u000fAre the graph axes labeled? Do the labels include\nappropriate units?\n\u000fIs there one or several performance metrics being\ngraphed?\n\u000fIf there is a single performance metric graphed, is it\non the vertical axes?\n\u000fAs big data performance is variable from measure-\nment to measurement, are error bars necessary?\n\u000fIf a line or curve is drawn connecting two markers,\nis it reasonable to extrapolate across a range of un-\nmeasured settings?\n\u000fIf there is a model of desirable behavior, is the model\nalso shown on the graph?\nBig data systems have evolved to the point where the\nmeaning of performance can be complex, and the number of\nrelevant metrics can be large. This is especially true when we\nconsider different big data processing engines not as stand-\nalone components, but as concurrently active frameworks\nsharing resources on the same cluster. Thus, we should\nmake every effort to ensure clear communication.\n\n[Página 7]\n1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing\n7\nFig. 6. Companion graph to Figure 5, showing MapReduce multi-tenant\nperformance.\n6.2 Following our own advice - Miracle checking\nEarlier we highlighted the need to check any miracle re-\nsults for their validity. In Figure 5, the fact that multi-\ntenant performance turned out better than modeled is an\nimmediate warning sign for a possible “miracle” result.\nSince Impala and MapReduce were concurrently active for\nthe multi-tenant scenario, the results would be reasonable\nif MapReduce multi-tenant performance suffered and was\nlower than modeled. The opposite happened, and the com-\npanion MapReduce multi-tenant performance also exceeded\nour model (Figure 6). This was indeed a “miracle” result\nworth understanding.\nTwo factors caused this result. First, our test scenarios\nrun through 56 pairs of concurrent MapReduce jobs and\nImpala queries, one pair at a time. For any given pair, either\nthe MapReduce job or Impala query would complete ﬁrst.\nThereafter, the remaining MapReduce job or Impala query\nwould receive the entire cluster’s resources. In other words,\nour test procedure systematically skewed the results in favor\nof being better than the model.\nAnother reason is the statistical multiplexing of hard-\nware resource demands. This is a subtle effect of multi-\ntenant processing. For our tests, a MapReduce job and an\nImpala query need different hardware resources at different\ntimes. The resource demands are frequently not overlap-\nping, i.e., statistically multiplexed. This multiplexing hap-\npens due to the range of processing covered in the 56 job-\nquery pairs and the different design of the MapReduce\nand Impala processing engines. In other words, the cluster\nhardware is better utilized when there are different kinds of\nprocessing present on the system.\nUnderstanding the cause of this “miracle” result helped\nus improve our test scenario. Our latest multi-tenant work-\nloads run many concurrent Impala queries and MapReduce\njobs, so that the system resources are fully utilized regard-\nless of statistical multiplexing. Also, we run continuous\nstreams of MapReduce jobs and Impala queries, such that\nfor the duration of measurement, there will always be two\ndifferent frameworks competing for resources.\n7 P RACTITIONER USE OF BENCHMARKS\nThere are few cases when a big data practitioner would need\nto run a benchmark:\u000fValidating an existing system following a system\nupgrade or migration\n\u000fCompare between technologies for a new system\n\u000fAssessing the impact of workload changes\nIn our experience, benchmarks are used in different ways\nin each scenario.\nWhen upgrading or migrating an existing system, bench-\nmarks validate whether the new infrastructure delivers\nexpected performance. It is key to ensure apples-to-apples\ncomparisons between different setups.\nThe new infrastructure should be validated with the\nexisting workload. If the workload includes batch jobs,\nsimply replicating data to the new system and running\nthe batch jobs is all that is required. If the workload is\nmore interactive, then a load-generation harness such as HP\nLoadRunner [18] or Apache JMeter [4] is often used.\nIn some cases, the speciﬁc production workload cannot\nbe replicated in the new environment. In those cases, it is\nvery common to choose an industry standard benchmark to\ntry to emulate the production workload.\nWhen trying to compare technologies for a newly de-\nsigned system, insist on full disclosure, and make sure the\nbenchmarks used are a good substitute for the workload\nplanned for the cluster. Speciﬁcally, ensure the benchmark\nreport makes apples-to-apples comparisons against compet-\ning technologies.\nSome common benchmarks used include: Terasort and\nSWIM [6] for MapReduce, TPC-DS [32] and TPC-H [31]\nfor SQL-on-Hadoop, and YCSB [33] for NoSQL key-value\nstores. Depends on the workload planned for the cluster,\nthey may or may not be appropriate.\nThe gold standard for validating results is indepen-\ndent audit. Some commericial vendors who use industry\nstandard benchmark show such results. An alternative to\nindependent audit is to try to reproduce the reported results\non a pre-production environment. We have seen cases where\na published performance result cannot be reproduced on\nidentical trial systems provided by cluster operators.\nWhen running a home-grown benchmark kit based on\nreal workloads, independent audit is nearly impossible and\nreproducing the result may simply reproduce built-in errors.\nThere, a good practice is to compare the measured perfor-\nmance to published results of similar systems, review the\ndifferences, and see whether the performance differences\ncan be explained with a reasonable model. We discussed\nsome examples of reviewing differences in Section 2 and the\nimportance of performance models in Section 4.\n8 R ELATED WORK\n8.1 Qualities of a good benchmark\nThe criteria for a good performance benchmark have been\nthe topic of decades of publications [15], [16], [19]. Prior\nwork has identiﬁed the following essential properties:\n\u000fRepresentative: The benchmark should measure per-\nformance under real life environments and use met-\nrics that are relevant to real life applications.\n\u000fPortable: The benchmark should be fair and portable\nto competing solutions that target the needs of the\nsame applications.\n\n[Página 8]\n1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing\n8\nBenchmark Publications\nSPECjbb (2000 - 2005) 1,050\nTPC-C 760\nSPEC SFS 730\nSPECweb (96 - 2009) 700\nTPC-D/H 650\nTABLE 1\nBenchmark Result Publications\n\u000fScalable: The benchmark should measure the perfor-\nmance of systems within a wide range of scale. As\ntechnology progresses, systems increase in scale and\nperformance capabilities. The benchmark should be\nable to accommodate for that increase.\n\u000fVeriﬁable: The benchmark should prescribe repeat-\nable measurements that produce the same results\nand can be independently veriﬁed.\n\u000fSimple: The conceptual elements of the benchmark\nshould be reduced to a minimum and made easily\nunderstandable. The benchmark should also abstract\naway details that represent case-by-case conﬁgura-\ntions or system administration choices that do not\naffect performance.\nSelecting a benchmark with the above qualities is a\nﬁrst step towards addressing many of the pitfalls identi-\nﬁed: non-representative benchmarks lead to the unrealis-\ntic benchmarks pitfall, non-portable benchmarks make it\neasier to commit the comparing apples-to-oranges pitfall,\nnon-scalable benchmarks lead to the not testing at scale\npitfall, non-veriﬁable benchmarks make it easier to believe\nin miracles, non-simple benchmarks make it easier to mis-\ncommunicate results.\nUnfortunately, the crowded ﬁeld of emerging big data\nbenchmarks often fall short on the “representative” charac-\nteristic. The two most critical shortcomings we see are (1)\nfailing to capture a multi-job, multi-query workload and (2)\nfailing to provide empirical evidence to justify the choice of\njobs, queries, and data that are included in the benchmark.\nOur prior work [8] contains a critique of several recent big\ndata benchmarks.\n8.2 Successful benchmarks and their making\nA few benchmarks have reached the level of active indus-\ntry standards. When it comes to benchmarks measuring\ncomplete or end-to-end systems, two organizations have\ndominated: SPEC and TPC.\nEach organization has published a number of bench-\nmarks with various degrees of success. One criteria for\nsuccess is the level at which the benchmark is being used\nby various organizations. While internal use is difﬁcult\nto quantify, external publication of benchmark results is\neasy to tally and represents a clear success criteria. Table 1\nshows the most published benchmarks from TPC [28] and\nSPEC [27].\nOf these benchmarks, TPC-C and TPC-D/H followed\na similar process of ﬁnding representative customer work-\nloads that provide insight regarding how to create a big data\nbenchmark. Little has been written about the insider’s views\nof the benchmarks deﬁnition process. The making of TPC-C\nis published only recently [10].The key ideas from this process are:\n\u000fGround the benchmark based on empirical survey\nof customer use cases, in TPC-Cs case a survey of\nhundreds of customers across multiple countries.\n\u000fDevelop abstract functions, datasets, and execution\nscheduling models that cover common characteris-\ntics across use cases without being burdened by the\nspeciﬁc quirks of any single use case.\n\u000fSpecify the benchmark in a technology agnostic fash-\nion to ensure the benchmark is portable.\n\u000fSpecify the benchmark with special attention to how\nshould the benchmark scale the functions, datasets,\nand execution scheduling.\n\u000fBuild the benchmark execution harness with spe-\ncial attention to how the harness can scale without\nadding overhead.\n\u000fEnsure the benchmark behaves deterministically, or\nat least within statistical bounds, so that the bench-\nmark can be rigorously audited.\nThe authors are involved in ongoing efforts to develop\nthe TPC Decision Support (TPC-DS) benchmark for big data.\nThese considerations present some of the hardest technical\nchallenges, especially because the benchmark has to ensure\nthe results have technical merit despite competing commeri-\ncial interests from different test sponsors.\n8.3 Parametric vs empirical models\nA more theoretical consideration brought about by big\ndata concerns what kind of models benchmarks should\nemploy to generate the load and the data. The traditional\napproach is to use analytical models with a small number\nof parameters. For example, a common parametric model\nfor arrival patterns is the Poisson or memoryless arrival\nmodel, used previously to generate network trafﬁc [22].\nA common parametric model for data patterns is the Zipf\nor long-tail frequency model, used for populating synthetic\ndatabases [17].\nThis approach works less well for big data, because\nthe complex, diverse, non-stationary nature of the customer\nworkloads make it hard to capture representative behav-\nior using simple statistical processes with a small number\nof parameters. An alternative is to use empirical models,\nwhere the workload traces arethe model. One can think\nof empirical models as models with an inﬁnite number of\nparameters.\nRecent work has started shifting towards empirical mod-\nels, for example, showing that TELNET and FTP session ar-\nrivals approximate Poisson processes whose average arrival\nrates are empirical constants that change at the hourly or\nﬁner granularity [22]. A recent and successful MapReduce\nbenchmark uses a fully empirical model, with the bench-\nmark test workload being a statistical sample of the original\nhistorical workload trace [6].\nThe shift is an interesting one, because it illustrates\nthat big data benchmarks sometimes need to compromise\nsimplicity (favoring analytic models) to achieve represen-\ntativeness (favoring empirical models). Furthermore, both\nkinds of models will fail to completely capture complex,\nnon-stationary behavior [23].\n\n[Página 9]\n1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See\nhttp://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\n10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing\n9\nOnce people reading benchmark results and people\nproducing benchmark results get past the basic pitfalls\ndiscussed earlier, they would confront deeper technical\nchallenges such as the choice of benchmark models, and\nwhether that helps or hinders understanding system behav-\nior in real-life.\n9 C ONCLUSION\nPerformance benchmarking is a challenging task. When\ndone well, benchmarks can guide ourselves as well as the\ncommunity. Cloudera is a leading vendor in big data, and\nwe make special effort to ensure our performance studies\nare fair, rigorous, and thus useful to ourselves and our\ncustomers. The stories here show that even with good intent\nand best practices, performance benchmarking is fraught\nwith challenges. Anyone can make benchmarking errors,\neveryone can learn from them, and everyone can beneﬁt\nfrom reviewing their own work.\nACKNOWLEDGMENTS\nWe would like to thank our colleagues at Cloudera who\nhelped with various studies cited in this paper, especially\nSandy Ryza, Karthik Kambatla, Jeff Bean, Justin Erickson,\nDavid Rorke, Dileep Kumar, and Arun Singla.\nREFERENCES\n[1] Apache Software Foundation, “Apache Hadoop NextGen\nMapReduce (YARN),” http://hadoop.apache.org/docs/current/\nhadoop-yarn/hadoop-yarn-site/YARN.html.\n[2] Apache Software Foundation, “Apache Sqoop,” http://sqoop.\napache.org.\n[3] Apache Software Foundation, “With Fair Scheduler, cluster can\nlogjam when all resources are consumed by AMs,” Apache Jira\nYARN-1913, https://issues.apache.org/jira/browse/YARN-1913.\n[4] Apache Software Foundation, “Apache JMeter,” http://jmeter.\napache.org/.\n[5] J. Bean, “Apache Hadoop YARN: Avoiding 6\nTime-Consuming ‘Gotchas’,” Cloudera Developer\nBlog, 2014, http://blog.cloudera.com/blog/2014/04/\napache-hadoop-yarn-avoiding-6-time-consuming-gotchas/.\n[6] Y. Chen, S. Alspaugh, A. Ganapathi, R. Grifﬁth, and R. Katz, “Sta-\ntistical workload injector for mapreduce,” https://github.com/\nSWIMProjectUCB/SWIM/wiki.\n[7] Y. Chen, S. Alspaugh, and R. Katz, “Interactive Query Processing\nin Big Data Systems: A Cross-Industry Study of MapReduce\nWorkloads,” in VLDB 2012 .\n[8] Y. Chen, A. Ganapathi, R. Grifﬁth, and R. Katz, “The Case for\nEvaluating MapReduce Performance Using Workload Suites,” in\nMASCOTS 2011 .\n[9] Y. Chen, P . Gokhale, and A. Singla, “Conﬁguring Impala\nand MapReduce for Multi-tenant Performance,” Cloudera\nDeveloper Blog, 2013, http://blog.cloudera.com/blog/2013/06/\nconﬁguring-impala-and-mapreduce-for-multi-tenant-performance/.\n[10] Y. Chen, F. Raab, and R. Katz, “From tpc-c to big data benchmarks:\nA functional workload model,” Lecture Notes on Computer Science ,\nvol. 8163, 2014.\n[11] Cloudera Inc., “Cloudera Impala,” http://www.cloudera.com/\ncontent/cloudera/en/products-and-services/cdh/impala.html.\n[12] J. Dean and S. Ghemawat, “Mapreduce: Simpliﬁed data processing\non large clusters,” in OSDI 2004 .\n[13] J. Erickson, M. Kornacker, and D. Kumar, “New SQL\nChoices in the Apache Hadoop Ecosystem: Why Impala\nContinues to Lead,” Cloudera Developer Blog, 2014,\nhttp://blog.cloudera.com/blog/2014/05/new-sql-choices-in-\nthe-apache-hadoop-ecosystem-why-impala-continues-to-lead.[14] J. Erickson, G. Rahn, M. Kornacker, and Y. Chen, “Impala Per-\nformance Update: Now Reaching DBMS-Class Speed,” Cloudera\nDeveloper Blog, 2014, http://blog.cloudera.com/blog/2014/01/\nimpala-performance-dbms-class-speed/.\n[15] D. Ferrari, Computer systems performance evaluation . Prentice-Hall,\n1978.\n[16] J. Gray, “The Benchmark Handbook For Database and Transaction\nProcessing Systems - Introduction,” in The Benchmark Handbook For\nDatabase and Transaction Processing Systems , J. Gray, Ed. Morgan\nKaufmann Publishers, 1993.\n[17] J. Gray, P . Sundaresan, S. Englert, K. Baclawski, and P . J. Wein-\nberger, “Quickly generating billion-record synthetic databases,” in\nSIGMOD 1994 .\n[18] Hewlett-Packard, “HP LoadRunner,” http://www8.hp.com/us/\nen/software-solutions/loadrunner-load-testing/.\n[19] K. Huppler, “The art of building a good benchmark,” in TPC\nTechnical Conference 2009 .\n[20] K. Kambatla and Y. Chen, “The Truth About MapReduce Perfor-\nmance on SSDs,” in LISA 2014 .\n[21] A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J. DeWitt, S. Mad-\nden, and M. Stonebraker, “A Comparison of Approaches to Large-\nscale Data Analysis,” in SIGMOD 2009 .\n[22] V . Paxson and S. Floyd, “Wide area trafﬁc: the failure of poisson\nmodeling,” Networking, IEEE/ACM Transactions on , vol. 3, no. 3,\nJun 1995.\n[23] V . Paxson, “Empirically derived analytic models of wide-area tcp\nconnections,” IEEE/ACM Trans. Netw. , vol. 2, no. 4, Aug. 1994.\n[24] S. Ryza, “Getting MapReduce 2 Up to Speed,” Cloudera\nDeveloper Blog, 2014, http://blog.cloudera.com/blog/2014/02/\ngetting-mapreduce-2-up-to-speed/.\n[25] The Apache Software Foundation Blog, “The Apache\nSoftware Foundation Announces Apache Hadoop 2,”\nhttps://blogs.apache.org/foundation/entry/the apache\nsoftware foundation announces48.\n[26] The Sort Benchmark, http://sortbenchmark.org/.\n[27] The Standard Performance Evaluation Corporation, http://www.\nspec.org/.\n[28] Transaction Processing Council, www.tpc.org.\n[29] Transaction Processing Council Auditors, http://www.tpc.org/\ninformation/who/whoweare.asp#auditors.\n[30] Transaction Processing Council, “TPC-C Benchmark,” http://\nwww.tpc.org/tpcc/.\n[31] Transaction Processing Council, “TPC-H Benchmark,” http://\nwww.tpc.org/tpch/.\n[32] Transaction Processing Council, “TPC-DS Benchmark,” http://\nwww.tpc.org/tpcds/.\n[33] YCSB Community, “Yahoo! Cloud Serving Benchmark,” https://\ngithub.com/brianfrankcooper/YCSB/.\nGwen Shapira is a Software Engineer on the\nPlatform Engineering team at Cloudera, working\non data ingest products. She has 15 years of\nexperience working with customers to design\nscalable data architectures. She specializes in\nmigrating data warehouses to Hadoop, integrat-\ning Hadoop with relational databases, building\nscalable data processing pipelines, and scaling\ncomplex data analysis algorithms.\nYanpei Chen is a software engineer on the\nPerformance Engineering team at Cloudera.\nHe works on multiple Hadoop ecosystem com-\nponents including MapReduce, Impala, Solr,\nHBase, and Hive, because someone has to\nmake sure the entire Hadoop ecosystem per-\nforms well together. His work involves internal\nengineering optimizations and external competi-\ntive benchmarking and customer support.",
      "metadata": {
        "filename": "shapira2016.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\shapira2016.pdf",
        "file_size": 1195023,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:40.464687",
        "content_length": 49812
      }
    },
    "cf15c644-78c4-4c4e-ade1-dbd066633f5f": {
      "id": "cf15c644-78c4-4c4e-ade1-dbd066633f5f",
      "content": "[Página 1]\nA Big Data Solution for Troubleshooting Mobile \nNetwork Performance Problems\nK. Skračić, I. Bodrušić\nEricsson Nikola Tesla, Zagreb, Croatia\nE-mail: kristian.skracic@ericsson.com\nAbstract - Big Data has become a major competitive \nadvantage for many org anizations. The analytical \ncapabilities made possible by Big Data analytics platforms \nare a key stepping stone for advancing the business of every \norganization. This paper illustrates the development of a big data analytics system for mobile telecommunication systems. The authors developed a solution for analyzing \ndata produced by mobile network nodes which contain data \nrelevant for predictive maintenance and troubleshooting purposes. The solution is built around the problem of \nworking with small files in t he Hadoop environment. The \nlogs collected from mobile ne twork nodes are small binary \nfiles between 5 and 15MB in size.  These binary log files need \nto be decoded to a readable format, and then analyzed to extract useful information. In this paper, the authors \nprovided a benchmark of various scenarios for collecting \nand decoding the binary log files in a Hadoop cluster. As a result, the scenario with the highest performance \nhas been\nused in the implementation of our solution. The developed \nsolution has been built and tested on a live Hadoop cluster \nusing real-world data obtained from several telecom operators around the world.\nI. I NTRODUCTION\nThe telecommunications industry is one of the largest \nand fastest growing industries in the world. Over the past \nfew decades we have witnessed the change from 2G, 3G, \n4G and now 5G in the near future [1]. With the rising demand for constant connectivity, the availability of telecommunication systems and their various componentshas never been more important. This trend is particularly apparent in the case of mobile networks. As shown in [2],\nduring 2016 there were 7.5 billion mobile subscriptions in \nthe world. The report in [2] estimates that in 2022 there will be 8.9 billion mobile subscriptions, 8 billion mobile broadband subscriptions and 6.1 billion unique mobile subscribers in the world. Mobile networks allow a subscriber to consume various telecommunication services via mobile phone from any place of coverage. Thus, they have become one of the most important resources today. This paper proposes a so lution for analyzing data \nproduced by mobile network nodes which containinformation relevant for pr edictive maintenance and\ntroubleshooting purposes.\nAs telecommunication systems are becoming larger \nand more advanced, it is necessary to constantly monitor and measure the performance of their various subsystems. \nHowever, larger networks and higher Internet access \nspeeds carry with them the need to analyze larger amounts of data in a short period of time, in order to prevent \nnetwork outages as soon as possible.\nSuch requirements \ncan be addressed by leveraging the analytical capabilities made possible by Big Data analytics platforms [3]. Apart from improved performance, Big Data can enable deeper analytics by providing access to historical data. For example, by storing network performance data it becomespossible to compare the current results with those obtained in past measurements. By storing the insight obtainedthrough troubleshooting, it becomes possible to predict and prevent the same failures  from happening again, or at \nleast to shorten the response time when the same or similar failures present themselves again [4].\nIn this paper, the authors developed a Big Data \nsolution for analyzing data produced by mobile network\nnodes, which contain important information used for \ntroubleshooting purposes. \nThe solution ingests large \namounts of logs which contain network event information. \nThe logs are gathered through an event-based monitoring (EBM) system, which is an embedded recording tool in the Ericsson EPG, SGSN and MME nodes [5].\nThis paper is organized as follows. Section II provides \nan overview of existing research on small files in the Hadoop environment, as well as the research on the applicability of Big Data solutions in the telecommunications industry. Section III shows the architecture of the developed solution and how the small files problem in Hadoop impacts it. Section IV shows the benchmark results for the scenarios laid out in Section III.\nII. R\nELATED WORK\nThis section provides an overview of the results of \nexisting research in the field of applying Big Data in the telecommunications industry, as well as previous work done on the analysis of small files in Hadoop. \nA. Big Data Analytics Platforms in the \nTelecommunications Industry\nBig Data solutions have become an important part of \ntoday’s industry for all types of businesses, such as finance [6], law enforcement [7], education [8] and others.To show the applicability of Big Data solutions in the telecommunications industry, we briefly summarizedsome existing use cases and their impact on the industry.\nTelecom operators have access to large amounts of \nvaluable data that can be used for various analytical usecases. The research in [9] shows a way to leverage Big \n472MIPRO 2017, May 22- 26, 2017, Opatija, Croatia\n\n[Página 2]\nData analytics for classifying subscribers based on their \nmovement. Reusing existing data for new use cases such \nas this is the first step in data monetization, which is expected to become a major source of income for all types of businesses in the near future [10]. The work in [11] is able to predict customer churn for telecom operators,which has a direct impact on the operator’s profitability.\nSimilarly, the work in [12] predicts customer experience \nwhen using over-the-top (OTT) applications such as WhatsApp or Viber. \nAs the Internet of Things (IoT) is evolving, it is \nexpected to have an impact in the way telecommunications providers analyze the large amounts of sensor data such systems bring with them [13]. We would also like to note that the move to Big Data has a big \nimpact on network infrastructure evolution , as such \nsystems require higher link speeds to transfer the data form one node to another [14].\nThe solution developed in this paper is focused on \nmobile network performance troubleshooting. Thus, it is \nused to calculate various key performance indicators \n(KPIs) relevant for this domain. KPI measurement is\nfrequently used by mobile operators and mobile network infrastructure vendors as a means to systematically search and identify network bottlenecks and anomalies [15].\nB. Analyzing Small Files in the Hadoop Environment\nHadoop is an open-source software framework used \nfor distributed storage and pr ocessing of very large data \nsets [16]. The Hadoop distributed file system (HDFS) has \nbeen widely adopted as the standard for storing data in Hadoop based clusters [17]. In the Hadoop ecosystem, access to stored data is handled by a system called Namenode, which manages the file system namespace and regulates client access. First the client asks the Namenode for instructions on where to find the files it needs to read, as well as the location of a free block it can write to [18].Figure 1 illustrates this process. DataNodes provide block storage and serve I/O requests from clients.\nNamenode Client\nDatanode Datanode Datanode1: Get block locations\n2: read/write \noperations 2: read/write \noperations\nFigure 1. HDFS system overview\nA major drawback of HDFS is its poor performance \nwith large numbers of small files, which has attracted significant attention [19]. According to the research in[19], the main reasons for such lower performance are:\n/g120large numbers of small files impose a heavy \nburden on NameNode memory;/g120correlations between small files are not \nconsidered for data placement;\n/g120no optimization mechanism, such as \nprefetching, is provided to improve I/O performance\nWe would like to note that when small files are stored \non HDFS, disk utilization is not a bottleneck. The research in [20] shows tha\nt a small file stored on HDFS does not \ntake up any more disk space than is required to store its contents. More precisely, a 6 MB file stored with an HDFS block size of 128 MB uses 6 MB of disk space, not 128 MB.\nHDFS is designed to read/write large files, and \nprovides no optimization for handling small files. In cases \nwhere large amounts of small files are accessed directly in \nHDFS, a mismatch of accessing patterns will emerge [21].HDFS will ignore the optimization offered by the native storage resource, which will lead to local disk access \nbecoming a bottleneck [22]. Additionally, in such a \nscenario data prefetching is not employed to improve access performance for HDFS [22].\nThe research in [21] considers all files smaller than \n16MB as small files, although no justification or proof were provided as to why this size was chosen as the cut-off point between large and small files in the context of HDFS\n.The research in [19] has quantified this cut-off \npoint through experimentation. The study indicates that \naccess efficiency starts to drop significantly with files smaller than 4.35 MB.\nThe small file processing problem in Hadoop has seen \nmany different solutions with various levels of success,depending on the nature of the data. One of these is the merging of multiple small files into a single bigger file,\nwhich has shown some significant performance \nimprovements [21], [23]. This  paper explores different \nscenarios in which this solution can be applied to mobile network data. The scenarios are explained in more detail in the following sections.\nIII. S\nYSTEM OVERVIEW\nThis section provides an overview of the developed \nBig Data solution for mobile network performance \ntroubleshooting .\nA. Data Collection\nEvent data has been used for various troubleshooting \npurposes [5]. The event data is collected from EPG, SGSN and MME nodes within the core network. The environment is based on the Evolved Packet Core (EPC) [24], as sown on Figure 2. This study uses only event data generated by these nodes. The authors used\nan event -\nbased monitoring (EBM) system, which is an embeddedrecording tool in the Ericsson EPG, SGSN and MME. We collected events on 2G, 3G and 4G networks. \nThe event data is collected in small log files which are \nbetween 5 and 15 MB in size, depending on the \nconfiguration. The overall size and and velocity of the \nlogs depends on the size of the network (e.g. number of base stations/eNodeB, number of EPG, SGSN and MME \n473\n\n[Página 3]\nnodes, overall network throughput). An average operator \nwill generate around 200 GB of logs per day.\nThe log files are stored in a binary format which needs \nto be decoded to text (usually CSV) in order to be processed. After the decoding process, the files are up to 10 times larger than in their binary format. EBM logs contain information that documents successful and unsuccessful events for comp leted mobility and session \nmanagement procedures.\nAs shown in the following section, the developed \nsolution was tested on several small and large networks around the world. \nGSM\nNetwork\neNodeBBSC\nRNC\nSGW PGWMMESGSNGGSN\nWCDMA\nNetwork\nLTE\nNetwork\nTrusted \nNon-3GPP\nNetwork\nUntrusted \nNon-3GPP\nNetworkRBSRBS\nPDN\nPDNGPRS\nNetwork\nEPC\nNetworkRadio\nNetworks External\nNetworks\nFigure 2 - Evolved Packet Core (EPC) schema\nB. Architecture\nFigure 3 shows the architecture of the developed \nsolution. Apache Flume is used to transfer the data from a network location to HDFS. Flume was chosen because it \nis a widely used distributed, reliable and available service \nfor efficiently collecting, aggregating and moving large amounts of streaming event data [25]. The binary logs are usually dumped to a server, which is usually somewhere in the operator’s network. Using Flume, the binary logs are transferred to the cluster that hosts the proposed \nsolution. Although the proposed solution can be deployed \nwithin the operator’s networ k, we argue that a centralized \noff-site deployment is more appropriate. A centralized \napproach enables data aggregation from various networks \ninto a single cluster, and thus enriches the data with variety that comes from different network configurations and environments.\nHDFS is used to store the raw binary log files until \nthey are decoded. A MapReduce job is used to decode the binary files into CSV. The decoding process is explained in more detail in the following section.\nThe decoded CSV files are imported into an Apache\nHive database [26]. Hive offers an SQL-like query language which enables data access. The developed \nsolution has a number of Hive queries that calculate \nvarious KPI’s, which provide insight about mobile network bottlenecks. This is a quick method of finding out \nwhich parts of the network are worth looking into during\ntroubleshooting. The results, or KPI’s calculated from such queries, are stored in a separate relation database, which is \nbased on PostgreSQL. External applications can \nalso connect to the Hive database and query the data to calculate KPI’s relevant for mobile network troubleshooting. One of the goals for this solution is to enable data mining. Mobile network experts can connect to the Hive database  either though the Hive shell or by \nusing a visualization tool like Tableau. Using the original measurement data stored in Hive, mobile network experts \ncan extract new insight and get to the root cause of a \nproblem. This is often not possible with aggregated KPI data because it hides much of the information it is derived from.\nHive provides several mechanisms for optimizing the \nstorage of the data and query performance. The developed solution makes use of partitioning and bucketing functionalities offered by Hive. Partitioning in Hive is the \nprocess of horizontally dividing the data into a number of \nsmaller and more manageable slices. Every partition is stored as a directory within a\ndata warehouse table in \nHive. The developed solution partitions the decoded CSV data based on the event identifier (or event name) \nattribute. \nBinary log \nfiles locationFlume\nHDFS\nHIVE SCRIPTS for \nKPI calculation\nVisualization\nplatformMapReduce \ndecoder\nHive\ndatabaseDecoded \nCSV log file\nPostgreSQL\ndatabase\nFigure 3 – Architecture of the developed solution\nBucketing is another technique of decomposing data \ninto more manageable parts. This optimization method distributes the data evenly across multiple files. It is used to distribute and organize the table or partition data into multiple files so that similar records are present in the \nsame file. The value of this column will be hashed into \nbuckets by a user-defined number. Bucketing has many performance benefits, most notably faster Map side joins, more efficient grouping using “Group By” statements and more efficient sampling. As the developed solution is used by mobile network experts, such statements are used very \noften when accessing the Hive database directly.\n474\n\n[Página 4]\nC. Log Decoder and the Im pact of Small Files in \nHadoop\nThe developed solution implements a MapReduce job \nto decode the binary files into the CSV format. The \nMapReduce job first reads the binary files in memory and then decodes them in parallel. The number of parallel decoding jobs is de fined by the number of input splits in \nHadoop. This is where the small files problem influences the developed solution. If each raw log file is decoded \nseparately, it will have a negative performance impact. In \nthe developed solution, we tried to influence the number of input splits by combining multiple raw log files into one larger file. More precisely, we tested the performance impact of using small files with the following scenarios: \n/g120Scenario 1: the raw logs are stored as small files \nin HDFS and they are directly used as input splits \nin the MapReduce job. A custom Hadoop input reader is used to read th e binary log files, which is \nbased on the native RecordReader class in \nHadoop \n(org.apache.hadoop.mapreduce.RecordReader )\n/g120Scenario 2: the raw logs are combined into larger\nfiles, stored in HDFS and then decoded using MapReduce jobs. Each line in the combined file is a hexadecimal representation of the binary log file. In this scenario, Flume is used to combine the raw log files. Thus, a native Hadoop input reader \nis used (located in \norg.apache.hadoop.mapreduce.lib.input.TextInput\nFormat )\n/g120Scenario 3: the raw logs are combined into larger\nfiles, stored in HDFS and decoded using MapReduce jobs with only mappers and no reducers. Like in Scenario 2, each line in the input \nfile contains a single file, and Flume is used to \ncombine the raw log files \nIV. R\nESULTS\nFor the purposes of this study, a Hadoop -based cluster \nwas used to evaluate the developed solution. The cluster is based on the Hortonworks Data Platform (HDP) [27] and \nis composed of 10 servers (2 masters and 8 slaves). The \nmaster nodes have 2 model E5-2630 CPU-s, 128GB of \nRAM and 6 hard disk drives (HDD), each with 3TB of \nspace. The slave nodes have 1 model E5-\n2623v3 CPU,\n64GB of RAM, and 8 HDDs, each with 2TB of space. Each node runs on top of CentOS v7, which is installed ona separate SSD disk which is not part of the HDFS. \nThe input data was collected from several small and \nlarge networks around the world, including operators from Europe, North and South America, and Southeast Asia. \nA. Small Files D\necoding Benchmark \nThe combined files in Scenario 2 and 3 were grouped \ninto larger files. Several performance tests were carried out on batches of 2, 9 and 33 GB of raw log files.\nTable 1 shows how the decoder performs in Scenario \n1, when the small log files are used directly. It can be seen that the MapReduce decoder is having difficulties \nprocessing even the smaller batches of 2 and 9 GB of raw \nlogs. As stated in\nprevious sections, the reason for this is \nthe large number of small files that is imposing a heavy burden on NameNode memory. In contrast, Scenario 2 (Table 2) shows a significant performance improvement of the MapReduce decoder. Also, we used 18 reducers in Scenario 2, one for each event type available through the logging system.\nUndoubtedly, the best performance \nwas achieved in \nScenario 3 by combining the input files into larger files and using map-only jobs (Table  3). The reason for such an \nimprovement is that both the shuffle-sort and the reduce \nphases of the MapReduce job are skipped, thus drastically reducing the amount of processing power and memory \nneeded to decode \na large input file. For the largest batch of \n33 GB, we can see that there is a 37% improvement compared to Scenario 2. We would like to note that this improvement increases with batch size (Figure 4). Figure 5 shows the performance gain for each scenario. The \nperformance improvements rise with the batch size, which \nis traditionally not the case for solutions that are not based \non Big Data.\nThe drawback of using the approach in Scenario 3 is \nthat the output of the decoder is split into several smaller files which need to be imported into Hive. This is due to the way MapReduce jobs work. The intermediate results of the mappers are shuffled and sorted before being delivered to the reducers. By having only map jobs in our decoder, the unsorted intermediate results become the \noutput. In contrast, when the reducers are used we can \ninfluence the number of files that will\nbe generated. For \nexample, each event type could be stored into a separate file. However, the files can easily be merged within Hive,as the output is textual (CSV). Also, the partitioning and bucketing in Hive restructures the physical layout of the data, so that the output of the map-only decoder does not influence the performance of the Hive queries.\nTABLE 1. D ECODER BENCHMARK FOR SCENARIO 1–USING SMALL LOG \nFILES AS INPUT\nRaw log size (GB) Seconds Minutes\n2.00 843 14.05\n9.00 3206.00 53.43\n33.00 13740 229.00\nTABLE 2. D ECODER BENCHMARK FOR SCENARIO 2–USING 18\nREDUCERS AND SEVERAL LARGER COMBINED INPUT FILES\nRaw log size (GB) Seconds Minutes\n2.00 428.00 7.13\n9.00 1405.00 23.41\n33.00 7282.00 121.36\nTABLE 3. D ECODER BENCHMARK FOR SCENARIO 3–USING ONLY \nMAPPERS AND SEVERAL LARGER COMBINED INPUT FILES\nRaw log size (GB) Seconds Minutes\n2.00 348.00 5.80\n9.00 883.00 14.71\n33.00 3194.00 53.23\n475\n\n[Página 5]\nB. Comparison with existing solutions\nIn order to clearly show the performance benefits \nwhen using big data solutions and technologies in this scenario, the study also shows the benchmark for decoding EBM logs without the proposed solution. Table 4 shows the performance benchmark on a single server, with the same hardware as the master node in the HDP cluster.\nAs shown in table Table 4, the proposed solution is up \nto 6 times faster than existing solutions. The largest set of \nlogs was unable to be measured since it was not possible \nwith the existing solutions.\nWe note that the performance gains represent only one \nbenefit of the proposed solution. The main advantage of the proposed solution is the ability to process a muchlarger set of logs than was possible with legacy solutions. \nAlso, the proposed solution provides a way to \ncontinuously gather and store logs for deeper analytics, which was no the case with legacy solutions.\nTABLE 4EXISTING SOLUTIONS BENCHMARK\nRaw log size (GB) Seconds Minutes\n2.00 1264.00 21.07\n9.00 5538.00 92.30\n33.00 N/A N/AV. C ONCLUSION\nBig Data analytics can \nprovide insight into the data available within the telecommunications industry.This paper demonstrates one usecase in which analytics can be leveraged to improve the efficiency and value of troubleshooting in mobile networks. The main advantage of the developed solution is its \nability to adapt to any new \nanalytical requests, as well as the \nability to adapt to changing input \nfile sizes. More precisely, the \nresults of this study show that the \ndeveloped solution is capable of processing small files in an efficient manner within the Hadoop environment, which was not built for processing large amounts of small files. Additionally, the study shows that by skipping the reduce phase we can decrease the execution time of the MapReduce job used for decoding. This was shown to be the most time-consumingprocess. The developed solution \nwas tested using log data \ncollected in various small and large networks from around the world, which demonstrates its \napplicability for mobile network troubleshooting. The developed solution has proven that Big Data platforms are suitable for processing large batches of mobile network data, and that they bring significant performance and scalability improvements co mpared to traditional \nsolutions. Future research may include the use of other Big Data tool that run on the Hadoop platform. Most notably, the use of Apache Cassandra instead of the Hive, and the use of Apache Spark as a replacement to the MapReduce decoder job.\nA\nCKNOWLEDGMENT \nThis study was fully funded by Ericsson Nikola Tesla. \nThe authors thank their leadership team for providing an environment in which it was possible to combine research and industry into one. \nR\nEFERENCES\n[1] G. Fettweis and S. Alamouti, “5G: Personal mobile internet \nbeyond what cellular did to telephony,” IEEE Communications \nMagazine , vol. 52, no. 2, pp. 140–145, Feb. 2014.\n[2] C. Patrik, L. Anette, and J. Peter, “Ericsson Mobility Report,” \nEAB-16:018498 Uen, Revision A, Nov. 2016.\n[3] D. Šipuš, “Big data analytics for communication service \nproviders,” in 2016 39th International Convention on Information \nand Communication Technology, Electronics and \nMicroelectronics (MIPRO) , 2016, pp. 513–517.\nFigure 4 – Batch processing benchmark for 3 considered Scenarios\nFigure 5 – Scenario to scenario processing time reduction comparison\n476\n\n[Página 6]\n[4] L. Yang, G. Kang, W. Cai, and Q. Zhou, “An Effective Process \nMining Approach against Diverse Logs Based on Case \nClassification,” in 2015 IEEE International Congress on Big \nData , 2015, pp. 351–358.\n[5] I. da Silva, Y. Wang, F. Mismar, and W. Su, “Event-based \nperformance monitoring for inter-system cell reselection: A SON enabler,” in 2012 International Symposium on Wireless \nCommunication Systems (ISWCS) , 2012, pp. 6–10.\n[6] A. Munar, E. Chiner, and I. Sales, “A Big Data Financial \nInformation Management Architecture for Global Ban king,” in \n2014 International Conference on Future Internet of Things and \nCloud , 2014, pp. 385–388.\n[7] A. Jain and V. Bhatnagar, “Crime Data Analysis Using Pig with \nHadoop,” Procedia Computer Science , vol. 78, pp. 571–578, Jan. \n2016.\n[8] F. Xhafa, D. Garcia, D. Ramirez, and S. Caballé, “Performance \nEvaluation of a MapReduce Hadoop-Based Implementation for \nProcessing Large Virtual Campus Log Files,” in 2015 10th \nInternational Conference on P2P, Parallel, Grid, Cloud and \nInternet Computing (3PGCIC) , 2015, pp. 200–206.\n[9] B. Furletti, L. Gabrielli, C. Renso, and S. Rinzivillo, “Analysis of \nGSM calls data for understanding user mobility behavior,” in \n2013 IEEE International Conference on Big Data , 2013, pp. 550–\n555.\n[10] H. Cao et al. , “SoLoMo analytics for telco Big Data \nmonetization,” IBM Journal of Research and Development , vol. \n58, no. 5/6, p. 9:1-9:13, Sep. 2014.\n[11] H. Li, D. Yang, L. Yang, YaoLu, and X. Lin, “Supervised \nMassive Data Analysis for Telecommunication Customer Churn \nPrediction,” in 2016 IEEE International Conferences on Big Data \nand Cloud Computing (BDCloud), Social Computing and \nNetworking (SocialCom), Sustainable Computing and \nCommunications (SustainCom) (BDCloud-SocialCom-SustainCom) , 2016, pp. 163–169.\n[12] E. Diaz-Aviles et al. , “Towards real-time customer experience \nprediction for telecommunication operators,” in 2015 IEEE \nInternational Conference on Big Data (Big Data) , 2015, pp. \n1063–1072.\n[13]\nS. Din, H. Ghayvat, A. Paul, A. Ahmad, M. M. Rathore, and I. \nShafi, “An architecture to analyze big data in the Internet of \nThings,” in 2015 9th International Conference on Sensing \nTechnology (ICST) , 2015, pp. 677–682.\n[14] I. Tomkos, C. Kachris, P. S. Khodashenas, and J. K. Soldatos, \n“Optical networ king solutions and technologies in the big data \nera,” in 2015 17th International Conference on Transparent \nOptical Networks (ICTON) , 2015, pp. 1– 1.[15] S. Singh, Y. Liu, W. Ding, and Z. Li, “Evaluation of Data Mining \nTools for Telecommunication Monitoring Data Using Design of \nExperiment,” in 2016 IEEE International Congress on Big Data \n(BigData Congress) , 2016, pp. 283–290.\n[16] K. Shvach ko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop \nDistributed File System,” 2010 IEEE 26th Symposium on Mass \nStorage Systems and Technologies (MSST) , pp. 1– 10, May 2010.\n[17] W. Tantisiriroj, S. Patil, and G. Gibson, “Data-intensive File \nSystems for Internet Services: A Rose by Any Other Name... (CMU-PDL-08-114),” Parallel Data Laboratory , Oct. 2008.\n[18] S. Bende and R. Shedge, “Dealing with Small Files Problem in \nHadoop Distributed File System,” Procedia Computer Science ,\nvol. 79, pp. 1001–1012, Jan. 2016.\n[19] B. Dong, Q. Zheng, F. Tian, K.-M. Chao, R. Ma, and R. Anane, “An optimized approach for storing and accessing small files on cloud storage,” Journal of Network and Computer Applications ,\nvol. 35, no. 6, pp. 1847–1862, Nov. 2012.\n[20] T. White, Hadoop: The Definitive Guide . O’Reilly Media, Inc., \n2009.\n[21] X. Liu, J. Han, Y. Zhong, C. Han, and X. He, “Implementing \nWebGIS on Hadoop: A case study of improving small file I/O \nperformance on HDFS,” in 2009 IEEE International Conference \non Cluster Computing and Workshops , 2009, pp. 1–8.\n[22] J. Shafer, S. Rixner, and A. L. Cox, “The Hadoop distributed \nfilesystem: Balancing portability and performance,” in 2010 IEEE \nInternational Symposium on Performance Analysis of Systems \nSoftware (ISPASS) , 2010, pp. 122–133.\n[23] P. Gohil, B. Panchal, and J. S. Dhobi, “A novel approach to \nimprove the performance of Hadoop in handling of small files,” in \n2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT) , 2015, pp. 1–5.\n[24] G. Kuhn, J. Eisl, and H. Bec ker, “Co-operative handover in 3G \nSystem Architecture Evolution,” in 32nd IEEE Conference on \nLocal Computer Networks (LCN 2007) , 2007, pp. 643–650.\n[25]\nP. B. Makeshwar, A. Kalra, N. S. Rajput, and K. P. Singh, \n“Computational scalability with Apache Flume and Mahout for \nlarge scale round the clock analysis of sensor networ k data,” in \n2015 National Conference on Recent Advances in Electronics \nComputer Engineering (RAECE) , 2015, pp. 306–311.\n[26] G. P. Haryono and Y. Zhou, “Profiling apache HIVE query from \nrun time logs,” in 2016 International Conference on Big Data and \nSmart Computing (BigComp) , 2016, pp. 61–68.\n[27] K. K. Gadiraju, M. Verma, K. C. Davis, and P. G. Talaga, \n“Benchmar king performance for migrating a relational application \nto a parallel implementation,” Future Generation Computer \nSystems , vol. 63, pp. 148–156, Oct. 2016.\n477",
      "metadata": {
        "filename": "skracic2017.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\skracic2017.pdf",
        "file_size": 187208,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:40.637726",
        "content_length": 28918
      }
    },
    "d914a09f-7c3b-40ea-b1b3-9430ba82db59": {
      "id": "d914a09f-7c3b-40ea-b1b3-9430ba82db59",
      "content": "[Página 1]\nExploring the Specificities and Challenges of Testing \nBig Data Systems \nDaniel Staegemann  \nMRCC VLBA \nOtto-von-Guericke University  \nMagdeburg, Germany \ndaniel.staegemann@ovgu.de Matthias Volk  \nMRCC VLBA \nOtto-von-Guericke University  \nMagdeburg, Germany \nmatthias.volk@ovgu.de  Abdulrahman Nahhas  \nMRCC VLBA \nOtto-von-Guericke University  \nMagdeburg, Germany \nabdulrahman.nahhas@ovgu.de \n   \n \n \nAbstract — Today, the amount and complexity of data that is \nglobally produced increases continuously, surpassing the \nabilities of traditional approaches. Therefore, to capture and \nanalyze those data, new concepts and techniques are utilized to  \nengineer powerful big data systems. However, despite the \nexistence of sophisticated approaches for the engineering of \nthose systems, the testing  is not sufficiently researched. Hen ce, \nin this contribution, a comparison of traditional software \ntesting, as a common procedure, and the requirements of big \ndata testing is drawn. The determined specificities in the big \ndata domain are mapped to their implications on the \nimplementation and the consequent challenges. Furthermore, \nthose findings are transferred into six guidelines for the test ing \nof big data systems. In the end, limitations and future prospec ts \nare highlighted.  \nKeywords— Big Data, System, Engineering, Verification, \nValidation, Testing, Benchmarking, Technologies, Guidelines \nI. INTRODUCTION \nBig data and its accompanying technologies dramatically \nchanged many aspects of today’s world by allowing the \npurposeful analysis of information that would have been \nforfeited just a few years ago [1]. The derived opportunities \nof those sources of knowledge affect a plethora of application \nareas like healthcare [2–4], civil protection [5, 6], business \n[7–10], opinion mining [11] and transportation [12]. Apart \nfrom the wide applicability, the ever increasing rate of data \ngeneration exemplifies the importance of technologies to \ncreate value from this fairly new resource that has been \ndescribed as digital oil [13]. While the amount of data created \nby modern industry in the year 2015 is stated with about 1000 \nexabytes, it is predicted to increase 20-fold by 2025 [14].  \nDue to its ubiquitousness, versatility, and impact, this \ntopic unites researchers and practitioners in their desire to \npush the boundaries and facilitate more and more advanced \nsolutions in numerous areas [15–17]. This interest shows in a \nmultitude of different facts and metric. From a scientific \nperspective, the number of publications, whose title \nincorporates “big data”, in the scientific literature meta-\ndatabase Scopus has grown immensely. While 3.056 of those \npublications are published between 2011 to 2014, the number \nbetween 2015 and 2018 adds up to 15.128 entries. \nAdditionally, as Fig. 1 Fig. 1 depicts, there is a continuing and \nsignificant annual growth in  numbers in this timespan.  \n \n \n \n \nFig. 1. \"Big data\" publications per year in Scopus.  \nHowever, not only the scientific figures thrive, but also \nthe economical point of view suggests an outstanding \nsignificance of the topic. The market intelligence provider \nInternational Data Cooperation (IDC), for example, predicts \nthat the worldwide market for big data and business analytics \nsolution will reach $189.1 billion in 2019 and increase to \n$274.3 billion by 2022 [18]. Accompanying this growth in \nfinancial volume, the number of big data companies has \nreached a four-digit number [19]. Furthermore, [20] have \nshown the potential for increasing a company’s productivity \nthrough the usage of big data analytics, therefore proving the \nfactual economical value. \nWhile the opportunities and attention are immense, the \nsame applies to the challenges accompanying them [21]. One \noften overlooked challenge is the testing of the created big \ndata systems landscape [22]. Even though the single \ncomponents might have been tested extensively on their own, \na s  i t  i s  o f t e n  t h e  c a s e  w i t h  p o p u l a r  a p p l i c a t i o n s ,  i t  i s  s t i l l  \nrequired to ensure the correctness of their interaction. Thus, \nto assure the validity of the obtained findings. In this regard , \nthe resulting task resembles traditional software testing. \nWhile the process of testing in the domain of conventional \nsoftware has already been extensively researched [23], the \nsame does not hold true for big data systems [24]. Therefore, \nto support the big data engineering procedure [25], the \nfollowing research question will be answered in the course of \nthis work: \n \n \n \n \nMohammad Abdallah \nDepartment of Software Engineering \nAl-Zaytoonah University of Jordan  \nAmman, Jordan \nm.abdallah@zuj.edu.jo   \nKlaus Turowski \nMRCC VLBA \nOtto-von-Guericke University  \nMagdeburg, Germany \nklaus.turowski@ovgu.de  \n2892019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS)\n978-1-7281-5686-6/19/$31.00 ©2019 IEEE\nDOI 10.1109/SITIS.2019.00055\nAuthorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 2]\nRQ1: What are the specificities of testing big data \napplications compared to common software testing?  \nRQ2: How can the identified differences be taken into \naccount in the creation of test scenarios? \nTo answer the research questions, the publication is \nstructured as follows. After introducing and motivating the \ntopic in the first section, the second section discusses the \nfundamentals of big data and software testing. Subsequently, \nbig data systems and software systems are compared, the \nchallenges and requirements of big data testing are explored \nand guidelines for the testing are presented and discussed. \nThe work ends with a conclusion that also includes \nlimitations and future prospects. \nII. FUNDAMENTALS \nIn the following, the domains of big data and software \ntesting, constituting the fundamentals of the publication at \nhand, are introduced and the most important concepts \nexplained. \nA. Big Data \nTo answer the research questions, at first it is necessary to \nclarify the meaning of big data. While the term itself has no \nuniversally applied definition, there are several explanations,  \nwhich are all describing the same phenomenon, but often \nslightly differ in detail. One of the most popular definitions is \nprovided by the National Institute of Standards and \nTechnology (NIST) and states, that big data “consists of \nextensive datasets primarily in the characteristics of volume, \nvelocity, variety, and/or variability that require a scalable \narchitecture for efficient storage, manipulation, and analysis”  \n[26].  \nVolume indicates the amount of data that has to be \nprocessed by the system to fulfill a given task. This can, on \nthe one hand, refer to the number of records that have to be \nmanaged and, on the other hand, to the size of the handled \ndata [27]. Either way, a tremendous increase of the volume is \nnoticeable. Sometimes the relevant metrics are changed from \ngigabytes/terabytes to petabytes or even zettabytes [28]. In \naddition, a growing number of data points is generated and \nregistered for the various subjects of interest [29]. Velocity \nalso refers to two, not necessarily identical challenges. It ca n \ndenominate the pace of incoming data that have to be handled \nby the system, but also the required speed when fulfilling a \nprocessing request [30]. Variety describes the multitude of \ndifferent sources, data types, structures (structured/ semi-\nstructured/ unstructured) and notational conventions that can \nbe present in a single data analytics application [31]. While \ngathering that diverse information can yield significant \nbenefits in terms of the gained insights, their integration can  \npose a major challenge. Variability represents the changes \nregarding the other dimensions. Since the real world is in a \nconstant state of change, the data that are deemed relevant, as  \nwell as their amount are also continuously evolving. This \ndynamic must be taken into account  during the design stage \nof big data systems to ensure a higher level of flexibility and  \nscalability. Furthermore, distinct events can cause a short-\nterm alteration of the composition of the received data, \ntherefore posing additional challenges in terms of the \nsystem’s flexibility [32]. Another important characteristic, \ndespite not being mentioned in the initial definition, is the \nveracity. It “refers to the accuracy of the data” [26] and \ndescribes the trustworthiness of different data sources. This in turn affects the effort that has to be put into the \npreprocessing of the data before the actual analysis can take \nplace. Furthermore, the validity signifies the temporal \ncomponent. While data might have a high veracity, it is \npossible, that they are too old for the contained information \nto be still useful for the purpose of certain analysis, possibly \neven leading to wrong results [26, 28]. \nAlthough the given definitions are providing some \norientation, there is, despite attempts for clarification [33, \n34], no universal definition at which point the characteristics , \ndepicted in Table 1 apply. \nTABLE 1. BIG DATA CHARACTERISTICS  \nCHARACTERISTIC  DESCRIPTION  \nVolume Volume represents the (high) amount of data the \nsystem is confronted with. \nVelocity  Velocity refers to the speed at which the data have to be \nhandled. \nVariety  Variety corresponds to the heterogeneity of the data and \nits sources. \nVariability Variability denominates the variation relating to t he \nother characteristics. \nVeracity Veracity stands for the accuracy and therefore the \ntrustworthiness of the data. \nValidity Validity signifies the task related assessment of the \ndata’s actuality. \n \nBecause the manifestation of those characteristics can \nimmensely vary, depending on the wanted results and the \nprevailing conditions, the required solutions and therefore the  \ndeveloped systems are highly individual [34, 35]. \nB. Software Testing \nSince the research questions are targeted on the potential \ndistinctions between common software testing and the testing \nof big data applications, it is self-evident, that an \nunderstanding of both is required. Hence, to emphasize the \npractice of software testing, the activity itself, the \nmotivations, and approaches are described in more detail. \nSoftware testing can be defined as “a process, or a series of \nprocesses, designed to make sure computer code does what it \nwas designed to do and, conversely, that it does not do \nanything unintended” [36]. Although the testing itself is no \nproductive endeavor, it constitutes a crucial, auxiliary activi ty \nto ensure the quality of the created software. The oversight of  \nexisting issues or the deliberate decision to ignore them can \ncause severe consequences [37]. As a result, companies \nworldwide spend about one-quarter of their total IT budget \non quality assurance and testing, exemplifying its  importance \n[38].  \nGenerally speaking, software testing comprises the \ndesign, the execution and analysis of test cases, and is \npossibly followed by the reporting of the results [39]. There \nare several different styles (black box, white box, gray box) \nthat reflect the tester’s amount of insights into the system \n[40]. Comprehensive testing usually occurs on different \nlevels of growing scale (unit, integration, system) [41]. \nFurthermore, the testing can be conducted with varying \nintentions and reasons. Examples of those are the initial \ntesting of new contents, ensuring, that the software runs at al l \n(smoke test), making sure, that changes did not negatively \naffect already functioning components (regression test) and \nassuring the principal’s satisfaction with the finished product  \n(acceptance test) [42]. An overview of those described \nconcepts, that in conjunction allow to categorize a testing \nendeavor, is given in Fig. 2. \n290\nAuthorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 3]\nFig. 2. Concepts in software testing.  \nIII. COMPARISON  OF BIG  DATA  AND  SOFTWARE  \nSYSTEMS \nEven though big data applications heavily rely on \nsoftware to fulfill their designated tasks, it would not be \nsufficient to observe them as common software, including the \naccompanying testing regimes. Instead, big data testing is a \ncategory of its own, which is concerned with hardware as \nwell as software aspects and additional challenges [22, 43]. \nA. Testing of the Systems \nSince the characteristics, depicted in Table 1, cannot be \nhandled by traditional approaches, they require the creation \nof suitably adapted systems. A certain magnitude of volume, \nfor instance, exceeds the possibilities of a system’s vertical \ngrowth. This results in the necessity to expand horizontally, \nleading to a widespread network of servers to collectively \nhandle the given tasks [26]. Additionally, it is prevalent to use \ncommodity hardware for the sake of cost reduction, which in \nturn increases the system’s heterogeneity [44]. This results in \nadditional risks and potential incompatibilities, which have to  \nbe covered in the testing process. The distributed nature also \nintroduces reliance on communication between the \ncomponents for the sake of coordination and task fulfillment. \nAccordingly, an outage of components is an omnipresent \nthreat that is amplified by the reduced reliability of the used  \ncommodity hardware in comparison to highly specialized \nbusiness solutions [45]. Hence, in the domain of big data, it \nis necessary to incorporate the examination of the resiliency \nof a system. While those decentralized structures also allow \nhandling the characteristic of velocity by distributing \nworkloads and tasks across numerous heterogeneous servers, \nthe specifications and therefore performance might vastly \ndiffer. Since the usefulness of big data applications often \nheavily relies on non-functional properties, those have to be \ntested under consideration of the aforementioned constraints \n[46]. Another consequence of a huge supply of data lies in the \nchallenge of determining which are to be used and how to do \nso. While typical software  commonly has a well-defined and \nv e r i f i a b l e  b e h a v i o r ,  b i g  d a t a  a p p l i c a t i o n s  a r e  c o m m o n l y  \nintended to generate new information out of existing data. For \nthis reason, there are additional aspects of the data sourcing and the reasoning of the underlying computations that have \nto be assessed. Hence, while in traditional software, the input s \nand desired outputs or reactions are usually known and the \ncongruency is verified, this approach does often not apply for \nbig data applications. In consequence of their nature and \npurpose, those systems often constitute black boxes without \na corresponding test oracle, which exacerbates the detection \nof logical flaws and therefore poses additional challenges \n[47]. Besides that task, the consequential variety of data \nrequires pre-processing to harmonize the different formats, \nstructures, and conventions as well as measures to \nincorporate inputs from a multitude of different sources, \nwhich might use completely different interfaces. \nFurthermore, it is common for data to be incorrect or \nincomplete [48], necessitati ng procedures fo r improving data \nqu al ity . Th us , to en su re th e qu ality  of t he obt ain ed res ult s. \nThose steps of integrating sources and preparing their data \nhave to be checked as well, which results in adding another \nlayer of complexity to the testing process. This leads to new \nchallenges, since there might be a big number of rules and \ndependencies, whose adherence has to be verified, because \nerrors in this stage can have a huge impact on the quality of \nthe later analysis [49]. \nAnother major difference in comparison to traditional \nsoftware testing can be caused by the variability in regards to  \nthe other characteristics. While the possible valid inputs of \ns o f t w a r e  a r e  u s u a l l y  k n o w n  a n d  m o d i f i c a t i o n s  a r e  p r e -\nplanned, the input-characteristics of big data applications can  \nchange over time or caused by events, therefore necessitating \na modification of the application itself [26, 32]. Those \nmodifications, in turn, add uncertainty, which has to be \nreflected by the testing. An additional factor of uncertainty \noccurs, when (a high number of) external sources are used. \nSince there is no option to directly control, how the data are \nprovided, manipulations by the source or through malicious \nattacks on the source are possible. Those encounters might \neither compromise the data quality (cp. veracity) or even try \nto invoke damage to the system itself. \nIn general, while software testing is limited to testing \nsoftware, testing big data systems has to deal with socio-\ntechnical systems, which adds additional aspects and \nchallenges that have to be factored in [22]. Therefore, even \nthough traditional software testing is already a demanding \ntask, testing in big data is even more challenging, because its  \ncharacteristics and stipulations cause increased complexity \nand uncertainty. This circumstance is depicted in Fig. 3, \nrepresenting a summary of the made investigations and \ntherefore an answer on the previously formulated RQ1. This \nresults in additional challenges that have to be overcome to \nrealize an effective quality assurance. While most of those \nchallenges originally stem from the aforementioned \ncharacteristics, the area of application also often plays a \ncrucial role in complicating the task by adding a certain \ndegree of complexity to the issue. These dimensions, \ninfluencing the baseline situation of big data endeavors, lead \nto specific conditions for the implementation of the aspired \nsolutions. As a result, the realization of the according system s \ni s  a c c o m p a n i e d  b y  a d d i t i o n a l  c h a l l e n g e s ,  w h i c h  a r e  n o t  a s  \nprevalent in traditional software engineering, but have to be \nconsidered and therefore also covered in big data engineering \nand the according testing.    \n291\nAuthorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 4]\nFig. 3. Mapping of the data characteristics, implementation det ails, and \noccurring challenges. \nB. Challenges and Requirements of Big Data Testing \nDue to the fact that big data applications highly rely on \nsoftware, many aspects and procedures of traditional \nsoftware testing also persist in the big data domain. \nTherefore, the approaches depicted in Fig. 2 are still relevant  \nand can in principle also be extended to the hardware \ncomponents and the composition of software and hardware. \nHowever, derived from the differences that have been \nhighlighted, additional tasks emerge, that have to be \nperformed, to provide comprehensive quality assurance of \nbig data systems. With regards to the content, it might, \ndepending on the use case, be necessary to constantly review \nthe undertaken calculations and the conducted usage of data \nsources for this purpose. This necessity arises due to the \nagility of the examined subjects and circumstances, \ndegradation of data sources, changes in the explored \nquestions, as well as the constantly emerging new \ntechnologies and insights in the field [26, 50]. Regarding \ntechnical aspects, the distributed and heterogeneous nature of \nthe big data landscapes requires extensive tests of the used components, since they are a part of the system under test and \nmight possibly induce errors due to faultiness [37]. \nFurthermore, the communication between the components, \nincluding possible data transformations, has to be tested and \nthe system’s reaction to the outage of nodes has to be \nassessed. Another consequence of the multitude of nodes and \nespecially sources, which might not be under direct control, \nis the plethora of potential weak points for possible attackers . \nThis might concern attacks on the system itself, but also \nattempts to manipulate the data and therefore the results of \nthe analysis. For this reason, the security of the system and, \ndepending on the use case, also the ability to detect \nmanipulations have to be tested regarding those \ncircumstances. In general, in the area of big data, it is an \nambitious task to create or find ways to validate data. This \nalso applies to the determination or creation of a test oracle.  \nBecause of the explorative nature of the applications, the \ndesired outcome is often not known, impeding the according \ntesting.  \nSince non-functional aspects, like response times, often \nplay a major role, it is also necessary to extensively \nbenchmark the application to assure conformity with those \nrequirements. Especially the timely detection of the need to \nscale and the ability to do so have to be taken into account, \nincluding the needed time and possible capacity limits. When \nan applications sole purpose is a timely evaluation of data, fo r \nexample in High-Frequency Trading, a delay of several \nseconds is not only a nuisance but might effectively render \nthe whole application useless, stressing the importance of this  \naspect [51]. \nAnother demand, derived from the dynamic and constant \nchange in the application areas is the convertibility and \nextensibility of the tests to allow the adjustment to changes \no f  t h e  s y s t e m  u n d e r  t e s t .  S i n c e  c h a n g e s  i n  t h e  u s e d  \nalgorithms, technologies and data sources are to be expected, \nthe testing solutions should provide an according amount of \nflexibility. A possible solution for this task might be a highl y \nmodular structure, which allows swapping elements \naccording to the prevailing needs [47].  \nTherefore, when facing big data applications, testers \nshould follow the six guidelines depicted in Fig. 4. Those \nconstitute the additionally needed steps on top of the common \nsoftware testing procedures to take account of the \nspecificities of those systems and represent an answer on the \nformulated RQ2. \n \nFig. 4. Developed guidelines for testing big data applications.  \n292\nAuthorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 5]\nIt should be noted, however, that the respective relevancy \nof each of those guidelines depends on the concrete use case. \nDepending on the circumstances, some of those might not, or \nonly to a lesser extent, be applicable to a testing scenario. F or \nexample, an application that has no direct or indirect external  \nconnections and only receives internal sensor data by \nproduction facilities is not prone to external attacks or \nmanipulation. Therefore, testing the resiliency versus \nmalicious attacks is unessential. On the other hand, when \nusing cloud services, the appropriateness of the hardware can \nrarely by verified, due to factors like a lack of direct access  \nand knowledge of the used components. Hence, while the \nguidelines help in understanding the specificities of big data \ntesting, their concrete application and implementation still \nremains in the big data engineers or testers responsibility. \nFurthermore, finding an appropriate test oracle in the big data  \ndomain remains a challenging and highly individual task that \nis facilitated neither by common software testing practices \nnor by the application of the guidelines. \nC. Discussion \nBecause big data systems are heavily relying on software, \nthe concepts of software testing that are depicted in Fig. 2 ar e \nalso applicable to the testing of big data systems. \nNevertheless, while the major intentions for a test scenario \nare the same, the three possible styles persist and the general  \noperations remain unchanged, the levels slightly differ. A \nunit, for instance, might comprise a combination of software \nand hardware, instead of just a piece of software. An example \ncould be a server, which can be considered a unit in big data \ntesting, even though it is not only relying on software or \nhardware, but on the conjunction of both. Though, since the \napplication areas and characteristics of big data create \nadditional challenges, it is necessary, to consider them in the  \ncreation of test scenarios. By applying the guidelines \npresented in Fig. 4, most of them can be tackled, increasing \nthe value of quality assurance. \nHowever, even considering all of those stipulations, \ndepicted in Fig. 3, and finding an applicable test oracle, does  \nnot assure a highly productive and effective big data \napplication. Since it is also necessary to incorporate further \naspects, besides the system itself, implementing a holistic \nquality assurance process, accommodating the socio-\ntechnical nature and the complexity of the endeavour is \nrequired [22]. Consequently, future research should focus on \nexamining the identified challenges one by one in detail, \nallowing for deeper insights and possibly new solution \napproaches. Considering both of the mentioned domains, this \ncan be approached from a software as well as a systems \nengineering perspective. Hence, in the future the testing of \nbig data systems would not only be observed in terms of the \ndeployed software and the correct way of functioning but also \nthe underlying architecture and its connections to the \nenvironment.  \nIV. CONCLUSION \nDue to the characteristics that define big data, properly \nhandling them and effectively generating value is a \ndemanding task. The paper at hand showed that the systems \nthat are used for this purpose exceed the scope of common \nsoftware since they combine sophisticated software with \ncomplex hardware formations. For this reason, traditional \nsoftware testing is not sufficient and further measures are \nrequired. Those are determined based on the additional \nchallenges depicted in Fig. 3 and constitute the six guidelines  for taking account of the specificities when testing big data \napplications in comparison to common software. The \nconjunction of the specificities with the guidelines also \nconstitutes the answer to the second research question. \nThe increased clarity concerning the domain and the \nguidelines will support scientists as well as practitioners in \ntheir endeavors to understand, apply and advance the subject \nof big data, therefore contributing to the formation of \ntomorrow’s system creation and management. \nThe following step could be the accumulation of best \npractices and use case independent techniques for each of \nthose guidelines to further support the dissemination of big \ndata analytics. Furthermore, reliable techniques for the \ndefinition of test oracles are still to be developed, leaving this \nas one of the most significant future challenges in the \nregarded domain. \nREFERENCES  \n[1] N. Khan  et al., “Big data: survey, technologies, \nopportunities, and challenges,” The Scientific World \nJournal , vol. 2014, pp. 1–18, 2014. \n[2] Y. Wang, L. Kung, W. Y. C. Wang, and C. Cegielski, \n“Developing a Big Data-Enabled Transformation \nModel in Healthcare: A Practice Based View,” in \nProceedings of Thirty Fifth International Conference \non Information Systems , 2014. \n[3] J. Kallinikos and N. Tempini, “Patient Data as Medical \nFacts: Social Media Practices as a Foundation for \nMedical Knowledge Creation,” Information Systems \nResearch , vol. 25, no. 4, pp. 817–833, 2014. \n[4] A. Farseev and T.-S. Chua, “Tweet Can Be Fit,” ACM \nTransactions on Information Systems , vol. 35, no. 4, \npp. 1–34, 2017. \n[5] K. Domdouzis, B. Akhgar, S. Andrews, H. Gibson, and \nL. Hirsch, “A social media and crowdsourcing data \nmining system for crime prevention during and post-\ncrisis situations,” Journal of Systems and Information \nTechnology , vol. 18, no. 4, pp. 364–382, 2016. \n[6] D. Wu and Y. Cui, “Disaster early warning and \ndamage assessment analysis using social media data \nand geo-location information,” Decision Support \nSystems , vol. 111, pp. 48–59, 2018. \n[7] D. Staegemann, M. Volk, and K. Turowski, “Mobile \nProcurement Management,” in Springer Reference \nWirtschaft, Handbuch Digitale Wirtschaft , T. \nKollmann, Ed., Wiesbaden: Springer Fachmedien \nWiesbaden, 2019, pp. 1–15. \n[8] T. Nguyen, L. Zhou, V. Spiegler, P. Ieromonachou, \nand Y. Lin, “Big data analytics in supply chain \nmanagement: A state-of-the-art literature review,” \nComputers & Operations Research , vol. 98, pp. 254–\n264, 2018. \n[9] K. Nagorny, P. Lima-Monteiro, J. Barata, and A. W. \nColombo, “Big Data Analysis in Smart Manufacturing: \nA Review,” International Journal of Communications, \nNetwork and System Sciences , vol. 10, no. 03, pp. 31–\n58, 2017. \n[10] Y. Tang, J. J. Xiong, Y. Luo, and Y.-C. Zhang, “How \nDo the Global Stock Markets Influence One Another? \nEvidence from Finance Big Data and Granger \nCausality Directed Network,” International Journal of \nElectronic Commerce , vol. 23, no. 1, pp. 85–109, 2019. \n293\nAuthorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 6]\n[11] X. Wu, X. Zhu, G.-Q. Wu, and W. Ding, “Data mining \nwith big data,” IEEE Transactions on Knowledge and \nData Engineering , vol. 26, no. 1, pp. 97–107, 2014. \n[12] H. Lee, N. Aydin, Y. Choi, S. Lekhavat, and Z. Irani, \n“A decision support system for vessel speed decision in \nmaritime logistics using weather archive big data,” \nComputers & Operations Research , vol. 98, pp. 330–\n342, 2018. \n[13] X. Yi, F. Liu, J. Liu, and H. Jin, “Building a network \nhighway for big data: architecture and challenges,” \nIEEE Network , vol. 28, no. 4, pp. 5–13, 2014. \n[14] S. Yin and O. Kaynak, “Big Data for Modern Industry: \nChallenges and Trends [Point of View],” Proceedings \nof the IEEE , vol. 103, n o. 2, pp. 143–146, 2 015. \n[15] P. Mikalef, I. O. Pappas, J. Krogstie, and M. \nGiannakos, “Big data analytics capabilities: a \nsystematic literature review and research agenda,” Inf \nSyst E-Bus Manage , vol. 16, no. 3, pp. 547–578, 2018. \n[16] Z. A. Al-Sai, R. Abdullah, and M. h. husin, “Big Data \nImpacts and Challenges: A Review,” in 2019 IEEE \nJordan International Joint Conference on Electrical \nEngineering and Information Technology (JEEIT) , \nAmman, Jordan, A pr. 2019 - Apr. 20 19, pp. 15 0–155. \n[17] A. Oussous, F.-Z. Benjelloun, A. Ait Lahcen, and S. \nBelfkih, “Big Data technologies: A survey,” Journal of \nKing Saud University - Computer and Information \nSciences , vol. 30, no. 4, pp. 431–448, 2018. \n[18] International Data Cooperation, IDC Forecasts \nRevenues for Big Data and Business Analytics \nSolutions Will Reach $189.1 Billion This Year with \nDouble-Digit Annual Growth Through 2022. [Online] \nAvailable: \nhttps://www.idc.com/getdoc.jsp?containerId=prUS449\n98419. Accessed on: May 21 2019. \n[19] M. Turck and D. Obayomi, The Big Data Landscape. \n[Online] Available: http://dfkoz.com/big-data-\nlandscape/. Accessed on: May 21 2019. \n[20] O. Müller, M. Fay, and J. Vom Brocke, “The Effect of \nBig Data and Analytics on Firm Performance: An \nEconometric Analysis Considering Industry \nCharacteristics,” Journal of Management Information \nSystems , vol. 35, no. 2, pp. 488–509, 2018. \n[21] O. Hummel, H. Eichelberger, A. Giloj, D. Werle, and \nK. Schmid, “A Collection of Software Engineering \nChallenges for Big Data System Development,” in 44th \nEuromicro Conference on Software Engineering and \nAdvanced Applications (SEAA) , Prague, 2018, pp. 362–\n369. \n[22] D. Staegemann, M. Volk, N. Jamous, and K. Turowski, \n“Understanding Issues in Big Data Applications - A \nMultidimensional Endeavor,” in Twenty-fifth Americas \nConference on Information Systems , Cancun, 2019. \n[23] V. Garousi and M. V. Mäntylä, “A systematic literature \nreview of literature reviews in software testing,” \nInformation and Software Technology , vol. 80, pp. \n195–216, 2016. \n[24] C. Tao and J. Gao, “Quality Assurance for Big Data \nApplication – Issuses, Challenges, and Needs,” in The \n28th International Conference on Software \nEngineering and Knowledge Engineering , 2016, pp. \n375–381. \n[25] M. Volk, D. Staegemann, M. Pohl, and K. Turowski, \n“Challenging Big Data Engineering: Positioning of \nCurrent and Future Development,” in Proceedings of \nthe 4th International Conference on Internet of Things, Big Data and Security , Heraklion, Crete, Greece, 2019, \npp. 351–358. \n[26] NIST, NIST Big Data Interoperability Framework: \nvolume 1, definitions, version 2. [Online] Available: \nhttps://bigdatawg.nist.gov/_uploadfiles/NIST.SP.1500-\n1r1.pdf. Accessed on: Jan. 31 2019. \n[27] P. Russom, Big Data Analytics: TDWI Best Practices \nReport Fourth Quarter 2011. [Online] Available: \nhttps://vivomente.com/wp-\ncontent/uploads/2016/04/big-data-analytics-white-\npaper.pdf. Accessed on: May 22 2019. \n[28] L. Cai and Y. Zhu, “The Challenges of Data Quality \nand Data Quality Assessment in the Big Data Era,” \nCODATA , vol. 14, no. 2, pp. 1–10, 2015. \n[29] S. Sagiroglu and D. Sinanc, “Big data: A review,” in \n2013 International Conference on Collaboration \nTechnologies and Systems (CTS) , 2013, pp. 42–47. \n[30] A. Gandomi and M. Haider, “Beyond the hype: Big \ndata concepts, methods, and analytics,” International \nJournal of Information Management , vol. 35, no. 2, pp. \n137–144, 2015. \n[31] A. Gani, A. Siddiqa, S. Shamshirband, and F. Hanum, \n“A survey on indexing techniques for big data: \ntaxonomy and performance evaluation,” Knowledge \nand Information Systems , vol. 46, no. 2, pp. 241–284, \n2016. \n[32] A. Katal, M. Wazid, and R. H. Goudar, “Big data: \nIssues, challenges, tools and Good practices,” in Sixth \nInternational Conference on Contemporary \nComputing , M. Parashar et al., Eds., 2013, pp. 404–\n409. \n[33] D. Laney, “Information Economics, Big Data and the \nArt of the Possible with Analytics,” 2012. \n[34] M. Volk, S. W. Hart, S. Bosse, and K. Turowski, “How \nmuch is Big Data? A Classification Framework for IT \nProjects and Technologies,” in Twenty-second \nAmericas Conference on Information Systems , San \nDiego, 2016. \n[35] A. Amado, P. Cortez, P. Rita, and S. Moro, “Research \ntrends on Big Data in Marketing: A text mining and \ntopic modeling based literature analysis,” European \nResearch on Management and Business Economics , \nvol. 24, no. 1, pp. 1–7, 2018. \n[36] G. J. Myers, T. Badgett, and C. Sandler, The art of \nsoftware testing, 3rd ed. Hoboken, N.J: J. Wiley & \nSons, 2011. \n[37] R. Patton, Software testing . Indianapolis: SAMS, 2001. \n[38] Capgemini, Sogeti, HPE, and Micro Focus, Proportion \nof budget allocated to quality assurance and testing as \na percentage of IT spend from 2012 to 2018. [Online] \nAvailable: \nhttps://www.statista.com/statistics/500641/worldwide-\nqa-budget-allocation-as-percent-it-spend/. Accessed \non: May 23 2019. \n[39] P. Ammann and J. Offutt, Introduction to software \ntesting . Cambridge: Cambridge University Press, 2008. \n[40] M. Kaur and R. Singh, “A Review of Software Testing \nTechniques,” International Journal of Electronic and \nElectrical Engineering , vol. 7, no. 5, pp . 463– 474, \n2014. \n[41] L. Copeland, A Practicioner's Guide to Software Test \nDesing, 11th ed. Boston: Artech House Publihsers, \n2010. \n294\nAuthorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore.  Restrictions apply.\n\n[Página 7]\n[42] R. Binder, Testing object-oriented systems: Models, \npatterns and tools, 7th ed. Boston, Madrid: Addison-\nWesley, 2006. \n[43] M. Abdallah, “Big Data Quality Challenges,” in 2019 \nInternational Conference on Big Data and \nComputational Intelligence (ICBDCI) , Pointe aux \nPiments, Mauritius, 2019, pp. 1–3. \n[44] A. McAfee and E. Brynjolfsson, “Big Data: The \nManagement Revolution: Exploiting vast new flows of \ninformation can radically improve your company’s \nperformance. But first you’ll have to change your \ndecision-making culture.,” Harvard Business Review , \nvol. 91, no. 5, pp. 1–9, 2012. \n[45] G. Wang, L. Zhang, and W. Xu, “What Can We Learn \nfrom Four Years of Data Center Hardware Failures?,” \nin 47th Annual IEEE/IFIP International Conference on \nDependable Systems and Networks: 26-29 June 2017, \nDenver, Colorado : proceedings , Denver, CO, USA, \n2017, pp. 25–36. \n[46] M. Gudipati, S. Rao, N. Mohan, and N. K. Gajja, “Big \nData : Testing Approach to Overcome Quality \nChallenges,” vol. 11, pp. 65–73, 2013. [47] D. Staegemann, J. Hintsch, and K. Turowski, “Testing \nin Big Data: An Architecture Pattern for a \nDevelopment Environment for Innovative, Integrated \nand Robust Applications,” in Proceedings of the \nWI2019 , 2019, pp. 279–284. \n[48] I. Taleb, M. A. Serhani, and R. Dssouli, “Big Data \nQuality: A Survey,” in 2018 IEEE International \nCongress on Big Data , 2018, pp. 166–173. \n[49] W. Verbeke, C. Bravo, and B. Baesens, Profit driven \nbusiness analytics: A practitioner's guide to \ntransforming big data into added value . Hoboken, New \nJersey: John Wiley & Sons, Inc, 2017. \n[50] D. Lazer, R. Kennedy, G. King, and A. Vespignani, \n“Big data. The parable of Google Flu: traps in big data \nanalysis,” (eng), Science (New York, N.Y.) , vol. 343, \nno. 6176, pp. 1203–1205, 2014. \n[51] B. Fang and P. Zhang, “Big Data in Finance,” in Big \nData Concepts, Theories, and Applications , S. Yu and \nS. Guo, Eds., Cham: Springer International Publishing, \n2016, pp. 391–412. \n \n \n295\nAuthorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore.  Restrictions apply.",
      "metadata": {
        "filename": "staegemann2019.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\staegemann2019.pdf",
        "file_size": 605902,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:41.211810",
        "content_length": 37631
      }
    },
    "04bb09c3-b758-4880-9d1e-517db37ff1a7": {
      "id": "04bb09c3-b758-4880-9d1e-517db37ff1a7",
      "content": "[Página 1]\nWhite-Box Testing of Big Data Analytics with Complex\nUser-Defined Functions\nMuhammad Ali Gulzar\nShaghayegh Mardani\nUniversity of California, Los Angeles\nUSAMadanlal Musuvathi\nMicrosoft Research, USAMiryung Kim\nUniversity of California, Los Angeles\nUSA\nABSTRACT\nData-intensive scalable computing (DISC) systems such as Google’s\nMapReduce, Apache Hadoop, and Apache Spark are being lever-\naged to process massive quantities of data in the cloud. Modern\nDISC applications pose new challenges in exhaustive, automatic\ntesting because they consist of dataflow operators, and complex\nuser-defined functions (UDF) are prevalent unlike SQL queries. We\ndesign a new white-box testing approach, called BigTest to reason\nabout the internal semantics of UDFs in tandem with the equiva-\nlence classes created by each dataflow and relational operator.\nOur evaluation shows that, despite ultra-large scale input data\nsize, real world DISC applications are often significantly skewed and\ninadequate in terms of test coverage, leaving 34% of Joint Dataflow\nand UDF (JDU) paths untested. BigTest shows the potential to min-\nimize data size for local testing by 105to 108orders of magnitude\nwhile revealing 2X more manually-injected faults than the previous\napproach. Our experiment shows that only few of the data records\n(order of tens) are actually required to achieve the same JDU cover-\nage as the entire production data. The reduction in test data also\nprovides CPU time saving of 194X on average, demonstrating that\ninteractive andfastlocal testing is feasible for big data analytics,\nobviating the need to test applications on huge production data.\nCCS CONCEPTS\n•Software and its engineering →Cloud computing ;Soft-\nware testing and debugging ;•Information systems →MapR-\neduce-based systems .\nKEYWORDS\nsymbolic execution, dataflow programs, data intensive scalable\ncomputing, map reduce, test generation\nACM Reference Format:\nMuhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miry-\nung Kim. 2019. White-Box Testing of Big Data Analytics with Complex\nUser-Defined Functions. In Proceedings of the 27th ACM Joint European Soft-\nware Engineering Conference and Symposium on the Foundations of Software\nEngineering (ESEC/FSE ’19), August 26–30, 2019, Tallinn, Estonia. ACM, New\nYork, NY, USA, 12 pages. https://doi.org/10.1145/3338906.3338953\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia\n©2019 Association for Computing Machinery.\nACM ISBN 978-1-4503-5572-8/19/08. . . $15.00\nhttps://doi.org/10.1145/3338906.33389531 INTRODUCTION\nData-intensive scalable computing (DISC) systems such as Mapre-\nduce [ 20], Apache Hadoop [ 1], Apache Spark [ 48] are commonly\nused today to process terabytes and petabytes of data. At this scale,\nrare and buggy corner cases frequently show up in production [ 49].\nThus, it is common for these applications to either crash after run-\nning for days or worse, silently produce corrupted output. Unfortu-\nnately, the common industry practice for testing these applications\nremains running them locally on randomly sampled inputs, which\nobviously does not flush out bugs hiding in corner cases.\nThis paper presents a systematic input generation tool, called\nBigTest , that embodies a new white-box testing technique for DISC\napplications. BigTest is motivated by the recent successes of sys-\ntematic test generation tools [ 22,24,39]. However, the nature of\nDISC applications requires extending these in important ways to be\neffective. Unlike general-purpose programs addressed by existing\ntesting tools, DISC applications use a combination of relational op-\nerators, such as join andgroup-by , and dataflow operators, such\nasmap,flatmap , along with user-defined functions (UDFs) written\nin general purpose languages such as C/C++, Java, or Scala.\nIn order to comprehensively test DISC applications, BigTest rea-\nsons about the combined behavior of UDFs with relational and\ndataflow operations. A trivial way is to replace these operations\nwith their implementations and symbolically execute the resulting\nprogram. However, existing tools are unlikely to scale to such large\nprograms, because dataflow implementation consists of almost 700\nKLOC in Apache Spark. Instead, BigTest includes a logical abstrac-\ntion for dataflow and relational operators when symbolically exe-\ncuting UDFs in the DISC application. The set of combined path con-\nstraints are transformed into SMT (satisfiability modulo theories)\nqueries and solved by leveraging an off-the-shelf theorem prover,\nZ3 or CVC4, to produce a set of concrete input records [ 11,19]. By\nusing such a combined approach, BigTest is more effective than\nprior DISC testing techniques [ 31,34] that either do not reason\nabout UDFs or treat them as uninterpreted functions.\nTo realize this approach, BigTest tackles three important chal-\nlenges that our evaluation shows are crucial for the effectiveness of\nthe tool. First, BigTest models terminating cases in addition to the\nusual non-terminating cases for each dataflow operator. For exam-\nple, the output of a join of two tables only includes rows with keys\nthat match both the input tables. To handle corner cases, BigTest\ncarefully considers terminating cases where a key is only present\nin the left table, the right table, and neither. Doing so is crucial,\nas based on the actual semantics of the join operator, the output\ncan contain rows with null entries, which are an important source\nof bugs. Second, BigTest models collections explicitly, which are\ncreated by flatmap and used by reduce . Prior approaches [ 31,34]\n290\n\n[Página 2]\nESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim\n1val x,y,z;\n2if(x<y)\n3 z = y/x; //PC1: x < y = true, Effect: z=y/x\n4else\n5 z = x/y; //PC2: x >= y = true, Effect: z=x/y\nFigure 1: Symbolic PathFinder produces a set of path con-\nstraints and their corresponding effects\ndo not support such operators, and thus are unable to detect bugs\nif code accesses an arbitrary element in a collection of objects or\nif the aggregation result is used within the control predicate of\nthe subsequent UDF. Third, BigTest analyzes string constraints\nbecause string manipulation is common in DISC applications and\nfrequent errors are ArrayIndexOutOfBoundException and String-\nIndexOutOfBoundsException during segmentation and parsing.\nTo evaluate BigTest , we use a benchmark set of 7 real-world\nApache Spark applications selected from previous work such as\nPigMix[ 35],Titian [28], and BigSift [25]. While these programs\nare representative of DISC applications, they do not adequately rep-\nresent failures that happen in this domain. To rectify this problem,\nwe perform a survey of DISC application bugs reported in Stack\nOverflow and mailing lists and identify seven categories of bugs.\nWe extend the existing benchmarks by manually introducing these\ncategories of faults into a total of 31 faulty DISC applications. To the\nbest of our knowledge, this is the first set of DISC application bench-\nmarks with representative real-world faults. Such benchmarks are\ncrucial for further research in this area.\nWe assess JDU (Joint Dataflow and UDF) path coverage, sym-\nbolic execution performance, and SMT query time. Our evaluation\nshows that real world datasets are often significantly skewed and\ninadequate in terms of test coverage of DISC applications, still leav-\ning 34% of JDU paths untested. Compared to Sedge [31],BigTest\nsignificantly enhances its capability to model DISC applications—In\n5 out of 7 applications, Sedge is unable to handle these applications\nat all, due to limited dataflow operator support and in the rest 2\napplications, Sedge covers only 23% of paths modeled by BigTest .\nWe show that JDU path coverage is directly related to improve-\nment in fault detection— BigTest reveals 2X more manually injected\nfaults than Sedge on average. BigTest can minimize data size for\nlocal testing by 105to 108orders of magnitude, achieving the CPU\ntime savings of 194X on average, compared to testing code on the\nentire production data. BigTest synthesizes concrete input records\nin 19 seconds on average for all remaining untested paths. Below,\nwe highlight the summary of contributions.\n•BigTest is the first piece of DISC white-box testing that\ncomprehensively models dataflow operators and the internal\npaths of user-defined functions in tandem.\n•BigTest makes three important enhancements to improve\nfault detection capability for DISC applications—(1) It con-\nsiders both terminating andnon-terminating cases of each\ndataflow operator; (2) It explicitly models collections created\nbyflatmap and translates aggregation logic into an iterative\naggregator; and (3) It models string constraints explicitly.\n•It puts forward a benchmark of manually injected DISC\napplication faults along with generated test data, inspired\nby the characteristics of real world DISC application faults\nevidenced by Stack Overflow and mailing lists.1val trips = sc.textFile(\"trips_table.csv\")\n2 .map{s =>\n3 val cols = s.split(\",\")\n4 (cols(1),cols(3).toInt/cols(4).toInt) }\n5 //Returns location and speed\n6val zip = sc.textFile(\"zipcode_table.csv\")\n7 .map{s =>\n8 val cols = s.split(\",\")\n9 (cols(1),cols(0) }\n10 // Returns location and its name\n11 .filter {\n12 s => s._ 2 == \"Palms\" }\n13 val joined = trips. join (zip)\n14 joined\n15 .map{s =>\n16 if (s._2._1 > 40) (\"car\",1)\n17 else if (s._2._1 > 15) (\"bus\",1)\n18 else (\"walk\",1)\n19 }\n20 .reduceByKey (_+_ )\n21 .saveAsTextFile(\"hdfs://...\")➊\n➋\n➌\n➍\n➎\nFigure 2: Alice’s program estimates the total number of trips\noriginated from “Palms.”\n•BigTest finds 2X more faults than Sedge , minimizes test data\nby orders of magnitude, and is fast and interactive.\nOur results demonstrate that interactive local testing of big data\nanalytics is feasible, and that developers should not need to test\ntheir program on the entire production data. For example, a user\nmay monitor path coverage with respect to the equivalent classes\nof paths generated from BigTest and skip records if they belong to\nthe already covered path, constructing a minimized sample of the\nproduction data for local development and testing.\nThe rest of the paper is organized as follows. Section 2 provides a\nbrief introduction to Apache Spark and symbolic execution. Section\n3 describes a motivating example. Section 4 describes the design of\nBigTest . Section 5 describes evaluation settings and results. Section\n6 discusses related work. Section 7 concludes the paper.\n2 BACKGROUND\nApache Spark. BigTest targets Apache Spark, a widely used data\nintensive scalable computing system. Spark extends the MapRe-\nduce programming model with direct support for dataflow and\ntraditional relational algebra operators ( e.g.,group-by ,join , and\nfilter ). Datasets can be loaded in Spark runtime using several\nAPIs that create Resilient Distributed Datasets (RDDs), an abstrac-\ntion of distributed collection [ 47]. RDDs can be transformed by\ninvoking dataflow operations on them ( e.g., val filterRdd =\nrdd.filter(_ >5) ). Dataflow operators such as map,reduce , and\nflatmap are implemented as higher-order functions that take a user\ndefined function (UDF) as an input parameter. The actual evalua-\ntion of an RDD occurs when an action such as count orcollect is\ncalled. Internally, Spark translates a series of RDD transformations\ninto a Directed Acyclic Graph (DAG) where each vertex represents\na transformation applied to the incoming RDD. The Spark scheduler\nexecutes each stage in a topological order.\nSymbolic Execution using Java Path Finder. BigTest builds on\nSymbolic Java PathFinder (SPF) [ 36]. Internally, SPF relies on the\n291\n\n[Página 3]\nWhite-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia\nTrips\nZipcode\nMap:\t𝑓map1\nMap:\t𝑓map2\nFilter:\t𝑓filter\nJoin:\t⨝\nMap:\t𝑓map3\nReduceByKey:\t𝑓Agg\n~𝑓filter(K2,\tV2)\nT1\nT4FalseTrue\n𝑓filter(K2,\tV2)\t⋀K1=\tK2\n(K1,\tV1)(K2,\tV2)(K1,\t(V1,\tV2))(S\t,1)(S\t,N)\n𝑓filter(K2,\tV2)\t⋀K1∉Zipcode\nK1∉ZipcodeK2∉Trips\n𝑓filter(K2,\tV2)\t⋀K2∉Trips\nT2\nT3TZ\n(a) Dataflow operators’ paths by BigTest\nMap:\t𝑓map1String : T\nT.split(“,”).length >= 5 ⋀isInt(T.split(“,”)(3)) ⋀isInt(T.split(“,”)(4)) ⋀T.split(“,”)(4).toInt!= 0 K1=T.split(“,”)(1)  V1=T.split(“,”)(3).toInt/T.split(“,”)(4).toInt\n=>String : Z\nZ.split(“,”).length >= 2=>\nMap:\t𝑓map2K2=Z.split(“,”)(1)V2=Z.split(“,”)(0)\nString : K2, String : V2\nV2 == “Palms”True\nFilter:\t𝑓filterString : K1, Int: V1, String : V2\nV1>40=>S=“car”\nMap:\t𝑓map315<V1≤40\nV1<15\n=>S=“public”=>S=“walk”String : S , Int[K] :  [a1,a2,a3,...,aK]\nReduceByKey:\t𝑓AggK==1=>N=a1\nK==2\nK==n\n=>N=𝑓Agg([a1 , a2]) =>N=𝑓Agg(a1,𝑓Agg(a2,…,𝑓Agg(an-1,an)…) . . .\n=>(b) Non-terminating path conditions of individual UDFs\nMap:\t𝑓map1String : T\nT.split(“,”).length < 5T.split(“,”).length >= 5 ⋀NotInt(T.split(“,”)(3))T.split(“,”).length >= 5 ⋀isInt(T.split(“,”)(3))  ⋀NotInt(T.split(“,”)(4))T.split(“,”).length >= 5 ⋀isInt(T.split(“,”)(3)) ⋀isInt(T.split(“,”)(4)) ⋀T.split(“,”)(4).toInt== 0\nZ.split(“,”).length < 2\n=>X=>X=>X=>X\nMap:\t𝑓map2String : Z\n=>X\n(c) Path constraints for terminating paths in UDFs➊➋\n➌\n➍\n➎➊ ➋ ➌\n➍ ➎\n➊ ➋\nFigure 3: Solid and dotted boxes represent transformations and path constraints, respectively. BigTest identifies path con-\nstraints for both non-terminating and terminating program paths while symbolically executing the program.\nanalysis engine of Java PathFinder (JPF) model checking [ 43]. It\ninterprets Java bytecode on symbolic inputs and produces a set of\nsymbolic constraints. Each constraint represents a unique path in\nthe program, and can be ingested by a theorem solver to generate\ntest inputs. Figure 1 illustrates an example symbolic execution result.\nBy attaching listeners to SPF, the path conditions and the effects of\neach path can be captured. For this program, SPF produces two path\nconditions: (1) the first path produces the effect of z=y/x , when\nthe path condition x<yholds true and (2) the second path produces\nz=x/y as an effect, when the path condition x≥yis satisfied.\n3 MOTIVATING EXAMPLE\nThis section presents a running example to motivate BigTest . Sup-\npose that Alice writes a DISC application in Spark to analyze the\nLos Angeles commuting dataset. She wants to find the total number\nof trips originating from the “Palms” neighborhood using: (1) a pub-\nlic transport whose speed is assumed to be faster than 15 but slower\nthan 40 mph, (2) a personal vehicle which is estimated to be faster\nthan 40 mph, and (3) on foot which is estimated as slower than 15\nmph. Each row in the Trips dataset represents a unique identifier\nfor the trip, the start and end location in terms of a zip code, the\ntrip distance in miles, and the trip duration in hours, for example,\n1,90034,90024, 10, 1 . To map an area zip code to its correspond-\ning area name, Alice uses another dataset that assigns a name to\neach zip code in the following manner: 90034,Culver City\nTo perform this analysis, Alice writes a Spark application in\nFigure 2. She loads both datasets (lines 1 and 6), parses each dataset,\nselects the start location of a trip as a key, and computes the average\nspeed as a value by dividing the distance by duration (lines 2-4).\nAlice outputs a zip code as a key and an area name as a value (lines\n7-9) and filters the area name with “Palms\" at line 12. She joins\nthe two data sets (line 13). In the subsequent mapoperation (line\n15-18), she categorizes the trips based on the average speed into\nthree categories. She finally counts the frequency of each trip kindand stores them (lines 20 and 21). Though this program is only 21\nlines long, it poses several challenges for modeling test paths.\nEquivalence Classes of Dataflow Operators. Consider filter\n➌at line 11. To exhaustively test this operator, we must consider\ntwo equivalence classes: the first where a data record satisfies the\nfilter and moves onto the next operator and the second where the\nfilter does not satisfy and its data flow terminates. If we only model\nnon-terminating case then test data would contain passing data\nrecords only and hence, would not detect a fault in which filter\nis removed from the DISC application. To model join at line 13,\nwe must have three equivalence classes—two terminating cases\nand one non-terminating case: (1) an input record in the left table\n(“Trip”) does not have a matching key on the right table (“ZipCode”),\nterminating its data flow, (2) an input in the right table does not\nhave a matching key on the left, terminating its data flow, and (3)\nthere exists a key that appears in both tables, passing the joined\nresult to the next operator. Modeling such terminating cases is\ncrucial otherwise test data generated produce the same output for\nboth join andleftOuterJoin and do not reveal faults that are\nbased on incorrect join type usage.\nUDF Paths. Consider the map➍at lines 15-18. There are three\ninternal path conditions: (1) speed >40 mph, (2) 15 mph <speed\n≤40 mph, and (3) speed ≤15 mph. The sub figure ➍in Figure 3b\nshows corresponding path conditions and effects.\nString Constraints. To analyze the second map➋at lines 7 to 9,\nwe must reason about the entailed string constraints. Given a string\nZin sub figure ➋in Figure 3b and 3a, to split the data into two\ncolumns, it must satisfy a string constraint Z.split(\",\").length\n≥2to produce the effect where the key K2is(Z.split(\",\")\n(1)and the value V2is(Z.split(\",\")(0)) . String manipulation\nis critical to many DISC applications. In the above example, at\nleast one test must contain a string zwithout delimiter \",\", so\nthatz.split(\",\")(1) leads to ArrayIndexOutOfBoundsException\nwhich will then expose the inability of the UDF to handle exceptions.\n292\n\n[Página 4]\nESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim\nTable 1: Generated input data where each row represents a unique path. Variables T,Z,V, and Kare defined in Figure 3a.\n# Constraint Trips Zipcode\nC1 T.split(\",\").length < 5 \"\" _ _\nC2 T.split(\",\").length ≥5∧NotInt(T.split(\",\")(3)) _, _, _, \"\", _ _, _\nC3 T.split(\",\").length ≥5∧isInt(T.split(\",\")(3)) ∧NotInt(T.split(\",\")(4)) _, _, _, \"-2\", \"\" _, _\nC4 T.split(\",\").length ≥5∧isInt(T.split(\",\")(3)) ∧isInt(T.split(\",\")(4)) ∧T.split(\",\")(4).toInt = 0 _, _, _, \"-2\", \"0\" _, _\nC5 Z.split(\",\").length < 2 _ \"\"\nC6 Z.split(\",\").length ≥2∧V2!= \"Palms\" _ _, \"\\x00\"\nC7T.split(\",\").length ≥5∧isInt(T.split(\",\")(3)) ∧isInt(T.split(\",\")(4))\n∧T.split(\",\")(4).toInt != 0 ∧Z.split(\",\").length ≥2∧V2= \"Palms\"∧K1<Zipcode_, \"!0!\", _, _, _ \"\\x00\", \"Palms\"\nC8 . . .∧V2= \"Palms\"∧K2<Trips _, \"\"!0!\", _, _, _ \"\\x00\", \"Palms\"\nC9 . . .∧V2= \"Palms\"∧K1= K 2∧V1> 40 _, \"\\x00\", _, \"41\", \"1\" \"\\x00\", \"Palms\"\nC10 . . .∧V2= \"Palms\"∧K1= K 2∧15< V 1< 40 _, \"\\x00\", _, \"16\", \"1\" \"\\x00\", \"Palms\"\nC11 . . .∧V2= \"Palms\"∧K1= K 2∧V1< 15 _, \"\\x00\", _, \"0\", \"1\" \"\\x00\", \"Palms\"\nOtherwise, this application may crash in production, when the input\nrecord does not have an expected delimiter.\nArrays. To analyze reduceByKey ➎at line 20 (also in Figure 3b),\nwe must model how the UDF operates on the input array of size K,\n[a1,a2,. . .,aK]and produces the corresponding output faдд(a1\n,faдд(a2. . .faдд(aK−1,aK). . .)). For example, the UDF (_+_)\nreturns the sum of two input arguments. When the array size Kis\ngiven by a user, the final output Nisa1+(a 2+. . .(aK−1+aK)).\nSummary. Due to the internal path conditions entailed by indi-\nvidual UDFs, instead of four high-level dataflow paths shown in\nFigure 3a, Alice must consider eleven paths in total, which are enu-\nmerated in Table 1. Figure 3 shows the symbolic execution tree at\nthe level of dataflow operators on the left and the internal symbolic\nexecution trees for individual UDFs on the right. Lastly, example\ndata generated by BigTest for each JDU path using Z3 is shown\nin Table 1. While these example data records may not look realis-\ntic, such data is necessary to exercise the downstream UDFs that\nare otherwise unreachable with the original dataset. For instance,\nfiltering a dataset without any passing data record will result in\nan empty set and consequently, the UDFs after the filter will\nnever get tested with the original data. Therefore, synthetic data is\nnecessary and crucial to expose downstream program behavior.\n4 APPROACH\nBigTest takes in an Apache Spark application in Scala as an input\nand generates test inputs to cover all paths of the program up to a\ngiven bound by leveraging theorem provers Z3 [ 19] and CVC4 [ 11].\n4.1 Dataflow Program Decomposition\nA DISC application is comprised of a direct acyclic graph where\neach node represents a dataflow operator such as reduce and cor-\nresponding UDFs. As the implementation of dataflow operators in\nApache Spark spans several hundred thousand lines of code, it is not\nfeasible to perform symbolic execution of a DISC application along\nwith the Spark framework code. Instead, we abstract the internal\nimplementation of a dataflow operator in terms of logical specifi-\ncations. We decompose a DISC application into a dataflow graph\nwhere a node calls each UDF and combine the symbolic execution\nof the UDFs using the logical specification of dataflow operators.\nUDF Extraction. BigTest compiles the DISC application into Java\nbytecode and traverses each Abstract Syntax Tree (AST) to search\nfor a method invocation corresponding to each dataflow operator.\nThe input parameters of such method invocation are UDFs repre-\nsented as anonymous functions as illustrated in Figure 4b. BigTest\nstores the UDF as a separate Java class shown in Figure 4c and1sc.textFile(\"zipcode.csv\"). map{...}\n2 .filter {_._2 == \"Palms\"}\n(a) DISC Application\nMethodDeclaration\nName\t:\t“main”\nBody\nMethodInvocation\nName\t:\t“fitler”\nparameter\nClassInstanceCreation\nUDFfilter\nAnonymousClassDeclaration\n.\t.\t.\t\n.\t.\t.\t.\t.\t.\t.\t.\t.\t.\t.\t.\t.\t.\t.\t\n.\t.\t.\t.\t.\t.\t\n(b) Generated AST1class filter {\n2 static void main(String args[]){\n3 apply(null);\n4 }\n5 static boolean apply(Tuple2 s){\n6 return s._2().equals(\"Palms\")\n7 }\n8}\n(c) Extracted Filter UDF\nFigure 4: BigTest extracts UDFs corresponding to dataflow op-\nerators through AST traversal.\ngenerates a configuration file required by JPF for symbolic execu-\ntion. BigTest also performs dependency analysis to include external\nclasses and methods referenced in the UDF.\n1def f(a:Int,b:Int){\n2 return a+b;\n3}\n4//Usage in reduce\n5...reduce {f}\n(a)1def f_reduce(arr:Array[Int]){\n2 var sum = 0;\n3 for(a <- 1 to K)//K is bound\n4 sum = udf(sum,arr(a));\n5 return sum; }\n(b)\nFigure 5: (a) a normal invocation of reduce with a corre-\nsponding UDF. (b) an equivalent iterative version with a\nbound K\nHandling Aggregator Logic. For aggregation operators, the at-\ntached UDF must be transformed. For example, the UDF for reduce\nis an associative binary function, which performs incremental ag-\ngregation over a collection shown in Figure 5a. We translate it into\nan iterative version with a loop shown in Figure 5b. To bound the\nsearch space of constraints, we bound the number of iterations to a\nuser provided bound K(default is 2).\n4.2 Logical Specifications of Dataflow\nOperators\nThis section describes the equivalence classes generated by each\ndataflow operator’s semantics. We use CIto represent a set of path\nconstraints on the input data, I, for a particular operator. A single\nelement cinCIcontains path constraints that must be satisfied\nto exercise a corresponding unique path. We define fas the set\n293\n\n[Página 5]\nWhite-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia\nTable 2: CIrepresents a set of incoming constraints from the input table I, where each constraint c∈CIrepresents a non-\nterminating path. c(t)represents that record t∈Imust satisfy constraint c.fdefines the set of path constraints generated by\nsymbolically executing ud fand f(t)represents the path constraint of a unique path exercised by input tuple t.\nOperator Inputs Logical Specification\nfilter( ud f)I: Input Table\nud f :t→BoolNon-Terminating ❶∃t:t∈I∧c∈CI∧c(t)∧f(t)\nTerminating ❷∃t:t∈I∧c∈CI∧c(t)∧¬ f(t)\nmap( ud f)I : Input Table, O : Output Table\nud f :t→t′where t′∈ONon-Terminating ❸∃t:t∈I∧c∈CI∧c(t)∧f(t)\nflatmap( ud f)I : Input Table, O: Output Table\nud f :t→Collection of t′where t′∈ONon-Terminating ❹∃t:tI∈I∧c∈CI∧c(t)∧f(t)\njoinR : Right Table, tR∈R\nL : Left Table, tL∈LNon-Terminating ❺∃tR,tL:cR∈CR∧cL∈CL∧cR(tR)∧tR,key=tL,key∧cL(tL)\nTerminating ❻∃key,tR:cR∈CR∧cL∈CL∧cR(tR)∧tR,key=key∧(∀tL∈L:cL(tL)∧tL,key,key)\nTerminating ❼∃key,tL:cR∈CR∧cL∈CL∧cL(tL)∧tL,key=key∧(∀tR∈R:cR(tR)∧tR,key,key)\ngroupByKeyI : Input Table\nt∈Iandt=(tkey,tvalue)Non-Terminating ❽∃t:t∈I∧c∈CI∧c(t)∧|{ X|x∈I∧xkey=tkey}|>0\nreduce( ud f)\nreduceByKey( ud f)I : Input Table, O: Output\nud f :(t,t)→t′where t′∈ONon-Terminatingud f′is an iterative version of the original UDF ud f, given as an input to reduce/reduceByKey.\nf′represents the set of path constraints generated from symbolic execution of ud f′.\n❾∃t1,t2,t3, . . . , tn∈I:c1,c2, . . . , cn∈CI∧c1(t1)∧c2(t2)∧. . . ..∧cn(tn)∧f′(I)\nof symbolic path constraints of a UDF where f(t)represents con-\nstraints of a unique path exercised by input t. By abstracting the\nimplementation of dataflow operators into logical specifications,\nBigTest does not have to symbolically execute the Spark framework\ncode (about 700KLOC), as it focuses on application level faults only\nas opposed to framework implementation faults which is out of the\nscope of this paper. BigTest supports all popular dataflow operators\nwith the exception of deprecated operators such as co-join .\nFilter. Filter takes a boolean function ud f deciding if an input\nrecord should be passed to downstream operators or not. Therefore,\nwe model two equivalence classes: (1) there exists a record tthat\nsatisfies ud f and one of the incoming constraints CIfrom input\ntable I(i.e.,the table produced by its upstream satisfying operator),\nshown in ❶Table 2; (2) there exists a record tthat satisfies one of\nthe incoming constraints but not ud f, shown in ❷Table 2.\nMap and Flatmap. Maptakes a UDF ud f as an input and applies\nit to each input record to produce an output record. It has one\nequivalence class, where there exists tuple tfrom the input table\nIsatisfying one of the incoming constraints, c∈CIand also one\nof the path constraints in fi.e.,path constraints generated by\nsymbolically executing ud f, shown in ❸Table 2. Mapis supported\nby the previous work Sedge butSedge considers the UDF ud f as a\nblack box, uninterpreted function. Flatmap splits an input record\nusing a ud f to generate a set of records, and thus the equivalence\nclass of flatmap is similar to that of map, as shown in ❹.BigTest\nhandles flatmap by explicitly modeling a collection, described in\nSection 4.3.\nJoin. Join performs an inner-join of two tables tron the right and\ntable tlon the left based on the equality of keys, assuming that\nrecords from both tables are of the type Tuple (key, value). We\nmodel the output records of join into three equivalence classes: (1)\nthe key of tuple tRin the right table matches with a key of tuple tL\non the left; (2) the key of tuple tRin the right table does not match\nwith any key of tuple tLon the left; and (3) the key of tuple tLin\nthe left table does not match with any key of tuple tRon the right.\n❺,❻, and❼in Table 2 represent the three equivalence classes.\nReduce and ReduceByKey. reduce takes a ud f and a collection\nas inputs and outputs an aggregated value, while reduceByKey\nperforms a similar operation per key. As discussed in Section 4.1,\nBigTest generates an equivalent iterative version of the ud f with\na loop. By this refactoring of ud f toud f′, the equivalence classes\ncould be modeled similar to that of map, where there exist inputrecords t1,t2, . . . , tn∈Ion which each of the corresponding non-\nterminating constraint ( c1,c2, . . . , cn)∈CIfrom the input table I\nholds true. In addition, each record must satisfy the constraints of\nud f′, satisfying f′([t1,t2, . . . , tn]), as shown in ❾of Table 2.\n4.3 Path Constraint Generation\nThis section describes several enhancements in Symbolic Path\nFinder (SPF) to tailor symbolic execution for DISC applications.\nDISC applications extensively use string manipulation operations\nand rely on a Tuple data structure to enable key-value based op-\nerations. Using an off-the-shelf SPF naïvely on a UDF would not\nproduce meaningful path conditions, thus, overlooking faults dur-\ning testing.\n1def parse(s:String){\n2 val cols = s.split(\",\")\n3 (cols(0) , cols(1)) }\nFigure 6: A UDF with string manipulation\nStrings. Operations such as split for converting an input record\ninto a key-value pair are common in DISC applications but are not\nsupported by SPF. BigTest extends SPF by capturing calls to split ,\nrecording the delimiter, and returning an array of symbolic strings.\nWhen an n-th element of this symbolic array is requested, SPF\nreturns a symbolic string encoded as splitn with a corresponding\nindex. By representing the effect of Figure 6 as (splitn(\",\",s,0),\nsplitn(\",\",s,1)) ,BigTest generates one terminating constraint,\nwhere scan only split into fewer than two segments, and one non-\nterminating constraint where scan split into at least two segments.\nDue to no split support, naïve SPF generates a string without any\ndelimiter as a test input e.g.,\"\\x00\" instead of \"\\x00,\\x00\". This input\nwould lead to ArrayIndexOutOfBoundsException while accessing\na string using split(\",\")(1) .\n1def agg(arr:Array[Int]){\n2 val sum = arr(0); // Bound K=3\n3 for(a <- 1 to min(arr.size,2)) sum += arr(a)<0 ? 0 : arr(a);\n4 sum }\nFigure 7: An iterative version of aggregator UDF\nCollections. Constructing and processing collections through op-\nerators such as flatmap are essential in DISC applications. There-\nfore, BigTest explicitly models the effect of applying a UDF on\na collection. In Figure 7, an iterative version of aggregator logic\n294\n\n[Página 6]\nESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim\nproduced by BigTest takes a collection as input and sums up each\nelement, if the element is greater than or equal to zero. Given a\nuser-provided bound K=3 BigTest unrolls the loop three time and\ngenerates four pairs of a path condition (P) and the corresponding\neffect (E):\n(1)P:a(1)<0∧a(2)<0 ,E:a(0)\n(2)P:a(1)≥0∧a(2)<0 ,E:a(0)+a(1)\n(3)P:a(1)<0∧a(2)≥0,E:a(0)+a(2)\n(4)P:a(1)≥0∧a(2)≥0,E:a(0)+a(1)+a(2)\nA naïve SPF does not handle collections well and thus may generate\nan array of length 1 only, not exercising line 3 in Figure 7. For\nexample, agg({3}) outputs the same sum of 3, when arr(a)<0 is\nmutated to arr(a)>0 ), because the loop starts from 1 instead of 0,\nandsumis initialize to the first element of the array. Thus, it is not\npossible to defect the fault using an array of length 1.\nExceptions. BigTest extends SPF to explicitly model exceptions.\nFor example, when an expression involves a division operator, divi-\nsion by zero is possible, which can lead to program termination. In\nFigure 1, BigTest creates two additional terminating path conditions,\ndue to division by zero (i.e., x<y∧x==0 andy≤x∧y==0 ).\nCombining UDF symbolic execution with equivalence classes.\nBigTest combines the path conditions of each UDF with the incom-\ning constraints from its upstream operator. For example, the UDF\noffilter (➌) in Section 3 produces a path condition of s._2 ==\n\"Palms\" . Suppose that the upstream operator mapproduces one\nnon-terminating path condition s.split(\",\").length ≥2with\nthe effect s._2 =splitn(s,\",\",1) . Inside the equivalence classes\noffilter —rows ❶and❷in Table 2, BigTest plugs in the incoming\npath conditions (/effects) of an upstream operator maptoCIand\nthe path conditions (/effects) of the filter ’s UDF to f, producing\nthe following path conditions.\n•c(t)∧f(t):s.split(\",\").length ≥2∧\nsplitn (s,\",\",1) == \"Palms\"\n•c(t)∧¬ f(t):s.split(\",\").length ≥2∧\n¬(splitn (s,\",\",1) == \"Palms\")\nJoint Dataflow and UDF Path. BigTest defines the final set of\npaths of a DISC application as Joint Dataflow and UDF (JDU) paths.\nWe define a JDU path as follows: let G=(D,E)represent a directed\nacyclic graph of a DISC application where Dis a set of vertices\nrepresenting dataflow operators and Erepresents directed edges\nconnecting dataflow operators. Imagine a DISC application con-\nstructed with a mapfollowed by filter andreduce . We represent\nthis dataflow graph as G=(D,E)such that D={d1,d2,d3,t1}\nandE={(d1,d2),(d2,d3),(d2,t1)}where d1,d2, and d3aremap,\nfilter , and reduce respectively. filter introduces a terminating\nedge(d2,t1)where a terminating vertex is t1.\nSince each dataflow operator takes a user-defined function f, for\na vertex di, we define a subgraph Gi=(Vi,Ei)which represents\nthe control flow graph of f. In this subgraph, a vertex v∈Vi\nrepresents a program point and an edge (va,vb)∈Eirepresents\nthe flow of control from vatovb.Gihasv1=start andvn=stop\ncorresponding to the first and last statements. Then from each\ndataflow operator node di, we add a call edge from dito the start\nnode of Giand from the stop node of Gito the di+1. Since some\nUDFs include a loop and thus have a cycle in the control flow graph,1(assert (= line2 (str.++ (str.++ line20 \",\") line21)))\n2(assert\n3 (= line1\n4 (str.++ (str.++ \" \" \",\")\n5 (str.++ (str.++ line11 \",\")\n6 (str.++ (str.++ \" \" \",\") (str.++ (str.++ line13 \",\")\nline14))))))\n7(assert\n8 (and (not (= (str.to.int line14) 0))\n9 (and (isinteger line14)\n10 (and (isinteger line13)\n11 (and (= \"Palms\" line21)\n12 (and (= x11 line20)\n13 (and (<= s21 15)\n14 (and (<= s21 40) (and (= s21 x621) (and (= s1 x61) (=\ns22 x622)))))))))))))))\n15 (assert\n16 (and (= x11 line11)\n17 (and (= x12 (/ (str.to.int line13) (str.to.int line14)))\n18 (and (= x61 x11)\n19 (and (= x621 x12) (and (= x622 x42) (and (= x71 \"walk\") (=\nx72 1))))))))))))\nFigure 8: Output SMT query constructed by BigTest to reflect\nJDU path constraint C11of Table 1 from motivating example.\nwe finitize the loop using a user provided bound Kand unroll the\nloop Ktimes.\nWe enumerate a set of all unique paths PKfor the graph Gwith\nexpanded subgraphs and call each unique path a Joint Dataflow and\nUDF (JDU) path . For an arbitrary test suite T, the JDU path coverage\nis measured as a set of covered paths, PK(T)={p|p∈PK,∃t∈\nT and t|=Cp}where a test input tsatisfies the path condition Cp\nof path p. Given a user-provided bound Kfor unrolling a loop, JDU\npath coverage is|PK(T)|\n|PK|.\n4.4 Test Data Generation\nBigTest rewrites path constraints into an SMT query. For constraints\non integer variables, BigTest uses analogous arithmetic and logical\noperators available in SMT. For string constraints, BigTest uses\noperations such as str.++ ,str.to.int , and str.at .BigTest in-\ntroduces a new splitn symbolic operation. If a path constraint con-\ntains a clause v = splitn(\",\" s,1) ,BigTest generates (assert\n(= s (str.++ \" \" (str.++ \",\" v)))) that is equivalent to s =\n\" ,v\"where vis a symbolic string. The path conditions produced\nbyBigTest do not contain arrays and instead model individual\nelements of an array up to a given bound K.\nBigTest generates interpreted functions for Java native methods\nnot supported by Z3. For example, BigTest replaces isInteger with\nan analogous Z3 function. BigTest executes each SMT query sepa-\nrately and finds satisfying assignments (i.e., test inputs) to exercise\na particular path. While executing each SMT query independently\nmay lead to redundant solving of overlapping constraints, in our\nexperiments, we do not find it as a performance bottleneck. Theoret-\nically, the number of path constraints increases exponentially due\nto branches and loops; however, empirically, our approach scales\nwell to DISC applications, because UDFs tend to be much smaller\n(in order of hundred lines) than DISC frameworks and we abstract\nthe framework implementation using logical specifications.\nFigure 8 shows an SMT query produced by BigTest for Figure 2.\nLines 1 to 6 constrict the first table to have four segments and the\nsecond table to have two segments separated by a comma. Lines 7 to\n10 restrict a string to be a valid integer. To enforce such constraint\n295\n\n[Página 7]\nWhite-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia\nTable 3: Subject Programs\nSubject # of Program Characteristics JDU Paths#ProgramOutputOperatorsOperatorsString Parsing # Branches # UDFs (K=2)\nP1 IncomeAggregate Total income of individuals earning ≤$300 weekly 3 map, filter, reduce ✓ 2 4 6\nP2 MovieRatings Total number of movies with rating ≥4 4 map, filter, reduceByKey ✓ 1 4 5\nP3 AirportLayover Total layer time of passengers per airport 3 map, filter, reduceByKey ✓ 2 4 14\nP4 CommuteTypeTotal number of people using each form of\ntransport for daily commute6map, fitler, join,\nreduceByKey✓ 3 5 11\nP5 PigMix-L2 PigMix performance benchmark 5 map, join ✓ 2 6 4\nP6 Grade Analysis List of classes with more than 5 failing students 5flatmap, filter,\nreduceByKey, map✓ 2 3 30\nP7 WordCount Finds the frequency of words 3 flatmap, map, reduceByKey ✓ 1 3 4\nP1 P2 P3 P4 P5 P6 P7020406080100\n100\n100\n100\n100\n100\n100\n10016.7\n40\n14.3\n18.2\n25\n13.3\n2566.7\n60\n28.6\n54.5\n75\n76.7\n100JDU Path Co veraдe\n(Normalized ,%)BigTest Sedge Original\nFigure 9: JDU path coverage of BigTest ,Sedge , and the original\ninput dataset\nthat crosses the boundary of strings and integers, BigTest uses a\ncustom function isinteger and Z3 function str.to.int . Lines 11\nto 14 enforce a record to contain “Palms” and the speed to be less\nthan or equal to 15. Lines 15 to 19 join these constraints generated\nfrom a UDF to the subsequent dataflow operator.\n5 EVALUATION\nWe evaluate the effectiveness and efficiency of BigTest using a\ndiverse set of benchmark DISC applications. We compare BigTest\nagainst Sedge in terms of path coverage, fault detection capability,\nand testing time. We compare test adequacy, input data size, and\npotential time saving against three alternative testing methods: (1)\nrandom sampling of k% records, and (2) using a subset of the first\nk% records, and (3) testing on the entire original data.\n•To what extent BigTest is applicable to DISC applications?\n•How much test coverage improvement can BigTest achieve?\n•How many faults can BigTest detect?\n•How much test data reduction does BigTest provide?\n•How long does BigTest take to generate test data?\nSubject Programs. In terms of benchmark programs, we use seven\nsubject programs from earlier works on testing [ 31] and debugging\nDISC applications [ 25,28], listed in Table 3. The PigMix bench-\nmark package contains a data generator script that generates large\nscale datasets. We utilize mapandflatmap with UDFs in Apache\nSpark to translate unsupported Pig operators like load As and\nsplit . Three programs MovieRating (P2), AirportLayover (P3),\nandWordCount (P7) are adapted from BigSift [25]. Each program\nis paired with a large scale dataset. The rest are self-created cus-\ntom Apache Spark applications to add heterogeneity in dataflow\noperators and UDFs. Table 3 shows detailed descriptions of subject\nprograms. All applications (1) involve complex string operations\nincluding split ,substring , and toInt , (2) perform complex arith-\nmetics, (3) use type Tuple for key-value pairs, and (4) generate and\nprocess a collection with custom logic using flatmap .\nExperimental Environment. We run all large-scale data process-\ning on a 16-node cluster. Each node is running at 3.40GHz and\nequipped with 4 cores, 32GB of RAM, and 1TB of storage allowingP1 P2 P3 P4 P5 P6 P7020406080100\n100\n100\n100\n100\n100\n100\n10066.7\n60\n28.6\n18.2\n75\n76.7\n10066.7\n60\n28.6\n18.2\n75\n76.7\n100JDU Path Co veraдe\n(Normalized ,%)BigTest Random Sample first 1% Data\nFigure 10: JDU path coverage of BigTest in comparison to al-\nternative sampling methods\nus to run up to 120 tasks simultaneously. For storage, we use HDFS\nversion 1.0.4 with a replication factor of 3. Due to a very small size\nof test data generated by BigTest , we leverage Apache Spark’s local\nrunning mode to perform experiments on a single machine.\n5.1 Dataflow Program Support\nBigTest supports a variety of dataflow operators prevalent in DISC\napplications. For instance, Apache Spark provides flatmap and\nreduceByKey for constructing and processing collections. The pre-\nvious approach Sedge is designed for PIG Latin with only a limited\nset of operators support [ 31].Sedge is neither open-source nor\nhave any implementation available for Apache Spark for direct\ncomparison. Therefore, we faithfully implement Sedge precisely\nbased on the technical details provided elsewhere [ 31]. We manu-\nally downgrade BigTest by removing symbolic execution for UDFs\nand equivalence classes for certain operators to emulate Sedge . The\nimplementations of both Sedge and BigTest are publicly available1.\nOut of seven benchmark applications written in Apache Spark, five\napplications contain flatmap andreduceByKey , therefore, Sedge\nis not able to generate testing data for these 5 applications.\n5.2 Joint Dataflow and UDF Path Coverage\nWe evaluate code coverage of BigTest ,Sedge , and the original input\ndataset based on JDU path coverage defined in Section 4.3.\nJDU Path Coverage Evaluation. We compare BigTest with three\nalternative sampling techniques: (1) random sampling of k% of the\noriginal dataset, (2) selection of the first k% of the original dataset,\nas developers often test DISC applications using head -n , and (3) a\nprior approach Sedge . To keep consistency in our experiment setting,\nwe enumerate JDU paths for a given user-provided bound K and\nmeasure how many of these paths are covered by each approach.\nFigure 9 compares the test coverage from BigTest ,Sedge , and the\noriginal dataset. Y axis represents the normalized JDU path coverage\nranging from 0% to 100%. Across seven subject programs, we ob-\nserve that Sedge covers significantly fewer JDU paths (22% of what\n1https://github.com/maligulzar/BigTest\n296\n\n[Página 8]\nESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim\n10−110010110220304050\nLoдscale k(%)JDU Path Co veraдe\n(Normalized ,%)k%Random Sample\nFirst k %of Data\n(a) JDU Path Coverage10−110010110205101520\nLoдscale k(%)Test runnin дtime(s)\nk%Random Sample\nFirst k %of Data\n(b) Test Execution time\nFigure 11: The number of JDU paths covered and the test exe-\ncution time when k%of the data is randomly selected and the\nf irst k %of data is selected for subject program CommuteType .\nis covered by BigTest ). By not modelling the internal paths of UDFs,\nSedge fails to explore many JDU paths. Even when the complete\ndataset is used, the JDU path coverage reaches only 66% of what\nBigTest could achieve. The entire dataset achieves better coverage\nthan Sedge but it still lacks coverage compared to BigTest . In other\nwords, using the entire bigdata for testing does not necessarily\nprovide high test adequacy.\nIn Figure 10, both random 1% sample andfirst 1% sample provide\n59% of what is covered by BigTest . We perform another experiment\nto measure the impact of different sample sizes on JDU path cover-\nage and test execution time. Figure 11a and Figure 11b present the\nresults on CommuteType . InCommuteType , the covered JDU paths\nincreases from two to six when the percentage of the selected data\nincreases from 0.1% to 50%. For those small samples, input tables\ndo not have matching keys to exercise downstream operators and\nthe time and distance columns may not have specific values to\nexercise all internal paths of the UDF. In terms of running time, as\nthe sample size ( k) increases, the test execution time also increases\nlinearly (see Figure 11b in which x-axis is in log scale).\n5.3 Fault Detection Capability\nWe evaluate BigTest ’s ability to detect faults by manually injecting\ncommonly occurring faults. Because DISC applications are rarely\nopen-sourced for data privacy reasons and there is no existing\nbenchmark of faulty DISC applications, we create a set of faulty\nDISC applications by studying the characteristics of real world\nDISC application bugs and injecting faults based on this study.\nWe carefully investigate Stack Overflow and Apache Spark Mail-\ning lists with keywords; Apache Spark exceptions, task errors, failures ,\nand wrong outputs and inspect top 50 posts. Many errors are re-\nlated to performance and configuration errors; thus, we filter out\nthose and analyze 23 posts related to coding errors. For each post,\nwe investigate the type of fault by reading the question, posted\ncode, error logs, answers, and accepted solutions. We categorize\nour findings into seven common fault types:\n(1)incorrect string offset: e.g., a user uses 1 instead of 0 as the start-\ning index in method substring and encounters StringIndex-\nOutOfBoundsException [7].\n(2)incorrect column selection: e.g., a user accesses a wrong col-\numn in a csv file and thus receives ArrayIndexOutOfBound-\nsException [5].\n(3)use of wrong delimiters:e.g., while splitting a string a user\nuses \"[ ]\" instead of \"\\[\\]\", leading to a wrong output [8].Table 4: Fault detection capabilities of BigTest and Sedge\nSubject program\nP1 P2 P3 P4 P5 P6 P7\nSeeded Faults 3 6 6 6 4 4 2\nDetected by BigTest 3 6 6 6 4 4 2\nDetected by Sedge 1 6 4 4 2 3 0\n(4)incorrect branch conditions: e.g., a user places a wrong order\nof control predicates, executing only one branch’s side [4].\n(5)wrong join types: e.g., a user uses a wrong relational operator\nsuch as cartesian join instead of inner join [3].\n(6)swapping a key with a value: e.g., a user tries to join two\ntables while the keys and values are interleaved [6].\n(7)other common mutations such as incorrect arithmetic or\nBoolean operator in UDFs.\nWhen applicable, we inject one of each fault type in every ap-\nplication. For example, fault types 1 and 3 could only be inserted\nwhen substr orsplit method is used. When a fault type is appli-\ncable to multiple locations, we select a location which is inspired\nby and similar to the fault location in the corresponding StackOver-\nflow/Mailing List post. For instance, for fault type (2) above, we\nmanually modify code to extract the first column instead of the\nsecond as a key in line 4 of Figure 2. Similarly, for fault type (3), we\nintroduce fault by replacing the delimiter \",\" with \":\". In total, our\nbenchmark comprises of 31 faulty DISC applications. While Sedge\nis not designed to handle string constraints, the main goal of this\nexercise is to justify the need to model UDFs and string constraints.\nSedge represents the internal UDFs as uninterpreted functions and,\ntherefore, is unable to model all internal UDF paths. Conversely,\nBigTest treats UDFs as interpreted functions by representing them\nsymbolically and models all internal UDF paths (up to bound k)\nwhich is crucial for high coverage testing of UDF’s internal.\nTable 4 shows a comparison of fault detection by BigTest and\nSedge .BigTest detects 2X more injected faults than Sedge . For in-\nstance, in application P4, BigTest detects 6 faults, whereas Sedge\ndetects 4 faults. Sedge uses concrete execution to model the UDF\nexercising line 16 of Figure 2 only. Therefore, it is unable to find an\ninput for detecting fault at line 17 when the binary operator \">\"\nis replaced with \"<\"(i.e.,s._2._1>15 tos._2._1<15 ). Similarly,\nwhen join in line 13 is changed to rightOuterJoin ,Sedge cannot\ndetect any difference in the output because the equivalence classes\ndo not model the terminating cases of join.\nTable 5: Modelling terminating and non-terminating cases\nOutput from programApproach Test Input DataOriginal Faulty\nBigTestTerminating CS100:41,01l\nNon-terminating CS200:0,0,0,0,0,0CS200CS100\nCS200\nAlternative Non-terminating CS200:0,0,0,0,0,0 CS200 CS200\nAs another example, application P6 identifies courses with more\nthan 5 failing students. A faulty version of P6 replaces the filter\npredicate count>5 tocount>0 to output courses with at least one\nfailing student. The original version of P6 uses mapandfilter to\nparse each row and identify failing students, reduceByKey to count\nthe number of failing students, and uses filter to find courses with\nmore than 5 failing students. BigTest generates at least two records\nto exercise both terminating and non-terminating cases of the last\nfilter ; thus, the original and faulty versions produce different\n297\n\n[Página 9]\nWhite-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia\nP1 P2 P3 P4 P5 P6 P71011071013\n6 514 1143064·109\n5.21·1054.48·1083.2·1082.4·108\n4·1071.11·108#of input records(loдscale)Minimal input data selected for maximal JDU coverage\nEntire data\nFigure 12: Reduction in the size of the testing data by BigTest\nP1 P2 P3 P4 P5 P6 P7020406080100\n15.5 13.759.9\n29.724.252.6\n2.2 2.9 4.2 6.4 4.2 5.31.8Test Runnin дTime(s) Entire data\nTest data generated by BigTest\nFigure 13: Test running time of entire data on large-scale\ncluster vs. testing on local machine with BigTest\noutcomes on this data. On the other hand, a record is generated to\nexercise a non-terminating case only. Such data would produce the\nsame outcome for both the original and the faulty versions, unable\nto detect the injected fault, as shown in Table 5.\n5.4 Testing Data Reduction\nTesting DISC applications on the entire dataset is expensive and\ntime-consuming. BigTest minimizes the size of the dataset, while\nmaintaining the same test coverage. It generates only a few data\nrecords (in order of tens) to achieve the same JDU path coverage\nas the entire production data. Four out of seven benchmarks have\nan accompanied dataset, whereas the rest relies on a synthetic\ndataset of around 20GB each. Figure 12 shows the comparison result.\nIn application P6, BigTest generates 30 rows of data to achieve\n33% more JDU path coverage than the entire dataset of 40 million\nrecords. In other words, BigTest produces testing data 106times\nsmaller than the original dataset. Across all benchmark applications,\nBigTest generates data ranging from 5 to 30 rows. This is 105to 108\ntimes smaller than the original dataset, showing the potential to\nsignificantly reduce dataset size for local testing.\n5.5 Time and Resource Saving\nBy minimizing test data without compromising JDU path coverage,\nBigTest consequently reduces the test running time. The benefit of\na smaller test data is twofolds: (1) the amount of time required to\nrun a test case decreases, and (2) the amount of resources (worker\nnodes, memory, disk space, etc.) for running tests also decreases.\nWe measure, on a single machine, the total running time by\nBigTest and compare it with the testing time on a 16-node cluster\nwith the entire input dataset. We present a breakdown of the total\nrunning time into test data generation vs. executing an application\non the generated data. Figure 13 represents the evaluation results.\nIn application P6, it takes 5.3 seconds on a single machine to test\nwith data from BigTest otherwise testing takes 387.2 CPU seconds\n(24.2 seconds x 16 machines) on the entire dataset, which still lacks\ncomplete JDU path coverage. Across the seven subject programs,P1 P2 P3 P4 P5 P6 P7020406080\n10.67.374.2\n23\n817.6\n4.78.44.470\n16.6\n4.112.3\n2.93.7 3.8 3.5 3.9 3.8 3.8 2.6Runnin дTime(s) Constraint Generation\nConstraint Solver\nTest Execution\nFigure 14: Breakdown of BigTest ’s running time\n1 2 3 4 5100102104\nDeдree of bound(K)#ofJDU PathsW ord Count Grades Anal ysis Income A ддreдate\n1 2 3 4 5101102103\nDeдree of bound(K)Test Generation Time (s)\nFigure 15: BigTest ’s performance when the degree of upper\nbound (K) on loop iteration and collection size changes\nBigTest improves the testing time by 194X, on average, compared\nto testing with the entire dataset.\nFigure 14 reports the complete breakdown of the total running\ntime of BigTest . The maximum test generation time observed is\n70 seconds for Airport Layover (P3) in which 66 seconds are\nconsumed by constraint solving. This is because the resulting JDU\npaths include integer arithmetics and complex string constraints\ntogether. Solving such constraints that cross the boundaries of\ndifferent dimensions (integer arithmetics vs. string constraints) is\ntime consuming even after BigTest ’s optimizations. If we combine\nboth the test running time and test generation time and compare\nBigTest with the testing time with the entire dataset, BigTest still\noutperforms. In fact, BigTest still is 59X faster than testing on the\nentire dataset.\n5.6 Bounded Depth Exploration\nBigTest takes a user-provided bound Kto bound the number of\ntimes a loop is unrolled. We assess the impact of varying Kfrom\n1 to 5 and present the results in Figure 15. At K=2, the number of\nJDU paths for GradeAnalysis is 36. When Kis 3, BigTest generates\n438 JDU paths. An exponential-like increase in the test generation\ntime can be seen across the subject program, as we increase K.\nWhen K=2 in GradeAnalysis ,BigTest takes 12 seconds and with\nK=3,BigTest takes 204 seconds. We empirically find K=2 to be a\nreasonable upper bound for loop iteration to avoid path explosion.\n5.7 Threats to Validity\nAs we manually seed faults in the benchmark applications, the loca-\ntion of faults may introduce a bias in fault detection rate of BigTest\nposing a threat to internal validity. However, as mentioned before,\nmost type of faults are only applicable to a single code location. If a\nfault type is applicable to multiple locations, we then select the fault\nlocation inspired by the corresponding StackOverflow/Mailing List\npost. In case of external validity, our classification of DISC faults\nmay not be representative of all possible DISC application faults\nout there, as the survey is based on 50 StackOverflow/mailing lists\n298\n\n[Página 10]\nESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim\nposts. Additionally, the selection of fault types in our evaluation\nmay be unfair to prior approaches. We attempt to mitigate this bias\nby restricting the evaluation to top seven most commonly occurring\nfaults in DISC applications. To eliminate this threat in the future,\nwe plan to perform a large scale study on DISC application faults.\n6 RELATED WORK\nTesting Map-Reduce Programs. Csallner et al. propose the idea\nof testing commutative and associative properties of Map-Reduce\nprograms by generating symbolic constraints [ 18]. Their goal is to\nidentify non-determinism in a Map-Reduce program arising from\na non-associative or non-commutative user-defined function in\nthereduce operator. They produce counter examples as evidence\nby running a constraint solver over symbolic path constraints. Xu\net al. add few more Map-Reduce program properties such as (1)\noperator selectivity , (2)operator statefulness , and (3) partition interfer-\nence [45] . Both of these techniques test only high-level properties\nof individual dataflow operators and they do not model the inter-\nnal program paths of user-defined functions. Olsten et al. generate\ndata for Pig Latin programs [ 34]. Their approach considers each\noperator in isolation and does not model internal program paths\nof UDFs—treated as black-box. Furthermore, Olsten et al. require\nknowing the inverse function of a UDF given to transform .\nLi et al. ( Sedge ) [31] is the most relevant approach to BigTest .\nSedge has three main limitations. First, its symbolic execution does\nnot analyze the internal paths of individual UDFs. It considers\nUDFs as black box procedures and encodes them into uninterpreted\nfunctions . Second, it does not support operators such as flatmap ,\nreduce , and reducebyKey , which are essential for constructing\na collection and aggregating results from a collection in big data\nanalytics. Third, the equivalence class modeling for each dataflow\noperator is not comprehensive, as it does not consider early termi-\nnating cases for some operators, where a data record does not flow\nto the next dataflow operator. Our empirical evaluation in Section 5\nfinds that these limitations lead to low defect detection in Sedge .\nTable 6 compares dataflow operator support for related approaches\nand shows that BigTest has the most comprehensive and advanced\nsupport for modern DISC applications.\nTest Generation in Databases. JDBC [ 41] or ODBC [ 2] enable\nsoftware developers to write applications that construct and exe-\ncute database queries at runtime. Testing such programs requires\ntest inputs and database states from a user. Emmi et al. perform con-\ncolic execution of a program embedded with an SQL query [ 21] by\nsymbolically executing the program till the point where a query is\nexecuted. Their approach is only applicable to basic SQL operations\nsuch as projection, selection, etc. ( e.g.,SELECT, WHERE ). Braberman\net al. select input data to test the logic of computing additional fields\nfrom existing columns in the database [ 13]. They do not handle\narbitrary UDFs which are prevalent in DISC applications.\nSymbolic Execution. Symbolic execution is a widely used tech-\nnique in software engineering [ 12,27,37] and is used to generate\ntest data using constraint solvers [ 14–16,23,32,33,40]. For ex-\nample, Visser et al. use JPF (Java PathFinder [ 29]) to generate test\ninput data [ 44]. However, the same approach cannot be applied to\nDISC applications directly because it would symbolically execute\nthe application as well as the underlying DISC framework. Such\npractice will produce an unnecessarily large number of complexTable 6: Support of dataflow operators in related work\nDataflow Operators Olston et al . Li et al . Emmi et al .Pan et al .BigTest\nLoad ✓ ✓ ✓ ✓ ✓\nMap (Select) ✓ ✓ ✓ ✓ ✓\nMap (Transform) Incomplete Incomplete ✗ ✗ ✓\nFilter (Where) ✓ ✓ ✓ ✓ ✓\nGroup ✓ ✓ ✗ ✗ ✓\nJoin Incomplete Incomplete ✗ Incomplete ✓\nUnion ✓ ✓ ✗ ✗ ✓\nFlatmap (Split) ✗ Incomplete ✗ ✗ ✓\nIntersection ✗ ✗ ✗ ✗ ✓\nReduce ✗ ✗ ✗ ✗ ✓\npath constraints, facing scalability issues. This justifies and moti-\nvates our approach that abstracts dataflow operators as a logical\nspecifications while performing symbolic execution for the UDFs.\nRosette is a framework for designing a solver-aided language [ 42]\nto ease the process of translating each language construct into sym-\nbolic constraints. BigTest and Rosette both translate higher-order\ntypes such as arrays into lower-level constraints. Bang et al. address\nthe problem of solving constraints crossing boundaries between\ndifferent theories (numerics, integer, and string constraints) [ 10].\nSuch cross-theory constraints are known to be difficult to solve\nwith Z3 or CVC4. They extend SPF by modeling strings into bit\nvectors and by integrating numeric model counting in ABC [ 9]\nwhich could be used for BigTest in the future.\nRegression Testing. Regression testing has been extensively stud-\nied in software testing. Safe regression testing selects only those test\ncases that exercise the updated regions of a program [ 26]. Rothermel\net al. summarize several regression testing techniques and evaluate\nthem under a controlled environment [ 38]. Test augmentation tech-\nniques help developers generate new test data to cover code not ex-\nercised by the available test cases using symbolic execution [ 17,30].\nXu et al. evaluate concolic and genetic test generation approaches\nand report trade-offs [ 46]. The aforementioned approaches are not\ndirectly applicable to DISC applications, as they do not explicitly\nmodel the combined behavior of dataflow (/relational) operators\nand the internal semantics of UDFs.\n7 CONCLUSION\nBig data analytics are now prevalent in many domains. However,\nsoftware engineering methods for DISC applications are relatively\nunder-developed. To enable efficient and effective testing of big\ndata analytics in real world settings, we present a novel white-box\ntesting technique that systematically explores the combined behav-\nior of dataflow operators and corresponding UDFs. This technique\ngenerates joint dataflow and UDF path constraints and leverages\ntheorem solvers to generate concrete test inputs. BigTest can de-\ntect 2X more faults than the previous approach and can consume\n194X less CPU time, on average than using the entire dataset. With\nBigTest ,fastlocal testing is feasible and testing DISC applications\non the entire dataset may not be necessary.\nACKNOWLEDGMENTS\nWe thank the anonymous reviewers for their comments. The par-\nticipants of this research are in part supported by Google PhD\nFellowship, NSF grants CCF-1764077, CCF-1527923, CCF-1460325,\nCCF-1723773, ONR grant N00014-18-1-2037, Intel CAPA grant, and\nSamsung grant. We would also like to thank Emina Torlak and\nKoushik Sen for their insightful discussions.\n299\n\n[Página 11]\nWhite-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia\nREFERENCES\n[1] [n.d.]. Hadoop. http://hadoop.apache.org/.\n[2][n.d.]. Microsoft Open Database Connectivity (ODBC). https://msdn.microsoft.\ncom/en-us/library/ms710252(v=vs.85).aspx.\n[3] 2015. . https://stackoverflow.com/questions/32190828.\n[4] 2016. . https://stackoverflow.com/questions/40494999.\n[5] 2017. . https://stackoverflow.com/questions/48021303.\n[6] 2017. . https://stackoverflow.com/questions/42459749.\n[7] 2018. . https://stackoverflow.com/questions/49505241.\n[8] 2018. . https://stackoverflow.com/questions/52083828.\n[9]Abdulbaki Aydin, Lucas Bang, and Tevfik Bultan. 2015. Automata-Based Model\nCounting for String Constraints. In Computer Aided Verification , Daniel Kroening\nand Corina S. Păsăreanu (Eds.). Springer International Publishing, Cham, 255–\n272.\n[10] Lucas Bang, Abdulbaki Aydin, Quoc-Sang Phan, Corina S. Păsăreanu, and Tevfik\nBultan. 2016. String Analysis for Side Channels with Segmented Oracles. In\nProceedings of the 2016 24th ACM SIGSOFT International Symposium on Foun-\ndations of Software Engineering (FSE 2016) . ACM, New York, NY, USA, 193–204.\nhttps://doi.org/10.1145/2950290.2950362\n[11] Clark Barrett, Christopher L. Conway, Morgan Deters, Liana Hadarean, Dejan\nJovanovi’c, Tim King, Andrew Reynolds, and Cesare Tinelli. 2011. CVC4. In\nProceedings of the 23rd International Conference on Computer Aided Verification\n(CAV ’11) (Lecture Notes in Computer Science) , Ganesh Gopalakrishnan and Shaz\nQadeer (Eds.), Vol. 6806. Springer, 171–177. http://www.cs.stanford.edu/~barrett/\npubs/BCD+11.pdf Snowbird, Utah.\n[12] Robert S. Boyer, Bernard Elspas, and Karl N. Levitt. 1975. SELECT&Mdash;a\nFormal System for Testing and Debugging Programs by Symbolic Execution. In\nProceedings of the International Conference on Reliable Software . ACM, New York,\nNY, USA, 234–245. https://doi.org/10.1145/800027.808445\n[13] Víctor Braberman, Diego Garbervetsky, Javier Godoy, Sebastian Uchitel, Guido\nde Caso, Ignacio Perez, and Santiago Perez. 2018. Testing and Validating End User\nProgrammed Calculated Fields. In Proceedings of the 2018 26th ACM Joint Meeting\non European Software Engineering Conference and Symposium on the Foundations\nof Software Engineering (ESEC/FSE 2018) . ACM, New York, NY, USA, 827–832.\nhttps://doi.org/10.1145/3236024.3275531\n[14] J. Burnim and K. Sen. 2008. Heuristics for Scalable Dynamic Test Generation. In\nProceedings of the 2008 23rd IEEE/ACM International Conference on Automated\nSoftware Engineering (ASE ’08) . IEEE Computer Society, Washington, DC, USA,\n443–446. https://doi.org/10.1109/ASE.2008.69\n[15] Cristian Cadar and Dawson Engler. 2005. Execution Generated Test Cases: How\nto Make Systems Code Crash Itself. In Proceedings of the 12th International Con-\nference on Model Checking Software (SPIN’05) . Springer-Verlag, Berlin, Heidelberg,\n2–23. https://doi.org/10.1007/11537328_2\n[16] Cristian Cadar, Vijay Ganesh, Peter M. Pawlowski, David L. Dill, and Dawson R.\nEngler. 2006. EXE: Automatically Generating Inputs of Death. In Proceedings of\nthe 13th ACM Conference on Computer and Communications Security (CCS ’06) .\nACM, New York, NY, USA, 322–335. https://doi.org/10.1145/1180405.1180445\n[17] Cristian Cadar, Patrice Godefroid, Sarfraz Khurshid, Corina S. Păsăreanu, Koushik\nSen, Nikolai Tillmann, and Willem Visser. 2011. Symbolic Execution for Software\nTesting in Practice: Preliminary Assessment. In Proceedings of the 33rd Interna-\ntional Conference on Software Engineering (ICSE ’11) . ACM, New York, NY, USA,\n1066–1071. https://doi.org/10.1145/1985793.1985995\n[18] Christoph Csallner, Leonidas Fegaras, and Chengkai Li. 2011. New Ideas\nTrack: Testing Mapreduce-style Programs. In Proceedings of the 19th ACM SIG-\nSOFT Symposium and the 13th European Conference on Foundations of Soft-\nware Engineering (ESEC/FSE ’11) . ACM, New York, NY, USA, 504–507. https:\n//doi.org/10.1145/2025113.2025204\n[19] Leonardo De Moura and Nikolaj Bjørner. 2008. Z3: An efficient SMT solver. In\nInternational conference on Tools and Algorithms for the Construction and Analysis\nof Systems . Springer, 337–340.\n[20] Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: simplified data processing\non large clusters. Commun. ACM 51, 1 (2008), 107–113.\n[21] Michael Emmi, Rupak Majumdar, and Koushik Sen. 2007. Dynamic Test Input\nGeneration for Database Applications. In Proceedings of the 2007 International\nSymposium on Software Testing and Analysis (ISSTA ’07) . ACM, New York, NY,\nUSA, 151–162. https://doi.org/10.1145/1273463.1273484\n[22] Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-\nmated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on\nProgramming Language Design and Implementation (PLDI ’05) . ACM, New York,\nNY, USA, 213–223. https://doi.org/10.1145/1065010.1065036\n[23] Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto-\nmated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on\nProgramming Language Design and Implementation (PLDI ’05) . ACM, New York,\nNY, USA, 213–223. https://doi.org/10.1145/1065010.1065036\n[24] Patrice Godefroid, Michael Y. Levin, and David A Molnar. 2008. Automated White-\nbox Fuzz Testing. In Network Distributed Security Symposium (NDSS) . Internet\nSociety. http://www.truststc.org/pubs/499.html[25] Muhammad Ali Gulzar, Matteo Interlandi, Xueyuan Han, Mingda Li, Tyson\nCondie, and Miryung Kim. 2017. Automated Debugging in Data-intensive Scal-\nable Computing. In Proceedings of the 2017 Symposium on Cloud Computing (SoCC\n’17). ACM, New York, NY, USA, 520–534. https://doi.org/10.1145/3127479.3131624\n[26] Mary Jean Harrold, James A. Jones, Tongyu Li, Donglin Liang, Alessandro Orso,\nMaikel Pennings, Saurabh Sinha, S. Alexander Spoon, and Ashish Gujarathi.\n2001. Regression Test Selection for Java Software. In Proceedings of the 16th\nACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages,\nand Applications (OOPSLA ’01) . ACM, New York, NY, USA, 312–326. https:\n//doi.org/10.1145/504282.504305\n[27] W. E. Howden. 1977. Symbolic Testing and the DISSECT Symbolic Evaluation\nSystem. IEEE Trans. Softw. Eng. 3, 4 (July 1977), 266–278. https://doi.org/10.1109/\nTSE.1977.231144\n[28] Matteo Interlandi, Ari Ekmekji, Kshitij Shah, Muhammad Ali Gulzar, Sai Deep\nTetali, Miryung Kim, Todd Millstein, and Tyson Condie. 2018. Adding data\nprovenance support to Apache Spark. The VLDB Journal 27, 5 (01 Oct 2018),\n595–615. https://doi.org/10.1007/s00778-017-0474-5\n[29] Sarfraz Khurshid, Corina S. Păsăreanu, and Willem Visser. 2003. Generalized\nSymbolic Execution for Model Checking and Testing. In Proceedings of the 9th\nInternational Conference on Tools and Algorithms for the Construction and Analysis\nof Systems (TACAS’03) . Springer-Verlag, Berlin, Heidelberg, 553–568. http://dl.\nacm.org/citation.cfm?id=1765871.1765924\n[30] James C. King. 1976. Symbolic Execution and Program Testing. Commun. ACM\n19, 7 (July 1976), 385–394. https://doi.org/10.1145/360248.360252\n[31] Kaituo Li, Christoph Reichenbach, Yannis Smaragdakis, Yanlei Diao, and\nChristoph Csallner. 2013. SEDGE: Symbolic example data generation for dataflow\nprograms. In Automated Software Engineering (ASE), 2013 IEEE/ACM 28th Inter-\nnational Conference on . IEEE, 235–245.\n[32] Rupak Majumdar and Koushik Sen. 2007. Hybrid Concolic Testing. In Proceedings\nof the 29th International Conference on Software Engineering (ICSE ’07) . IEEE\nComputer Society, Washington, DC, USA, 416–426. https://doi.org/10.1109/ICSE.\n2007.41\n[33] David Molnar, Xue Cong Li, and David A. Wagner. 2009. Dynamic Test Gener-\nation to Find Integer Bugs in x86 Binary Linux Programs. In Proceedings of the\n18th Conference on USENIX Security Symposium (SSYM’09) . USENIX Association,\nBerkeley, CA, USA, 67–82. http://dl.acm.org/citation.cfm?id=1855768.1855773\n[34] Christopher Olston, Shubham Chopra, and Utkarsh Srivastava. 2009. Generating\nExample Data for Dataflow Programs. In Proceedings of the 2009 ACM SIGMOD\nInternational Conference on Management of Data (SIGMOD ’09) . ACM, New York,\nNY, USA, 245–256. https://doi.org/10.1145/1559845.1559873\n[35] K. Ouaknine, M. Carey, and S. Kirkpatrick. 2015. The PigMix Benchmark on Pig,\nMapReduce, and HPCC Systems. In 2015 IEEE International Congress on Big Data .\n643–648. https://doi.org/10.1109/BigDataCongress.2015.99\n[36] Corina S. P ˇasˇareanu, Peter C. Mehlitz, David H. Bushnell, Karen Gundy-Burlet,\nMichael Lowry, Suzette Person, and Mark Pape. 2008. Combining Unit-level\nSymbolic Execution and System-level Concrete Execution for Testing Nasa Soft-\nware. In Proceedings of the 2008 International Symposium on Software Testing and\nAnalysis (ISSTA ’08) . ACM, New York, NY, USA, 15–26. https://doi.org/10.1145/\n1390630.1390635\n[37] C. V. Ramamoorthy, S. B. F. Ho, and W. T. Chen. 1976. On the Automated\nGeneration of Program Test Data. IEEE Trans. Softw. Eng. 2, 4 (July 1976), 293–300.\nhttps://doi.org/10.1109/TSE.1976.233835\n[38] Gregg Rothermel and Mary Jean Harrold. 1996. Analyzing Regression Test\nSelection Techniques. IEEE Trans. Softw. Eng. 22, 8 (Aug. 1996), 529–551. https:\n//doi.org/10.1109/32.536955\n[39] Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A Concolic Unit\nTesting Engine for C. In Proceedings of the 10th European Software Engineering\nConference Held Jointly with 13th ACM SIGSOFT International Symposium on\nFoundations of Software Engineering (ESEC/FSE-13) . ACM, New York, NY, USA,\n263–272. https://doi.org/10.1145/1081706.1081750\n[40] Matt Staats and Corina P ˇasˇareanu. 2010. Parallel Symbolic Execution for Struc-\ntural Test Generation. In Proceedings of the 19th International Symposium on\nSoftware Testing and Analysis (ISSTA ’10) . ACM, New York, NY, USA, 183–194.\nhttps://doi.org/10.1145/1831708.1831732\n[41] Art Taylor. 2002. Jdbc: Database Programming with J2Ee with Cdrom . Prentice\nHall Professional Technical Reference.\n[42] Emina Torlak and Rastislav Bodik. 2014. A Lightweight Symbolic Virtual Machine\nfor Solver-aided Host Languages. In Proceedings of the 35th ACM SIGPLAN Con-\nference on Programming Language Design and Implementation (PLDI ’14) . ACM,\nNew York, NY, USA, 530–541. https://doi.org/10.1145/2594291.2594340\n[43] Willem Visser, Klaus Havelund, Guillaume Brat, Seungjoon Park, and Flavio\nLerda. 2003. Model Checking Programs. Automated Software Engg. 10, 2 (April\n2003), 203–232. https://doi.org/10.1023/A:1022920129859\n[44] Willem Visser, Corina S. P ˇasˇareanu, and Sarfraz Khurshid. 2004. Test Input Gener-\nation with Java PathFinder. In Proceedings of the 2004 ACM SIGSOFT International\nSymposium on Software Testing and Analysis (ISSTA ’04) . ACM, New York, NY,\nUSA, 97–107. https://doi.org/10.1145/1007512.1007526\n300\n\n[Página 12]\nESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim\n[45] Z. Xu, M. Hirzel, G. Rothermel, and K. L. Wu. 2013. Testing properties of dataflow\nprogram operators. In 2013 28th IEEE/ACM International Conference on Automated\nSoftware Engineering (ASE) . 103–113. https://doi.org/10.1109/ASE.2013.6693071\n[46] Zhihong Xu, Yunho Kim, Moonzoo Kim, Gregg Rothermel, and Myra B. Co-\nhen. 2010. Directed Test Suite Augmentation: Techniques and Tradeoffs. In\nProceedings of the Eighteenth ACM SIGSOFT International Symposium on Foun-\ndations of Software Engineering (FSE ’10) . ACM, New York, NY, USA, 257–266.\nhttps://doi.org/10.1145/1882291.1882330\n[47] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma,\nMurphy McCauley, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2012. Re-\nsilient Distributed Datasets: A Fault-tolerant Abstraction for In-memory Cluster\nComputing. In Proceedings of the 9th USENIX Conference on Networked SystemsDesign and Implementation (NSDI’12) . USENIX Association, Berkeley, CA, USA,\n2–2. http://dl.acm.org/citation.cfm?id=2228298.2228301\n[48] Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and\nIon Stoica. 2010. Spark: Cluster Computing with Working Sets. In Proceedings\nof the 2Nd USENIX Conference on Hot Topics in Cloud Computing (HotCloud’10) .\nUSENIX Association, Berkeley, CA, USA, 10–10. http://dl.acm.org/citation.cfm?\nid=1863103.1863113\n[49] Hucheng Zhou, Jian-Guang Lou, Hongyu Zhang, Haibo Lin, Haoxiang Lin, and\nTingting Qin. 2015. An Empirical Study on Quality Issues of Production Big\nData Platform. In Proceedings of the 37th International Conference on Software\nEngineering - Volume 2 (ICSE ’15) . IEEE Press, Piscataway, NJ, USA, 17–26. http:\n//dl.acm.org/citation.cfm?id=2819009.2819014\n301",
      "metadata": {
        "filename": "White-Box Testing of Big Data Analytics with Complex.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\White-Box Testing of Big Data Analytics with Complex.pdf",
        "file_size": 1221435,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:41.719028",
        "content_length": 73795
      }
    },
    "e4385e1a-3342-4d4e-aaf7-50e16ca2859f": {
      "id": "e4385e1a-3342-4d4e-aaf7-50e16ca2859f",
      "content": "[Página 1]\nQoS-Aware Proactive Data Replication for Big Data Analytics in\nEdge Clouds\nQiufen Xia\nqiufenxia@dlut.edu.cn\nDalian University of Technology\nDalian, Liaoning, ChinaLuyao Bai\nbailuyao1997@outlook.com\nDalian University of Technology\nDalian , Liaoning, ChinaWeifa Liang\nwliang@cs.anu.edu.au\nAustralian National University\nCanberra, ACT, Australia\nZichuan Xu\nz.xu@dlut.edu.cn\nDalian University of Technology\nDalian, Liaoning, ChinaLin Yao\nyaolin@dlut.edu.cn\nDalian University of Technology\nDalian, Liaoning, ChinaLei Wang\nlei.wang@dlut.edu.cn\nDalian University of Technology\nDalian, Liaoning, China\nABSTRACT\nWe are in the era of big data and cloud computing, large quantity\nof computing resource is desperately needed to detect invaluable\ninformation hidden in the coarse big data through query evaluation.\nUsers demand big data analytic services with various Quality of\nService (QoS) requirements. However, cloud computing is facing\nnew challenges in meeting stringent QoS requirements of users due\nto the remoteness from its users. Edge computing has emerged as\na new paradigm to address such shortcomings by bringing cloud\nservices to the edge of the operation network in proximity of users\nfor performance improvement. To satisfy the QoS requirements of\nusers for big data analytics in edge computing, the data replication\nand placement problem must be properly dealt with such that user\nrequests can be efficiently and promptly responded. In this paper,\nwe consider data replication and placement for big data analytic\nquery evaluation. We first cast a novel proactive data replication\nand placement problem of big data analytics in a two-tier edge\ncloud environment, we then devise an approximation algorithm\nwith an approximation ratio for it, we finally evaluate the proposed\nalgorithm against existing benchmarks, using both simulation and\nexperiment in a testbed based on real datasets, the evaluation results\nshow that the proposed algorithm is promising.\nKEYWORDS\nData replication and placement; big data analytics; edge clouds;\nquery evaluation\nACM Reference Format:\nQiufen Xia, Luyao Bai, Weifa Liang, Zichuan Xu, Lin Yao, and Lei Wang.\n2019. QoS-Aware Proactive Data Replication for Big Data Analytics in Edge\nClouds. In 48th International Conference on Parallel Processing: Workshops\n(ICPP 2019), August 5–8, 2019, Kyoto, Japan. ACM, New York, NY, USA,\n10 pages. https://doi.org/10.1145/3339186.3339207\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nICPP 2019, August 5–8, 2019, Kyoto, Japan\n©2019 Association for Computing Machinery.\nACM ISBN 978-1-4503-7196-4/19/08. . . $15.00\nhttps://doi.org/10.1145/3339186.33392071 INTRODUCTION\nCloud platforms have been receiving ever-growing attentions in\nrecent years to provide services in a wide range of information tech-\nnology (IT) domains, and offer on-demand processing, storage and\nbandwidth resources. Many services have been deployed on clouds\nand generate big data there, the big data are analyzed to obtain\nhidden valuable information for business advantages and decision-\nmakings. However, cloud computing is facing new challenges in\nmeeting the quality of service (QoS) requirements of emerging ap-\nplications, such as augmented reality, autonomous vehicles, timely\nquery evaluation for big data analytics, to name a few. We argue\nthat the most pressing requirement of those emerging applications\nis response latency, which is the time duration from submitting a\nrequest to the cloud to receiving the query result by the request user.\nThe remote cloud data centers are not appropriate for achieving\nsmall response latencies, as it could suffer from limitations due to\nhigh transmission latency and risk of heavy workload as well as\nnetwork bottlenecks.\nOne promising solution to tackle the mentioned challenges is\nEdge Computing, which can exploit processing and storage capa-\nbilities at the edge of the network as near as possible to end-users.\nIn this regard, the deployment of edge cloudlets in network access\npoints can achieve remarkable benefits in terms of low-latency\ninteractions and economic computing resource. Query evaluation\nfor big data analytics demands large quantity of computing re-\nsource and low response latency, by leveraging edge computing\ntechnologies, the response time to big data analytics queries can\nbe significantly reduced. To this end, an important approach is\nto proactively replicate a large dataset to multiple data centers or\ncloudlets so that query users can obtain their desired query results\nwithin their specified time duration.\nAlthough data replication and placement can improve system\nperformance, it does not necessarily imply that more replicas will\nlead to better system performance, due to the fact that the mainte-\nnance of data consistency between the original dataset and its slave\nreplicas in the network does incur cost. To maximize the benefit\nof query processing and dataset replications, strategic replicating\nand placing replicas of each dataset in a two-tier edge cloud is\ncrucial. One fundamental problem thus is how to place the replicas\nof datasets to different data centers or cloudlets in the two-tier\nedge cloud so that big data analytics queries can be evaluated, the\n\n[Página 2]\nICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and Bai, et al.\nvolume of datasets demanded by admitted queries is maximized,\nwithout violating the resource capacity constraints and delay re-\nquirements of users. Notice that, one main reason that we aim to\nmaximize the volume of datasets demanded by admitted queries is\nas follows. Cloud service providers such as Amazon offer users a\npay-as-you-go approach for pricing [ 3], maximizing the volume of\ndatasets demanded by admitted queries means that users pay more\nfor evaluating queries to the cloud service providers who can thus\nobtain maximum income.\nSeveral studies on data replication and placement have been\nconducted in the past [ 1,6,26]. However, most these studies consid-\nered neither data replications of the generated big data [ 1,26] nor\nQoS requirements of users [ 1,6,26]. In addition, there are several\ninvestigations on query evaluation and data placement [ 17,20].\nAlthough some of them considered the data transmission cost, they\ndid not incorporate the QoS requirements of users [ 17], or data\nreplications and placements [ 20]. In this paper, we study proactive\ndata replication and placement of query evaluation for big data\nanalytics in a two-tier edge cloud with the aim to maximize the\nvolume of datasets demanded by admitted queries while meeting\nusers’ QoS requirements, subject to various resource capacities on\nan edge cloud network.\nThe main contributions of this paper are as follows.\n•We first formulate a novel proactive QoS-aware data repli-\ncation and placement problem for big data analytic query\nevaluation in a two-tier edge cloud environment. We aim\nto maximize the volume of datasets demanded by admitted\nqueries while meeting users’ end-to-end delay requirements.\n•We then propose an efficient approximation algorithm with\nprovable approximation ratio for the problem through a\nprimal-dual dynamic update technique.\n•We finally evaluate the performance of the proposed algo-\nrithm through experimental simulations and in a testbed\nusing real datasets. The simulation results show that the per-\nformance of the proposed algorithm is promising, placing\nsignificantly higher volume of datasets demanded by queries\nadmitted compared to some existing work.\n•To the best of our knowledge, this is the first time that the\nproactive QoS-aware data replication and placement prob-\nlem for big data analytics query evaluation in two-tier edge\nclouds is considered, and an efficient approximation algo-\nrithm is devised.\nThe remainder of this paper is organized as follows. Section 2 in-\ntroduces the system model and problem definition, followed by an\napproximation algorithm for the problem in Section 3. The per-\nformance evaluation of the proposed algorithm is conducted in\nSection 4. The related work is presented in Section 5, and conclu-\nsions are given in Section 6.\n2 PRELIMINARIES\nIn this section, we first introduce the system model. We then give\nnotations on big data analytics evaluation in the two-tier edge cloud\nunder QoS requirements of users. We finally define the problem\nprecisely.2.1 System model\nWe consider a two-tier edge cloud G=(BS∪SW∪CL∪DC,E),\nwhich consists of a set BSof base stations through which users\nconnect to edge cloudlets, a set SWof switches in a Wireless Met-\nropolitan Area Network (WMAN), a set CLof edge cloudlets co-\nlocated with some switches in SW, and a setDCof data centers\nlocated at different geographical locations that are connected to\nthe WMAN via the Internet to/from gateway nodes in SW.\nThese edge cloudlets, switches (or access points), and data centers\nare inter-connected by a set Eof communication links, and e∈Eis\na link between two cloudlets, two switches, a cloudlet and a switch,\nor a gateway node and a data center.\nLetCLibe an edge cloudlet in CL, and DCjbe a data center\ninDC. The computing resource of each edge cloudlet CLiand\neach data center DCjcan be used for processing data to evaluate\nqueries, while their storage resource is used to store the query\nresults and data replicas. The quantity of available computing re-\nsource of each data center or edge cloudlet is limited, especially\nfor cloudlets which usually consist of several servers to fit into\nsmall machine rooms located in metropolitan areas. Denote by\nB(DCj)andB(CLi)the computing capacities of data center DCj\nand cloudlet CLi, respectively. Denote by A(CLi)andA(DCj)the\navailable computing resources of edge cloudlet CLiand data center\nDCjat the moment. Evaluating queries of big data analytics in edge\ncloudlets and data centers consumes their computing resources.\nLetrmbe the amount of computing resource allocated to process\na unit data. We do not restrict the capacity of storage resource of\ncloudlets and data centers, as the storage resource usually is abun-\ndant and inexpensive, compared with the expensive computing\nresource [26].\nThe processing and transmission of data in Gconsume comput-\ning and bandwidth resources of edge clouds and thus incur process-\ning and transmission delays. Let d(CLi)andd(DCj)be the delays\nincurred by processing a unit data per unit computing resource in\ncloudlet CLiand data center DCjanddt(e)the transmission delay\non link e∈Efor transferring a unit data.\nFor simplicity, let V={CL∪DC}, and each node vl∈V\nrepresents either an edge cloudlet or a data center. An example of\na two-tiered edge cloud Gis illustrated in Fig. 1.\n2.2 Big data processing in the edge cloud\nWith the wide adoption of cloud services, enterprise users usually\nhave large scale of legacy services being outsourced to remote data\ncenters, and these services generate large volume of data from their\noutsourced services, such as web logs, click streams, sensory data.\nMeanwhile, with the support of network service providers, more\nand more cloud services are deployed in edge cloudlets within the\nproximity of users to reduce the response time. To obtain valuable\ninformation and interesting patterns from such big data generated\nby services deployed at data centers and cloudlets, users may con-\nduct analysis on big data that are stored in remote data centers and\nedge cloudlets by issuing queries.\nPerforming big data analytics in remote data centers causes\nvery high latency, because large volume of intermediate results\ngenerated by processing the big data need to be transferred to\nedge cloudlets and join with the intermediate results there, the\n\n[Página 3]\nQoS-Aware Proactive Data Replication for Big Data Analytics ICPP 2019, August 5–8, 2019, Kyoto, Japan\nFigure 1: An example of a two-tier edge cloud G.\ndelay requirements required by users may be violated ultimately.\nTherefore, proactively replicating big data from the remote cloud to\nedge cloudlets is an effective way to reduce data transmission delay\nand guarantee the timeliness of big data analytics. Meanwhile, the\ncomputing capacity of an edge cloudlet is very limited, it takes long\ntime to evaluate queries, sometimes the computing resource of an\nedge-cloudlet even cannot satisfy the resource demands of big data\nquery evaluation, so the big data generated in edge cloudlets can be\nproactively placed to the remote data centers and processed there,\nthereby reducing the processing delay to guarantee the timeline\nof queries and satisfying the computing resource requirements of\nqueries. We thus assume that the big data and their replicas can be\nreplicated to the edge cloudlets or remote data centers in advance,\nsuch that the delay incurred by the joint analysis of datasets or\ntransmission of intermediate results is no greater than the delay\nrequirements of queries.\nLetSbe the collection of datasets generated by all services in\nremote data centers, denote by Sna dataset inS, where 1≤n≤|S|\nwith|S|representing the number of datasets in S. Denote by qm\na query for big data analytics. Each query qmusually requires to\nbe evaluated based on a collection of datasets. Let S(qm)be the\ncollection of datasets required by query qm.\nEvaluating a query qmis to abstract the intermediate results\nfrom its requested datasets that possibly are in different data centers\nor cloudlets, and aggregate the intermediate results at the home\nlocation of the query. Let hmbe the home location of query qm,\nwhich can be a data center or a cloudlet. Without loss of generality,\nwe assume that the size of an intermediate result on each dataset Sn\nevaluated by query qmis a fraction size αnmofSn, i.e.,αnm·|Sn|,\nwhereαnmis with 0<αnm≤1[21] and|Sn|is the volume of\ndataset Sn.\n2.3 User QoS requirements\nAs we consider query evaluation for big data analytics within strin-\ngent delay requirements, we refer to the delay requirement of a\nquery as its quality of service (QoS) requirement , where the delay\nexperienced by the query is defined as the duration from the query\nis issued to the evaluation result is received. Since the size of aquery is usually small, the transfer delay of the query from a user\nlocation to the edge cloud network is negligible.\nEach query may require multiple datasets or datasets’ replicas\nplaced at different locations, the processing datasets and trans-\nmitting intermediate results can be performed in parallel among\ndifferent datasets, therefore the delay experienced by qmdemand-\ning multiple datasets is the maximum sum of the delays incurred\nin processing a dataset and transmitting the intermediate results\nof the dataset inS(qm)accessed by qm, i.e., arдmax{(d(vl)·|Sn|+\ndt(pvl,hm)·|Sn|·αnm)}. Denote by dqmthe maximum tolerable\ndelay of query qm, that is to say, dqmis the QoS in terms of delay\nrequirement of query qm. To make datasets in the two-tier edge\ncloud highly available, reliable and scalable, the datasets usually\nhave several replicas, while in order to reduce the cost for data\nconsistency, we thus assume that each dataset Snhas at most K\nreplicas in the system with K∈Z+, the replication of datasets\nto data centers or cloudlets are conducted in advance before the\nevaluation of queries, and the delay incurred by dataset replications\nis not accounted into the QoS requirement of queries.\n2.4 Problem definition\nGiven a collection Sof datasets, a set of queries Q={qm|1≤\nm≤M}for big data analytics, and a two-tier edge cloud network\nG=(BS∪SW∪V,E), where V=CL∪DC, the computing\nresource of each node vl∈Vis capacitated. Different queries have\ndifferent QoS requirements.\nThe proactive data replication and placement problem for query\nevaluation of big data analytics in the two-tiered edge cloud network\nGis to place at most Kreplicas for each dataset Sn∈Sto cloudlets\nor data centers in advance such that the volume of placed datasets\ndemanded by admitted queries is maximized while meeting the\ndelay requirements of all admitted queries, subject to the computing\nresource capacities on edge cloudlets and data centers, where Kis\na given small integer with K≥1.\nHere, a query is admitted if the QoS requirement of the query\ncan be satisfied and the computing capacity of each cloudlet and\ndata center is not violated, the admitted queries will be evaluated\nby the cloudlets or data centers. Notice that, we here only consider\nthe proactive replication and placement for static data, as for the\ndynamic aspect of data, we set a threshold, which is a ratio of the\nvolume of new generated data to the volume of original data at a\ntime point. When the ratio of the volume of new generated data\nachieves the threshold, an update operation is made between the\noriginal data and its replicas to keep data consistent in the whole\nnetwork.\n3 AN APPROXIMATION ALGORITHM FOR\nPROACTIVE DATA REPLICATION AND\nPLACEMENT\nIn this section, we first give an overview of the proposed algorithm,\nwe then formulate an Integer Linear Programming (ILP) solution to\nthe proactive data replication and placement problem for query eval-\nuation of big data analytics, and devise an approximation algorithm\nwith an approximation ratio by the primal-dual dynamic-update\ntechnique, we finally analyze the correctness and time complexity\nof the approximation algorithm.\n\n[Página 4]\nICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and Bai, et al.\n3.1 Algorithm overview\nIn the proactive data replication and placement, each query can\ndemand several datasets each time, and a dataset can be demanded\nby multiple different queries at each time. It is NP-hard [ 5] to find an\noptimal solution for the problem. However, for a special case where\neach query demands only one dataset, there is an approximation\nalgorithm based on the primal-dual dynamic-update technique.\nTherefore for a general case where each query demands multiple\ndatasets, we can also get an approximation algorithm by invoking\nthe proposed approximation algorithm in the special case.\n3.2 Integer linear programming\nWe formulate the problem as an integer linear programming (ILP).\nWe first define a set of decision variables. Recall that, in the problem,\nthere are a set Qof queries and a collection Sof datasets, these\nqueries demand datasets for evaluation with different delay require-\nments, some replicas of the datasets should be created and placed at\nappropriate locations in G, such that the volume of dataset replicas\ndemanded by admitted queries is maximized while satisfying the de-\nlay requirements of the queries, subject to the capacity constraints\non data centers and edge cloudlets. As maintaining data consistency\nbetween an original dataset Sn∈Sand its replicas incurs cost, we\nassume that each dataset has at most Kreplicas in the edge cloud.\nTherefore, the proactive data replication and placement problem\nis equivalent to determining where the replicas of each dataset\nshould be proactively placed, and which queries should be assigned\nto which data centers or edge cloudlets for evaluation. Recall that\nV=CL∪DCis the set of edge cloudlets and data centers in G,\neach location node vl∈Vis either an edge cloudlet or a data center,\n1≤l≤|C L∪DC|. We thus use a binary decision variable xnl\nindicating whether a replica of dataset Snis placed at a location\nnodevlinG. Similarly, we use a binary variable πmlto indicate\nwhether a query qmis assigned to a location node vlto access the\nreplica of dataset Sn∈S( qm). Once a query qmis assigned to a\nlocation node vlwhere the replicas demanded by qmare placed,\nthe processed intermediate results will be transferred to the home\nlocation node hmofqm, via a shortest path whose transmission\ndelay is the minimum one.\nWe then formulate the objective of the proactive data replica-\ntion and placement problem, which is to maximize the volume of\ndatasets demanded by admitted queries that can be expressed by\nmaximizeÕ\nqm∈QÕ\nvl∈V|Sqm|·πml (1)\nsubject to the following constraints,\nÕ\nqm∈Q|Sqm|·rm·πml≤A(vl),∀vl∈V (2)\nπml−xqml≤0,∀qm∈Qand∀vl∈V (3)\n|Sqm|·[d(vl)+dt(pvl,hm)·αqm]·πml≤dqm,\n∀qm∈Q,∀vl∈V (4)Õ\nvl∈Vxnl≤K,∀Sqm∈S (5)\nπml∈{0,1}, (6)\nxnl∈{0,1}, (7)where Constraint (2) ensures that the computing resource of node\nvlallocated to evaluate queries that demand dataset Sqmis no\ngreater than the available computing resource of vl. Constraint (3)\nensures that only when the dataset Snrequired by query qmis\nplaced at node vl, query qmcan then be assigned to vl. Con-\nstraint (4) guarantees that the delay requirement dqmof each query\nqmis met. Constraint (5) ensures that each dataset has at most K\nreplicas in G.\n3.3 An approximation algorithm\nWe consider the above ILP for the proactive data replication and\nplacement problem as the Primal problem. We first calculate the\nDual of the Primal, we then devise an approximation algorithm\nfor the Dual problem. To be specific, we define four dual variables\nθl,yml,ηmlandµqm, then the dual of the Primal problem can be\nformulated as\nminÕ\nvl∈VA(vl)·θl+Õ\nqm∈QÕ\nvl∈Vdqm·ηml+Õ\nqm∈QK·µqm(8)\nsubject to the following constraints,\n|Sqm|·rm·θl+yml+|Sqm|·[d(vl)+dt(pvl,hm)·αqm]\n·ηnl≥|Sqm|,∀qm∈Qand∀vl∈V (9)Õ\nSqm∈Sµqm−Õ\nqm∈Qyml≥0,∀vl∈V (10)\nθl≥0, (11)\nyml≥0, (12)\nηml≥0, (13)\nµqm≥0. (14)\nThe primal complementary slackness conditions are as follows:\n•For each query qm∈Qand each node vl∈V, ifπml>0\nthen\n|Sqm|·rm·θl+yml+|Sqm|·[d(vl)+\ndt(pvl,hm)·αqm]·ηnl=|Sqm|\n(15)\n•For each node vl, ifxqml>0then\nÕ\nSqm∈Sµqm−Õ\nqm∈Qyml=0 (16)\nWe apply the complementary slackness approach to the approx-\nimation algorithm by defining relaxed complementary slack-\nness . The relaxed primal complementary slackness conditions are\nas follows:\n•For each query qm∈Qand each node vl∈V, ifπml>0\nthen\n|Sqm|≤|Sqm|·rm·θl+yml+|Sqm|·[d(vl)+\ndt(pvl,hm)·αqm]·ηnl≤β·|Sqm|(17)\n•For each node vl, ifxqml>0then\nÕ\nSqm∈Sµqm−Õ\nqm∈Qyml=0 (18)\n\n[Página 5]\nQoS-Aware Proactive Data Replication for Big Data Analytics ICPP 2019, August 5–8, 2019, Kyoto, Japan\nAlgorithm 1 An approximation algorithm Appro-S for the proactive\ndata placement problem where a query demands only one single\ndataset each time .\nInput: The set Qof queries, the set Sof datasets, the set Vof\nnodes in the two-tier edge cloud.\nOutput: The maximum volume of datasets demanded by admitted\nqueries.\n1:Q′←∅ // set of admitted queries ;\n2:V′←∅ //set of nodes where the replicas of the datasets are\nplaced;\n3:S′←∅ // set of placed replicas;\n4:θ←0,y←0,η←0,µ←0;\n5:N←0//the volume of datasets demanded by admitted queries;\n6:while eachµqm≤KorQ′,Qdo\n7: Uniformly increase µqmby 1 in a unit time, that is we create\none replica of dataset Sqmdemanded by qm;\n8: Increaseymluniformly, i.e., increase ymlby 1 in a unit time.\nAfter a while, Eq. 10 becomes tight, that isÍ\nSqm∈Sµqm−Í\nqm∈Qyml=0, i.e.,µqm−yml=0as each query qmonly\ndemands one single dataset Sqmeach time;\n9: Increase uniformly all θlandηnlsimultaneously, that is in a\nunit timeθlandηnlincrease 1. After a while Eq. 9 becomes\ntight, it means we have yml=|Sqm|−|Sqm|·rm·θl−|Sqm|·\n[d(vl)+dt(pvl,hm)·αqm]·ηnl;\n10: AddqmintoQ′, remove qmfrom Q, that is Q′←Q′∪{qm},\nandQ←Q\\{qm};\n11: Add SqmintoS′, i.e.,S′←S′∪{Sqm};\n12: AddvlintoV′, i.e., V′←V′∪{vl};\n13: Declare that query qmandSqmare assigned to node vl;\n14: N←N+|Sqm|;\n15:Return N.\nObservations: From the dual we can observe the meanings of\ndual variables: θlmeans the computing cost for evaluating qm\non nodevl;ymlrepresents the cost by assigning qmtovl;ηml\nis the cost for satisfying the delay requirement of qm, ifqmis\nassigned to node vl;µqmis the cost for creating a replica of a dataset\ndemanded by query qm. Based on the observations, for a special\ncase where a query demands only one single dataset, we can devise\nan approximation algorithm that calculates the placed datasets S′\nand admitted queries Q′, and satisfies the delay requirements of\nqueries. For simplicity, we refer algorithm 1 as Appro-S .\nNotice that approximation algorithm Appro-S works in a special\ncase where each query demands only one single dataset. In contrast,\nfor a general case where each query demands multiple datasets\neach time, we can still use algorithm Appro-S to derive another\napproximation algorithm, that is once a query demands a dataset\nwe invoke algorithm Appro-S . The specific algorithm is detailed in\nalgorithm 2, which is referred as Appro-G .\nTheorem 1. The approximation algorithm Appro-S gives an ap-\nproximation ratio arдmax(|Q|,|V|/K), the approximation algorithm\nAppro-G gives an approximation ratio arдmax(|Q|·|S| ,|V|·|S|/ K),\nwhere|Q|is the number of queries in the system, |V|is the numberAlgorithm 2 An approximation algorithm Appro-G for the proactive\ndata placement problem where a query demands multiple datasets\neach time .\nInput: The set Qof queries, the set Sof datasets, the set Vof\nnodes in the two-tier edge cloud.\nOutput: The maximum volume of datasets demanded by admitted\nqueries.\n1:N′←0// the total volume of datasets demanded by admitted\nqueries;\n2:N←0// the volume of datasets demanded by admitted queries\ninAppro-S ;\n3:foreach qm∈Qand each dataset Sn∈S(qm)do\n4: Invoke algorithm 1 ;\n5: N′←N′+N;\n6:Return N′.\nof cloudlets and data centers, |S|is the number of datasets, and Kis\nthe maximum number of replicas of each dataset.\nProof. Letθ,y,ηandµbe the returned dual-feasible solution.\nTo prove the approximation ratio, we need to compare the max-\nimum volume of datasets demanded by admitted queries of the\napproximated solution, which isÍ\nqm∈QÍ\nvl∈V|Sqm|, to the cost\nof the dual feasible solution ( θ,y,ηandµ), which isÍ\nvl∈VA(vl)·\nθl+Í\nqm∈QÍ\nvl∈Vdqm·ηml+Í\nqm∈QK·µqm.\nAs described in algorithm Appro-S , after some recurrence, we\nhave qmandvlwhich make µqm−yml=0, based on formula (10)\nwe have\n|Sqm|≤\u0000|Sqm|·rm·θl+µqm+\n|Sqm|·[d(vl)+dt(pvl,hm)·αqm]·ηnl\u0001, (19)\nthen we have\nÕ\nqm∈QÕ\nvl∈V|Sqm|≤\nÕ\nqm∈QÕ\nvl∈V|Sqm|·rm·θl+Õ\nqm∈QÕ\nvl∈Vµqm\n+Õ\nqm∈QÕ\nvl∈V|Sqm|·[d(vl)+dt(pvl,hm)·αqm]·ηnl\n=Õ\nvl∈VA(vl)·θl·Í\nqm∈Q|Sqm|·rm\nA(vl)+Õ\nqm∈Q\nÕ\nvl∈Vdqm·ηnl·|Sqm|·[d(vl)+dt(pvl,hm)·αqm]\ndqm\n+Õ\nqm∈QK·µqm·|V|\nK\n≤Õ\nvl∈VA(vl)·|Q|+Õ\nqm∈QÕ\nvl∈Vdqm·ηnl+\nÕ\nqm∈QK·µqm·|V|\nK(20)\nTherefore, the approximation ratio of algorithm Appro-S ismax{|Q|,|V|\nK}.\nAs each query can maximumly demand |S|datasets at each time,\n\n[Página 6]\nICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and Bai, et al.\nso the approximation ratio of algorithm Appro-G isarдmax(|Q|·\n|S|,|V|·|S|/ K). □\n4 PERFORMANCE EVALUATION\nIn this section, we evaluate the performance of the proposed algo-\nrithms Appro-S andAppro-G , and investigate the impact of impor-\ntant parameters on the algorithmic performance, by both simula-\ntions and a proof-of-concept in a real test-bed using real datasets.\n4.1 Experimental environment\nFor simulation, we consider a two-tier edge cloud consisting of 6\ndata centers, 24cloudlets and 2switches, there is a link between\neach pair of nodes (data centers, cloudlet, and switches) with a\nprobability of 0.2, generated by the GT-ITM tool [ 8]. The delay The\ncomputing capacities of each data center and cloudlet are randomly\ndrawn from a value interval [ 200,700] and [ 8,16] units (GHz) [ 26]\nrespectively. Each user produces several Gigabytes of data, we thus\nemulate the volume of the dataset generated by each user is in the\nrange of [1, 6] GB [ 26], and the amount of computing resource\nassigned to the processing of 1GB data is a value in the range of\n[0.75, 1.25] GHz [ 2,4]. The numbers of datasets and queries in\nthe system are randomly drawn in the range of [5, 20] and [10,\n100], respectively. The number of datasets required by the query is\nrandomly drawn from interval [1, 7]. Taking the transfer delay in\nreal cables into consideration, the QoS in terms of delay requirement\nof each query depends on the size of dataset demanded by the query,\nthe reason is to avoid some users who demand more dataset require\nthe same delay as users who demand few dataset. Unless otherwise\nspecified, we will adopt the default settings in our experiments.\nEach value in the figures is the mean of the results by applying\neach mentioned algorithm on 15different topologies of the two-tier\nedge cloud.\nWe evaluate the performance of the proposed algorithms against\ntwo benchmarks. The first benchmark adopts a greedy strategy, it\nselects a data center or cloudlet with largest available computing\nresource to place a replica of a dataset. If the delay requirement\ncannot be satisfied, it then selects a data center or a cloudlet with\nthe second largest available computing resource to place the replica.\nThis procedure continues until the query is admitted or there are\nalready Kreplicas of the dataset in the system. Another benchmark\nis from an existing work [ 10] that places Kreplicas for each dataset\nat data centers or cloudlets, if the delay requirement of the query\ncan be satisfied by evaluating the replica at the data center or the\ncloudlet. This procedure continues until the query is admitted or\nthere are already Kreplicas of the dataset in the system. It then\nmakes a graph partitioning with maximum volume of datasets\ndemanded by admitted queries. For simplicity, we refer to the two\nbenchmarks as Greedy-S andGraph-S for the special case where\neach query only demands a single dataset, while for the general\ncase where each query demands multiple datasets we refer to them\nasGreedy-G andGraph-G , respectively.\nIn addition, we also evaluate the proposed algorithms in a real\ntestbed. For which, we leased a number of virtual machines from\na cloud service provider DigitalOcean [ 9]. These virtual machines\nare located at geo-distributed locations. A two-tier edge cloud is\ndeployed by making use of both the leased virtual machines andlocal servers, based on which we evaluate the proposed algorithms\nagainst an existing work [ 13]. The benchmark work first calculates\nthe popularity of a node (cloudlet and data center) according to\nthe ratio of the number of dataset replicas on the node to the to-\ntal number of dataset replicas of all nodes. It then selects a node\nwith the highest popularity for each dataset, and places a replica\nof the dataset if the delay requirement of a query can be satisfied;\notherwise, if then selects another node with the second highest\npopularity to place the replica; this procedure continues until the\nquery is admitted or there are already Kreplicas of the dataset.\nAs we consider the special case where each query only demands\none single dataset and a general case where each query demands\nmultiple datasets, we thus refer to the benchmark as algorithm\nPopularity-S for the special case and Popularity-G for the gen-\neral case for simplicity, respectively.\n4.2 Performance evaluation of different\nalgorithms by simulations\n(a) The volume of datasets demanded by admitted\nqueries.\n(b) The system throughput.\nFigure 2: The performance of different algorithms Appro-S ,\nGreedy-S and Graph-S in terms of the volume of datasets\ndemanded by admitted queries and the system throughput,\nwhere each query demands a single dataset each time.\nWe first evaluate the proposed algorithm Appro-S against al-\ngorithms Greedy-S andGraph-S by varying the network size for\nthe special case where each query demands a single dataset each\ntime, in terms of the volume of datasets demanded by admitted\nqueries and the system throughput which is a ratio of the number\nof admitted queries to the total number of queries in the system. It\ncan be seen from Fig. 2(a) and Fig. 2(b) that the volume of datasets\n\n[Página 7]\nQoS-Aware Proactive Data Replication for Big Data Analytics ICPP 2019, August 5–8, 2019, Kyoto, Japan\ndemanded by admitted queries is over 4times than that by algo-\nrithm Greedy-S and 2 times than that by algorithm Graph-S , the\nsystem throughput by Appro-S is15% higher than that by algo-\nrithm Greedy-S and10%higher than that by Graph-S , respectively.\nThe rationale behind is that Appro-S places the replicas of datasets\nfrom an overall perspective, it jointly considers data replication\nand query assignment by smartly finding appropriate number and\nplacement locations of replicas for each dataset, it also fully utilizes\nthe available computing resource and the delay requirements of\nqueries when placing replicas. Whereas Greedy-S intends to place\na replica at a location with largest available computing resource\nwhile pays less attention to the delay requirement when choosing\nlocations to place replicas; similarly Graph-S places replicas at lo-\ncations under the constraints of location (data center or cloudlets)\ncapacities and delay requirements of queries, it then use graph\npartitioning with maximum volume of datasets demanded by ad-\nmitted queries, it thus can better user the resources of locations\ncompared with Greedy-S but not fully make use of the resources\nand admit as many queries as possible compared with Appro−S.\nNotice that when the network size is too high, e.g., 200, the system\nthroughput and volume of datasets demanded by admitted queries\nslightly decrease, this is because when the network size is too large,\ntransmission delay of some paths from evaluation locations to home\nlocations of queries has a higher probability to increase which may\nviolate the delay requirements of some queries, thereby reducing\nthe system throughput and the volume of datasets demanded by\nadmitted queries.\n(a) The impact on the volume of datasets de-\nmanded by admitted queries.\n(b) The impact on the system throughput.\nFigure 3: Impacts of the maximum number of datasets de-\nmanded by each query on the performance by Appro-G ,\nGreedy-G and Graph-G .We then evaluate the proposed algorithm Appro-G against al-\ngorithms Greedy-G andGraph-G by varying the network size for\nthe general case where each query demands multiple datasets each\ntime, in terms of the volume of datasets demanded by admitted\nqueries and the system throughput. It can be seen from Fig. 3(a)\nand Fig. 3(b) that the volume of datasets demanded by admitted\nqueries is 5and1.7times than those by algorithms Greedy-G and\nGraph-G , respectively. The system throughput by Appro-G is2.1\nand 1.5times than those by algorithms Greedy-G andGraph-G ,\nrespectively. The arguments are the same as that in Fig. 2, we do\nnot repeat here.\n(a) The volume of datasets demanded by admitted\nqueries.\n(b) The system throughput.\nFigure 4: The performance of different algorithms Appro-G ,\nGreedy-G and Graph-G in terms of the volume of datasets\ndemanded by admitted queries and the system throughput,\nwhere each query demands multiple datasets each time.\nImpact of the maximum number of datasets demanded by\neach query on the algorithmic performance: We now eval-\nuate the impact of the maximum number of datasets demanded\nby each query by varying the number from 1 to 6 for the general\ncase where each query demands multiple datasets each time, on\nthe performance of algorithms Appro-G ,Greedy-G andGraph-G ,\nin terms of the volume of datasets demanded by admitted queries\nand the system throughput. Notice that we did not evaluate the im-\npact of the maximum number of datasets demanded by each query\non the algorithmic performance for the special case, as a query\nonly demands a single dataset each time for the special case. For\nsimplicity, we refer to the maximum number of datasets demanded\nby each query as F. From Fig. 4(a) we can see that the system\nthroughput of three algorithms decreases with the growth of F,\n\n[Página 8]\nICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and Bai, et al.\nthe rationale is that a query could be admitted by the system, only\nwhen the delay for evaluating all datasets demanded by the query\nis no greater than the delay requirement of the query, that is to say,\nthe more number of datasets is demanded by a query, the harder\nthe QoS requirements of queries would be satisfied, so the harder\nthe query would be admitted. Although the system throughput\ndecreases with the growth of F, the volume of datasets demanded\nby admitted queries firstly increases with the growth of Ffrom 1 to\n5, and then slightly decreases after F=5. The reason is that before\nF=5, the total number of datasets demanded by admitted queries\nincreases as queries demand more datasets, however when Fis\n6, so many queries are rejected by the system due to the violated\ndelay requirements, the volume of datasets demanded by admitted\nqueries thus decreases. It can be clearly seen that the volume of\ndatasets demanded by admitted queries and system throughput by\nalgorithm Appro-G is higher than those by algorithms Greedy-G\nandGraph-G , the reasons are similar those of Figs. 2(a) and 2(b).\n(a) The impact of Kon the volume of datasets\ndemanded by admitted queries.\n(b) The impact of Kon the system throughput.\nFigure 5: Impacts of the maximum number Kof replicas of\neach dataset on the performance by Appro-G ,Greedy-G , and\nGraph-G in terms of the volume of datasets demanded by ad-\nmitted queries and the system throughput.\nImpacts of the maximum number Kof replicas on the algo-\nrithmic performance: We then evaluate the impact of the maxi-\nmum number Kof replicas of a dataset by varying Kfrom 1 to 7 for\nthe general case where each query demands multiple datasets each\ntime, on the performance of Appro-G ,Greedy-G andGraph-G in\nterms of the volume of datasets demanded by admitted queries and\nthe system throughput. From Fig. 5(a) and Fig. 5(b) we can see that\nthe volume of datasets demanded by admitted queries and systemthroughput are increasing with the growth of the value of K, the\nrationale is that as more replicas of each dataset are placed in the\nsystem, the delay requirements of queries are easier to be satis-\nfied, thus the system throughput and volume of datasets demanded\nby admitted queries increase. Obviously, the volume of datasets\ndemanded by admitted queries and system throughput achieved\nbyAppro-G are significantly higher than those by Greedy-G and\nGraph-G . The reason is that Appro-G places the replicas of datasets\nfrom the perspective of all the system to optimize the use of system\nresources, it jointly considers data replication and query assignment\nby smartly finding appropriate number and placement locations of\nreplicas for all datasets, it also fully utilizes the available comput-\ning resource and the delay requirements of queries when placing\nreplicas.\n4.3 Performance evaluation in a real test-bed\nWe now evaluate the performance of the proposed algorithms in\na real testbed that is composed of virtual machines in different\ngeo-locations that are provided by a cloud service provider, and a\ncontroller that executes the proposed algorithms.\nTestbed settings: We lease 20 virtual machines (VMs) from a cloud\nservice provider DigitalOcean [ 9], these VMs are located at loca-\ntions San Francisco, New York, Toronto, and Singapore. It must\nbe mentioned that since we focus on the replica placement in a\ntwo-tier edge cloud, we use 4 VMs to represent data centers, and 16\nVMs to represent cloudlets in the edge cloud network G, we also use\na local server as a controller to control the running of algorithms\nand 2 switches. Although the scale of each node representing a data\ncenter in this testbed may not be comparable to a large-scale data\ncenter, the implementation can be easily extended to a test-bed\nwith large-scale data centers. An illustration of the testbed is in\nFig. 6.\nFigure 6: The topology of the testbed with leased VMs.\nDatasets: The datasets used in the experiment are mobile appli-\ncation usage information from 3 million anonymous mobile users\nfor a period of three months. We divide the data into a number of\ndatasets according to the data creation time, and randomly distrib-\nute the datasets into the data centers and cloudlets of the testbed.\nBig data analytic queries are issued to find some evaluation results:\n\n[Página 9]\nQoS-Aware Proactive Data Replication for Big Data Analytics ICPP 2019, August 5–8, 2019, Kyoto, Japan\nsuch as the most popular applications, at what time the found ap-\nplications would be used, and the usage pattern of some mobile\napplications, etc.\nResults: We first evaluate the performance of the proposed algo-\nrithm Appro-S against a benchmark Popularity-S for the special\ncase where a query demands one single dataset each time by vary-\ning the maximum number of datasets demanded by each query.\nDue to page limits, we here put only a set of figures about the\nimpact of the maximum number Fof datasets demanded by each\nquery on the performance of algorithm Appro-S against benchmark\nPopularity-S illustrated in Fig. 7. From Figs. 7(a) and 7(b) we can\nsee that, algorithm Appro-S outperforms algorithm Popularity-S\nby delivering a higher volume of datasets demanded by admitted\nqueries and system throughput. We can see that the volume of\ndatasets demanded by admitted queries increases with the growth\nofFfrom Fig. 7(a), and the system throughput decreases as the\nvalue of Fincreases from Fig. 7(b), the arguments are similar with\nthose in Figs. 4(a) and 4(b).\n(a) The volume of datasets demanded by admitted\nqueries by Appro-S and Popularity-S on the\nreal testbed.\n(b) The system throughput by Appro-S and\nPopularity-S on the real testbed.\nFigure 7: The performance evaluation of the proposed algo-\nrithm Appro-S against benchmark Popularity-S on the real\ntestbed for the special case.\nWe then investigate the performance of the proposed algorithm\nAppro-G against benchmark Popularity-G for a general case where\neach query demands multiple datasets each time, by varying the\nnumber Kof dataset replicas. Comparably, because of page limits\nand pattern similarity, we here put only one set of figures about\nthe impact of the maximum number Kof replicas of each dataset\non the algorithmic performance. It can be seen from Figs. 8(a) and8(b) that Appro-G achieves a higher volume of datasets demanded\nby admitted queries and a higher system throughput than those\nbyPopularity-G . The rationale behind is that Appro-G places the\nreplicas of datasets from the perspective of the whole system by\nsmartly finding appropriate number and placement locations of\nreplicas for all datasets, it also fully utilizes the available computing\nresource and the delay requirements of queries when placing repli-\ncas. The volume of datasets demanded by admitted queries and the\nsystem throughput increase with the growth of K. This is because\nas more replicas of each dataset are placed in the system, the delay\nrequirements of queries are easier to be satisfied, thus the volume\nof datasets demanded by admitted queries and system throughput\nincrease.\n(a) The volume of datasets demanded by admitted\nqueries by Appro-G and Popularity-G on the\nreal testbed.\n(b) The system throughput by Appro-G and\nPopularity-G on the real testbed.\nFigure 8: The performance evaluation of the proposed algo-\nrithm Appro-G against benchmark Popularity-G on the real\ntestbed for the general case.\n5 RELATED WORK\nSeveral studies on data placement and query evaluation have been\nconducted in the past [ 1,6,7,17,18,20,22–26], and the others\nfocused on multi-layered network architecture and edge clouds for\ndealing with big data [ 11,14–16,27]. Most of these studies either\ndid not consider data replications of generated big data [ 1,11,14–\n16,20,26,27] or ignored the QoS requirement of users [ 1,6,17,23,\n26], or some of them only considered traffic cost while neglecting\nother costs [17].\nFor example, Baev et. al. [6] considered a problem of placing\nreplicated data in arbitrary networks to minimize the total storage\n\n[Página 10]\nICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and Bai, et al.\nand access cost. Golab et al. [10] studied a data placement problem\nto determine where to store the data and where to evaluate data-\nintensive tasks with a goal to minimize the data traffic cost. Kayyoor\net. al [17] addressed a problem of minimizing average query span,\nwhich is the number of servers involved in answering a query. They\nignored other costs and QoS requirements of users [ 6,17], and did\nnot consider data replications [ 10]. Agarwal et al. [1] proposed a\ndata placement mechanism Volley for geo-distributed cloud services\nto minimize the user-perceived latency. Xia et. al [26] considered a\nbig data management problem in distributed cloud environments to\nmaximize the system throughput while minimizing the operational\ncost of service providers. No data replications and QoS require-\nments of users are discussed in the two works [ 1,26]. Pu et al. [20]\npresented a system for low latency geo-distributed analytics, which\nused an heuristic to redistribute datasets among the data centers\nprior to queries’ arrivals, and placed the queries to reduce network\nbottlenecks during the query’s execution. Heintz et al. [12] studied\nthe tradeoff between the delay and errors of obtained results in\nstreaming analytics in an architecture consisting of a single center\nand multiple edge servers. In the study [ 20], authors did not con-\nsider data replications of datasets. The work in [ 16] considered a\nlayered architecture for the satellite-based data center infrastruc-\nture, and big data storage by leveraging such data centers. The\nauthors [ 14,15] studied a service provisioning problem in the edge\ncloud network, with an objective to maximize the profit of network\noperators. No data replication is considered in these works [ 14–16].\nIn contrast, we studied the proactive QoS-aware data replication\nand placement problem for query evaluation of big data analytics\nin a two-tier edge cloud environment, where the number of replicas\nof big datasets should be appropriately determined and the loca-\ntions to place the replicas should be strategically selected, with an\nobjective to maximize the volume of datasets demanded by admit-\nted queries such that the service providers can obtain maximum\nbenefits by offering a pay-as-you-go pricing approach to process\nthe datasets, while meeting the QoS requirements of queries and\nresource capacity constraints.\n6 CONCLUSIONS\nIn this paper, we studied query evaluation of big data analytics in\na two-tier edge cloud network through efficient and effective data\nreplication and placement with the aim to maximize the volume\nof datasets demanded by admitted queries, subject to computing\nresource capacities on data centers and edge cloudlets, while meet-\ning various delay requirements of user queries. To this end, we first\nformulated a novel QoS-aware data replication and placement prob-\nlem of query evaluation for big data analytics. We then proposed\nan efficient approximation algorithm with provable approximation\nratio for the problem. We finally evaluated the performance of the\nproposed algorithm through experimental simulations in a real\ntestbed based on real datasets. Simulation results demonstrate that\nthe proposed algorithm achieves several times higher volume of\ndatasets demanded by admitted queries and system throughput\nthan existing works.ACKNOWLEDGEMENT\nThe work of Qiufen Xia and Zichuan Xu is partially supported by the\nNational Natural Science Foundation of China (Grant No. 61802047,\n61802048, 61772113, 61872053), the fundamental research funds\nfor the central universities in China (Grant No. DUT19RC(4)035,\nDUT19RC(5)001, DUT19GJ204), and the “Xinghai Scholar” Program\nat Dalian University of Technology, China.\nREFERENCES\n[1]S. Agarwal, J. Dunagan, N. Jain, S. Saroiu, A. Wolman, and H. Bhogan. Volley: au-\ntomated data placement for geo-distributed cloud services. Proc. of NSDI , USENIX,\n2010.\n[2] https://aws.amazon.com/ec2/ , accessed in Jan. 2019.\n[3] https://aws.amazon.com/pricing/?nc1=h_ls , accessed in Jan. 2019.\n[4] https://aws.amazon.com/s3/ , accessed in Jan. 2019.\n[5]H. An, M. Singh, and O. Svensson. LP-based algorithms for capacitated facility\nlocation. Proc. of FOCS’14 , IEEE, 2014.\n[6]I, Baev, R. Rajaraman, and C. Swamy. Approximation algorithms for data placement\nproblems. SIAM J. on Computing , Vol.38, No.4, pp.1411-1429, 2008.\n[7]M. W. Convolbo, J. Chou, and S. Lu. DRASH: A data replication-aware scheduler\nin geo-distributed data centers. Proc. of CloudCom , IEEE, 2016.\n[8] K. Calvert, and E. Zegura. Gt-itm: georgia tech internetwork topology models.\n[9] Digital Ocean. https://www.digitalocean.com , accessed in Jan. 2019.\n[10] L. Golab, M. Hadjieleftheriou, H. Karloff, and B. Saha. Distributed data placement\nto minimize communication costs via graph partitioning. Proc. of SSDBM , ACM,\n2014.\n[11] S. Guo,D. Zeng, L. Gu, and J. Luo. When green energy meets cloud radio access\nnetwork: joint optimization towards brown energy minimization. Mobile Networks\nand Applications , Springer, pp.1-9, 2018.\n[12] B. Heintz, A. Chandra, and R. K. Sitaraman. Trading timeliness and accuracy in\ngeo-distributed streaming analytics Proc. of SoCC , ACM, 2016.\n[13] T. Hou, G. Feng, S. Qin, and W. Jiang. Proactive content caching by exploiting\ntransfer learning for mobile edge computing. International Journal of Communica-\ntion Systems , Vol, 31, No. 2, 2017.\n[14] H. Huang, and S. Guo Adaptive service provisioning for mobile edge cloud. ZTE\nCommunications , Vol. 15, No. 2, pp.1-9, 2017.\n[15] H. Huang, and S. Guo Service provisioning update scheme for mobile application\nusers in a cloudlet network Proc. of ICC , IEEE, 2017.\n[16] H. Huang, S. Guo, and K. Wang. Envisioned wireless big data storage for low-\nearth-orbit satellite-based cloud. IEEE Wireless Communications , Vol.25, No.1,\npp.26-31, 2018.\n[17] A. K. Kayyoor, A. Deshpande, and S. Khuller. Data placement and replica selection\nfor improving co-location in distributed environments. Computing Research\nRepository (CoRR), arXiv:1302.4168, 2012.\n[18] P. Li , S. Guo, T, Miyazaki, X. Liao, H. Jin, A. Y. Zomaya, and K. Wang. Traffic-\naware geo-distributed big data analytics with predictable job completion time.\nIEEE Trans. on Parallel and Distributed Systems , Vol.28, No.6, pp.1785-1796, 2017.\n[19] H. Li, H. Xu, and S. Nutanong. Bohr: similarity aware geo-distributed data\nanalytics. Open Access Media , USENIX, 2017.\n[20] Q. Pu, G. Ananthanarayanan, P. Bodik, S. Kandula, A. Akella, P. Bahl, and I. Stoica.\nLow latency analytics of geo-distributed data in the wide area. Proc. of SIGCOMM ,\nACM, 2015.\n[21] S. Rao, R. Ramakrishnan, A. Silberstein, M. Ovsiannikov, and D. Reeves. Sailfish:\na framework for large scale data processing. Proc. of SoCC , ACM, 2012.\n[22] W. Xiao, W. Bao, X. Zhu, and L. Liu. Cost-aware big data processing across geo-\ndistributed data centers. IEEE Trans. on Parallel and Distributed Systems , Vol.28,\nNo.11, pp.3114-3127, 2017.\n[23] Q. Xia, W. Liang, and Z. Xu. The operational cost minimization in distributed\nclouds via community-aware user data placements of social networks. Computer\nNetworks , Vol.112, pp.263-278, 2017.\n[24] Z. Xu and W. Liang. Operational cost minimization for distributed data centers\nthrough exploring electricity price diversity. Computer Networks , Vol. 83, pp.59-75,\nElsevier, 2015.\n[25] Z. Xu, W. Liang, and Q. Xia. Electricity cost minimization in distributed clouds by\nexploring heterogeneities of cloud resources and user demands. Proc. of ICPADS’15 ,\nIEEE, 2015.\n[26] Q. Xia, Z. Xu, W. Liang, and A. Zomaya. Collaboration- and fairness-aware big\ndata management in distributed clouds. IEEE Trans. on Parallel and Distributed\nSystems , Vol.27, No.7, pp.1941-1953, 2016.\n[27] S. Yu, M. Liu, W. Dou, X. Liu, and S. Zhou. Networking for big data: A survey.\nIEEE Communications Surveys & Tutorials , Vol. 19, No.1, pp. 531-549, 2017.",
      "metadata": {
        "filename": "xia2019.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\xia2019.pdf",
        "file_size": 1320308,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:41.963862",
        "content_length": 51683
      }
    },
    "2530e8bd-807c-4462-90f4-b2bd1edb9db4": {
      "id": "2530e8bd-807c-4462-90f4-b2bd1edb9db4",
      "content": "[Página 1]\nA survey on quality assurance techniques for big data applications\nPengcheng Zhang1, Xuewu Zhou1, Wenrui Li2, Jerry Gao3,4\n1College of Computer and Information, Hohai University, Nanjing, China  \n2School of Mathematics & Information Technology, Nanjing Xiaozhuang University, Nanjing, P.R. China\n3San Jose State University, San Jose, CA, \u0017Taiyuan University of Technology, China\nEmail Address: {pchzhang@hhu.edu.cn; jerry.gao@sjsu.edu} \nAbstractüüWith the rapid \u0003advance of big data and cloud \ncomputing, building high quality big data systems in different \napplication fields has gradually became a popular research \ntopic in academia and industry as well as government \nagencies. However, more quality problems lead to application \nerrors. Although the current research work has discussed how \nto ensure the quality of big data applications from several\naspects, there is no systematic discussion on how to ensure \nthe quality of large data applications. Therefore, a systematic \nstudy on big data application quality assurance is very \nnecessary and critical. This paper focuses on the survey of\nquality assurance techni ques of big data applications, and it \nintroduces big data properties and quality attributes. It mainly \ndiscusses the key approaches to ensure the quality of big data \napplications and they are testing, model-driven architecture \n(MDA), monitoring, fault tolerance, verification and also \nprediction techniques. In addition, this paper also discusses \nthe impact of big data characteristics on big data applications.\nIndex Terms üQuality Assurance, Big data, Big data \napplication, MDA, Testing, Verification, Fault tolerance, \nMonitoring, Prediction\nI. INTRODUCTION\nAccording to IDC report, the Big Data technology \nmarket will grow at \"a 27% compound annual growth rate \n(CAGR) to $32.4 billion through 2017” [1]. It shows that \nlarge-scale data computing and big data application services \nbecome more and more popular and have more influences on\npeople's daily lives. Big data applications are now widely \nused in many aspects, such as monitoring systems ,\nforecasting , and statistical reporting applications. However, \nbig data applications pose new challenges for Quality \nAssurance (QA) engineers due to the large big data \ncharacteristics (e.g., velocity of arriving data, volume of data)\n[2], [3]. For examples, because of the volume and timeliness \nof the data, verification the accuracy of big data prediction \nsystems is a difficult task, and it is a hard job to validate the\ncorrectness of a big data prediction system due to the large \nscale data size and the feature of timeliness. Therefore, \nquality assurance techniques for big data applications become \na key concern and research topic. Although there are many\npublished papers addressing data quality assurance in the past, \na few of them focused on the systematic study on the quality\nassurance techniques for big data applications. Towards this \nresearch direction, the main purpose of this paper is to \ninvestigate literature relevant for the quality assurance \ntechniques for big data applications so that it can provide a \ncomprehensive reference to the challenges of quality \nassurance approaches for big data applications.Unlike existing work, this paper provides the \ncontributions in the following aspects: \nx It discusses quality assurance approaches for big data \napplications, mainly from the six aspects: testing, \nmodel-driven architecture (MDA), monitoring, fault \ntolerance, verification and prediction for big data \napplications.\nx It also combines quality assurance techniques with big \ndata characteristics while it considers the quality \nassurance of big data applications, and it explores the \nbig data 4V properties of existing quality assurance \ntechniques for big data application.\nThe rest of the paper is organized as follows. Section II \nreviews related work. Section III introduces the different \ntypes of big data applications, and the quality assurance \napproaches. Section IV provides an overview and comparison \nof the existing approaches for quality assurance of big data \napplications, specifically in testing, model-driven architecture \n(MDA), monitoring, fault tolerance, verification and \nprediction. Section V discussed big data 4V properties and \nthe quality assurance of big data applications. Section VI \nconcludes the paper.\nII. RELATED SURVEY\nMany scholars have investigated the analysis of big data \nquality assurance. Let us cons ider the most interesting\napproaches from our point of view results obtained by them.\nBecause of the widespread use of big data applications, big \ndata quality assurance research has been tried by scholars. \nHowever, due to the huge volume of generated data, the fast\nvelocity of arriving data, and the large variety of \nheterogeneous data, the quality of data is far from perfect [4] .\nTherefore, big data quality assurance in big data service \napplications and academic research has become an important \nand critical issue due to 4V in big data applications. In \ngeneral, big data quality assurance refers to the study and \napplication of various assurance processes, methods, \nstandards, criteria, and systems to ensure the quality of big \ndata in terms of a set of quality parameters. \nGao et al. [2] provide informative discussions for big \ndata validation and quality assurance, including the essential \nconcepts, focuses, and validation process. Moreover, they \npresent a comparison among big data validation tools and \nseveral major players in industry are discussed. Also, they \ndiscuss the big data quality assurance issues, challenges and \nneeds. Furthermore, these discussions may bring great \nbenefits to the future of large data quality assurance. We have \ncollected some data quality parameters from the published \npapers, and we have presented in Table I. It includes quality \nparameter and the corresponding attribute meaning.\n2017 IEEE Third International Conference on Big Data Computing Service and Applications\n978-1-5090-6318-5/17 $31.00 © 2017 IEEE\nDOI 10.1109/BigDataService.2017.42313\n\n[Página 2]\nTable I. Quality Parameters for Big Data\nQuality \nParametersAttribute Meaning\nData accuracy It refers to the degree of closeness between \nthe observed result and the true value or \nvalue that is accepted as true. Therefore, we \ncan know this quality parameter is typically \nused to measure the collected sensor data by \ncomparing the multiple sources.\nData \ncorrectnessThis data quality parameter is much helpful \nto evaluate the correctness of big data sets in \nterm of data types, formats, and so on.\nData \nconsistencyData consistency refers to data collection \nmethods, schedules, and locations. It is much \nhelpful to evaluate the consistency of the big \ndata sets in abundant and different angles. \nData security This quality parameter could be helpful to \nevaluate the security of the given big data \nsets in different perspectives.\nThey also discussed big data quality verification tools \nand players. They compare with tools in terms of operating \nenvironment, supported data sources, data validation, and \ncurrent successful applications.\nNow, when big data quality a ssurance is discussed, the \nquality of big data applications is also concerned. Of course, \nthe quality factors of big data applications have gradually \nopened the mystery. Conventional quality factor such as \nperformance ,robustness ,security , etc., can be applicable \nonto big data applications. From the published papers in [5], \nTao et al. focus on big data system validation and quality \nassurance, and the paper includes informative discussions \nabout essential quality parameters ,primary focuses , and \nvalidation process . Compared with traditional software \ntesting, they discussed the big data application specific test \nprocess. The test procedure comprises the following steps\n[6] .\nStep 1: System function testing, including rich oracles, \nintelligent algorithms, learning capability, as well as \ndomain-specific functions; \nStep 2: System non-function testing, including system \nconsistency, security, robustness, and QoS (Quality of \nService);\nStep 3: System feature testing, checks usability, \nsystem evolution, visualization, and so on; \nStep 4: System timeliness testing, targets time related \nfeature testing, including co ntinuous testing, real-time \ntesting, life-time testing, and others.\nIn addition, they also discuss the quality factors of \ndifferent systems, including prediction systems, \nrecommendation systems and so on. Based on those, we \ncan draw out the quality factors of big data applications, \nand presented below:\nx Performance: This factor indicates the performance \nof the big data applications, such as availability, \nresponse time, etc.\nx Reliability: This factor helps to evaluate the \ndurability of the big data applications when the \nrequired function is performed within a specified time \nperiod under specified conditions. x Correctness: This is a quality factor used to assess \nthe correctness of big data applications.\nx Scalability: This quality factor means that big data \napplication should be able to support large data sets \nnow and in the future, and all components of big data \napplication can be extended to address the growing \ncomplexity of complex data sets. \nx Security: This factor helps to evaluate security of the \nbig data application in various perspectives at the \ndifferent levels. \nOur brief survey of the litera ture has demonstrated that \nalthough big data quality assurance has been studied, \nquality assurance techniques for big data applications has \nalso been studied.  However, th ere has been little scientific \nresearch aimed at understanding, defining, classifying and \ncommunicating quality assurance techniques of big data \napplications.  Consequently, there is no clear way to deal \nwith quality assurance of big data applications.  Therefore, \ndiscussing quality assurance techniques of big data \napplications is very necessary.\nIII. The SURVEY FRAMEWORK\nIn this section, we brie fly summarize the articles \nwhich we researched, and we describe the articles in \nseveral sections. We have studied new research results in \nthe last five years, discussed the application domain of big \ndata applications, and show whether the quality assurance \ntechnique is applied at design-time or run-time.  In addition, \nwe also discussed the big data applications functional \nproperties or non-functional properties (e.g., performance, \nreliability, availability, etc.), which are very important. \nWe all know that big data has its own properties, such \nas Volume ,Velocity ,Variety andVeracity. Volume means\nthe sheer size of the databases. Variety means the different \ntypes of data which can be stored within a single data \ncontainer, and everything from discrete numeric and string \nvalues to texts and images and to video films and audio \nrecordings. All of this can be stored and retrieved in \nvarious sequences or combinations [7] . Velocity means the \nspeed with which the objects can be retrieved and put \ntogether. The search algorithms are constructed in such a \nway that many multiple search paths are executed parallel \nto one another. In the end the results of the different \nsearches are joined together to form a consistent whole. We \ndiscussed the quality assurance of big data application, so \nbig data itself unique 4V properties  (i.e., volume, velocity, \nvariety, and veracity) are also focused. For quality \nassurance, quality assurance techniques are particularly \nimportant. Therefore, we are mainly from these aspects to \nanalyze the article. In the Table II, we have a simple \ninduction for the articles which we have researched.\nThrough the analysis of Table II, and related articles, \nlarge data applications are widely applicable to many areas, \nespecially in recent years. Quality assurance tec hnology of \nbig data applications are rapid developed. Consequently, \nwe can conclude that there are six main ways, including \ntesting, model-driven arch itecture (MDA), monitoring, \nfault tolerance, verification and prediction to typically \nensure the quality of big data applications. In the next part, \n314\n\n[Página 3]\nwe will conduct a detailed description and analysis of the approaches of thesesix aspects. \n Table II. Comparison of Quality Assura nce Approaches of Big Data Application\nYear Reference Application Domain Technique Design-time or \nRun-timeFunctional or \nNon-functional \nPropertiesProperties\n2014 [8] Application Testing Testing Design-time Performance 4V characteristics of \nbig data\n2015 [7] Big data bases Testing Design-time Validity \nConsistencyVolume, Variety\n2015 [9] Big Data and Cloud \nComputing to process \nlarge data.Testing Design-time NULL Volume, Variety, \nVelocity\n2015 [10] Data-intensive software \nsystemsMDA Design-time Reliability Safety \nEfficiencyVolume, Velocity\n2015 [11] Not Mentioned Specific \nApplication DomainMDA Design-time Performance Not Mentioned\n2012 [12] Enterprise Application \nPerformance \nManagementMonitoring Not Mentioned Performance Velocity\n2016 [22] Not Mentioned Specific \nApplication DomainMonitoring Design-time Reliability Volume, Velocity\n2015 [13] Distributed storage \nsystemsFault tolerance Design-time Performance Velocity\n2012 [14] Modern cloud computing \nsystems and so onPrediction Run-time Reliability Volume\n2015 [15] MapReduce 9HUL¿FDWLRQ Design-time Integrity, \nPerformanceNot Mentioned\nIV. THE SURVEY APPROACHES\nResearch shows that quality assurance techniques of \nbig data application are mainly these aspects – MDA,\nTesting, Verification, Fault tolerance, Monitoring, and \nPrediction.\nA. Model-Driven Architecture (MDA)\nMDA derives from the well-known idea of separating \nthe specification of system operations from the system. \nMDA provides a way (through related tools) to standardize \na platform-independent application, selects a specific \nimplementation platform for the application, and then \ntransforms application specifications to a specific \nimplementation platform. The three main goals of MDA \nare: to achieve portability, interoperability, and reusability \nthrough architectural separation [16], [17] . \nThe model driven approach is a well-known one and \nhas been widely exploited in many areas of software \nengineering. The goal of the MDA is to design applications \nin a model-driven approach which is more abstract than the \nimplementation of the techniques. For example, Alodib et \nal. [11] propose an extension to automate the integration of \nthe Hadoop platform. This is intended to break up each \nproblem into multiple su b-tasks using a simple \nprogramming model (MapReduce). After the analysis is \ncalculated, the results are submitted to the Score table\nlinked to the protocol service. The approach harnesses the \ncapability of Model-Driven Architecture (MDA) to \nautomate the creation, and integration of the architecture. Largely, due to existing models and QA techniques \nignore properties of data such as volumes, velocity and so \non. Casale et al. [10] present the research agenda of DICE. \nIt is a quality-aware MDE technology for big data cloud \napplications. And its goal is to developing a quality \nengineering tool chain offering simulation, verification, and \narchitectural optimization for Big Data applications. They \nalso present the main challenge in this approach. These \nchallenges are due to the fact that data operations and data \ncharacteristics cannot be fully described. \nEtani [18] describes database application model and \nits service for drug discovery introducing their proposed \nsoftware development process in MDA into their research \nprocess. The issue of veracity can be solved when pinpoint \ndata are selected from drug properties in big data analytics \nwith domain model. Our approach of software development \nprocess in MDA will be useful  for developing a big data \napplication and a new service by “veracity” of big data. \nAll in all, MDA provides a complete solution for \nintegration of big data applications at different lifecycle \nstages. It advocates the use of formalized system models as \nthe core of application integration. Consequently, we can \nknow MDA is an important method for quality assurance of \nbig data application. \nB. Testing\nApplication testing is a test of the entire product to \nverify whether the application meets the requirements \nspecification definition, and to identify inconsistent with \nthe requirements specification or contradictory places, so as \nto propose a more complete solution.\n315\n\n[Página 4]\nThe volume and variety of big data presents a \nparticular challenge to the testing of the big data \napplication. Therefore, Sneed et al. [7] consider that there \nis no other way but to automate the test process to test the \napplications. Due to the volume and variety of big data, \nthey think it is impossible to test big data application \nmanually, and testing big data need new processes and \nhigher degree of automation. People need automated tools \nto scan through the big data and check the validity and \nconsistency of the content.\nThe performance of big data applications is \nparticularly important.  Performance testing is a test method, \nwhich belongs to a typically non-functional testing. During \nperformance testing, the system tests by simulating various \nnormal and abnormal peak load conditions to reduce \noperational, upgrade, or patch deployment risk through \nperformance testing (such as information systems) to \nachieve a user response time load. But the existing \nperformance testing techniques are not suitable for the big \ndata application. Liu [8] proposes test technique for \nperformance testing. The technique provided testing goal \nanalysis, testing design, load design for big data \napplications. The characters for different big data \napplications could be supported to consider specific \nmultiple test data design method under this framework. \nThis performance technique is used to test some \napplications and demonstrated its effectiveness.\nJesús Morán et al. [9] propose a testing technique \nnamed MRFlow, which is based on data flow test criteria \nand oriented to transformations analysis between the input \nand the output, and it can test defects in MapReduce \nprograms. MapReduce is a programming model for parallel \ncomputing of large-scale data sets. MapReduce achieves \nreliability by distributing the large-scale operations on the \ndata set to each node on the network. Moreover, they tested \nthe technology, and the testing results are better.\nIn summary, the testing for big data quality assurance \nis very important, especially because of the big data \nproperties, testing is a very good quality assurance method.\nAnd we can learn that the testing will work in many cases \nwhich we meet.\nC. Verification\nApplications based on big data are now widely used, \nsuch as recommendation, prediction and decision systems. \nResearch shows that current research rarely explores how \nto effectively verify big data applications to ensure the \nquality of big data applications. Big data properties have \ntaken many challenges for big data applications. For \nexample, because of the volume of data and the timeliness \nof data, it is a very difficult task to verify the correctness of \nbig data applications.\nGao et al. [5] have discussed the validation methods \nfor big data application. They discussed and reviewed \nexisting research results in software testing methods that \nhave been used to validate various types of big data \napplications, including data mining programs, \nbioinformatics programs, and learning-based applications. \nAnd those methods include program-based software testing, \nclassification-based testing, metamorphic testing (MT), \nlearning-based testing, crowd-sourced testing, data model-\nbased testing, rule-based software testing and so on. Result integrity is one of the most important security \nissues in cloud-based big data computing scenarios. Wang\net al. [15] present MtMR, a Merkle tree-based verification \nmethod to ensure the high integrity of the MapReduce tasks. \nMtMR covers MapReduce in a hybrid cloud environment \nand performs two rounds of Merkle tree-based verification \nfor the pre-reduction and restoration phases. In each round \nof verification, MtMR samples a small portion of the \nreduced task input/output records on the private cloud, and \nthen performs Merkle tree-based verification of all task \ninput/output records. After analysis, they believe that \nMTMR can significantly improve the comprehensive, \nwhile can produce moderate performance overhead.\nTraditional software verification models and standards \nhave been unable to meet the quality requirements of big \ndata applications (because of the existence of big data\nproperties)[19] . Although many scholars have studied the \nquality verification problem of big data applications, but \nnot enough, the quality verification and assurance of big \ndata application challenges remain.\nD. Fault tolerance\nThe so-called fault tolerance refers to the existence of \nthe fault in the case of the system does not fail, still is able \nto work properly. Fault tolerance is rather a fault, not an \nerror. The use of fault toleran ce to ensure the quality of big \ndata applications can usually measure in terms of \napplication reliability, availability, and testability. \nDue to the trends towards Big Data, people want to \nprovide large storage systems, and those are accessible by \nmany servers. The shared storage has been the performance \nbottleneck and a single-point of failure. Lundberg et al. [13]\nsuggest that we introduce a cac he in the distributed storage \nsystem. The cache system must be fault tolerant so that no \ndata is lost when the hardwa re failure happened. According \nto the study, we know that the cache system is a way to \nimprove the performance of most systems.\nAs we all known, NoSQL databases are critical for \nsupporting big data applications, because they can handle a \nlarge number (i.e., volume) of highly variable (i.e., variety)\nuser-generated content while guaranteeing fault tolerance, \navailability, and scalability. However, all NoSQLs are \nsomewhat different from each other, even if they are \nconsidered to belong to the same database family. \nScavuzzo et al. [20] pose an efficient and fault tolerance \ndata migration method. In general, data migration should be \nable to tolerate faults or inte rruptions by recovering to the \nlast correct state, since NoSQL typically stores large \namounts of data, which means long-running migration tasks, \nbut on the contrary, higher risk of faults will happen. \nHowever, their approach tolerates a sudden fault of any \ncomponent involved in the data migration process without \nany data loss. Experiments show that the method used to \nperform the data migration is efficient, fault tolerance, and \nreally can improve the NoSQL technology interoperability.\nLikewise, there is an in creasing interest in the \nreliability and availability of big data cloud applications. \nAnd fault tolerance is a very effective means to solve the \nproblem of reliability and usability. Jhawar et al. [21] focus \non describing repetitive faults in typical cloud computing \napplications, analyzing the im pact of faults on user \napplications, and investigatin g fault tolerance solutions \ncorresponding to each type of failure. And they also talk \n316\n\n[Página 5]\nabout providing fault tolerance as a service to user \napplications as an effective means of addressing reliability \nand availability issues.\nFrom those researches, we can know that the fault \ntolerance is helpful to quality assurance of big data \napplications.\nE. Monitoring\nIn recent years, a large number of structured, semi-\nstructured and unstructured data is generated. These data\nare huge, complex, and rapidly changing. If the data cannot \nbe filtered, the real-time monitoring of information cannot \nbe achieved. Therefore, one of the biggest challenges with \nbig data applications is how to analyze and process huge \namounts of data in real time. And real-time monitoring is \nan effective way to ensure the quality of large data \napplications. Therefore, improving the real-time \nperformance of large data monitoring is very necessary. \nIn order to improve the real-time performance of big \ndata monitoring, Shi et al. [22] dish a dual cloud \narchitecture to take full advantage of cloud resources and \nnetwork bandwidth. They also propose a real-time \nmonitoring algorithm based on user evaluation in Hadoop \nplatform, which uses a combination of computing nodes. \nThe monitoring algorithm can eliminate nonsense data such \nas spam, malice evaluation, brush score, brush reputation \nand brush list by establishing user evaluation system. As a \nresult, it can significantly reduce the amount of data, but \nalso can greatly improve the operational efficiency. Thus, it \ncan ensure real-time monitoring information, reliability and \naccuracy.\nDistributed systems are typically big data applications. \nState monitoring has been widely used to detect critical \nevents and anomalies in distributed systems. Unfortunately,\nexisting distributed state monitoring methods are usually \ndesigned based on the condition that we assume always-\nonline distributed monitoring nodes and reliable inter-node \ncommunicate. Therefore, based on these methods, it often \nproduces misleading results, which leads to various \nproblems being introduced to rely on state monitoring \nresults to perform automatic management tasks of the user. \nMeng et al. [23] introduced a new state monitoring \napproach, and this method exposed and handled \ncommunication dynamics such as message delay and loss \nin Cloud monitoring environments. Firstly, by \nquantitatively estimating the accuracy of monitoring results, \nit can capture uncertainties which are introduced by \nmessaging dynamics. This characteristic is useful to \ndistinguish trustworthy monito ring results from one heavily \ndeviated from the truth. Secondly, they can configure the \nmonitoring algorithm, which minimizes monitoring errors.         \nAnd there are also other methods related to monitoring, \nwhich we can find in paper [24],  [25] . \nTherefore, we can know that big data brings some \ntrouble to big data applications, and using special \nmonitoring approaches can improve the quality assurance \nof big data applications and improve reliability,\nperformance and other non-functional properties. \nF. Prediction\nBig data applications will have a variety of failures. If\nwe can predict the upcoming failure; it will greatly improve the quality of large data applications. Therefore, the \nprediction technique for big data quality assurance is an \neffective way.\nYang et al. [26] design a general framework named \nHdoctor for hard drive failure prediction. Hdoctor\ndemonstrated a number of innovations, and building time-\ndependent features to characterize Self-monitoring, \nAnalysis and Reporting Technology (SMART) value \ntransitions during disk failures is the important one. \nMeanwhile, Hdoctor automatically collects/labels samples \nand updates model, and works well for all kinds of disk \nfailure prediction in their intelligent data center.\nExisting production applicatio ns are short of real-time \nperformance status of production process active perception, \nresulting in the production abnormal conditions processed \nlag, leading to the frequency problems of deviations in \nproduction tasks execution and planning. To address this \nproblem, Zhang et al. [27] advance they should extend an \nadvanced identification technology to the manufacturing \nfield to acquire the real-time performance data. Based on \nthe sensed real-time manufacturing data, they present a \nprediction method which applies the Dynamic Bayesian \nNetworks (DBN) theory and methods. Achieving the \nprediction of the performance status of production system \nand potential anomalies is the goal of the method, and it \ncan provide the important and abundant prediction \ninformation. All in all, Dynamic Bayesian Networks theory \nand method is used to make the mathematical modeling of \nperformance prediction for production system based on \nmanufacturing big data. \nIn modern cloud computing systems, thousands of \ncloud servers are interconnected through multiple layers of \nnetworks. Faults are common in such large and complex \nsystems. In order to predict the failure, we should monitor \nthe system implementation process, and collect health-\nrelated runtime performance data. Guan et al. [14] present\nan unsupervised failure detection method based on an \nensemble of Bayesian models. It characterizes the normal \nsystem execution state and detects anomalous behavior. \nThe tagged data is available after the system administrator \nverifies the exception. Then, supervised learning based on \ndecision tree classifier is used to predict future failures. \nThere are other predictive methods in paper [28], [29] and \nother papers which we do not know.\nDealing with faults which have been happened may be \nvery difficult, and fault prediction is particularly important. \nTherefore, it is necessary to discuss the fault prediction \nmethod of big data applications. Therefore, I think \nprediction will play an important role in quality assurance \nof big data applications.\nV Discussion\nBy reading a lot of literature, we can summarize a \nnumber of approaches to ensure the quality of big data \napplications, including MDA, Testing, Verification, Fault \ntolerance, Monitoring, and Prediction. In TABLE III, we \nfurther summarize functional or non-functional properties \ninvolved in these six aspects, as well as the big data \nproperties.\nAs we can see from the TABLE III , in the process of \nconsidering big data application quality assurance, \n317\n\n[Página 6]\nperformance of this non-functional property is basically the \nmain consideration. However, the big data properties have \na great impact on quality assurance of big data application. \nAs shown in TABLE III, we know one of big data \nproperties which are common for mostly approaches is \nvariety. However, the variety often is solved by NOSQL \nwhich can handle structured data, semi-structured data, and \nunstructured data of big data. NoSQL databases are key to \nsupporting Big Data applications, since they enable \nhandling large quantities (i.e., volume) of highly-variable \n(i.e., variety), user-generated contents while guaranteeing \nfault tolerance, availability (i.e., velocity) and scalability \n[20] . \nTABLE III. Approaches, Functional or Non-functional\nProperties, 4V properties of big data application\nApproaches Functional or \nNon-functional \nProperties4V properties\nModel-Driven \nArchitecture \n(MDA)Performance, \nScalabilityVeracity, Volume, \nVariety\nTesting Availability, \nPerformanceVariety, Velocity\nVerification Performance,\nReliabilityVolume, Variety\nFault tolerance Performance, \nScalabilityVariety, Volume\nMonitoring Performance, real-\ntimeVariety, Velocity\nPrediction Performance,\nDependabilityVariety, Veracity\nNot only that, according to the research, we can get \nbig data properties of the challenges, and how to use the \nnovel technique to solve the problems. Consequently, we \ncan summarize the big data properties, challenges, as well \nas the techniques for those challenges in following form.\nTABLE IV. Properties, Challenges and Techniques \nProperties Challenge Novel Technique\nVolume Storage/Scale Distributed File \nSystems\nVelocity Fast  Processing Parallel Programming\nVariety Heterogeneity NOSQL Databases\nWhen we consider the big data properties and quality \nrequirements, it is anticipated to aid requirement analysts in \nthe specification of quality requirements while keeping big \ndata properties in mind.\nThere are some major issues and challenges in big \ndata application quality assurance. Here are typical ones.\nIssue #1 - Lack of awareness and good understanding of \nquality assurance techniques for big data applications. \nWith the fast development of big data technologies \nand analytics approaches, more big data applications and \nservice systems are developed to be used in many areas of \nour daily life. Consequently the increasing deployment of \nbig data applications and services dishes quality assurance \nconcerns. Then, most people will find ways to solve a \nspecific problem until the big data application problems \nhappened. Hence, according to real world practitioners, \nthere is a clear demand on understanding the quality assurance of big data application. This brings the first \ndemand of big data application quality assurance. \nNeed #1 - Full understanding the quality assurance\ntechniques to solve the special functions and needs of big \ndata applications and services.\nIssue #2 - Lack of approaches to solve quality assurance\nissues in different big data applications.\nFor specific big data applications, there are specific ways to \nsolve the quality assurance pr oblem. However, there is \ncurrently no strictly defined approach to solve the problem.\nTherefore, it brings the second demand of big data\napplication quality assurance. For example, testing oracle\nmay be a big issue for big data applications due to the 4V \nproperties. \nNeed #2 - Define and develop well-defined big data\napplication quality assurance standards, and define some \napproaches to solve quality assurance issues. Those \napproaches can be extracted from the six aspects of this \npaper.\nIssue #3 – Lack of solutions to coordinate big data \nproperties with quality assurance techniques. \nToday, big data applications, such as social media, generate\nmore data in a short period of time than was previously \navailable requiring new techniques for quality assurance. \nExisting techniques have no adequate scalability and facing \nchallenges because of big data properties such as Volume, \nVelocity, Variety and Veracity. Therefore, it brings the last \ndemand of big data application quality assurance. \nNeed #3 - Consider functional or non-functional properties \nwith big data properties together to ensure the quality of big \ndata applications.\nIn addition, we have general approaches to deal with \nbig data properties: \n- Distributed File Systems for Volume;\n- Parallel Programming for Velocity;\n- NOSQL Databases for Variety (structured, semi-\nstructured and unstructured data).\nVI. Conclusion and Future Work\nThis paper focuses on the quality assurance of big data \napplication. It mainly discusses the state-of-art approaches \nto ensure the quality of big data applications. The surveyed\napproaches are mainly testing, model-driven architecture \n(MDA), monitoring, fault tolerance, verification and \nprediction. In addition, this paper discusses the impact of \nbig data characteristics on big data applications. \nAlthough researchers have proposed some quality \nassurance techniques for big data applications, the \nchallenge of big data applicati ons still exists. Consequently, \nhow to effectively ensure the quality of big data \napplications is still a hot res earch issue. In the follow-up \nstudy, we should conduct more research based on big data \n4V properties. We can try to deal with big data 4V\nproblems and we can also consider functional or non-\nfunctional properties of big data applications with big data \nproperties together to ensure the quality of big data \napplications.\n318\n\n[Página 7]\nAcknowledgement\nThis work is supported by the National Natural Science \nFoundation of China (No. 61572171) and the Fundamental \nResearch Funds for the Central Universities (No.\nB15020191).  \nREFERENCES\n[1] Big Data Technology and Services at $32.4 Billion in \n2017 - IDC[J]. San/lan, 2013.\n[2] Gao J, Xie C, Tao C. Big Data Validation and Quality \nAssurance -- Issuses, Challenges, and Needs[C]// \nIEEE, IEEE International Symposium on Service-\nOriented System Engineering. IEEE, 2016: 433-441.\n[ 3 ]  G a r g  N ,  S i n g l a  S ,  J a n g r a  S .  C h a l l e n g e s  a n d  \nTechniques for Testing of Big Data[J]. Procedia \nComputer Science, 2016, 85: 940-948.\n[4] Yesudas M, Menon S G, Nair S K. High-Volume \nPerformance Test Framework using Big Data[C]// \nInternational Workshop on Large-Scale Testing. \nACM, 2015: 13-16. \n[5] Tao C, Gao J. Quality Assurance for Big Data \nApplications– Issues, Challenges, and Needs[C]// The \nTwenty-Eighth International Conference on Software \nEngineering and Knowledge Engineering. 2016.\n[6] Guerriero M, Tajfar S, Tamburri D A, et al. Towards \na model-driven design tool for big data \narchitectures[C]// The, International Workshop. 2016: \n37-43.\n[7] Sneed H M, Erdoes K. Testing big data (Assuring the \nquality of large databases) [C]// IEEE Eighth \nInternational Conference on Software Testing, \nVerification and Validation Workshops. IEEE, 2015: \n1-6.\n[8] Liu Z. Research of performance test technology for \nbig data applications[C]// IEEE International \nConference on Information and Automation. IEEE, \n2014: 53-58.\n[9] Jesús Morán, Riva C D L, Tuya J. Testing data \ntransformations in MapReduce programs[C]// The, \nInternational Workshop. 2015: 20-25.\n[10] Casale G, Ardagna D, Artac M, et al. DICE: Quality-\nDriven Development of Data-Intensive Cloud \nApplications[C]// IEEE/ACM, International Work- \nshop on Modeling in Software Engineering. ACM, \n2015: 78-83.\n[11] Alodib M, Malik Z. A Big Data approach to enhance \nthe integration of Access Control Policies for Web \nservices[C]// IEEE/ACIS, International Conference \non Computer and Information Science. IEEE, 2015: \n41-46.\n[12] Rabl T, Mez-Villamor S, Sadoghi M, et al. Solving \nbig data challenges for enterprise application \nperformance management[J]. Proceedings of the Vldb \nEndowment, 2012, 5(12): 1724-1735.\n[13] Lundberg L, Grahn H, Ilie D, et al. Cache Support in \na High Performance Fault-Tolerant Distributed \nStorage System for Cloud and Big Data[C]// Parallel \nand Distributed Processing Symposium Workshop. \nIEEE, 2015: 537-546.\n[ 1 4 ]  G u a n  Q ,  Z h a n g  Z ,  F u  S .  E n s e m b l e  o f  B a y e s i a n  \nPredictors and Decision Trees for Proactive Failure Management in Cloud Computing Systems[J]. \nJournal of Communications, 2012, 7(1): 52-61.\n[15] Wang Y, Shen Y, Wang H, et al. MtMR: Ensuring \nMapReduce Computation Integrity with Merkle Tree-\nbased Verifications[J]. 2016: 1-1.\n[16] Xuan P, Zheng Y, Sarupria S, et al. SciFlow: A \nDataflow-Driven Model Architecture for Scientific \nComputing using Hadoop[C]// IEEE Big Data 2013 \nWorkshops: Big Data and Science - Infrastructure \nand Services. IEEE, 2013: 36-44.\n[17] Klein J, Buglak R, Blockow D, et al. A reference \narchitecture for big data systems in the national \nsecurity domain[C]// International Workshop on Big \nData Software Engineering. 2016: 51-57.\n[18] Etani N. Database application model and its service \nfor drug discovery in Model-driven architecture[J]. \nJournal of Big Data, 2015, 2(1): 1-17.\n[19] Hussain M, Almourad M B, Mathew S S. Collect, \nScope, and Verify Big Data -- A Framework for \nInstitution Accreditation[C]// International Conferen-\nce on Advanced Information NETWORKING and \nApplications Workshops. IEEE, 2016: 187-192.\n[20] Scavuzzo M, Tamburri D A, Nitto E D. Providing big \ndata applications with fault-tolerant data migration \nacross heterogeneous NoSQL databases[C]// \nInternational Workshop on Big Data Software \nEngineering. 2016: 26-32.\n[21] Jhawar R, Piuri V. Chapter 7 - Fault Tolerance and \nResilience in Cloud Computing Environments[M]// \nComputer and Information Security Handbook. \nElsevier Inc. 2013: 125-141.\n[22] Shi G, Wang H. Research on Big Data Real-Time \nPublic Opinion Monitoring under the Double Cloud \nArchitecture[C]// IEEE Second International \nConference on Multimedia Big Data. IEEE Computer \nSociety, 2016: 416-419.\n[23] Meng S, Iyengar A K, Rouvellou I M, et al. Reliable \nState Monitoring in Cloud Datacenters[C]// IEEE, \nInternational Conference on Cloud Computing. IEEE, \n2012: 951-958.\n[24] Iuhasz G, Dragan I. An Overview of Monitoring \nTools for Big Data and Cloud Applications[C]// \nInternational Symposium on Symbolic and Numeric \nAlgorithms for Scientific Computing. 2015: 363-366.\n[25] Zareian S, Fokaefs M, Khazaei H, et al. A big data \nframework for cloud monitoring[C]// The, \nInternational Workshop. 2016: 58-64.\n[26] Yang W, Hu D, Liu Y, et al. Hard Drive Failure \nPrediction Using Big Data[C]// Reliable Distributed \nSystems Workshop. IEEE, 2015: 13-18.\n[27] Zhang Y, Liu S, Si S, et al. Production system \nperformance prediction model based on manufactu- \nring big data[C]// IEEE, International Conference on \nNetworking, Sensing and Control. IEEE, 2015.\n[28] Xu J, Li H. The Failure Prediction of Cluster Systems \nBased on System Logs[M]// Knowledge Science, \nEngineering and Management. Springer Berlin \nHeidelberg, 2013:526-537.\n[29] D ai D, Chen Y, Kimpe D, et al. Provenance-based \nobject storage prediction scheme for scientific big \ndata applications[C]// IEEE International Conference \non Big Data. IEEE, 2014: 271-280.\n319",
      "metadata": {
        "filename": "zhang2017.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\zhang2017.pdf",
        "file_size": 315421,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:42.157896",
        "content_length": 40902
      }
    },
    "5255389e-58cb-4ab8-ae65-8c6c390357b3": {
      "id": "5255389e-58cb-4ab8-ae65-8c6c390357b3",
      "content": "[Página 1]\nQuantCloud: A Software with Automated Parallel \nPython for Quantitative Finance Applications\nPeng Zhang\nApplied Mathematics Department\nStony Brook University\nNY 11794, United States\nPeng.Zhang@Stonybrook.eduYuxiang Gao\nMidea Emerging Technology Center\nCA 95134 , United States\nYuxiang1.Gao@Midea.comXiang Shi\nAdvanced Risk & Portfolio Management\n(ARPM) ,\nNY 10023 United States\nXiang.Shi@arpm.co\nAbstract —Quantitative Finance is a field that replies on data \nanalysis and big data enabling software to discover market signals. \nIn this, a decisive factor is the speed that concerns execution speed \nand software development speed. So, an efficient software plays a\nkey role in helping trading firms. Inspired by this, we present a \nnovel software: QuantCloud to integrate a parallel Python system \nwith a C++-coded Big Data system. C++ is used to implement this \nbig data system and Python is used to code the user methods. The \nautomated parallel execution of Python codes is built upon a \ncoprocess-based parallel strategy. We test our software using two \npopular algorithms: moving -window and autoregressive moving-\naverage (ARMA). We conduct an extensive comparative study \nbetween Intel Xeon E5 and Xeon Phi processors. The results show\nthat our method achieved a nearly linear speedup for executing \nPython codes in parallel , prefect for today’s multicore processor s.\nKeywords —Quantitative Finance Software, Parallel Python, \nBig Data, Cloud computing.\nI. I NTRODUCTION\nQuantitative Finance is a field that extends mathematical \nmodels to the finance problem thus it is also known as \ncomputational finance. In this field, the revolution of \ncomputational technologies has be en shaping the best practice \nand future of quantitative finance. In the high-frequency trading\nage, a program trading system is developed to use powerful \ncomputers to transact a large number of orders as quickly as \npossible. The whole order and withdraw process may happen in \na microsecond level or even less [1] . However, as the age of big \ndata arrives, the science, social and economic including \nquantitative finance have been undergoing a fierce yet great \nrevolution. In the past, the high-frequency traders had been \npursuing a high speed between exchanges for facilitating the \nbuying and selling of shares, curr encies and other assets [2, 3].\nAt present, the finance firms want to compete on strategies as \nthe race for transacting speed among high-frequency traders hit \npeak [4, 5]. Traders are building more complicated data analysis \nmodels to derive deeper profitable signals out of the big finance \ndatasets [6]. Thus, there is a need to construct a novel software \nthat allows fast-developing and fast-testing strategies. This need \ninspires this work.\nSpeed is always a decisive factor in maintaining a finance \nfirm’s competitive advantage but its meaning is extend ing. To \nhave a transaction speed without prediction is of no practical \nvalue. Currently, the speed concerns with not only the executionspeed of a big data analysis model but also the software \ndevelopment speed of a complicated mathematical model. In the \npractice of this field, Python is the most preferred high-level \nprogramming language as it requires fewer lines of code and \nalso has wide availability of statistics libraries in timeseries \nanalysis. On the other hand, C++ is the most ideal language to \nimplement the big data infrastructure system that is able to \nhandle massive amounts of market data as it provides high speed \nof execution. Considering these facts, we develop an integration \nsystem that combines a C++-based big data infrastructure and \nan automated parallel Python system. As being applied to \nquantitative finance, this system handles the timeseries market \ninformation by this big data infrastructure and meanwhile \nperforms timeseries analysis models that are coded in Python.\nRevolutionary technological advances have been stimulating \nevolutionary industrial adaptation. In this trend, the quantitative \nfinance is a grand pioneer for adapting advanced technologies \nsuch as novel multicore processor architectures. Of these, Intel \nXeon Phi processor, codenamed as Knights Landing (KNL), is \na representative of modern multi-core processors. Different \nfrom its former processor families, Intel’s KNL has a higher \ndensity of processor cores and thus it is optimized more for \nhighly parallel workloads. However, fully exploiting the power \nof such kind of high-density multi-core processor is by no means \na trivial challenge. This needs an effective yet agile way to bring \na degree of parallelism to the execution of programs. To this end, \nwe develop a coprocess-based parallel execution for Python.\nContribution synopsis: the main contribution of this work is \nthe design of a software suite, QuantCloud that combines a big \ndata infrastructure with an automated parallel Python system. \nWe also conducted an extensive application-level comparative \nstudy using commodity hardware. The results show the efficacy \nof this software and characterize all essential aspects of \nperformance such as the wallclock time, the speedup and \nparallel efficiency for the codes in Python, the tick-level latency \nfor commonly-used QF applications on real-world market data.\nII. B ACKGROUND AND MOTIVATION\nA. Big Data in Quantittative Finance\nBig data is becoming a critical issue in finance, particularly \nthe quantitative finance with multiple applications, wider usage, \ngiven advances in enabling technologies [7]. Big data in finance \nhas covered all principle interests of Big Data such as the data \n3882018 IEEE International Conference on Software Quality, Reliability and Security\n978-1-5386-7757-5/18/$31.00 ©2018 IEEE\nDOI 10.1109/QRS.2018.00052\n\n[Página 2]\nvolume, velocity and variety [8-10]. Data volume of market \ninformation has been ever increasing at a tremendous rate. For \ninstance, the total shares changed hand is tenfold of 20 years ago \nand the total number of transactio ns is increased by 50 times, \nwith this number being more than 120 times during the financial \ncrisis [8]. The prevalence of high-frequency trades (HFTs) has \nspurred up growth of high-speed data in trading activities. For \nexample, about 70% of the U.S. equity trades are computer \ndriven [10].\nB. Python for Quantitative Finance\nIn daily practice of most trading systems, Python is the most \npreferred language. For example, Quartz is Bank of America \nMerrill Lynch’s integrated trading, position management, \npricing and risk management platform and its entire tech stack \nuses Python. Athena is J.P. Morgan’s next -generation pricing, \nrisk management, analysis and tr ade management platform, and \nis a Python-based rapid develo pment environment. Meanwhile, \na compiled language C++ is used  for the high-performance core \nof this system, while Python is used for building logic and apps. \nPython is becoming more and more popular for being easier to \nuse and faster to program than traditional languages including \nthe C++ programming language. So far, there’s been a huge \nspike in demand for Python in the investment banks including \nBank of America and J.P. Morgan that are using Python to \nreplace historic legacy systems built in Java/C++. From a \npractical perspective in quantitative finance, we choose Python \nas a language to program timeseries analysis algorithms and \nmodels on the finance big data.\nC. Python Limitation\nCPython is the default and most widely-used interpreter for \nthe Python programming language. It is written in C and offers \nrich extensions with several languages including C. In CPython, \nglobal interpreter lock, or GIL, is a mutex lock that prevents \nconcurrent executions of multiple native threads within one \nprocess [11]. In other words, Python is implemented in such a \nway that only one thread can be accessing the interpreter at a \ntime. The exceptions are few: for example, while a thread is \nwaiting for I/O, the interpreter is released so other threads can \nrun [12]. In this literature, the GIL becomes a key limitation in \nmultithreading with Python. As usual, multithreading actually \nperforms worse than serial code [13, 14]. However, the GIL is \nnecessary because CPython’s memory management is not \nthread-safe. A solution to this issue is to use multiple full \nprocesses instead of threads [15], where each process uses its \nown GIL. To overcome this limitation, we present a coprocess-\nbased approach and bring parallel performance to Python code. \nSophisticated parallelism and wo rkflow management is hidden \nin QuantCloud system.\nD. High-Density Multicore Processors\nTechnology has been shaping financial markets so much, \nthat the traders are competing for the fastest equipment rather \nthan the transaction itself. The heart of modern quantitative \nfinance is to reduce the execution time of more complicated \nmodels by using more advanced machines. In this battlefield for \nspeed, the processor plays a key role. Simply, a more powerful \nprocessor makes the analytics algorithm execute quicker so there is more room to crunch more data and harness more \ncomplex models while without sacrificing time. Faster \ncomputing means more doing. This is of practical interest to \ntime-critical applications in the field. I n  t o d a y ’ s  p r o c e s s o r  \nmarkets, Intel’s 2nd-generation Xeon Phi processor, codename \nas Knights Landing (KNL) [16, 17], is a novel high-density \nmulticore processor and it has 64~72 cores per processor, \noptimized for a highly-parallel application.\nIII. S YSTEM DESIGN\nOur system incorporates two parts: a big data infrastructure \nsystem and its integration interface with Python. The overview \nof this integrated system is shown in Figure 1. The design of this \nbig data infrastructure system is extension of our previous work \n[18]. In this system, an approp riate embedded Python interface \nis built for effortless integration with this big data infrastructure. \nData communication between the main C++ program and \nembedded Python scripts is through a shared memory system. \nThe Python script is used as a high-level language to program \nthe sequential execution of an algorithm. The coprocesses that \nexecute a code in Python seem to run sequentially instead of \nparallel and they are transparent at the user-application level. \nThere have been no code changes in the Python scripts so this \n“as-is” embedding approach supplies a simple yet efficient \nmethod to use Python embedded in a C++ program for a \ncomplex big data application.\nFigure 1. Overview of the integrated Big Data and Parallel Python \narchitecture in the QuantCloud suite\nA. Big Data Softwar e Infrastructure\nThis Big Data infrastructure includes three components: \nUser, Client and Server, in Figure 1. User part is an XML-script \nportal that is able to receive an application-user job and return \nCSV-format results to end users. Client part is a platform that \nexecutes the computing jobs. It parses a user job, queries the \nrequired data from the Server and conducts the job. The Server \npart is a platform that provides data-centric services. This is a \ndistributed application architecture and adopts a sever-client \nmodel that partitions jobs and data between the Client and the \n389\n\n[Página 3]\nServer. In this system, Client and Server are the provider of \ncomputing and data services and User is a service requester. To \ncommunicate, Internet communication is between user and \nclient, and Intranet is between server and its clients.\nThis system enables Cloud platforms as providers for \nfinance big data analytics. Essence of cloud computing is \nEverything-as-a-Service. In this field, Server may reside on a \nStorage-as-a-Service provider operating on a cost-per- byte-\nstored and cost-per-byte-transferred basis. Client may use an \nInfrastructure-as-a-Service provider provisioning scalable \ncomputing resources and operating on a pay-per-use basis. \nMeanwhile, the finance big data analytics algorithms and \nmodels could be supplied as Software-as-a-Service in the Client \nand defined as pay-per-use software. Simply, User just runs a \nlight-weight kernel thus it is able to be operated on ultra-portable \ndevices. This design helps the big data analytics research \nproducts to quickly enter the market with the advent of cloud \ncomputing technologies.\nThe Server part manages the historical market information \nsuch as stock transactions. The market information is organized \nas multiple timeseries and indexed by its date and stock symbol. \nHere, data is first compressed then hashed before stored on the \nstorage. Data compression is for saving space and hashing for \nsecurity reason. In addition to data storage, Server responds to \ntimeseries queries [18]. Before querying data, a Client needs to \nregister a Server and establishes a link between data provider \n(Server) and data requester (Client).\nThe Client part responds to users’ requesters and processes \nuser jobs. Specifically, a user job describes: (1) the requested \ndata information, such as data duration, message type and stock \nsymbols; (2) big data analytics models, such as moving-window \ntimeseries analysis and autoregressive moving-average models; \nand (3) user-specific analytics codes in Python. Upon arrival of \na user job, the Client parses the requested data information then \nqueries timeseries data from its Server. Data analytics starts as \nsoon as the queried data streams flow into this Client node. Only \ntimeseries methods take into account possible internal structure \non the market data streams. In Section 5, we would present two \npopular analytics methods: movi ng-window analysis method of \nfinancial timeseries data and autoregressive moving-average \n(ARMA) model. The user-specific analytics code is embedded \nto apply an analytics method on the managed timeseries. Here, \nour focus is to enable embedded-Python API that allows finance \nengineers to: (1) easily implement a method in Python and (2) \neffortless integrate their method with this big data system for \nultra-fast low-latency execution. The detail of integration with \nPython is presented in the following section.\nB. Automated Parallel Python Software System\nTransparency is of essence in the design of embedded \nparallelized Python APIs in this big data infrastructure system. \nSo, the Python script that is embedded stays unchanged and is \nintegrated “ as-is”. Figure 2 shows the integration flowchart of \nthe codes in Python in the QuantCloud system. It adopts a \ncoprocess -based mechanism. A parent process stands for a \nthread of main process and manages the timeseries data streams. \nAt its inception, it spawns a child coprocess that is able to \noffload its workload. Communication between a parent and its child coprocess is using a parent-child shared memory. A shared \nmemory is attached and its associated parent-child \nsynchronization channel is established at the same time. To \nexecute a code in Python, the child coprocess serializes the \ntimeseries data in the C++ environment, transfers serialized data \npackets to the Python environm ent where serialized data is \ndeserialized and re -formatted as data structures in Python. This \ncompletes data conversion from C++ to Python. The interpreter \nis called to execute the script as long as data is ready to use. The \nresults from the Python code are serialized then returned to the \nchild coprocess. The child coprocess deserializes data packets \nthen restores structures. The results are finally transmitted back \nto memory space of the parent process and then an acknowledge \nsignal is sent to this parent process upon completion of this job.\nThe code in Python is executed in a single-thread model. A\nchild coprocess is operating in a multi -threaded asynchronous \nmodel. It has a built-in job queue able to buffer multiple jobs at \nthe same time and executes the buffe red jobs as first-in first-out \n(FIFO). It operates three threads: thread 1 is for messaging with \na parent process; thread 2 for ex ecuting the code in Python and \nthread 3 for deserializing results from the Python environment. \nThis asynchronous execution mechanism, though complicates \nthe implementation as requires more thread-safe codes, could \neffectively overlap the data serialization and the data analytics \noperations, thus it helps reduce the latency that is caused by the \nextra serialization operations. Optionally, an additional thread is \nconfigured to monitor the health state of the parent process \nperiodically and provide fault tolerance. Particularly, it performs \na safe shutdown when a failure is detected. Otherwise, an orphan \nprocess appears at occurrence of program faulty and error.\nFigure 2. Integration of automated parallel Python system in the big \ndata infrastructure in the QuantCloud software suite\nIV. P ROTOTYPE IMPLEMENTATIONS\nWe build a prototype to study the performance \ncharacteristics of this proposed system. In this section, we \npresent the software stack about the prototype implementation \nand the hardware that we use to benchmark this prototype. In \nnext section, we present the finance big data analytics models \nthat use this prototype for test and describe market data and \nperformance measurements.\nA. Software Stack\nThe prototype is coded in C++. The input script is in XML \nformat and result is reported as in CSV file. The communication \namong User, Client and Server uses the TCP/IP protocol. The \ndatabase on the Server and the query of timeseries data on the \n390\n\n[Página 4]\nClient follow our previous work [18]. The automated parallel \nPython API is provided on the Client.\nWithin one Client instance, the multithreaded programming \nis used for intra-node parallelism on shared memory. In this, the \nthread pool is used to manage the threads. The Python code is \nrun in a coprocess that is referred to as child in Fig. 2. To interact \nwith the main process, a memory segment is shared among the \nparent (the main program) and its child (the coprocess). This \naddresses the data transfer between the parent and its child. This \nchild is responsible to interact with Python. The workflow of a \nchild is as follows: the incomi ng timeseries is serialized and \ntransferred to Python; in Python, serialized timeseries is restored \nand reformatted as Python data types; then GIL is acquired to \nconduct the code in Python; last the produced result is \nreformatted, serialized and transmitted back to the child where \nthe result is returned to its par ent’s memory. Upon task \naccomplishment, the child sends an acknowledgement signal to \nits parent. This completes the work cycle of a child coprocess.\nB. Hardware Platform\nSystem 1: Dell PowerEdge R720, installed with two Intel \nXeon E5- 2603 processors at 1.8 GH z; a total of eight cores per \nserver; 32 GB DDR3 RAM and 500 GB SATA hard drive. Max \nmemory bandwidth is 34.1 GB/s. In this system, operating \nsystem is CentOS Linux 7.3 and compiler is GCC 4.8.5. This \nprocessor launched on Q1’12 and already discontinued at Q2’15 \nso it represents a legacy processor.\nSystem 2: QCT QuantaPlex S41T-2U4N system. Each node \nhas one Intel Xeon Phi 7230F processor, codenamed Knights \nLanding (KNL). Each KNL has a total of 64 cores (1.3 GHz); \n128 GB DDR4 RAM and 1 TB SATA SSD. Max memory \nbandwidth is 115.2 GB/s. In this  system, operating system is Red \nHat Enterprise Linux 7.2 and compiler is ICC 17.0.1. This \nprocessor launched on Q4’16 and it features a high -density \ncompute optimized solution.\nV. E XPERMENTS\nWe experiment the real-world financial analysis models and \nmarket tick datasets using this prototype. In this section, we first \nintroduce the moving-window analysis and the autoregressive \nmoving-average (ARMA) analysis of financial timeseries data. \nThen we describe the financial market tick datasets that we use \nto conduct the tests. Last, we describe the measurement in tests.\nA. Financial Analysis Models\n1) Moving-Window Analysis\nA moving window method in financial timeseries is one of \nthe most common approaches in many models [19]. We hereby \nsimulate this approach by coding the user-specific data-process \nfunctions in Python. The input timeseries stream of tick data is \nmanaged by the big data infrastructure, including querying the \ntimeseries, computing the logarithmic returns and formatting a \nfixed- length window. After these steps, the preprocessed data \nstreams flow into downstream embedded-Python nodes where \nthe Python-coded functions are applied to the data. Final result \nis exported in CSV files. In this test, two data process functions: \n‘abs’ and ‘ mean ’ are programmed in Python.The flowchart for these tests is shown in Figure 3. Among \nthese, we test three scenarios: (a) “1 -node abs”: a single \nembedded-Python node is added to find absolute values of \nlogarithmic returns; (b) “1-node abs + mean”: a single \nembedded-Python node is added to find the average of absolute \nlogarithmic returns; (c) “2 -node abs + mean”: two embedded -\nPython nodes are added, where first node is added for finding \nabsolute values of logarithmic returns and second node for \nfinding the average of absolute values from first node. Actually, \nthe result of case (c) is the same as that of case (b) and the \ndifference is number of embedded-Python nodes.\nFigure 3. Flowchart for moving -window analysis: 1-node abs (left), \n1-node abs+mean (middle) and 2-node abs+mean (right).\n2) Autoregressive Moving Average (ARMA)\nOutlier detection and data cleaning is the first must-have step \nof most financial modeling pipelines [7, 20]. We hereby test the \nautoregressive moving average (ARMA) model on the financial \ntimeseries data. The ARMA model is a tool to understand the \nvalues of timeseries. Same to previous study, the input dataset is \nthe tick data for S&P500 trade. The output is the result of the \nARMA model within certain fixed-length window of financial \ntimeseries. In this test, the ARMA model is coded in Python. In \naddition, the Hampel method [21, 22] is coded in Python and is \nused for truncating the outliers. Similarly, data flow is managed \nby this big data infrastructure. That is, the raw market tick data \nis preprocessed before entering the Hampel and AMRA nodes. \nThe flowchart is in Figure 4. In this test, two Python nodes are \ninserted in this process pipeline.\nFigure 4:Flowchart for ARMA with Hampel method\nB. Performance Measurements\nWallclock time is the amount of elapsed time from the start \nto the completion of a process pipeline, including the time that \n391\n\n[Página 5]\nqueries data from database, prepares timeseries, computes \nlogarithmic returns, executes Python scripts and writes results to \ndisk. This timing result is called as overall wallclock time.\nMeanwhile, we measure the cumulative time that is spent in \nexecuting the codes in Python. In the practice, the time spent in \nexecuting a code in Python is the elapsed time from entering to \nleaving the Python code. This me asurement is done at the child \ncoprocess side and includes the delay of C-Python API. This \ntiming result is called as embedded Python wallclock time. The \ntime measure is with a resolution of microseconds.\nLatency is reported in microseconds per tick and represents \nthe amount of time elapsed for processing a single tick message \non average. It is computed as the overall wallclock time divided \nby the total number of tick messages. Speedup for the codes in \nPython is defined as a ratio of ܶ(1)over ܶ(݊) ,where ܶ(1)and \nܶ(݊)are the elapsed times of 1 and ݊child coprocesses for the \ncodes in Python: ܵ(݊)(ܶ= 1 ) / )݊(ܶ .Parallel efficiency is \nܧ(݊)ܵ=(݊)݊/ .This speedup ratio ܵ(݊)and parallel \nefficiency ܧ(݊)is used to provide an estimate for how well this \nembedded- Python system speeds up. It is used to generate a plot \nof the elapsed time vs. the number of coprocesses and to \nunderstand the behavior of the parallelized Python scripts on \nmulticore processor architectures.\nVI. R ESULTS AND DISCUSSION\nA. Performance on Intel Xeon E5-2603 CPU\n1) Moving-Window Analysis\nFigs. 5 and 6 present the overall wallclock time (in second) \nand the latency (in μs/tick), respectively, for processing S&P500 \ntrade tick data for 7 days. These results show that: Our approach \nbrings great parallel performance to the codes in Python. I n “1-\nnode: abs” case, wallclock time is reduced from 177 to 56 sec \nand performance is improved by 68% (Fig. 5) and the latency is \nreduced to 0.82 μs/tick (Fig. 6). Calling more functions is more \nexpensive. For example, the performance of the “1 -node: \nabs+m ean” case is ever worse than that of the “1 -node: abs” case \n(Fig. 5 ). The former calls two Python functions and the latter \ncalls only one once.\nFigure 5. Wallclock time in seconds for processing trade tick data of \nS&P500 stocks in 7 days (window period: 20)Figure 6. The latency in μs per tick for processing trade tick data of \nS&P500 stocks in 7 days (window period: 20)\n2) ARMA model\nFigs. 7 and 8 present the overall and embedded -Python \nwallclock times (in seconds), respectively. In this test, we tested \ntwo cases: 16-stock and 8-stock. From results, this test affirmed \nthe parallel performance our approach brings to the code in \nPython. As the number of coprocesses increases, the overall and \nthe embedded-Python wallclock times are reduced in Figs. 7 and \n8. The overall performance for 16-stock and 8-stock are\nimproved by 51% and 43%, respectively (Fig. 7). The optimal \nnumber of coprocesses for the 16-stock case is 8 (Fig. 7).\nFigure 7. Overall wallclock time for the ARMA model.\nFigure 8. Embedded-Python wallclock time for ARMA.319\n115288\n88177\n56\n0100200300400\n048 1 2 1 6Overall WallClock Time in Second\nNumber of Coprocess20-point/S&P500/7-day Trade\n2-node: abs + mean\n1-node: abs + mean\n1-node: abs1.69\n1.29\n0.82\n012345\n0 4 8 12 16Speed: microsecond per tick\nNumber of Coprocess20-point/S&P500/7-day Trade\n2-node: abs + mean\n1-node: abs + mean\n1-node: abs\n2570\n19131567\n12501308\n1282 1068\n799732\n05001,0001,5002,0002,5003,000\n1248 1 6Overall WallClock Time in Second\nNumber of CoprocessARMA/16 stocks/7-day trade\nARMA/8 stocks/7-day trade\n2552\n1753\n1164\n996\n6841275 981\n640\n436\n05001,0001,5002,0002,5003,000\n1248 1 6Embedded-Python WallClock Time in \nSecond\nNumber of CoprocessARMA/16 stocks/7-day trade\nARMA/8 stocks/7-day trade\n392\n\n[Página 6]\nB. Performance on Intel Xeon Phi 7230F\n1) Moving-Window Analysis\nFig. 9 presents the overall and embedded-Python wallclock \ntimes. This test reaffirmed the sc alability of our approach. With \nthe increase of coprocesses, the performance is consistently \nimproved and the speedup for Python codes is almost linear.\n2) ARMA model\nWe use 64 stocks and 7 -day trade in this test. Fig. 11 presents \nthe overall wallclock time vs. the number of coprocesses. In this \nfigure, both axes are in logarithmic scale to clarify the \nscalability. This test restated  that our platform could bring \nsignificant performance improvement on this time-consuming \nARMA model: the wallclock time is reduced from 6 days \n(21,008 seconds) to 20 minutes (1,183 seconds) in Fig. 11. The \nPython part is speeded up by a factor of 28.4. In most \nconfigurations, the parallel efficiency in the Python part is no \nless than 90%. Optimal number of coprocesses is 32 for this test.\nFigure 9. Overall and embedded-Python wallclock time for \nprocessing trade tick data of S&P500 stocks for 7 days.\nFigure 10. Overall wallclock time in second for the ARMA model on\nIntel Xeon Phi processor.\nC. Intel Xeon E5 vs. Intel Xeon Phi 2\nWe compare two Intel Xeon processor architectures at the \nfinance big data analytics level in this subsection.\n1) Moving-Window Analysis\nIn this test, we choose the “2 -node: abs+mean” case as it i s\nthe most time-consuming case among three cases. Wallclock time is shown in Fig. 12. This comparison shows that Intel Xeon \nPhi processor consistently outperforms Intel Xeon E5 processor. \nThe former is 2.67 times faster than the latter in this test. Our \nmethod improved the overall performance by 64% on Xeon E5 \nCPU and 84% on Xeon Phi CPU. The latency is reduced to 1.69 \nμs/tick on Xeon E5 and 0.63 μs/tick on Xeon Phi. The latter’s \nlatency is only 1/3 of the former processor model.\nFigure 11. Overall wallclock time for processing S&P500 trade data \nfor 7 days, using 2-node: abs and mean.\n2) ARMA model\nWe compare the performance of ARMA model on Xeon E5 \nand Xeon Phi processors in two test sets as follows.\nTest 1: we test the same problem size: a total of 16 stocks \nand 7-day trade tick market data. The overall clock time vs. the \nnumber of coprocess is shown and compared in Fig. 12.\nThis comparison shows when a small amount of coprocesses \nare used, Intel Xeon Phi processor actually performs worse than \nIntel Xeon E5. For example, when the Python code runs in a \nsingle process mode (i.e., number of coprocess is one), it takes \nXeon Phi 5262 seconds, while Xeon E5 needs only 2570 \nseconds that is ~2x faster than the Xeon Phi processor. However, \nas the number of coprocesses increases, Xeon Phi quickly \noutperforms Xeon E5 and shows its superiority with aid of \nmassive parallel execution of Python codes. In this test, the Xeon \nPhi reduces the overall wallclock time to 418 seconds and is ~3x \nfaster than the Xeon E5.\nFigure 12. Overall wallclock time of ARMA for 16 stocks and 7-day \ntrade on Intel Xeon E5 and Xeon Phi CPUs.277\n144\n82\n5043144173236\n118\n62\n31168 4\n0100200300\n1 2 4 8 16 32 64WallClock Time in Second\nNumber of Coprocess20-point/S&P500/7-day Trade\nOverall\nEmbedded-Python\n21,008\n10,494\n5,425\n2,940\n1,506\n1,1831,384\n1,0242,0484,0968,19216,38432,768\n1 2 4 8 16 32 64Overall WallClock Time in Second\nNumber of CoprocessARMA/64 stocks/7-day trade319\n115277\n43\n0100200300400\n1 2 4 8 16 32 64Overall Wallclock Time in Second\nNumber of Coprocess20-point/S&P500/7-day Trade\nIntel Xeon E5-2603 processor\nIntel Xeon Phi 7230F processor\n2,570\n1,9131,567\n1,250 1,3085,262\n2,761\n1,421\n753\n41801,0002,0003,0004,0005,0006,000\n1248 16Overall WallClock Time in Second\nNumber of CoprocessARMA/16 stocks/7-day trade\nIntel Xeon E5 Processor\nIntel Xeon Phi Processor\n393\n\n[Página 7]\nThus, it is no coincidence that effective parallel execution of \nthe Python code is essential at exploring the performance of the \nupcoming multicore platforms such as the Knights Landing and \nits processor family. In this work, we have already showed an \neffective coprocess-based parallelism for the codes in Python \nand their seamless integration with a big data infrastructure.\nTest 2: we conduct a scalability test by using two problem \nsizes: 16 stocks for Xeon E5 an d 64 stocks for Xeon Phi. 7- day \ntrade tick data is used for both. In results, the per-stock latency \n(i.e., second per stock) is presented in Fig. 13. These results \nshow that: Xeon Phi processor consistently outperforms the \nXeon E5, with the increase of coprocesses, in terms of the per-\nstock latency. The Xeon Phi processor excels at lowering the \nlatency than the Xeon E5. The Xeon Phi is able to improve the \nlatency by 95% but the Xeon E5 only by 52%. The Xeon Phi \nhas a higher workload throughput than the Xeon E5. The Xeon \nPhi is able to process as many as 195 stocks in an hour while \nthe Xeon E5 could just do a maximum of 46 stocks in an hour. \nIn this measure, the Xeon Phi is 4.2x faster than the Xeon E5.\nFigure 13. Per-stock latency (second per stock) of ARMA for 16 \nstocks on Intel Xeon E5 and 64 stocks on Xeon Phi CPUs.\nCollectively, by observing these results in Tests 1 and 2, we can \nsee that: (1) Intel Xeon E5 proce ssor is about 2x times faster \nthan Intel Xeon Phi processor when the Python code is simply \nexecuted in a single process; (2) the power of a novel Intel Xeon \nPhi processor cannot be explored until the parallel execution of \nthe Python code is employed.  In other words, if the application \ncannot execute in parallel, it can hardly take advantage of novel \nmodern processors’ capabilities a nd it may end up with an ev en \nworse performance. This appears true not only for these finance \nbig data applications in this work but also for a wide range of \nmodern applications [23-25].\nD. Summary and Discussion\nIn this section, we have tested the moving-window analysis and \nthe ARMA model and compared the performance of Intel Xeon \nE5 and Xeon Phi processors. These results can demonstrate \nthat: our approach, a coprocess-based parallel execution of the \ncode in Python could bring significant parallel performance to \nall cases. Specifically, the overall wallclock time and the time for the Python code is greatly reduced, as more coprocesses \nused. The speedup for the Python code is nearly linear and the \nparallel efficiency is around 90%.\nOur approach plays a key role at fully delivering the power of \nmodern high density multicore processors. Clearly, the peak \nperformance of Intel Xeon Phi pr ocessor is greatly larger than \nthat of its previous Intel processors. However, fully exploring \nthe power of such modern multico re processors requires highly \nparallel application programs. In the meanwhile, programming \ndifficulty increases with increased complexity of architectures. \nWith the number of cores in mainstream processors predicted \nto scale to hundreds today, there is an urgent need to develop a \nhighly- parallel application -specific platform for the real-world \napplication users. To this aim, this work presents an approach \nthat effectively brings coprocess-based parallel execution to the \nPython while integrates with a finance big data infrastructure. \nThis yields an integrated big da ta and parallel python platform \ntowards modern quantitative finance applications. This effort is \nof practical interest in the research and industry.\nVII. C ONCLUSION\nThis work presents an integr ated big data and parallel Python \nplatform for the quantitative fina nce applications. This platform \nincorporates a big data infrastructure system to manage finance \ntimeseries market data and a built-in embedded-Python API to \nexecute the Python code on these managed timeseries streams. \nThe code in Python is able to be executed in highly parallel. We \nprototype this proposed system and test our prototype with two \npopular applications and the NYSE tick data. Results show that \nour system could bring significant parallel performance to all \nof the cases; the execution time is greatly reduced as the number \nof processes increases; the spee dup for parallel Python is nearly \nlinear and the parallel efficiency for most cases is as high as \naround 90%. These achievements demonstrated the efficacy of \nour proposed system model and the efficiency of our software \nprototype at the real-world applications level.\nREFERENCES\n[1] J. J. Angel, L. E. Harris, and C. S. Spatt, \"Equity trading in the 21st \ncentury: An update,\" The Quarterly Journal of Finance, vol. 5, p. 1550002, \n2015.\n[2] A. Carrion, \"Very fast money: High-frequency trading on the NASDAQ,\" \nJournal of Financial Markets, vol. 16, pp. 680-711, 2013.\n[3] I. Aldridge, High-frequency trading: a practical guide to algorithmic \nstrategies and trading systems vol. 459: John Wiley and Sons, 2009.\n[4] M. A. Goldstein, P. Kumar, and F. C. Graves, \"Computerized and High ̺\nFrequency Trading,\" Financial Review, vol. 49, pp. 177-202, 2014.\n[5] J.-P. Serbera and P. Paumard, \"The fall of high-frequency trading: A \nsurvey of competition and profits,\" Research in International Busin ess and \nFinance, vol. 36, pp. 271-287, 2016.\n[6] G. Meyer and N. Bullock. (2017, March 30). Race for speed among algo \ntraders hits peak. Available: https://www.ft.com/content/6961129e-14fa-\n11e7-80f4-13e067d5072c\n[7] X. Shi, P. Zhang, and S. U. Khan, \"Quantitative Data Analysis in \nFinance,\" in Handbook of Big Data Technologies, A. Y. Zomaya and S. \nSakr, Eds., ed Cham: Springer International Publishing, 2017, pp. 719 -\n753.328\n164\n85\n4624 18 22161\n12098\n78 82\n050100150200250300350\n1248 1 6 3 2 6 4Latency: second per stock\nNumber of CoprocessARMA/7-day trade\n64-stock on Xeon Phi\n16-stock on Xeon E5\n394\n\n[Página 8]\n[8] B. Fang and P. Zhang, \"Big Data in Finance,\" in Big Data Concepts, \nTheories, and Applications, S. Yu and S. Guo, Eds., ed Cham: Springer \nInternational Publishing, 2016, pp. 391-412.\n[9] M. Peat, \"Big data in finance,\" InFinance: The Magazine for Finsia \nMembers, vol. 127, p. 34, 2013.\n[10] T. Seth and V. Chaudhary, \"Big Data in Finance,\" ed, 2015.\n[11] D. Beazley, \"Understanding the python gil,\" in PyCON Python \nConference. Atlanta, Georgia, 2010.\n[12] K. Kinder, \"Event-driven programming with Twisted and Python,\" Linux \njournal, vol. 2005, p. 6, 2005.\n[13] (2015). SciPy Cookbook. url: http://scipy-cookbook.readthedocs.io/\n[14] R. Odaira, J. G. Castanos, and H. Tomari, \"Eliminating global interpreter \nlocks in ruby through hardware transactional memory,\" in ACM \nSIGPLAN Notices, 2014, pp. 131-142.\n[15] L. Dalcin, R. Paz, and M. A. Storti, \"MPI for Python,\" J. Parallel Distrib. \nComput., vol. 65, pp. 1108-1115, 2005.\n[16] Y. Gao and W.-M. Chen, \"Family Relationship Inference Using Knights \nLanding Platform,\" in Cyber Security and Cloud Computing (CSCloud), \n2017 IEEE 4th International Conference on, 2017, pp. 27-30.\n[17] C. Zhou, Y. Gao, and W. Howard, \"Evaluation of Combining Bootstrap \nwith Multiple Imputation Using R on Knights Landing Platform,\" in \nCyber Security and Cloud Computing (CSCloud), 2017 IEEE 4th \nInternational Conference on, 2017, pp. 14-17.[18] P. Zhang, K. Yu, J. Yu, and S. Khan, \"QuantCloud: Big Data \nInfrastructure for Quantitative Finance on the Cloud,\" IEEE Transactions \non Big Data (in press) doi: 10.1109/TBDATA.2017.2649544, 2017.\n[19] N. R. Swanson, \"Money and output viewed through a rolling window,\" \nJournal of monetary Economics, vol. 41, pp. 455-474, 1998.\n[20] S. T. Rachev, S. V. Stoyanov, and F. J. Fabozzi, Risk and Uncertainty vol. \n211: John Wiley & Sons, 2011.\n[21] R. K. Pearson, \"Outliers in process modeling and identification,\" IEEE \nTransactions on control systems technology, vol. 10, pp. 55-63, 2002.\n[22] H. Liu, S. Shah, and W. Jiang, \"On-line outlier detection and data \ncleaning,\" Computers & chemical engineering, vol. 28, pp. 1635-1647, \n2004.\n[23] G. Lawson, M. Sosonkina, T. Ezer, and Y. Shen, \"Empirical Mode \nDecomposition for Modeling of Parallel Applications on Intel Xeon Phi \nProcessors,\" in Proceedings of the 17th IEEE/ACM International \nSymposium on Cluster, Cloud and Grid Computing, 2017, pp. 1000-1008.\n[24] S. J. Pennycook, C. J. Hughes, M. Smelyanskiy, and S. A. Jarvis, \n\"Exploring simd for molecular dynamics, using intel® xeon® processors \nand intel® xeon phi coprocessors,\" in Parallel & Distributed Processing \n(IPDPS), 2013 IEEE 27th International Symposium on, 2013, pp. 1085-\n1097.\n[25] J. Reinders, \"An overview of programming for Intel Xeon processors and \nIntel Xeon Phi coprocessors,\" Intel Corporation, Santa Clara, 2012. \n395\n\n[Página 9]\n[1] J. J. Angel, L. E. Harris, and C. S. Spatt, \"Equity trading in the \n21st century: An update,\" The Quarterly Journal of Finance, \nvol. 5, p. 1550002, 2015.\n[2] A. Carrion, \"Very fast money: High-frequency trading on the \nNASDAQ,\" Journal of Financial Markets, vol. 16, pp. 680-711, \n2013.\n[3] I. Aldridge, High- frequency trading: a practical guide to \nalgorithmic strategies and trading systems vol. 459: John Wiley \nand Sons, 2009.\n[4] M. A. Goldstein, P. Kumar, and F. C. Graves, \"Computerized \nand High‐Frequency Trading,\" Financial Review, vol. 49, pp. \n177-202, 2014.\n[5] J.-P. Serbera and P. Paumard, \"The fall of high-frequency \ntrading: A survey of competition and profits,\" Research in \nInternational Business and Finance, vol. 36, pp. 271-287, 2016.\n[6] G. Meyer and N. Bullock. (2017, March 30). Race for speed \namong algo traders hits peak . Available: \nhttps://www.ft.com/content/6961129e-14fa-11e7-80f4-\n13e067d5072c\n[7] X. Shi, P. Zhang, and S. U. Khan, \"Quantitative Data Analysis \nin Finance,\" in Handbook of Big Data Technologies , A. Y. \nZomaya and S. Sakr, Eds., ed Cham: Sp ringer International \nPublishing, 2017, pp. 719-753.\n[8] B. Fang and P. Zhang, \"Big Data in Finance,\" in Big Data \nConcepts, Theories, and Applications , S. Yu and S. Guo, Eds., \ned Cham: Springer International Publishing, 2016, pp. 391-412.\n[9] M. Peat, \"Big data in finance,\" InFinance: The Magazine for \nFinsia Members, vol. 127, p. 34, 2013.\n[10] T. Seth and V. Chaudhary, \"Big Data in Finance,\" ed, 2015.\n[11] D. Beazley, \"Understanding the python gil,\" in PyCON Python \nConference. Atlanta, Georgia , 2010.\n[12] K. Kinder, \"Event-driven programming with Twisted and \nPython,\" Linux journal, vol. 2005, p. 6, 2005.\n[13] (2015). SciPy Cookbook . Available: http://scipy-\ncookbook.readthedocs.io/\n[14] R. Odaira, J. G. Castanos, and H. Tomari, \"Eliminating global \ninterpreter locks in ruby through hardware transactional \nmemory,\" in ACM SIGPLAN Notices , 2014, pp. 131-142.\n[15] L. Dalcin, R. Paz, and M. A. Storti, \"MPI for Python,\" J. \nParallel Distrib. Comput., vol. 65, pp. 1108-1115, 2005.\n[16] Y. Gao and W.- M. Chen, \"Family Relationship Inference Using \nKnights Landing Platform,\" in Cyber Security and Cloud \nComputing (CSCloud), 2017 IEEE 4th International Conference \non, 2017, pp. 27-30.\n[17] C. Zhou, Y. Gao, and W. Howard, \"Evaluation of Combining \nBootstrap with Multiple Imputation Using R on Knights \nLanding Platform,\" in Cyber Security and Cloud Computing \n(CSCloud), 2017 IEEE 4th International Conference on , 2017, \npp. 14-17.\n[18] P. Zhang, K. Yu, J. Yu, and S. Khan, \"QuantCloud: Big Data \nInfrastructure for Quantitative Finance on the Cloud,\" IEEE \nTransactions on Big Data (in press) doi: \n10.1109/TBDATA.2017.2649544, 2017.\n[19] N. R. Swanson, \"Money and output viewed through a rolling \nwindow,\" Journal of monetary Economics, vol. 41, pp. 455-474, \n1998.\n[20] S. T. Rachev, S. V. Stoyanov, and F. J. Fabozzi, Risk and \nUncertainty vol. 211: John Wiley & Sons, 2011.\n[21] R. K. Pearson, \"Outliers in process modeling and \nidentification,\" IEEE Transactions on control systems \ntechnology, vol. 10, pp. 55-63, 2002.\n[22] H. Liu, S. Shah, and W. Jiang, \"On-line outlier detection and \ndata cleaning,\" Computers & chemical engineering, vol. 28, pp. \n1635- 1647, 2004.\n[23] G. Lawson, M. Sosonkina, T. Ezer, and Y. Shen, \"Empirical \nMode Decomposition for Modeling of Parallel Applications on \nIntel Xeon Phi Processors,\" in Proceedings of the 17th \nIEEE/ACM International Symposium on Cluster, Cloud and \nGrid Computing , 2017, pp. 1000-1008.\n[24] S. J. Pennycook, C. J. Hughes, M. Smelyanskiy, and S. A. \nJarvis, \"Exploring simd for molecular dynamics, using intel® xeon® processors and intel® xeon phi coprocessors,\" in \nParallel & Distributed Processing (IPDPS), 2013 IEEE 27th \nInternational Symposium on , 2013, pp. 1085-1097.\n[25] J. Reinders, \"An overview of programming for Intel Xeon \nprocessors and Intel Xeon Phi coprocessors,\" Intel Corporation, \nSanta Clara, 2012.\n396",
      "metadata": {
        "filename": "zhang2018.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\zhang2018.pdf",
        "file_size": 466633,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:42.412246",
        "content_length": 42897
      }
    },
    "e0187208-dd3a-48a3-b50c-e5bb5e74fef4": {
      "id": "e0187208-dd3a-48a3-b50c-e5bb5e74fef4",
      "content": "[Página 1]\nSPECIAL SECTION ON ADVANCED OPTICAL IMAGING FOR EXTREME ENVIRONMENTS\nReceived August 12, 2019, accepted August 20, 2019, date of publication September 4, 2019, date of current version October 1, 2019.\nDigital Object Identifier 10.1 109/ACCESS.2019.2939158\nOptimizing the Electronic Health Records Through\nBig Data Analytics: A Knowledge-Based View\nCAIFENG ZHANG1, RUI MA2, SHIWEI SUN3,4, YUJIE LI\n5, YICHUAN WANG\n2,\nAND ZHIJUN YAN3,4\n1Kaifeng Hospital of Traditional Chinese Medicine, Kaifeng 300193, China\n2Shef\u001celd University Management School, The University of Shef\u001celd, Shef\u001celd S10 2TN, U.K.\n3Beijing Institute of Technology, Beijing 100811, China\n4Sustainable Development Research Institute for Economy and Society, Beijing, China\n5Yangzhou University, Yangzhou 225009, China\nCorresponding author: Shiwei Sun (shiweisun@bit.edu.cn)\nThis work was supported in part by the Beijing Institute of Technology Research Fund Program for Young Scholars under Grant\n3210012221902, and in part by the Taiyuan National Sustainable Development Agenda Major Project under Grant 3210041910009.\nABSTRACT Many hospitals are suffering from ineffective use of big data analytics with electronic health\nrecords (EHRs) to generate high quality insights for their clinical practices. Organizational learning has been\na key role in improving the use of big data analytics with EHRs. Drawing on the knowledge-based view and\nbig data lifecycle, we investigate how the three modes of knowledge can achieve meaningful use of big data\nanalytics with EHRs. To test the associations in the proposed research model, we surveyed 580 nurses of a\nlarge hospital in China in 2019. Structural equation modelling was used to examine relationships between\nknowledge mode of EHRs and meaningful use of EHRs. The results reveal that know-what about EHRs\nutilization, know-how EHRs storage and utilization, and know-why storage and utilization can improve\nnurses' meaningful use of big data analytics with EHRs. This study contributes to the existing digital health\nand big data literature by exploring the proper adaptation of analytical tools to EHRs from the different\nknowledge mode in order to shape meaningful use of big data analytics with EHRs.\nINDEX TERMS Big data analytics, electronic health records and impacts, knowledge-based view.\nI. INTRODUCTION\nWith the aim of improving quality of care through the mean-\ningful use of electronic health records (EHRs), the China\ngovernment has promulgated the Electronic Health Record\nArchitecture and Data Standard in 2009 as a guide for the\nhospitals. In this guide, EHRs are de\u001cned as ``a complete\ncollection of digital clinical information documenting the\nclinical care rendered to an individual in the Chinese EHR\nStandard'' [1]. Over two decades, EHRs has been suggested\nto enhance the healthcare service ef\u001cciency and effectiveness,\nbut it does not mean that simply adopting the EHRs system\ncould lead to those bene\u001cts. Healthcare providers need to\nmake the EHR a routine in the daily work system in order to\nrealize the payback. Thus, Health Information Technology for\nEconomic and Clinical Health (HITECH) Act introduces the\n``meaningful use'' of EHR as the goal of adoption. The main\nobjective of Act is to create meaningful and useful digital\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Huimin Lu.medical records, including the entry and storage of EHRs,\nand optimize the utilization of EHRs.\nAs of 2011, clinical data had reached 150 exabytes\n(1 EB D1018 bytes) worldwide, mainly in the form of\nEHRs [2]. Yet, considerable uncertainty still remains about\nthe use of big data analytics within EHRs and its impact on\nclinical performance [3]. Such struggles are due to not only\ninsuf\u001ccient fund and biased resource allocation at the national\nlevel but also lack of planning and governance for the use of\nbig data analytics within EHRs at the hospital level [3], [4].\nTo address this challenge, although many hospitals in\nChina have invested a great deal of cost, time and resources\nin learning the implementation and utilization of EHRs, they\nare still suffering from ineffective use of big data analytics\nwithin EHRs to generate high quality information for deci-\nsion making and reduce health disparities [3], [9]. One of the\nkey reasons for this dif\u001cculty is the lack of full consideration\nof EHRs \u001ctness to the speci\u001cc situations of the particular\norganization [9]. It is important for healthcare practitioners to\npay greater attention to understand how to absorb the diverse\nVOLUME 7, 2019 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/ 136223\n\n[Página 2]\nC. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics\nFIGURE 1. Optimizing the electronic health records through big data\nanalytics.\nknowledge of EHRs. As such, little attention has been paid\nto understanding the role of knowledge mode in improving\nthe use of big data analytics within EHRs. In this study, thus,\nwe examine the relationship between the knowledge about\nbig data analytics within EHRs and the outcome of EHRs\nadoption (i.e., meaningful use of EHRs).\nThe remainder of this paper is structured as follows: the\nnext section serves as our theoretical background, which\nleads to the development of the research model and associ-\nated hypothesis; followed by our research method, \u001cndings\nand discussions, contributions to research, implications for\npractice and recommendations, then limitations and future\nresearch directions are discussed as our conclusion.\nII. OPTIMIZING THE ELECTRONIC HEALTH RECORDS\nTHROUGH BIG DATA ANALYTICS\nThe meaningful use of EHRs is crucial for improving clinical\noperations and healthcare service [5]. Big data analytics is a\ntool that enables healthcare organizations to reach this goal by\noptimizing EHRs through analytical algorithms. For exam-\nple, Texas Health Harris Methodist Hospital Alliance uti-\nlizes medical sensor data to analyze patients' movements and\nmonitor their actions throughout their hospital stay. In this\nway they can provide healthcare services more ef\u001cciently and\naccurately, optimize existing operations, and prevent some\nmedical risks [6]. Indeed, the use of big data analtyics within\nEHRs is rooted in the concept of data life cycle framework\nthat consists of three components: data collection, data stor-\nage, and data utilization, as shown in Figure 1. These logical\ncomponents that perform speci\u001cc functions enable healthcare\npractitioner to understand how to transform the EHRs into\nmeaningful clinical insights through big data analtyics.\nData collection. This component contains all the data\nsources and content type of EHRs. In general, The EHRs aredivided into structured data (e.g., patient demographics, med-\nication history, health status and lab results) and unstructured\ndata (e.g., diagnosis notes, clinical graphics, and medical\nimages). These data are collected from various clinical units\ninside the hospital or from external units.\nData storage. The EHRs are stored into appropriate\ndatabases depending on the source of data and content for-\nmat. This component aims to handle data from the vari-\nous data sources by two steps: transformation and storage.\nThe transformation engine is capable of moving, cleaning,\nsplitting, translating, merging, sorting, and validating EHRs.\nFor instance, structured EHRs data will be extracted from\nhealthcare information systems and converted into a speci\u001cc\nstandard data format, sorted by the criterion (e.g., patient\nidentity, health status medication history), and then the record\nin the right place. In the next step, the EHRs are loaded\ninto the target databases (e.g., Database Management System;\nDBMS, Hadoop distributed \u001cle systems; HDFS, or in a cloud)\nfor further analysis.\nData Utilization. This component is used to process\nall kinds of EHRs and report the summarized results for\nclinical decision making. The analysis of EHRs includes\nMap/Reduce, stream computing, and in-database analytics,\ndepending on the type of data and the purpose of the analy-\nsis. Map/Reduce can provide the ability to process massive\nunstructured and structured EHRs in batch form in a mas-\nsively parallel processing environment. Stream computing\ncan support near real time or real time analysis for EHRs.\nThough stream computing, medical staffs can track EHRs in\nmotion in order to respond to unexpected events and deter-\nmine next-best actions. In-database analytics is commonly\nused data mining approach that allows EHRs to be analyzed\nwithin database. It can provide high-speed parallel processing\nand offer a safe environment to process con\u001cdential patient\ninformation. This component also generates various visu-\nalization reporting and real-time and meaningful business\ninsights derived from the analysis. The reporting system is\na critical big data analtyics feature that allows EHRs to be\nvisualized in a meaningful way to support medical staff day-\nto-day operations and clinical decisions.\nIII. RESEARCH MODEL AND HYPOTHESIS DEVELOPMENT\nPrior research has acknowledged that organizational learn-\ning has been an important enabler for improving the use of\nbig data analytics within EHRs [7]\u0015[9]. From the aspect of\ninformation technology (IT) adoption, learning process plays\na key role in the outcomes of the IT adoption. When the new\nIT is introduced to the organization, it implies that a large\namount of knowledge is brought in [10], [11]. Organizations\nneed to adopt a series of learning processes to merge the\ngap between what needs to be known and what is already\nknown in order to understand how to use this knowledge\neffectively and ef\u001cciently [10]. From the knowledge-based\nview (KBV), knowledge plays a pivotal role in increasing the\norganizations' competitive advantage and \u001cnancial perfor-\nmance [12], [13]. Effective knowledge activities in healthcare\n136224 VOLUME 7, 2019\n\n[Página 3]\nC. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics\nFIGURE 2. Proposed model of how three mode of knowledge about the\nuse of big data analytics within EHRs for achieving meaningful use of\nEHRs.\nnot only improve the existing operational capabilities of\nhealthcare service but also reduce the care delivery costs and\nprevent potential medical errors [14], [15].\nDrawing on the knowledge-based view (KBV), we develop\nour research model and associated hypotheses, as shown\nin Figure 2. KBV posits that organizational knowledge is\nviewed as a strategic resource of an organization. It also\nemphasizes that creating knowledge for the production of\ngoods and services can acquire competitive advantage and\norganizational performance [12], [15]. In the context of EHRs\nimplementation, an effective knowledge creation from EHRs\nis likely to be achieved by all medical staffs knowing how,\nwhy, what EHRs can be used properly.\nTo understand the creation of knowledge, it is essential\nto explore the mode of knowledge. In general, the mode of\nknowledge activities can be classi\u001ced into three categories\naccording to the level of material involvement with the knowl-\nedge: knowing-what, knowing-how, and knowing-why [18].\nKnowing-what refers to a declarative knowledge that con-\ntains information about activities and relationships [18]. This\nknowledge allows organizations to understand the digital\nhealth technologies in certain detail, such as the principle and\ncharacteristics of the technology, and to generate to a certain\ntangible products or outcomes. In the context of EHRs, hos-\npitals need to understand what EHRs are, its features, and\nproblems when it applies in practice. When they learn about\nEHRs, hospitals would perceive an attitude towards it and\nform the basic idea of how to adopt it effectively. Thus, we\npropose the following hypotheses.\nHypothesis 1a (H1a): Knowing-what about the data collec-\ntion of EHRs will facilitate meaningful use of EHRs.\nHypothesis 1b (H1b): Knowing-what about the data stor-\nage of EHRs will facilitate meaningful use of EHRs.\nHypothesis 1c (H1c): Knowing-what about the data utiliza-\ntion of EHRs will facilitate meaningful use of EHRs.\nKnowing-how is a procedural knowledge that includes the\nstep-by-step procedures executable in a speci\u001cc system [16].Data analysts within healthcare organizations need to gain\nthis type of knowledge in order to process EHRs effectively\nand meaningfully. For example, Tracking EHRs can generate\nreal-time monitoring patient information such as alerts and\nproactive noti\u001ccations. Data analysts need to know what the\nmost important outputs are and how to display them and send\nto interested users or made available in the form of dash-\nboards in real time. Knowing-how about processing EHRs\ncan explore patterns of care and provide exceptional support\nfor evidence based medical practices. Using knowing-how,\nhealthcare organizations can also address data quality issue\nthrough knowing well-de\u001cned procedures and rules in an\nEHRs system. Thus, we propose the following hypotheses.\nHypothesis 2a (H2a): Knowing-how about the data collec-\ntion of EHRs will facilitate meaningful use of EHRs.\nHypothesis 2b (H2b): Knowing-how about the data storage\nof EHRs will facilitate meaningful use of EHRs.\nHypothesis 2c (H2c): Knowing-how about the data utiliza-\ntion of EHRs will facilitate meaningful use of EHRs.\nKnowing-why is a contextual knowledge that enables users\nto solve the problems based on understanding contextual\nreasons and axiomatic principles [16], [17]. This knowledge\nprovides explanations for rationalization about technology. In\nthe context of EHRs, hospitals realize why EHRs should be\nused to generate better clinical performance. This includes\nthe examination of the speci\u001cc situation of their organizations\nand comparison of other alternative solutions. Also, organiza-\ntions should be aware of the impacts and consequences of uti-\nlizing EHRs. Besides the \u001cnancial and organizational impact\nof EHRs, hospitals also have to harness the possible chal-\nlenges when they use the EHRs system. In hospitals, a high\nlevel of knowing-why about EHRs can be accumulated by\nunderstanding of knowing-what and knowing-how involved\nin data collection, storage, and utilization of EHRs in the\nclinical system. Thus, we propose the following hypotheses.\nHypothesis 3a (H3a): Knowing-why about the data collec-\ntion of EHRs will facilitate meaningful use of EHRs.\nHypothesis 3b (H3b): Knowing-why about the data storage\nof EHRs will facilitate meaningful use of EHRs.\nHypothesis 3c (H3c): Knowing-why about the data utiliza-\ntion of EHRs will facilitate meaningful use of EHRs.\nIV. METHODS\nA. SAMPLE AND DATA COLLECTION\nWe investigate the relationship between knowledge mode\nof EHRs and meaningful use of EHRs among healthcare\nworkers in China, primarily surveyed nurses after receiving\nethics approval. An initial population set of 1,000 nurses was\nobtained from a large hospital in Henan province, China. The\n\u001crst round of 1,000 questionnaires resulted in 351 invitations\nbeing rejected due to the availability. Of the 649 invitations\nthat were seen by potential respondents, 580 responses were\nreturned, completed and usable for the data analysis, showing\na response rate of 89.37%.\nVOLUME 7, 2019 136225\n\n[Página 4]\nC. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics\nTABLE 1. Demographic characteristics of the final sample with\ninformation of the participants ( nD580).\nNon-response bias was assessed by comparing the \u001crst\n25 percent with the last 25 percent of the responses for each\nvariable using paired sample t-tests [18]. The results showed\nno statistically signi\u001ccant difference (p >0.05) between\nthese two groups, indicating that non-response bias did not\npresent a problem for this study.\nThe demographic characteristics of the respondents are\nshown in Table 1. Among the 580 respondents, 86.20% were\nfemale. Most nurses (92.20%) were younger than 40 years:23.30% were younger than 25 years, 40.30%were 25\u001530\nyears of age, 21.70% were 31\u001535 years of age, and 6.90%\nwere 36\u001540 years of age. Most respondents had a bachelor's\ndegree (91.40%). The respondent seniority (years of employ-\nment) was evenly distributed, and the largest group had a\nseniority of 6\u001510 years (31.60%). A plurality of respondents\n(33.28%) worked in the internal medicine department.\nB. VARIABLES AND INSTRUMENTS\nThe instrument used in this study was adapted from previ-\nously validated instruments (presented in Appendix 5). All\nindependent and dependent variables were collected using an\nonline survey completed by each participant. The scale of\nknowing-what, knowing-how, and knowing-why about EHRs\nwas adapted from Lee and Strong's study [16] who proposed\nthe three mode of knowledge underlying data collection,\nstorage, and utilization and examined how knowledge held by\ndifferent work roles affects data quality. This scale was used\nto rate the knowledge level of EHRs by which each participant\nacquires. A seven-point Likert-type scale was used to capture\nthe responses, ranging from 1 Dvery small extent, through\n4Daverage, to 7 Dvery large extent.\nThe measurement of meaningful use of the EHR was\ndeveloped from the regulation published by Department of\nHealth and Human Services (DHHS) for the year 2011-\n2012 [19]. Leading by Centers for Medicare and Medicaid\nServices, DHHS developed a list of criteria for meaningful\nuse requirements on January 16, 2010 based on the call from\nHealth Information Technology for Economic and Clinical\nHealth (HITECH). Five items were developed according to\nthose regulations to measure the performance of the adopted\nEHRs in hospital. A seven-point Likert-type scale was used\nto capture the responses, ranging from 1 Dstrongly disagree\nto 7Dstrongly agree.\nC. MEASUREMENT VALIDITY AND RELIABILITY\nThe validity and reliability of measurements were assessed\nfrom the sample data set (n D580) collected for this study.\nAs shown in Table 2, the loadings are all within accept-\nable ranges, and all but three items for knowing-what about\nEHRs storage, knowing-what about EHRs utilization, and\nknowing-how about EHRs utilization have loadings above the\nthreshold of 0.5. All of the reliability coef\u001ccients (Cronbach's\nalphas) are above 0.80 (Table 2), con\u001crming that the mea-\nsurements are reliable. The correlations for each construct are\npresented in Table 3.\nConvergent validity was assessed by three criteria: (1)\nitem loading; (2) composite reliability; and (3) average vari-\nance extracted (A VE) [20]. The composite reliability scores\nrange from 0.579 to 0.881. Each A VE is above 0.4, but\nKHEU (Table 2), which is acceptable. We assessed discrim-\ninant validity by checking whether each item loads more\nhighly on its assigned construct than on other constructs,\nas suggested by Gefen, Straub and Boudreau [21]. Each\nitem loading in the cross-loading table is markedly higher\non its assigned construct than on the other variables. Thus,\n136226 VOLUME 7, 2019\n\n[Página 5]\nC. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics\nTABLE 2. Reliability and validity measures of the research model.\nTABLE 3. Inter-construct correlations.\nour measurements demonstrate acceptable discriminant and\nconvergent validities.\nIn addition, we assessed the potential effect of common\nmethod bias statistically by conducting Harman's one-factor\ntest [22] generated ten principal constructs; the unrotated\nfactor solution shows that the \u001crst construct explains only\n11.11% of the variance, indicating that our data do not suf-\nfer from high common method bias. Consequently, this test\nsuggest that common method bias is not a major concern for\nthis study.\nV. RESULTS\nThe results from the regression analysis are shown in Table 4.\nThe hypotheses were assessed by checking the direction and\nsigni\u001ccance of path coef\u001ccients ( \f) between dependent and\nindependent variables. Our proposed research model is a\ngood predictor of meaningful use of EHRs in the context of\nnursing department as the R2 accounts for 60.70% of the\nvariance. According to the results, we found that different\nmodes of knowledge can be used to improve nurses' effective\nuse of EHRs. For example, our \u001cnding reveals that know-\nwhat, know-how and know-why about EHRs utilization can\nlead improved meaningful use of EHRs, thus H1c, H2c, and\nH3c are supported. This implies that EHRs utilization playsTABLE 4. Standardized regression coefficients \f) with p value ( \u000b0.05).\nan important role in developing meaningful use of EHRs\npractice. In addition to the EHRs utilization, we also found\nthat if nurses know how and why EHRs are stored, they are\nmost likely to use EHRs effectively. Thus, H2b and H3b are\nsupported. Surprisingly, knowing what, how, and why about\nhow EHRs are collected does not improve meaningful use of\nEHRs, which H1a, H2a, and H3a are not supported.\nVI. THEORETICAL AND PRACTICAL CONTRIBUTIONS\nTo strategically meaningful use of EHRs, prior work has\ndeveloped many analytical approaches to effectively process\nEHRs. However, what kind of knowledge about the use of\nbig data analtyics within EHRs should be created remains\nunknown. By addressing this research gap, the theoretical and\npractical contributions of this study are three-fold. Firstly,\nour \u001cndings have partially con\u001crmed knowledge about the\nuse of big data analytics within EHRs matters for meaningful\nuse of EHRs. This is among the \u001crst study to investigate the\nuse of big data analytics within EHRs from a knowledge-\nbased view. Three mode of knowledge about the use of big\ndata analytics for EHRs are identi\u001ced and tested their impact\non improving meaningful use of EHRs practices. Based on\nour \u001cndings, healthcare organizations can make a strategic\ndecision as to which type of knowledge and big data analytics\ncomponents need to be enhanced to improve meaningful use\nof EHRs. For example, improving meaningful use of EHRs\ndoes not require nurses to understand how, why, and what\nEHRs are collected within a hospital.\nSecondly, we found meaningful use of EHRs is highly\nin\u001duenced by knowing-what, knowing-how and knowing-\nwhy about data utilization of EHRs as generally re\u001dected in\ncommon sense. It is particularly important to gain knowl-\nedge regarding why various analtyics such as descriptive\nanaltyics and predictive analytics can be used for EHRs.\nThis result is consistent with Lee and Strong's [16] \u001cnding\nwho recognizes the critical role that knowing-why plays in\nproducing high data quality. Indeed, constant increasing large\nvolume of EHRs is challenging healthcare organization's data\nmanagement capabilities [23]\u0015[26]. Needs for knowing-why\nabout data utilization of EHRs is not unique for healthcare\nVOLUME 7, 2019 136227\n\n[Página 6]\nC. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics\nTABLE 5. The items in the questionnaire and the results of EFA.\n136228 VOLUME 7, 2019\n\n[Página 7]\nC. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics\nTABLE 5. (Continued.) The items in the questionnaire and the results of EFA.\nbut more important because the results extracted from the\nanalysis of EHRs concerns patients' quality of care and well-\nbeing. A poor data utilization of EHRs may lead to issues such\nas billing errors, intentional frauds, or medical mistakes.\nThirdly, our \u001cndings show that knowledge about data col-\nlection of EHRs does not matter for improving meaningful\nuse of EHRs. A potential explanation is that in practice nurses\nare data collectors that know more about collecting accurate\nand complete healthcare records. Thus, knowledge about how\nto collect EHRs would not play an important role in improv-\ning meaningful use of EHRs. Instead, they are interested\nin knowing more about making data relevant to their daily\nclinical tasks.\nVII. CONCLUSION\nThis study has some limitations that may create interest-\ning opportunities for future research. First, this study only\ncollects data from a large hospital as the research sample.\nAlthough suf\u001ccient number of data points and high response\nrate may represent a large portion of population in a region of\nChina, there is still a need to collect the data from the different\nhospitals to better generalize our research \u001cndings. Future\nresearch may assess potential difference among age groups,\namong working experience groups, and among different clin-\nical department groups, with a more representative sample.\nSecond, future research could consider applying qualitative\nmethods to complement the general lack of adequate survey\nmethods. Third, examining the knowledge mode of EHRs\nwith linear methods does not support the comprehensive view\nrequired to capture the non-linear interaction among these\nknowledge modes [6]. Future research could consider using\nfuzzy-set Qualitative Comparative Analysis as a data analysis\napproach to better explain how different knowledge mode ofEHRs simultaneously combine to achieve meaningful use of\nEHRs.\nOur study contributes to the existing digital health, big\ndate literature and nursing literature in three ways. First, this\nresearch explores the proper adaptation of analytical tools\nto EHRs from the different knowledge mode in order to\nimprove meaningful use of big data analytics within EHRs\n[29], [30]. Second, we identi\u001ced the important the knowledge\nmodes of EHRs (e.g., know-how, know-what, and know-why\nabout EHRs utilization) that provides evidence regarding the\nways in which how training programs/course of EHRs can be\ndesigned [29]. This also extends and deepens understanding\nof how meaningful use of EHRs practices can be improved\n[30]. It could be a useful guidance for hospital practitioners,\noutlining a variety of knowledge mode of EHRs that they\ncan focus [23], [31], [32]. Third, this research proposes a\nconceptual model with a knowledge-based view to explicate\nthe different knowledge mode of EHRs in the meaningful use\nof EHRs practice for nursing professionals. To the best of our\nknowledge, as yet, no previous studies have considered the\nknowledge mode of EHRs driving meaning use of EHRs in\nthe nursing context.\nAPPENDIX\nSee Table 5.\nREFERENCES\n[1] W. Xu, Z. Guan, H. Cao, M. Lu, T. Li, and H. Zhang, ``Analysis and\nevaluation of the electronic health record standard in China: A comparison\nwith the American national standard ASTM E 1384,'' Int. J. Med. Inform. ,\nvol. 80, no. 8, pp. 555\u0015561, Aug. 2011.\n[2] A. L. Kellermann and S. S. Jones, ``What it will take to achieve the as-\nyet-unful\u001clled promises of health information technology,'' Health Affairs ,\nvol. 32, no. 1, pp. 63\u001568, Jan. 2013.\nVOLUME 7, 2019 136229\n\n[Página 8]\nC. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics\n[3] H. Lu, M. Kondo, Y. Li, J. Tan, H. Kim, S. Murakami, T. Aoki, and\nS. Kido, ``Extraction of GGO candidate regions on thoracic CT images\nusing SuperVoxel-based graph cuts for healthcare systems,'' Mobile Netw.\nAppl. , vol. 23, no. 6, pp. 1669\u00151679, Dec. 2018.\n[4] Y. He and C. Johnson, ``Challenges of information security incident learn-\ning: An industrial case study in a Chinese healthcare organization,'' Inform.\nHealth Soc. Care , vol. 42, no. 4, pp. 393\u0015408, Oct. 2017.\n[5] C. Vuppalapati, A. Ilapakurti, and S. Kedari, ``The role of big data in cre-\nating sense EHR, an integrated approach to create next generation mobile\nsensor and wearable data driven electronic health record (EHR),'' in Proc.\nIEEE 2nd Int. Conf. Big Data Comput. Service Appl. (BigDataService) ,\nOxford, U.K., Mar./Apr. 2016, pp. 293\u0015296.\n[6] Y. Wang, L. Kung, and T. A. Byrd, ``Big data analytics: Understanding its\ncapabilities and potential bene\u001cts for healthcare organizations,'' Technol.\nForecasting Social Change , vol. 126, no. 1, pp. 3\u001513, Jan. 2018.\n[7] J. L. Reardon and E. Davidson, ``An organizational learning perspective\non the assimilation of electronic medical records among small physician\npractices,'' Eur. J. Inf. Syst. , vol. 16, no. 6, pp. 681\u0015694, Dec. 2007.\n[8] V. Venkatesh, X. Zhang, and T. A. Sykes, ```Doctors do too little tech-\nnology': A longitudinal \u001celd study of an electronic healthcare system\nimplementation,'' Inf. Syst. Res. , vol. 22, no. 3, pp. 419\u0015684, Sep. 2011.\n[9] Y. Wang and T. A. Byrd, ``Business analytics-enabled decision-making\neffectiveness through knowledge absorptive capacity in health care,''\nJ. Knowl. Manage. , vol. 21, no. 3, pp. 517\u0015539, May 2017.\n[10] W. Ke and K. K. Wei, ``Organizational learning process: Its antecedents\nand consequences in enterprise system implementation,'' JGIM , vol. 14,\nno. 1, pp. 1\u001522, Jan. 2006.\n[11] R. L. Purvis, V. Sambamurthy, and R. W. Zmud, ``The assimilation of\nknowledge platforms in organizations: An empirical investigation,'' Org.\nSci., vol. 12, no. 2, pp. 117\u0015135, Apr. 2001.\n[12] R. M. Grant, ``Toward a knowledge-based theory of the \u001crm,'' Strategic\nManage. J. , vol. 17, no. 2, pp. 109\u0015122, Dec. 1996.\n[13] M. Zack, J. McKeen, and S. Singh, ``Knowledge management and orga-\nnizational performance: An exploratory analysis,'' J. Knowl. Manage. ,\nvol. 13, no. 6, pp. 392\u0015409, Oct. 2009.\n[14] R. Agarwal, G. G. Gao, C. DesRoches, and A. K. Jha, ``Research\ncommentary\u0016The digital transformation of healthcare: Current status and\nthe road ahead,'' Inf. Syst. Res. , vol. 21, no. 4, pp. 796\u0015809, Dec. 2010.\n[15] B. Kogut and U. Zander, ``Knowledge of the \u001crm, combinative capabilities,\nand the replication of technology,'' Org. Sci. , vol. 3, no. 3, pp. 383\u0015397,\nAug. 1992.\n[16] Y. W. Lee and D. M. Strong, ``Knowing-why about data processes\nand data quality,'' J. Manage. Inf. Syst. , vol. 20, no. 3, pp. 13\u001539,\nDec. 2003.\n[17] E. B. Swanson and N. C. Ramiller, ``Innovating mindfully with information\ntechnology,'' MIS Quart. , vol. 28, no. 4, pp. 553\u0015583, Dec. 2004.\n[18] J. S. Armstrong and T. S. Overton, ``Estimating nonresponse bias in mail\nsurveys,'' J. Marking Res. , vol. 14, no. 3, pp. 396\u0015402, Aug. 1977.\n[19] D. Blumenthal and M. Tavenner, ``The `Meaningful Use' regulation\nfor electronic health records,'' New England J. Med. , vol. 363, no. 6,\npp. 501\u0015504, Aug. 2010.\n[20] C. Fornell and D. F. Larcker, ``Evaluating structural equation models with\nunobservable variables and measurement error,'' J. Marketing Res. , vol. 18,\nno. 1, pp. 39\u001550, 1981.\n[21] D. Gefen, D. Straub, and M. C. Boudreau, ``Structural equation modeling\nand regression: Guidelines for research practice,'' Commun. Assoc. Inf.\nSyst., vol. 4, no. 1, p. 7, 2000.\n[22] P. M. Podsakoff, S. B. MacKenzie, J. Y. Lee, and N. P. Podsakoff,\n``Common method biases in behavioral research: A critical review of the\nliterature and recommended remedies,'' J. Appl. Psychol. , vol. 88, no. 5,\npp. 879\u0015903, 2003.\n[23] J. M. Ferranti, M. K. Langman, D. Tanaka, J. McCall, and A. Ahmad,\n``Bridging the gap: Leveraging business intelligence tools in support of\npatient safety and \u001cnancial effectiveness,'' J. Amer. Med. Inform. Assoc. ,\nvol. 17, no. 2, pp. 136\u0015143, Mar. 2010.\n[24] W. Raghupathi and V. Raghupathi, ``Big data analytics in healthcare:\nPromise and potential,'' Health Inf. Sci. Syst. , vol. 2, no. 1, p. 3,\n2014.\n[25] H. Monem, M. Afrasiabi, P. Rezvan, and S. AbediDehkordi, ``The impact\nof user quality and information quality on the IS success in health-\ncare context,'' J. Basic Appl. Sci. Res. , vol. 3, no. 10, pp. 40\u001551,\nAug. 2013.[26] M. J. Ward, K. A. Marsolo, and C. M. Froehle, ``Applications of business\nanalytics in healthcare,'' Bus. Horizons , vol. 57, no. 5, pp. 571\u0015582,\nSep./Oct. 2014.\n[27] Y. Wang, L. Kung, S. Gupta, and S. Ozdemir, ``Leveraging big data\nanalytics to improve quality of care in healthcare organizations: A con-\n\u001cgurational perspective,'' Brit. J. Manage. , vol. 30, no. 2, pp. 362\u0015388,\nApr. 2019.\n[28] X. Li, R. S. Sedeh, L. Wang, and Y. Yang, ``Patient-record level integration\nof de-identi\u001ced healthcare big databases,'' in Proc. IEEE Int. Conf. Big\nData (Big Data) , Washington, DC, USA, Dec. 2016, pp. 1784\u00151786.\n[29] L. Baillie, S. Chadwick, R. Mann, and M. Brooke-Read, ``A survey of\nstudent nurses' and midwives' experiences of learning to use electronic\nhealth record systems in practice,'' Nurse Edu. Pract. , vol. 13, no. 5,\npp. 437\u0015441, Sep. 2013.\n[30] Y. Ming and T. Zhang, ``Ef\u001ccient privacy-preserving access control scheme\nin electronic health records system,'' Sensors , vol. 18, no. 10, p. 3520,\nOct. 2018.\n[31] S. M. R. Islam, D. Kwak, M. H. Kabir, M. Hossain, and\nK.-S. Kwak, ``The Internet of Things for health care: A comprehensive\nsurvey,'' IEEE Access , vol. 3, pp. 678\u0015708, 2015.\n[32] Y. Hao, L. Peng, H. Lu, M. M. Hassan, and A. Alamri, ``Energy harvesting\nbased body area networks for smart health,'' Sensors , vol. 17, no. 7, p. 1602,\n2017.\nCAIFENG ZHANG was born in Henan, China, in\n1962. She received the master's degree in regional\neconomics from Henan University, China. She\nserves as the Associate Dean of the Kaifeng Hospi-\ntal of Traditional Chinese Medicine and also with\nthe Hospital of Henan University of Traditional\nChinese Medicine. Her research interests include\nbig data analytics in healthcare, health communi-\ncation, medical tourism, and nursing management.\nShe has published 16 articles in her research \u001celd.\nRUI MA is currently a Doctoral Researcher with\nthe Shef\u001celd University Management School, The\nUniversity of Shef\u001celd, U.K. Her current research\ninterests include big data analytics in healthcare,\nmedical tourism, sustainable development. She\nreceived the Excellent Presentation Award at the\n10th International Conference on Systematic Inno-\nvation (ICSI).\nSHIWEI SUN received the Ph.D. degree in MIS\nfrom the Harbert College of Business, Auburn\nUniversity, USA, in 2017. He is currently an\nAssistant Professor and an Associate Research\nFellow with the School of Economics and Man-\nagement, Beijing Institute of Technology, China.\nHis research interests include information tech-\nnology diffusion, innovation management, social\nmedia, and social networks. His research has\nappeared in several journals such as the Journal\nof Computer Information Systems ,Expert Systems with Applications , the\nInternational Journal of Information Management , and some other leading\nconference proceedings AMCIS, PACIS, and DSI.\n136230 VOLUME 7, 2019\n\n[Página 9]\nC. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics\nYUJIE LI received the B.S. degree in computer\nscience and technology from Yangzhou Univer-\nsity, in 2009, the M.S. degree in electrical engi-\nneering from the Kyushu Institute of Technology\nand Yangzhou University, in 2012, respectively,\nand the Ph.D. degree from the Kyushu Institute\nof Technology, in 2015. From 2016 to 2017, she\nwas a Lecturer with Yangzhou University. She\nis currently an Assistant Professor with Fukuoka\nUniversity, Japan, and also a JSPS Research Fel-\nlow with the Kyushu Institute of Technology, Japan. Her research interests\ninclude computer vision, sensors, and image segmentation.\nYICHUAN WANG received the Ph.D. degree\nin business and information systems from the\nRaymond J. Harbert College of Business, Auburn\nUniversity, USA. He is currently an Associate\nProfessor of digital marketing with The Univer-\nsity of Shef\u001celd. His research interests include\nexamining the impact of digital technologies and\ninformation systems (e.g., big data analytics, AI,\nand social media) in in\u001duencing practices in\nmarketing, healthcare management, and tourism\nmanagement.\nHe has authored or coauthored more than 50 publications, including\n5 book chapters and 30 refereed journal articles\u0016attracting in excess of\n1300 citations. His research has appeared in journals, including the IEEE\nTRANSACTIONS ON ENGINEERING MANAGEMENT , the British Journal of Man-\nagement ,Information and Management ,Annals of Tourism Research , the\nJournal of Travel Research , the Journal of Business Research ,Industrial\nMarketing Management , theInternational Journal of Production Economics ,\nTechnological Forecasting and Social Change , and Computers in Human\nBehavior , among others.\nHe received the Best Paper Award at the Global Marketing Conference and\nwas listed as a \u001cnalist for the Best Paper Award at the 20th Americas Con-\nference on Information Systems (AMCIS 2014) and the 13th International\nConference on Operations and Supply Chain Management (ICOSCM 2019).\nHe was also a recipient of the Research Excellence Award from Newcastle\nUniversity, in 2018.\nZHIJUN YAN received the Ph.D. degree from the\nBeijing Institute of Technology. His research inter-\nests include health data analytics, health care man-\nagement, digital health, and electronic commerce.\nHe is currently a Professor with the Department of\nManagement Engineering and the Associate Dean\nof the School of Management and Economics,\nBeijing Institute of Technology. He has published\nin the Journal of Management Information Sys-\ntems,Information and Management ,Information\nTechnology and People , the Journal of Electronic Commerce Research , and\nmany Chinese journals.\nVOLUME 7, 2019 136231",
      "metadata": {
        "filename": "zhang2019.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\zhang2019.pdf",
        "file_size": 9378431,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:44.842374",
        "content_length": 37011
      }
    },
    "94ed8e1d-b515-406b-96c7-a65b89c140da": {
      "id": "94ed8e1d-b515-406b-96c7-a65b89c140da",
      "content": "[Página 1]\nA Reliability Benchmark for Big Data Systems on JointCloud \nYingying Zheng1,2, Lijie Xu1, Wei Wang1,2, Wei Zhou3, Ying Ding4* \n1Institute of Software, Chinese Academy of Sciences \n2University of Chinese Academy of Sciences,  3KSYUN \n4Institute of space optoelectronic technology, Changchun Univers ity of Science and Technology\nAbstract —\u0003\u0003JointCloud provides a large-scale, flexible, and elas-\ntic computing resource platform. Big data systems such as \nMapReduce and Spark are widely deployed on this platform \nfor big data processing. How to choose a cloud platform in \naccordance with the need of customers is a problem. Current \nperformance benchmarking suites can choose suitable cloud \nplatforms for customers. However, they do not consider the \nreliability of applications running atop big data systems. Thes e \nsystems have high scalability, but the applications running \natop them often generate runtime errors, such as out of \nmemory errors, I/O exceptions, and task timeouts. For users, \nthey want to know whether the developed applications have \npotential application faults. For system designers and manag-\ners, they want to know whether the deployed/updated systems \nhave potential system faults. In addition, current benchmarks \nfor big data system are also only designed for performance \ntesting. To fill this gap, we propose a reliability benchmark, \nwhich contains representative applications, an abnormal data \ngenerator, and a configuration combination generator. Differ-\nent from performance benchmarks, this benchmark (1) gener-\nates abnormal test data according to the application character-\nistics, and (2) reduces the configuration combination space \nbased on configuration features. Currently, we implemented \nthis benchmark on Spark system. In our preliminary test, we \nfound three types of errors (i.e., out of memory errors, timeou t \nand wrong results) in five SQL, Machine Learning, and Graph \napplications. \nKeywords-reliability; benchmark; big data system; Spark; \ncloud computing; \nI.  INTRODUCTION  \nJointCloud, as a new generation of cloud computing plat-\nform, provides joint cloud services and complex calculations. \nIts pay-as-you-go computing model provides a scalable plat-\nform for big data systems. Due to high scalability, big data \nsystems such as MapReduce [ 1], Spark [ 2], and Flink [ 3] are \nnow widely deployed on cloud platforms for big data pro-\ncessing. Because of the large number of cloud providers, \nthere is a problem that how to choose a cloud platform in \naccordance with the need of customers. Previous studies [ 4, \n5] provide performance benchmarking suites to choose a \nsuitable cloud for customer. There is not a benchmark for \nchoosing cloud for reliability of applications. However, the \napplications deployed on big data systems often suffer from \nruntime errors, such as out of memory errors [ 6], I/O excep-\ntions [ 7], and task timeouts [ 8]. These errors can directly lead \nto application failures and cannot be tolerated by current \nfault-tolerant mechanisms. \n                                                           \n\u0003 * Corresponding author A big data application can be denoted as ( input data , con-\nfigurations , user code ). The input data is usually stored as \ndata blocks on distributed file system. Configurations include \nsystem-specific configurations (e.g., input block size , parti-\ntion number ) and application-specific configurations (e.g., K-\nmeans  application’s cluster k). User code refers to the user-\ndefined functions, such as map() , reduce() , and join() , which \nprocess the input or intermediate data. \nPrevious empirical studies [ 6, 7, 9, 10 ] have summarized  \nthe root causes of applications’ runtime errors: (1) applica-\ntion faults , including improper configurations, abnormal \ndata, and  user code defects. Improper configurations refer to \nlarge input  data block size , small partition number , unbal-\nanced partition  function , etc. Abnormal data refers to ex-\nceptional input/intermediate/output data, such as skewed \ndata and high dimension  data. User code defects contain \nmemory leak, high time/space  complexity, etc. (2) system \nfaults , including hardware  faults  and software faults. \nHardware faults refer to CPU, memory,  network and disk \nfailures. Software faults include logic-specific bugs, data \nraces, etc. \nFor users, they want to know whether the developed \napplications have potential application faults. For system \ndesigners  and managers, they want to know whether the \ndeployed/updated systems have potential system faults. \nTesting is a promising approach, but current  benchmarks [ 11, \n12, 13, 17 ] for big data systems are designed for perfor-\nmance  testing. Since these benchmarks use normal input \ndata and  fixed configurations, they cannot be directly \nused to detect  potential faults. \nIn this paper, we propose a reliability benchmark for \nbig data systems. To detect the potential application/system \nfaults, this benchmark g enerates abnormal input  data, and \ncombines both system-specific and application-specific \nconfigurations to test the applications. Different from  per-\nformance benchmarks, this benchmark (1) generates ab-\nnormal input data according to the application characteris-\ntics, and (2) reduces the configuration combination space \nbased on  configuration features. \nWe implemented this benchmark on Spark system. \nThis benchmark  currently contains 10 representative appli-\ncations, an abnormal  data generator, a configuration com-\nbination generator, and a test report  generator. In our pre-\nliminary test, we found five errors: (1) out of memory error \nin a SQL operation where a small table inner joins a large \ntable with skewed data; (2) wrong results in a SQL operation \nwhere a table participates in multiple join operations but not \nrenamed; (3) out of memory error in RandomForest  with \nhigh dimension data; (4) out of memory and timeout errors \nin LogisticRegression  with high dimension, and abnormal \n2017 IEEE 37th International Conference on Distributed Computing Systems Workshops\n1545-0678/17 $31.00 © 2017 IEEE\nDOI 10.1109/ICDCSW.2017.18306\n2017 IEEE 37th International Conference on Distributed Computing Systems Workshops\n2332-5666/17 $31.00 © 2017 IEEE\nDOI 10.1109/ICDCSW.2017.18306\n\n[Página 2]\ndistribution data; (5) out of memory error in PageRank ap-\nplication with large and sparse data.   \nIn summary, our main contributions are as follows: \nx A reliability benchmark is designed for big data sys-\ntems, which generates abnormal test data according \nto the application’s characteristics. \nx A greedy configuration combination method is de-\nsigned to reduce the configuration combination \nspace through analyzing the configuration independ-\nence and correlation. \nx We found three types of errors (i.e., out of memory \nerror, timeout, and wrong results) in five applica-\ntions. \nII. BENCHMARK DESIGN AND IMPLEMENTATION  \nThe reliability benchmark mainly contains four parts, as \nshown in Fig. 1: \n1) Representative applications selection: It selects \nwidely-used SQL, Machine Learing, and Graph applications.  \n2) Abnormal input data generation : It summarizes appli-\ncations’ computational characteristics and generate abnormal \ninput data according to the characteristics.  \n3) Configuration combination test : It combines system-\n/application-specific configurations and reduce the combina-\ntion space to test the applications. \n4) Test report generation : It analyzes testing results and \nreports the errors/faults. \n \nFig. 1. The modules of the reliability benchmark  \nA. Representative app lications selection  \nWe select the representative applications based on the \nfollowing criteria: (1) the application represents a basic data  \noperation or a widely-used algorithm; (2) the application has \na standard or well-tested implementation. At present, we \nselected 10 applications from previous benchmark [ 11, 12, \n13, 17 ] or Spark’s libraries such as Spark SQL, MLlib and \nGraphX library in Spark. Table I illustrates the type and \ncomputational characteristics of each selected applications. \n1) Scan: A data filter query like SELECT * FROM \nTableA WHERE columnValue > x . \n2) Aggregate: A data aggregation query like SELECT  \ncolumnA , sum ( columnB ) AS total FROM TableA GROUP \nBY columnA ORDER BY total. \n3) Join: A data join query like SELECT * FROM \nTableA INNER JOIN TableB ON A.columnA = B.columnA . \n4) Mix: A query contains data f ilter, aggreg ation and \njoin. 5) Logistic Regression: An iterative classification al-\ngorithm used to predict continuous or classified data. \nThis algorithm uses a stochastic gradient-descent or L-\nBFGS algorithm to train  the classification model. The mod-\nel parameters are calculated,  updated, and propagated in \neach iteration. \n6) K-means: an unsupervised clustering algorithm \nwhich iteratively computes the K centers. \n7) Decision Tree: a supervised classification algorithm \nwhich builds a tree to classify the data. \n8) Random Forest: Different from Decision Tree, it builds \nmultiple trees and combines them to classify the data. It uses \nrandom sampling to train each tree. \n9) PageRank: An algorithm used by Google Search to \nrank websites in their search engine results. \n10) Triangle C ount: It counts the number of different  \ntriangles in a directed or undirected graph. \nTABLE I.  REPRESENTATIVE APPLICATIONS  \nType Application  Computational  Chara c-\nteristics Abnormal \nData Cha r-\nacteristics \nSQL \nquery Scan Filter operation \nLD, SKD Aggregate Aggregated operation \nJoin Associated operation \nMix Filter,Aggregated and \nAssociated operation \nMachine \nLearning Logistic \nRegression classification algorithm, \nIterative computation LD, SD, \nHD, AD K-means Clustering Algorithm, \nIterative calculation \nDecision Tree Classification/Regressio\nn, Breadth-first tree LD , HD, \nAD Random Forest Classification/Regressio\nn, Breadth-first tree \nGraph PageRank Iterative calculation LD, SD, AD TriangleCount Iterative calculation \nB. Abnormal input data generation \nBased on the summarized computational characteristics, \nthis benchmark generates abnormal data for each application \nas shown in Table I. Abnormal data characteristics include: \nlarge data volume  ( L D ) ,  skewed data  (SKD), sparse data  \n(SD), high dimension data (HD), and abnormal distribution  \ndata (AD). How to select the abno rmal data characteristics \nfor different types of applications is summarized below. \n1) SQL Query: The Scan , Aggregate , Join applications \ndeal with key/value pairs. The computation complexity of \nthe filter, aggreated, and associated operations are related to  \nthe  key distribution, so this benchmark selects skewed data \n(i.e., generating uneven key distribution) as the abnormal \ninput data. Join application is also related to the order of the \noperations. \n2) Machine Learning: Machine Learning applications  \nsuch as Logistic Regression  a n d  K-means  take matrix-like \nfeatures as input data, so the  related data characteristics are : \n(1) matrix total size, (2) matrix dimension, (3) distribution \nof each matrix column, and (4) matrix sparsity. Other tree-\nbased applications such as Decision Tree  a n d  Random \nForest  hold breadth-first trees in memory and use random \nsampling to train the trees. When the data dimension is high, \nthe resource utilization will be high too. In addition, the \n307\n307\n\n[Página 3]\nrandom sampling method will affect the stability of the \ncomputing results.  \n3) Graph: PageRank  a n d  Triangle Count  applications \nuse vertex-centric partition. In each interation, each vertex \nneeds to send its computation resu lts to its adjacent vertices.  \nSo, the computation complexity of these applications are \nrelated to the edge distribution (i.e., the degree distribution  \nof vertices). As a result, this benchmark generates skewed \ngraph (i.e., some vertex has too many adjacent vertices) as \nthe input data. \nFig. 2 illustrates the process of generating abnormal in-\nput data for the Random Forest  application. The computa-\ntional characteristics of Random Forest  application are \nbreadth-first tree and random sampling. Then, we select the \ncorresponding data features, namely large-scale, high-\ndimensional, and abnormal distribution, to generate the ab-\nnormal input data. \n \nFig. 2. Generate abnormal data for the Random Forest applicatio n \nC. Configuration combination test \nAfter generating the input data, the next task is to com-\nbine system-specific and app lication-specific configurations \n(e.g., as shown in Table II) to test the applications. \nTABLE II.  CONFIGURATIONS OF THE RANDOM FOREST \nAPPLICATION  \nType Configuration Description \nSystem-\nspecific co n-\nfigurations Input split \nnumber Data parallelism \nPartition  \nnumber Task parallelism \nApplication-\nspecific co n-\nfigurations maxBins Maximum number of bins used for \nsplitting features \nnumClasses Number of classes for classification \nnumTrees Number of trees in the random \nforest \nmaxDepth Maximum depth of the tree \nThe main problem is that the configuration combination \nspace is too large. Suppose that an application has n config-\nurations, where the i-th configuration has mi optional values. \nSo, the combination space is O(m1*m 2*···*m n). However, if \nthe configurations satisfy the following two assumptions, \nthe combination space can be reduced to O(n). \n1. The configurations are independent of each other.  \n2. The mi values of configuration i are positively or nega-\ntively correlated with the applications’ performance \n(e.g., execution time or resource usage). \nBased on the above two assumptions, the application’s \nperformance will become worst (may trigger runtime errors) when the configurations take boundary values. Accordingly, \nwe designed a greedy algorithm (Algorithm 1) to combine \nthe configurations. \nFor example in Fig. 3 a), the application has three con-\nfigurations. At first, this algorithm chooses the low boundary \nvalue of each configuration (i.e., 2-1-1). Then it changes the \nfirst configuration combination to be the high boundary val-\nue 100. Now the configuration is 100-1-2. If the applica-\ntion’s resource utilization of (2-1-1) is less than that of (10 0-\n1-2), the algorithm will fix 100 as the first configuration. \nNext, the algorithm repeats this selection on the other two \nconfigurations as shown in Fig. 3 b), c) and d). Finally, the \napplication may generate runtime errors under the worst con-\nfiguration combination (i.e., 100-10-2) in Fig. 3 d). \nHowever, if the configurations do not satisfy the given \ntwo assumptions, this benchmark uses binary search to select \nthe worst value in each configuration. The average compu-\nting complexity is O(logm1*logm2*…* logmn). \nAlgorithm 1: Greedy configuration combination test \n1. Give the range of each configuration. \n2. Select a combination of each threshold value of each configura tion, \nthen test, and record the resource occupancy. \n3. Change the val ue of a configuration to another threshold, then test, \nand record the resource occupancy. \n4. Compare the resource usage in the last two combinations of conf ig-\nurations, and preserve the critical value of poor performance. \n5. Return to step 2, and repeat until the exception or ends of test. If an \nexception was found, the configuration was found which can caus e \nfailures. If no exception was found, the configuration with wor st \nresource usage or worst performance was found. \n \n \nFig. 3. The process of reducing configuration combination space  \nD. Test report generation \nAfter generating the abnormal data and running configu-\nration combination test, a task report generator is performed \nto analyze the application’s runtime information and gener-\nate test reports. The test reports mainly include: 1) what the \nruntime error is, 2) what abnormal data causes the runtime \nerror, 3) what the worst configurations are. \n308\n308\n\n[Página 4]\nTABLE III.  PRELIMINARY TEST RESULTS  \nApplication Input Data Configurations Errors \nJoin 10GB, skewed data Small table inner join big table OOM \nMix 10GB, skewed data A table participates in multiple join opera-\ntions but not rename it  Wrong results \nRandom Forest 1 million instances, 1000-dimensional,  \nGamma-Poisson distribution numTrees = 100, maxDepth = 30, \ndimensions = 1000 OOM \nLogistic  Regression 1.05GB sparse data with 1000 di mensions 4 executor (2 cores, 8G), split=134.13MB, \npartition number = 8 OOM, \nTimeout \nPageRank 10G data, 1 million vertices, 20 million edge  4 executor (2 cores, 8G), \nconvergence accuracy = 0.001 OOM \nIII. PRELIMINARY RESULTS  \nA. Experimental setup \nWe performed this reliability benchmark on a 10-node \ncluster (including 1 master node and 9 slave nodes) using \nSpark-2.0 on Ubuntu-11.04 Operation System. Each node \nhas 4 CPU, 16GB RAM and 2*1TB Disks. We tested each \napplication 5 times and use the mean value. \nFor SQL applications, the input table schemas (shown in \nTable IV) are as same as that used in Pavlo et al. [15]. How-\never, the input data of all the applications are generated by \nabnormal input data generator. \nTABLE IV.  TABLE SCHEMAS  \nTable name Column name Data type \nRankings pageURL  VARCHAR \npageRank  INT \navgDuration  INT  \nUserVisits  sourceIP  VARCHAR \ndestURL  VARCHAR \nvisitDate  DATE \nadRevenue  FLOAT \nuserAgent VARCHAR \ncountryCode  VARCHAR \nlanguageCode  VARCHAR \nsearchWord  VARCHAR \nduration  INT  \nB. Results  \nThe preliminary results are shown in Table III. We found \nthree types of errors (i.e., out of memory (OOM), timeout \nand wrong results) in five applications. \nC. Case studies \n1) SQL Join \nWhen testing the Join query in Spark SQL, this bench-\nmark generates both normal data and abnormal data (skewed \ndata) for each table shown in Table IV. Since the Join opera-\ntion is a binary operation, the Join order can be changed. So, \nthe Join query has two sub-queries as shown in Table V. \nBigSmallJoin  denotes Uservisits  (large table) inner join \nRankings  (small table), while SmallBigJoin  denotes Rank-\nings (small table) inner join Uservisits (large table). Table IV \nshows the results of the two Join operations. Out of memory \nerror occurs in the second SmallBigJoin , where a small table \ninner joins a large table with skewed data. The execution \ntime of the parallel tasks in BigSmallJoin  and SmallBigJoin  \napplications are shown in Fig. 4. \n  \nWhen a given data set is skewed, the number of pro-\ncessed records on a certain task increases significantly. The \nreason is that when the same key has too many values, these \nvalues will be pushed to the same task in shuffle phase. In \nthis situation, the execution time of this task is far longer \nthan that of the other tasks. By analyzing the inner join im-\nplementation in Spark, we found that: when two tables inner \njoin each other, the first table is considered as a driven tabl e, \nand the second table is considered as a buffer table. It will \ntraverse each record in the drive table, look for the corre-\nsponding matching records in the buffer table, and put rec-\nords into the matching table. So when we consider a large \ntable as a buffer table, the matching records will be huge. If \nthere is a large table with a seriously skewed data, the match-\ning table will occupy much more memory, and out of \nmemory error will occur when we query the relevant key. \nTABLE V.  TEST RESULTS OF JOIN QUERY  \nSQL Type Data type Execution Time \nBigSmallJoin Normal data ˄large table ˅ 51s \nSkewed data ˄small table ˅ 59s \nSmallBigJoin Normal data ˄small table ˅ 56s \nSkewed data ˄large table ˅ Failed \n \nFig. 4. Comparison of normal and skew data  \n2) Random Forest Application \nThe configurations of the Random Forest  application are \nshown in Table II. The generated abnormal data is 23.7GB \nwith 104 dimensions. The data distribution is Gaussian dis-\ntribution. The test results are shown in Table VI. Configura-\ntions in group A are the initial values. Group B changes the \nconfiguration numTrees  f rom  2 to  1 00 .  A f ter th at , th e  c on -\nfiguration combination test found that the time and GC time \nincreased significantly. Therefore, the configuration combi-\nnation algorithm keeps the configuration numTrees  to be 100 \nin group C. The next test is to change maxDepth  to be 100. \nOut of memory error occurs in group C. If we continue test-\ning using the configurations in group D, out of memory error \nwill also occur. So, the worst configuration combination is \n309\n309\n\n[Página 5]\n100-5-32-10. However, for the configurations in group C and \nD, the out of memory errors will disappear if the data distri-\nbution is changed to the uniform distribution. It indicates tha t \nthe application has potential faults while processing the data \nwith Gaussian distribution. \nTABLE VI.  TEST RESULTS OF RANDOM FOREST  \nConfigurations  A B C D \nnumTrees 2 100 100 100 \nmaxDepth 5 5 100 5 \nmaxBins 5 5 5 32 \nPartition num 10 10 10 10 \nRunning time 6.4min 41min OOM OOM \nIV. RELATED WORK  \nThe reliability of big data applications/systems has \nemerged as a critical problem for both academia and indus-\ntry. Many researchers have performed empirical studies on \nbig data application/system failures. However, the current \nbenchmarks are not designed for reliability testing. \nFailure study on big data  applications/systems: Li et \nal. [9] studied 250 failures in SCOPE jobs in Microsoft big \ndata platform, and found 84.5% failures are caused by de-\nfects in data processing. They also found 3 OOM errors that \nare caused by accumulating large data (e.g, all input rows) \nin memory. Xu et al . [6]  s t u d i e d  1 2 3  O O M  e r r o r s  i n  r e a l -\nworld Hadoop/Spark applications and found three causes of \nout of memory errors: improper configurations, abnormal \ndataflow and memory-consuming user code. Kavulya et al. \n[7] analyzed 4100 failed Hadoop jobs, and found 36% fail-\nures are array indexing errors and 23% failures are IOEx-\nceptions. Zhou et al . [16] studied the quality issues of big \ndata platform in Microsoft. They found 36% issues are \ncaused by system side defects and 2 issues (1%) are \nm e m o r y  i s s u e s .  G u n a w i  et al . [10] studied 3655 develop-\nment and deployment issues in cloud systems such as Ha-\ndoop MapReduce, HDFS, and HBase. They found 87% is-\nsues are software faults, while 13% issues are hardware \nfaults. They also reported 1 OOM error in HBase (users \nsubmit queries on large data sets) and 1 OOM error in Ha-\ndoop File System (users create thousands of small files in \nparallel). These studies help us design the abnormal data \ngenerator and configuration generator. \nBig data benchmarks:  Pavlo [ 15] designed a big SQL \nbenchmark to compare the performance between MapRe-\nduce and relational databases. Berkeley AMPLab developed \na SQL benchmark [ 12] to compare the performance among \nSpark, Hive, Impala, etc. HiBench [ 13] is designed to test the \nperformance of Hadoop and Spark. BigDataBench [ 17] in-\ncludes 14 real-world data sets, and 34 big data workloads. \nThese benchmarks use normal data and fixed configurations \nto test the performance of big data systems. \nV. CONCLUSION AND FUTURE WORK \nBig data applications deployed on the cloud platform \nfrequently suffer from runtime errors. However, current \nbenchmarks are designed for performance testing and can-\nnot be directly used for detecting potential faults. In this \npaper, we design a reliability benchmark for big data sys-tems and implement it on Spark. This benchmark first gen-\nerates abnormal input data according to the application \ncharacteristics, and then uses g reedy algorithm to combine \nsystem-/application-specific conf igurations for testing. Pre-\nliminary results show that this benchmark can detect appli-\ncation faults. In the future, we will build more applications \ninto the benchmark and implement this benchmark on more \nsystems such as Flink.  \nACKNOWLEDGMENT  \nThis work was supported by the National Key Research \nand Development Program of China (2016YFB1000103) and \nYouth Innovation Promotion Association, CAS (No. \n2015088). \nREFERENCES  \n[1] J. Dean and S. Ghemawat, “Mapreduce: Simplified data processing  \non large clusters,” in 6th Symposium on Operating System \nDesign and  Implementation (OSDI) , 2004, pp. 137–150. \n[2] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly, M . \nJ. Franklin, S. Shenker, and I. Stoica, “Resilient distributed \ndatasets: A  fault-tolerant abstraction for in-memory cluster \ncomputing,” in NSDI , 2012, pp. 15–28. \n[3] “Apache Flink.” [Online]. Available: https://flink.apache.org/.  \n[4] A. Li, et al. \"CloudCmp: comparing public cloud providers,\" in \nProceedings of the 10th ACM SIGCOMM conference on Internet \nmeasurement (SIGCOMM). ,2010. \n[5] Lenk, Alexander, et al. \"What are you paying for? performance \nbenchmarking for infrastructure-as-a-service offerings.\" Cloud \nComputing (CLOUD), 2011 IEEE International Conference on. IEEE, \n2011. \n[6] L. Xu, W. Dou, F. Zhu, C. Gao, J. Liu, H. Zhong, and J. Wei, \n“Experi-  ence report: A characteristic study on out of memory errors \nin distributed  data-parallel applications,” in 26th IEEE \nInternational Symposium on  Software Reliability Engineering \n(ISSRE) , 2015, pp. 518–529. \n[7] S. Kavulya, J. Tan, R. Gandhi, and P. Narasimhan, “An analysis of \ntraces  from a production mapreduce cluster,” in 10th IEEE/ACM \nInternational  Conference on Cluster, Cloud and Grid Computing \n(CCGrid) , 2010. \n[8] “Spark     reduce      operation      taking      too      long .”  [On-  \nline]. Available: http://stackoverflow.com/questions/33558593/   \nspark-reduce-operation-taking-too-long.  \n[9] S. Li, H. Zhou, H. Lin, T. Xiao, H. Lin, W. Lin, and T. Xie, “A  \ncharacteristic study on failures of production distributed data -\nparallel  programs,” in 35th International Conference on Software \nEngineering  (ICSE) , 2013, pp. 963–972. \n[10] H. S. Gunawi, M. Hao, T. Leesatapornwongsa, T. Patana-anake, T.  \nDo, J. Adityatama, K. J. Eliazar, A. Laksono, J. F. Lukman, V. Martin, \nand A. D. Satria, “What bugs live in the cloud? A study of 3000 + \nissues in cloud systems,” in Proceedings of the ACM Symposium on \nCloud Computing (SoCC) , 2014, pp. 7:1– 7:14. \n[11] “Spark Performance Tests.” [Online]. Available: https://github.com/   \ndatabricks/spark-perf.  \n[12] “ S p a r k  S Q L  B e n c h m a r k . ”  [ O n l i n e ] .  A v a i l a b l e :  https://amplab.cs.   \nberkeley.edu/benchmark/.  \n[13] “HiBench: the bigdata micro benchmark suite.” [Online]. Availab le:  \nhttps://github.com/intel-hadoop/HiBench.  \n[14] M.  Armbrust,  et al.  “Spark SQL: relational data processing in \nspark,” in Proceedings of  the 2015 ACM SIGMOD International \nConference on Management of  Data (SIGMOD) , 2015. \n[15] Pavlo, Andrew, et al. \"A comparison of approaches to large-scal e data \nanalysis.\" in Proceedings of the 2009 ACM SIGMOD International \nConference on Management of Data (SIGMOD) , 2009. \n[16] H .  Z h o u ,  J . - G .  L o u ,  H .  Z h a n g ,  H .  L i n ,  H .  L i n ,  a n d  T .  Q i n ,  “ A n  \nempirical study on quality issues of production big data platfo rm,” in \nICSE , 2015. \n[17] L. Wang, et al, “Bigdatabench: A big data benchmark suite from \ninternet services,” in HPCA , 2014. \n \n310\n310",
      "metadata": {
        "filename": "zheng2017.pdf",
        "file_path": "C:\\Users\\Icaro\\Documents\\projetos-google-cli\\data-quality-chatbot\\docs_to_import\\RSL-Daase2024\\zheng2017.pdf",
        "file_size": 412899,
        "file_type": ".pdf",
        "imported_at": "2025-12-17T21:23:45.087579",
        "content_length": 27243
      }
    },
    "00253a0a-b357-4fd1-808b-956aaa892e73": {
      "content": "This is a test document about data quality testing.",
      "metadata": {
        "filename": "test.txt",
        "size": 51
      },
      "id": "00253a0a-b357-4fd1-808b-956aaa892e73"
    },
    "cc081639-d2f6-4f15-8b61-4806dffc3e0b": {
      "content": "Data quality is essential for accurate analytics and reporting.",
      "metadata": {
        "filename": "quality.txt",
        "size": 63
      },
      "id": "cc081639-d2f6-4f15-8b61-4806dffc3e0b"
    },
    "6da978c2-403a-4cce-a412-6a45c6091397": {
      "content": "\n        Data quality testing ensures that data meets the required standards.\n        It includes validation of accuracy, completeness, consistency, and reliability.\n        Testing should be performed regularly to maintain data integrity.\n        ",
      "metadata": {
        "filename": "test_guide.txt",
        "size": 248
      },
      "id": "6da978c2-403a-4cce-a412-6a45c6091397"
    },
    "b3ba56fe-df43-4648-abda-15f1454eafd2": {
      "content": "\n        Data quality testing ensures that data meets the required standards.\n        It includes validation of accuracy, completeness, consistency, and reliability.\n        Testing should be performed regularly to maintain data integrity.\n        ",
      "metadata": {
        "filename": "test_guide.txt",
        "size": 248
      },
      "id": "b3ba56fe-df43-4648-abda-15f1454eafd2"
    },
    "2c83d205-e2c9-416a-8a1d-9b0a4528e979": {
      "content": "\n        Data quality testing ensures that data meets the required standards.\n        It includes validation of accuracy, completeness, consistency, and reliability.\n        Testing should be performed regularly to maintain data integrity.\n        ",
      "metadata": {
        "filename": "test_guide.txt",
        "size": 248
      },
      "id": "2c83d205-e2c9-416a-8a1d-9b0a4528e979"
    },
    "d35b90d1-1cf6-45c6-969f-2854cbf20029": {
      "content": "\n        Data quality testing ensures that data meets the required standards.\n        It includes validation of accuracy, completeness, consistency, and reliability.\n        Testing should be performed regularly to maintain data integrity.\n        ",
      "metadata": {
        "filename": "test_guide.txt",
        "size": 248
      },
      "id": "d35b90d1-1cf6-45c6-969f-2854cbf20029"
    },
    "81c441fb-2d00-42d2-b3b0-c40f3c09703e": {
      "content": "Test content about data validation",
      "metadata": {
        "filename": "test.txt"
      },
      "id": "81c441fb-2d00-42d2-b3b0-c40f3c09703e"
    },
    "0e80936e-137c-477a-a883-8955c12fa14b": {
      "content": "Persistence test content",
      "metadata": {
        "filename": "persist.txt"
      },
      "id": "0e80936e-137c-477a-a883-8955c12fa14b"
    },
    "3416ba8e-6c9a-414a-b381-b15d61798675": {
      "content": "This is a test document about data quality testing.",
      "metadata": {
        "filename": "test.txt",
        "size": 51
      },
      "id": "3416ba8e-6c9a-414a-b381-b15d61798675"
    },
    "97214baf-18f4-4c83-8008-3664a9548301": {
      "content": "Data quality is essential for accurate analytics and reporting.",
      "metadata": {
        "filename": "quality.txt",
        "size": 63
      },
      "id": "97214baf-18f4-4c83-8008-3664a9548301"
    },
    "ff649f97-f509-44be-9238-2207a3f99849": {
      "content": "\n        Data quality testing ensures that data meets the required standards.\n        It includes validation of accuracy, completeness, consistency, and reliability.\n        Testing should be performed regularly to maintain data integrity.\n        ",
      "metadata": {
        "filename": "test_guide.txt",
        "size": 248
      },
      "id": "ff649f97-f509-44be-9238-2207a3f99849"
    },
    "2f8ae8f7-230f-4cfa-8f01-5410ece5b9f6": {
      "content": "\n        Data quality testing ensures that data meets the required standards.\n        It includes validation of accuracy, completeness, consistency, and reliability.\n        Testing should be performed regularly to maintain data integrity.\n        ",
      "metadata": {
        "filename": "test_guide.txt",
        "size": 248
      },
      "id": "2f8ae8f7-230f-4cfa-8f01-5410ece5b9f6"
    },
    "8d168bd2-4708-41c7-ada3-056769ffd527": {
      "content": "\n        Data quality testing ensures that data meets the required standards.\n        It includes validation of accuracy, completeness, consistency, and reliability.\n        Testing should be performed regularly to maintain data integrity.\n        ",
      "metadata": {
        "filename": "test_guide.txt",
        "size": 248
      },
      "id": "8d168bd2-4708-41c7-ada3-056769ffd527"
    },
    "9e63d88e-d5dd-4429-9826-e4f2f210d968": {
      "content": "\n        Data quality testing ensures that data meets the required standards.\n        It includes validation of accuracy, completeness, consistency, and reliability.\n        Testing should be performed regularly to maintain data integrity.\n        ",
      "metadata": {
        "filename": "test_guide.txt",
        "size": 248
      },
      "id": "9e63d88e-d5dd-4429-9826-e4f2f210d968"
    },
    "f903b1e9-e47e-4d17-844f-f656effb52b0": {
      "content": "Test content about data validation",
      "metadata": {
        "filename": "test.txt"
      },
      "id": "f903b1e9-e47e-4d17-844f-f656effb52b0"
    },
    "b9ea5edc-6448-4976-acb5-b547128a6674": {
      "content": "Persistence test content",
      "metadata": {
        "filename": "persist.txt"
      },
      "id": "b9ea5edc-6448-4976-acb5-b547128a6674"
    }
  },
  "chunks": {
    "328a07f5-149e-46ee-b0de-0163aedfcde3": [
      {
        "text": "Apache Spark Best Practices for Data Quality Introduction: Apache Spark is a powerful distributed computing framework that excels at processing large datasets. When implementing data quality checks, following best practices ensures optimal performance and reliable results. Key Principles: 1. Partition Strategy - Use appropriate partitioning to avoid skewed data - Consider partition size (aim for 128MB-1GB per partition) - Use repartition() vs coalesce() appropriately 2. Caching Strategy - Cache DataFrames that are accessed multiple times - Use appropriate storage levels (MEMORY_AND_DISK_SER) - Unpersist when no longer needed 3. Data Quality Patterns - Implement schema validation early in the pipeline - Use built-in Spark functions for better performance - Avoid collect() on large datasets 4. Error Handling - Implement graceful degradation for data quality issues - Log quality metrics",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "- Implement graceful degradation for data quality issues - Log quality metrics for monitoring - Use checkpoints for long-running processes 5. Resource Management - Configure executor memory and cores appropriately - Monitor garbage collection patterns - Use dynamic allocation when possible Example Code Patterns: def validate_data_quality(df): # Count nulls efficiently null_counts = df.select([ sum(col(c).isNull().cast(\"int\")).alias(f\"{c}_nulls\") for c in df.columns ]).collect()[0] # Check for duplicates total_rows = df.count() distinct_rows = df.distinct().count() duplicate_rate = (total_rows - distinct_rows) / total_rows return { 'null_counts': null_counts.asDict(), 'duplicate_rate': duplicate_rate, 'total_rows': total_rows } Performance Tips: - Use broadcast joins for small lookup tables - Prefer DataFrames over RDDs for better optimization - Use columnar formats like Parquet for better I/O - Enable adaptive query execution (AQE) in Spark 3.0+",
        "start_idx": 116,
        "end_idx": 237
      },
      {
        "text": "execution (AQE) in Spark 3.0+",
        "start_idx": 232,
        "end_idx": 237
      }
    ],
    "bcd1c4e9-600f-4286-8599-481901d79c71": [
      {
        "text": "# Estratégias de Validação de Dados ## Validação em Tempo Real vs Batch ### Validação Batch - Processa grandes volumes de dados históricos - Permite análises complexas e estatísticas - Ideal para relatórios e auditorias - Menos recursos em tempo real ### Validação em Tempo Real - Verifica dados conforme chegam - Permite correção imediata - Requer mais recursos de infraestrutura - Crítica para sistemas transacionais ## Implementação com Kafka + Spark Streaming ```python from pyspark.streaming import StreamingContext from pyspark.sql import SparkSession from pyspark.sql.functions import * def process_streaming_data(df, epoch_id): # Validações em tempo real # 1. Schema validation expected_schema = [\"id\", \"timestamp\", \"value\", \"status\"] if set(df.columns) != set(expected_schema): raise ValueError(\"Schema mismatch detected\") # 2. Business rules validation invalid_records = df.filter( (col(\"value\") < 0) | (col(\"value\") > 1000) |",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "validation invalid_records = df.filter( (col(\"value\") < 0) | (col(\"value\") > 1000) | (col(\"status\").isNull()) ) invalid_count = invalid_records.count() if invalid_count > 0: print(f\"Warning: {invalid_count} invalid records found\") # Log para sistema de monitoramento # 3. Store valid records valid_records = df.subtract(invalid_records) valid_records.write.mode(\"append\").saveAsTable(\"clean_data\") # Setup streaming spark = SparkSession.builder.appName(\"DataQualityStream\").getOrCreate() df = spark.readStream.format(\"kafka\") \\ .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\ .option(\"subscribe\", \"data-topic\") \\ .load() query = df.writeStream \\ .foreachBatch(process_streaming_data) \\ .start() query.awaitTermination() ``` ## Padrões de Quarentena ### Isolamento de Dados Problemáticos 1. **Quarentena Automática**: Dados que falham validação básica 2. **Quarentena Manual**: Dados suspeitos para revisão humana 3. **Quarentena Temporária**: Dados aguardando informações adicionais ### Estrutura de Quarentena ``` quarantine/ ├── schema_errors/ ├── business_rule_violations/ ├── data_quality_issues/ └── pending_review/ ``` ## Métricas de Qualidade - **Completude**: % de campos preenchidos - **Validade**: % de",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "de Qualidade - **Completude**: % de campos preenchidos - **Validade**: % de dados em formato correto - **Consistência**: % de dados consistentes entre fontes - **Precisão**: % de dados corretos - **Atualidade**: % de dados recentes",
        "start_idx": 232,
        "end_idx": 268
      }
    ],
    "1f4646b3-0d61-4f89-94b5-2c5e534ce81c": [
      {
        "text": "# Guia de Testes de Performance ## Introdução Testes de performance são essenciais para garantir que aplicações de dados funcionem adequadamente sob carga. Este documento aborda estratégias e técnicas para testar sistemas de big data. ## Tipos de Testes de Performance ### 1. Testes de Carga - Verificar comportamento sob carga normal - Identificar limites de capacidade - Monitorar tempo de resposta e throughput ### 2. Testes de Stress - Testar além dos limites normais - Identificar ponto de quebra do sistema - Verificar recuperação após sobrecarga ### 3. Testes de Volume - Grandes volumes de dados - Avaliar escalabilidade - Testar limites de armazenamento ## PySpark para Performance ```python from pyspark.sql import SparkSession from pyspark.sql.functions import * # Configuração otimizada spark = SparkSession.builder \\ .appName(\"PerformanceTest\") \\ .config(\"spark.sql.adaptive.enabled\",",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "import * # Configuração otimizada spark = SparkSession.builder \\ .appName(\"PerformanceTest\") \\ .config(\"spark.sql.adaptive.enabled\", \"true\") \\ .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\ .config(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\ .getOrCreate() # Monitoramento de performance def monitor_query_performance(df, query_name): start_time = time.time() result = df.count() # ou qualquer operação end_time = time.time() print(f\"Query: {query_name}\") print(f\"Tempo: {end_time - start_time:.2f}s\") print(f\"Registros: {result}\") return result ``` ## Métricas Importantes - **Latência**: Tempo de resposta individual - **Throughput**: Operações por segundo - **Utilização de CPU**: Percentual de uso - **Memória**: Consumo e garbage collection - **I/O**: Leitura/escrita de dados ## Ferramentas de Monitoramento - Spark UI para análise de jobs - Ganglia para métricas de cluster - Grafana para dashboards - JProfiler para análise de JVM",
        "start_idx": 116,
        "end_idx": 227
      }
    ],
    "877f4762-0d7c-4c31-ab5f-f50d0487235e": [
      {
        "text": "[Página 1] Advancing beyond technicism when managing big data in companies ’decision-making Francesco Caputo, Barbara Keller, Michael Möhring, Luca Carrubbo andRainer Schmidt Abstract Purpose –In recognising the key role of business intelligence and big data analytics in influencing companies’ decision-making processes, this paper aims to codify the main phases through which companies can approach, develop and manage big data analytics. Design/methodology/approach –By adopting a research strategy based on case studies, this paper depicts the main phases and challenges that companies ‘‘live’’ through in approaching big data analytics as a way to support their decision-making processes. The analysis of case studies has been chosen as the main research method because it offers the possibility for different data sources to describe aphenomenon and subsequently to develop and test theories. Findings",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "sources to describe aphenomenon and subsequently to develop and test theories. Findings –This paper provides a possible depiction of the main phases and challenges through which the approach(es) to big data analytics can emerge and evolve over time with reference to companies’decision-making processes. Research limitations/implications –This paper recalls the attention of researchers in defining clear patterns through which technology-based approaches should be developed. In its depiction of the main phases of the development of big data analytics in companies’ decision-making processes, this paper highlights the possible domains in which to define and renovate approaches to value. The proposed conceptual model derives from the adoption of an inductive approach. Despite its validity, it is discussedand questioned through multiple case studies. In addition, its generalisability requires further discussion and analysis",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "multiple case studies. In addition, its generalisability requires further discussion and analysis in the light of alternative interpretative perspectives. Practical implications –The reflections herein offer practitioners interested in company management the possibility to develop performance measurement tools that can evaluate how each phase can contribute to companies’ value creation processes. Originality/value –This paper contributes to the ongoing debate about the role of digital technologies in influencing managerial and social models. This paper provides a conceptual model that is able to support both researchers and practitioners in understanding through which phases big data analytics can be approached and managed to enhance value processes. Keywords Big data, Big data analytics, Companies’ decision-making, Smarter management Paper type Technical paper 1. Preliminary reflections In the past few decades, socio-economic configurations have profoundly",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "1. Preliminary reflections In the past few decades, socio-economic configurations have profoundly changed because of the increasing use and accessibility of Information and Communication Technologies (ICT) in multiple domains of everyday life ( Forester, 1987 ;Turban et al. ,1 9 9 8 ;Drucker, 2011 ; Caputo et al. , 2019b ). Consolidated views based on the representation of technologies for data management as “simple instruments” for supporting decision-making activities have progressively shown that they are incapable of explaining ongoing dynamics and trends (Caputo et al. , 2019c ). Similarly, new interpretative approaches and managerial models are strongly required by researchers and practitioners interested in effectively understandingFrancesco Caputo is based at the Department of Economics,Management and Institutions,University of Naples Federico II, Naples, Italy. Barbara Keller is based at theDuale",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "of Naples Federico II, Naples, Italy. Barbara Keller is based at theDuale Hochschule Baden-Wu¨rttemberg Stuttgart, Stuttgart, Germany.Michael Mo ¨hring is based at the Department of Informatics – HHZ Reutlingen University,Reutlingen, Germany.Luca Carrubbo is based at theDepartment of Managementand Innovation Systems,University of Salerno, Salerno,Italy. Rainer Schmidt is based at the Department of ComputerScience and Mathematics,University of Applied SciencesMunich, Munich, Germany. Received 8 October 2022 Revised 26 January 2023Accepted 25 February 2023 Corrigendum : It has come to the attention of the publisher that the article: Caputo, F., Keller, B., Mo ¨hring, M., Carrubbo, L. and Schmidt, R.(2023), “Advancing beyondtechnicism when managing bigdata in companies’ decision-making”, Journal of Knowledge Management , Vol. ahead-of- print No. ahead-of-print. https:// doi.org/10.1108/JKM-10-2022- 0794 did not accurately display Mo¨hring, M.‘s affiliation. Our guidelines state that",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "0794 did not accurately display Mo¨hring, M.‘s affiliation. Our guidelines state that affiliations should be supplied in full when the article issubmitted. The city corresponding to Reutlingen University has been amended from Munich toReutlingen. DOI10.1108/JKM-10-2022-0794 VOL. 27 NO. 10 2023, pp. 2797-2809, ©Emerald Publishing Limited, ISSN 1367-3270 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2797 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 2] what the main implications, consequences and effects of the increasing use of ICT in business and social dynamics are ( Castells, 1999 ;Markus and Topi, 2015 ). [2015] Building upon this widely recognised need, in recent decades, a challenging debate has emerged around the topic of big data analytics as “a way of extracting value from thesehuge volumes of information, and it drives",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "way of extracting value from thesehuge volumes of information, and it drives new market opportunities and maximizes customer retention” ( Zakir et al. ,2 0 1 5 , p. 81). Several contributions have been provided with reference to the multiple advantages that it is possible to obtain for companies from a“new” approach in the collection, coding and management of data related to the multiple dimensions of shopping expeditions and evaluations ( Griffin et al. ,2 0 0 0 ;Mummalaneni, 2005 ;Demangeot and Broderick, 2006 ;Amendola et al. , 2018 ;Ardito et al. ,2 0 1 8 ). Multiple stimuli for reflections have also been provided with reference to the ways in which people,processes and technologies can be combined to improve the quality of companies’ and markets’ approaches in",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "be combined to improve the quality of companies’ and markets’ approaches in data collection and use ( Alter, 2006 ;Singh and Del Giudice, 2019 ). As effectively summarised by Demchenko et al. (2012 , p. 614), “Data Science is becoming a new technology driver and requires re-thinking a number of infrastructure, components,solutions and processes to address the following general challenges: Exponential growth of data volume produced by different research instruments and/or collected from sensors; Need to consolidate e-Infrastructure as [a] persistent research platform to ensure researchcontinuity and oration, deliver/offer persistent services, with [an] adequate governancemodel.” According to the authors’ reflections, the challenging domain about big data should mainly refer to the infrastructure and processes required for ensuring the effective collection and organisation of a huge volume of data.",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "ensuring the effective collection and organisation of a huge volume of data. Despite the relevance of the aforementioned dimensions, it only represents a “small” part of the multiple reflections that seem to require the ongoing transitions towards a knowledge era based on technology infrastructure. Several relevant elements related to human approaches to big data, the consequences of big data analytics in companies’ decision-making processes and the antecedents capable of addressing the ongoing digital transition( Caputo et al. , 2019a ;Chinnaswamy et al. ,2 0 1 8 ), among others, seem to be vastly underestimated. Accordingly, the paper proposes extending current perspectives in the study of big data analytics by focusing attention on the intriguing domain of big dataanalytics, specifically “the extraction of hidden sight about consumer behaviour from",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "big dataanalytics, specifically “the extraction of hidden sight about consumer behaviour from bigdata and the exploitation of that insight through advantageous interpretation” ( Erevelles et al. ,2 0 1 6 , p. 897). Thanks to the adoption of a research strategy based on case studies, the paper aims to depict the main phases that companies face in the process of reshapingdecision-making processes through big data analytics. The analysis of case studies has been chosen as the main research method because it offers the possibility for different data sources to describe a phenomenon and subsequently to develop and test theories. The paper is structured as follows. In Section 2, the theoretical background will be presented by focusing attention on smart management and on the role of big data analytics",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "attention on smart management and on the role of big data analytics in companies’ decision-making processes as relevant domains with reference to which proposed reflections have been developed. In Section 3, the method and data collection ofthe proposed research will be reported, whilst in Section 4, the results of the proposedresearch will be summarised to enrich the current debate about the role of big data analytics in reshaping companies’ decision-making processes. Finally, in Section 5, the study’s preliminary conclusions, main limitations, implications and possible future directionswill be presented. 2. Theoretical background The way in which organisations apply data analysis has changed over time ( Chen et al. , 2012 ). In recent years, different methods have been developed that depend on the different data sources and related data",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "been developed that depend on the different data sources and related data structures. PAGE 2798jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 3] In general, different data sources with structured and/or unstructured data can be part of big data projects ( Gandomi and Haider, 2015 ). In the past, enterprises were only able to analyse structured datasets like customer order data coming from, for example, CRM orERP systems ( Chen et al. , 2012 ). The data used for analyses mainly consisted of numbers or categorial variables, for example. The way of collecting, storing and analysing data was less complex in comparison to more recent data sources containing unstructured data( Buneman et al. , 1997 ;Blumberg",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "recent data sources containing unstructured data( Buneman et al. , 1997 ;Blumberg and Atre, 2003 ;Baker and Thien, 2020 ;Del Giudice et al. , 2021 ). Today, however, up to 90% of the collected data is unstructured data like texts, images, audio and video ( Harbart, 2021 ). The analysis of unstructured data is currently challenging organisations because of its unsuitability for use in conventional data models( Harbart, 2021 ). The use of unstructured data together with structured data is manifold. For instance, it can be used to improve the quality and the possibilities of prediction within big data analytics ( Davenport et al. , 2021 ). Nevertheless, the more data types are included in analytical projects, the more different methods must be used. Today, more and more",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "projects, the more different methods must be used. Today, more and more IoT-related data sources like connected home appliances ( Bayer et al. , 2020 ) or services like Google Popular times ( Mo¨hring et al. ,2 0 2 0 ) can be used to predict and better understand customer behaviour. These new data sources must be integrated within the analytical landscape to be used in related analysis. Another interesting use case that highlights thechallenges of the benefits of big data analytics is product returns in e-commerce. This fieldis even more important because it meets both customer behaviour and the sustainability concept, as well as helping to easily understand the facets appearing in big data analysis. For instance, if an organisation wants to use online customer reviews (unstructured",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "For instance, if an organisation wants to use online customer reviews (unstructured textualdata) to predict the product returns probability ( Schmidt and Mo ¨hring, 2013 ;Mo¨hring et al. , 2013 ), past customer order data from the CRM and ERP system (structured data) as well as images (unstructured image data) from offered goods should also be integrated into the analysis to enhance the quality of the prediction. Therefore, they must apply differentmethods like text mining for textual data, image pattern recognition for images and traditional data mining techniques like regression or correlation analysis. In turn, this means that different results, various key figures and quality criteria must be aggregated andharmonised within one comprehensive result ( Kaur et al. , 2019 ). Furthermore, the data must be stored in",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "et al. , 2019 ). Furthermore, the data must be stored in different locations like relational databases for the order data and/or within NoSql databases ( Stonebraker, 2010 ) like document-based databases for textual data. In sum, all these requirements will increase the complexity of big data analytics projects and generate challenges for organisations running an analytical project. In line with the identified methodological complexity and storing issues, thecomputational complexity also increases. The more variables are included in analyticalapproaches, the more steps for information processing and result calculation are necessary. Therefore, organisations that are considering applying big data analytics must explore the option of scalable public cloud computing services at major sites like AmazonAWS, Microsoft Azure and Google Cloud to capture the limitations of traditional non-scalable systems (",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "and Google Cloud to capture the limitations of traditional non-scalable systems ( Schmidt and Mo ¨hring, 2013 ). 2.1 Challenges and dynamics of smart management Nowadays, the dynamics in decision-making in all contexts are increasingly guided and conditioned by the reception, filtering, processing and use of data ( Raisinghani, 2000 ). The evolution of new technologies favours the development of virtuous processes [thanks to bigdata analytics techniques, data mining, machine learning, artificial intelligence (AI), etc.] that support decision-making processes ( Nutt, 2008 ;Yang et al. , 2019 ). The growing uncertainty in all application areas accentuates the importance of the way in whichdecisions are made, especially if they involve significant consequences for the community.Decision making is a multidisciplinary topic that lends itself to different levels of analysis VOL.",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "a multidisciplinary topic that lends itself to different levels of analysis VOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2799 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 4] when we focus on the various elements (including technological ones) that condition or facilitate it ( Papadakis et al. ,1 9 9 8 ). Decision-making processes are increasingly data-driven. Therefore, the decisions are more “informed” because the exchange of information is rapid (often in real-time); hence, it canbe precise, punctual, efficient and valid. From the electronic medical record (now fully operational) to the development of information systems to new communication protocols, it is possible to record a continuous flow of data, information (and the contained in it) and the inputs to be filtered, processed,",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "(and the contained in it) and the inputs to be filtered, processed, used and managed in a timely manner ( Sharma et al. , 2014 ). The risk of “data-deluge” and the difficulty of having useful elements is very high, while the possibility of making quick, accurate, thoughtful decisions becomes more and more necessary, indeed fundamental ( Sabherwal and King, 1995 ;Citroen, 2011 ). In this sense, the evolution of decision-support-systems (DSS) assumes increasing importance inmany critical “moments”, both for descriptive-analytics (e.g. diagnostics, evaluations and monitoring), as well as in the follow-up analytics in the operational phases and even for forecasting possible choices in the future and related reasons through predictive analyticsand prescriptive analytics ( Boonstra, 2003 ). In general, information sharing with shared databases, data-storage, data extraction",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "2003 ). In general, information sharing with shared databases, data-storage, data extraction and data processing favours the design of a more functional, versatile, scalable, context-friendly service provision, where the smart management can make a difference thus deserves to be furtherexplored. For this, it becomes important to study the main characteristics of data that can be acquired. Here, the so-called “10V[s] of big-data” (Volume, Velocity, Variety, Veracity, Value, Validity, Variability, Venue, Vocabulary and Vagueness) are often taken into consideration tounderstand how new knowledge is generated and, consequently, how much decision-making processes are affected; particularly with reference to the possible advantages of meta dating, data modelling, architecture and data integration ( Manogaran et al. ,2 0 2 2 ). Similarly, the most frequently used methods are studied to improve decision-making",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "). Similarly, the most frequently used methods are studied to improve decision-making from data management’s point of view. Typical topics of interest here are cloud computing for information sharing, artificial intelligence for the data interpretation available and the generation of new ones like data mining and machine learning. The aim here is to betterunderstand how the information flow works, what criticalities it presents, how it feeds the activation and management of known protocols, how it integrates the various data-sources and how it supports the management of queries ( Hicks et al. ,2 0 0 6 ). All this effectively integrates decision-making techniques (cost benefits, grid analysis, paired comparison, compensatory strategies, etc.), with particular reference to conditions of uncertainty because of, for example, systematic errors, cognitive biases, risk situations,",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "of uncertainty because of, for example, systematic errors, cognitive biases, risk situations, external distortions, information asymmetries, misalignments, internal friction, misunderstandings, technical oradministrative misunderstandings, legal aspects, technological crashes or even weak signals escaping, somatic markers and negative contingencies. These issues are so fundamental and interesting that in the period between 2021 and 2027, European investments will be geared towards building a smarter Europe throughinnovation, digitalisation, economic transformation and support for small- and medium- sized enterprises. EIT Digital has launched the 2022 call to promote entrepreneurship and education for the construction of a strong digital Europe and contribute to the developmentof digital technology, digital industry, digital cities, digital wellbeing and digital finance. Since 2014, the European Commission has spoken out in favour of a thriving data-driven economy ( European Commission,",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "spoken out in favour of a thriving data-driven economy ( European Commission, 2014 ); in 2015, it discussed a strategy for the digital single market in Europe ( European Commission, 2015 ). In 2018, the International Data Corporation estimated an increase of 16 trillion gigabytes of data, with an annual growth rate of 236% in terms of data generation to date; they linked this to the fact that decisions based on knowledge generated by big data can lead to increased productivity andcompetitiveness and GDP (equal to 1.9% by 2020) ( Reinsel et al. , 2018 ). PAGE 2800jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 5] Today, the evolving trend of Big Data Analyses is an",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "[Página 5] Today, the evolving trend of Big Data Analyses is an integral part of a new digital market. According to the European Commission, it guarantees the development of innovative and competitive business models. However, while having to comply with the EU data protectionframework, big data can involve significant risks and challenges, especially in fundamentalrights like privacy and data protection. More recently, the European Parliament discussedthe role of the data-based economy in the strategy for the digital union against the backgroundof all stakeholders and their daily life situations, such as consumers (ease of use, efficiency and savings), businesses (industry 4.0) and public administration (e-government), housing (smart cities), science, medicine (Mhealth), disaster response capacity and the fight againstcrime, etc. 2.2 Big data in companies’ decision-making processes Scientists and researchers",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "againstcrime, etc. 2.2 Big data in companies’ decision-making processes Scientists and researchers have long since faced the challenges of data management,focusing their attention on possible ways to collect data both directly and indirectly( Sapsford and Jupp, 1996 ;Hajian and Domingo-Ferrer, 2012 ). Several experiments have been conducted aiming to define the processes and protocols that enhance the effectiveness of data collection as a relevant way to extend consolidated knowledge aboutthe reasons, antecedents and motivations behind actors’ behaviours and decisions inmultiple domains ( Grant and Mayer, 2009 ;Guiot and Roux, 2010 ;Daunt and Harris, 2012 ; Rahrovani and Pinsonneault, 2020 ). Along this line, studies focusing on companies’ decision-making have also been developed and multiple approaches for collecting andanalysing data have been investigated ( Goulding, 1999 ;Rokka and Uusitalo,",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "collecting andanalysing data have been investigated ( Goulding, 1999 ;Rokka and Uusitalo, 2008 ;Pac¸o and Lavrador, 2017 ). Nowadays, all these approaches and contributions seem to be outmoded against the background of the disruptive role of big data analytics in the data and knowledge managementprocesses ( Pauleen and Wang, 2017 ). Today, big data infrastructure supports the handling of data operations by facilitating the source’s integration and collaboration in real time with highstandards for control and data safety ( Sagiroglu and Sinanc, 2013 ). Demchenko et al. (2014 , p. 105) reports “the Big Data definition as having the following 5V properties: Volume, Velocity, Variety that constitute native/original Big Data properties, andValue and Veracity as acquired as a result of data[’s] initial classification and processing in the context",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "as a result of data[’s] initial classification and processing in the context of a specific process or model.” These properties effectively summarise the relevant contributions that big data can provide the management of a high volume of data inreal time without “damaging” the granularity of information to ensure a realisticrepresentation of the phenomenon ( Polyakova et al. ,2 0 1 9 ). According to Erevelles et al. (2016) , the properties of big data seem to provide a valuable solution for organisations striving to find an answer to environmental and social changesthrough predictive approaches about market trends. More comprehensively, big data offersorganisations the opportunities to increase: /H17039their dynamic capabilities –their “ability to respond to change incorporates skills and knowledge embedded within the organization to alter existing resources and",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "skills and knowledge embedded within the organization to alter existing resources and createnew value” ( Erevelles et al. , 2016 , pp. 898 –899); and /H17039adaptive capabilities –as capabilities that do not derive “from a specific change in organizational structure but from the overall ability to capture consumer activities andextract hidden in-sights” ( Erevelles et al. , 2016 , p. 899). Recognising the disruptive role of big data in reinventing firms’ market approaches, it is possible to underline its contribution in supporting enterprises in innovating theirrelationships with the market by focusing on the “implementation of creative ideas”( Gumusluoglu and Ilsev, 2009 , p. 61). From this perspective, big data analytics can be seen as a valuable approach that supports firms to enforce their relationship by focusing on the",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "approach that supports firms to enforce their relationship by focusing on the VOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2801 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 6] definition of the innovation management path based on their “ability to effectively acquire and exploit new information” ( Chaston et al. ,2 0 0 1 , p. 147). Data acquisition and exploitation became the bridge with the capacity to link innovation management, information management and market analysis under the common umbrella of big data analytics; this offers the opportunity to understand current interest in developing an effective model for information management, allowing firms to better understand (and predict) market trends and expectations based on big data analytics ( Erevelles et al. ,2 0",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "expectations based on big data analytics ( Erevelles et al. ,2 0 1 6 ). In a nutshell, big data can be considered a disruptive innovation ( Caputo et al. ,2 0 1 7 ) that is potentially able to reinvent firms’ approach to market analysis. Accordingly, Davenport et al. (2012 , p. 43) stated that big data supports firms “to understand their business environments at a more granular level, [ ...] creating new products and services, and [ ...] responding more quickly to change as it occurs.” As a result, a new challenge emerges concerning how to decode the pattern for companies’ decision-making processes through big data analytics. 3. Method and data collection With the aim to enrich current debate about the role of big data in",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "aim to enrich current debate about the role of big data in companies’ decision-making, a case study approach was set as the research strategy ( Kohlbacher, 2016 ). The reasons why this approach was chosen are multi-faceted. On the one hand, the approach follows the recommendations of Yin (2003) , who described the importance of case study research when a contemporary phenomenon is investigated in its real-world setting, and the boundaries between the phenomena itself and the related context are blurred. As a matter of fact, this method allows for a variety of research methods ( Yin, 2003 ;Kohlbacher, 2016 ). Case studies allow researchers to combine different data sources (such as interviews, texts and observations), as well as using qualitative and quantitative data analysis. Therefore, they can",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "as well as using qualitative and quantitative data analysis. Therefore, they can be used to describe a phenomenon and Subsequently to develop and test theories ( Darke et al. ,1 9 9 8 ). A widespread procedure is to use case studies in qualitative inquiries ( Stake, 2000 ; Kohlbacher, 2016 ). This is especially relevant in contexts where the “why” and the “how” of a phenomenon are the focus of an investigation. Consequently, a case study research strategy with a qualitative inquiry thus seems to be an appropriate approach for an investigation and the provision of new insights. It is therefore unsurprising that case studies are an appropriate and popular way of investigating the implementation and use of information systems within organisations. This is particularly true in",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "and use of information systems within organisations. This is particularly true in information systems research and related scientific areas, in which it is quite important to examine and understand the context of the phenomenon, because often researchers are unclear about how a phenomenon arises or how individuals’ experiences and doings are critical to its actions and effects. Furthermore, numerous research approaches demand that with regards to the research question the number and topic of the cases must be determined at the outset. Whilst a single case study is applied to gain deep and rich insights, multi-case studies have the advantage of allowing replications (literal, theoretical) and comparisons between cases ( Darke et al. ,1 9 9 8 ). Here, a topic highly related to information systems research is",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "8 ). Here, a topic highly related to information systems research is investigated. Besides managerial and human factors, the research question also aims to understand the technical issues and their related problems. Following the recommendations given in the literature, as described previously, a multiple case study research strategy was chosen as an appropriate approach in line with our research question. As the research focuses on different aspects, a single case study approach did not seem to be appropriate to best gain the desired insights about the subject. Therefore, multiple cases were investigated by collecting different data from different sources and conducting a qualitative analysis ( Yin, 1994 ,2012 ). PAGE 2802jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "NO. 10 2023 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 7] Consequently, three different cases were examined. The investigated cases were a manufacturing enterprise, an enterprise from the IT sector and a supplier for IT solutions. It is assumed that all the branches are equally affected by the challenges of implementing big data analytics. In addition, the cases highlight and clarify that all sectors are affected by thechallenges of Big Data Analysis. The IT sector is no exception. The investigated enterpriseshave different sizes and turnovers. This circumstance is useful in terms of the generalisabilityof the findings. More details about the companies’ characteristics are reported in Table 1 . In all cases, the process to implement the possibility of big data analytics was accompanied and",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "process to implement the possibility of big data analytics was accompanied and supported by at least one of the researchers. As a result, a minimum of one person wasinvolved as an “action researcher” within the organisations ( Walsham, 1995 ). Subsequently, both the data and the contextual insights gathered are very rich and useful. Every case wascomprehensively investigated and hence a strong understanding of the phenomenon wasachieved ( Darke et al. , 1998 ). Furthermore, the action researchers accompanied different big data analytics projects within the companies chosen as cases. This allowed them to prove andcontrol the generalisability of the insights and findings in different settings ( Darke et al. ,1 9 9 8 ). As recommended in the literature, different data sources such as observations, interviews andquestionnaires",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "recommended in the literature, different data sources such as observations, interviews andquestionnaires were picked-up and combined ( Darke et al. ,1 9 9 8 ). An overview about the data sources used in this investigation is provided in Table 1 . For the data analysis, the Grounded Theory approach was conducted ( Strauss and Corbin, 1994 ). This approach is very common and widespread in Information Systems research (Aarnikoivu et al. , 2019 ). In the first step, the open coding process was conducted. The data was investigated, and the relevant aspects were tagged with abstract labels. This step isfollowed by the so-called axial coding process. As the second step of the procedure, the axialcoding process examines the relationships between the labels and tries to build networkscontaining relevant",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "examines the relationships between the labels and tries to build networkscontaining relevant aspects. Hence, the identified labels were aggregated and networks werebuilt. In the third step, selective coding was applied, meaning that the networks were subsumedinto categories. In each step, all the team members did the coding process alone and theresults were discussed afterwards. 4. Results The data analysis revealed that in all cases along the project’s timeline specific patternsoccurred at special points in time. The findings are summarised in Table 2 and explained in more detail subsequently. Phase (a) : Nearly all enterprises have recognised that the customer data they own is a hidden gem. Hence, it is not surprising that companies want to exploit this potential. Consequently, organisations have recognised the need for big data analytics",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "this potential. Consequently, organisations have recognised the need for big data analytics to realise thebenefits provided by the data. Often, the top management takes the initiative to createplans for big data analytics projects. They set ambitious goals and objectives thatfrequently consist of a mix of dreams, wishes and reality. In many cases, the intended big Table 1 Overview of case studies and data gathering process Enterprise no. 1 (case 1) Enterprise no. 2 (case 2) Enterprise no. 3 (case 3) Sector Manufacturing IT IT solution supplier Company size Large Medium Small No. employees >550 >200 63 Turnover /C24200 Mio e /C24200 Mio e /C245 Mio e Observations by accompanying/supportive researcher x x x Cross-divisional e-mail traffic x x x Interviews and expert talks x x x Surveys x",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "x x x Interviews and expert talks x x x Surveys x Source: Authors’ elaboration VOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2803 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 8] data analytics projects are not realisable for several reasons. Firstly, the company lacks concrete processes, possibilities and outcomes along with the initial vague and imaginative assumptions. Hence, big data analytics projects begin similarly and specific requirements are often not respected because of the company’s inexperience with such projects. Subsequently, wrong estimations in terms of budget and staffing, as well as time and scope occur. In addition, some of the most prominent aspects in big data analytics projects are also neglected. Furthermore, the availability of data is a crucial factor that",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "also neglected. Furthermore, the availability of data is a crucial factor that is often misjudged. Organisations trust in their databases. However, it is not uncommon for data to be unusable because of poor data management and questionable data quality. There are also often assumptions about data sources that do not, in fact, exist in the reality of the company. In one of the cases in this study, an expert in case (1) stated that the management proclaimed that all the needed data is stored and available in their proAlpha ERP system. However, it turned out that this was a false estimation from the management. Even if the data is available, wrong judgement can be taken as case (3) revealed. The responsible persons in case (3) assumed that they",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "case (3) revealed. The responsible persons in case (3) assumed that they have high quality data about their customers and their behaviour. Although data about the customers was available, it did not meet the requirements. Relevant aspects of customers’ behaviour were missing and, therefore, the potential for the analysis was quite restricted. Phase (b) : Once a project is started, challenges because of human factors, as well as technical issues arise. On the human side, the challenges are two fold. On the one hand, it might be that the assigned employees did not have the relevant knowledge for conducting the project or cannot be identified. During the project, the management of case (1) discovered that their internal staff were not able to implement the AI models into their",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "internal staff were not able to implement the AI models into their systems. Therefore, they had to find an external service provider who was able to cope with this challenge. On the other hand, missing openness and/or a restricted mindset are a critical human factor too. This often results in staff hiding their knowledge to avoid changes that could lead to more work or that has a negative impact on their job position. Besides challenges occurring because of human factors, we also observed technical aspects that were crucial for the continuation of big data analytics projects. On the technicalTable 2 Main results about companies ’approach to big data Phase (a.): Before/at the beginning of the project Phase (b.): During the projectPhase (c.): At the end/ finalization of the",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "Phase (b.): During the projectPhase (c.): At the end/ finalization of the project Need for big data analytics Staff with adequate knowledge is missing or cannot be foundNot all requirements/ automation tasks can be fulfilled Mix-up of dreams, wishes and realityMissing openness/restricted mindsetPredictions by the algorithms are not always better than the human ones Budget and available staff Data sources (e.g. databases) do not fitUsability issues Implementation/time horizon Identification of the best Big Data algorithm(s)Time, costs and effort was underestimated (run of time and budget) Trust in databases Must re-design the project and re-start IT infrastructure is old and notflexible Data protection rules Source: Authors’ elaboration PAGE 2804jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 9]",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 9] side, it might be the case that the database does not even contain the expected data or that the data did not fit the requirements, as described previously. In many cases, the missingdata cannot be procured because the IT infrastructure is too old and inflexible. There aremissing interfaces, hence new analytical systems can connect to it and collect the dataneeded for analysis (in cases 1, 2 and 3). Modern standard application interfaces like REST-APIs ( Masse, 2011 ) were not provided, which hindered the seamless collection of data. Furthermore, the implementation of modern big data analytic and data visualisationtools into old systems might be difficult. Both human and technical factors might stop or delay the project.",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "difficult. Both human and technical factors might stop or delay the project. In all cases (1) –(3), it was hard to find the correct experts with business and domain specific know-how. In cases (1) and (3), often the most suitable employees for the task were also not known by themanagement. Sometimes, a step back to the first phase was needed to re-define theresponsibilities and even the technical possibilities. In cases (1) and (3), the project had to be restarted (a). In case (1), adjustments during the project were done. Hence, the aim of the project must be reviewed and re-defined. Another aspect that sometimes occurs is thatthe best algorithm cannot be found. In all cases (1) –(3), there was no generally available algorithm or approach fitting the project’s",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "–(3), there was no generally available algorithm or approach fitting the project’s goal that would deliver a result within theexpected quality range from the very beginning. Furthermore, the available IT infrastructure resources (e.g. CPU, RAM, disk) for the analysis hindered the evaluation of different algorithms. For example, (sample) data was split and patterns were reconstructed toevaluate the algorithms. Different algorithms were combined in all cases to accommodateissues such as linear and non-linear behaviour (e.g. linear regression and neuronal networks) and selected based on different rules (rule-based algorithm selection and combination), as well as patterns that could only be identified during the actual dataanalysis. For instance, after starting the project the responsible persons in case (1) foundout that their systems could not be used to run analytical services. They",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "that their systems could not be used to run analytical services. They did not anticipate in advance that the necessary infrastructure capabilities (e.g. CPU/RAM) would be missing. Phase (c) : In the final project phase, further patterns were identified within the selected cases. Regarding the definition and targets of the big data projects at the beginning of theproject, not all requirements and automation tasks could be fulfilled. This is often a consequence of the fact that the challenges from the two preceding project phases could not be sufficiently taken into account. In cases (1) and (3), only a minor set of requirementscould be fulfilled because of the issues in the prior project phases. It was only in case (2)that important requirements during the project could be delivered. Sometimes,",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "in case (2)that important requirements during the project could be delivered. Sometimes, theprediction of human experts with years of experience is faster and more accurate compared to the developed systems. This might mean that not all the necessary data is available and the data behaviour patterns may not be recognised by the system. This wasparticularly true of case (1), where the system was not accurate compared to experiencedexperts. The developed big data systems are very complex. Therefore, their usability and user friendliness are severely limited. Experts must configure the systems in advance by entering specific parameters. Consequently, the staff must be trained to use the system andto interpret the results with regards to the specific business demands. In all cases (1) –(3), the effort needed to complete the",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "demands. In all cases (1) –(3), the effort needed to complete the project, in terms of, for instance, time, costs and budget, was underestimated. In cases (1) and (3), the project ran out of time and budget and had to be adjusted. Again, this might be a consequence of the identified patterns in the first twophases of the projects (a) and (b). 5. Conclusions, implications, limitations and future research In the past few years, big data and big data analytics tools have been presented as the new“miracle” for efficiency, survival and increased performance for any type of organisedentities ( Schmarzo, 2013 ). These approaches attracted the interest of multiple researchers and the investment of multiple companies interested in the possibility of obtaining multiple VOL. 27 NO. 10 2023",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "interested in the possibility of obtaining multiple VOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2805 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 10] advantages by simply buying new instruments, software and digital devices. Despite the summarised scenarios, the proposed research shows different scenarios in which collectedand analysed data demonstrate that the predictions made by the algorithms do not naturally offer value in isolation. Sometimes, human predictions are even better because they can involve more variable factors and are more intuitive. In such a perspective, the research offers several practical implications because it underlines how automation may not even be possible, and several manual steps are needed as the usability of the tool decreases. Sometimes, users cannot work with the system because",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "of the tool decreases. Sometimes, users cannot work with the system because it is hard to handle or because they are not able to interpret the output of the system and relate it to adequate strategical or operational measures. In addition,because of delays and re-definitions the project may run out of time and budget. Thus, the expenses overcome the estimated benefit. Sometimes, projects must even be abandoned. Furthermore, issues related, e.g. to the technical foundation of the enterprises, used algorithms and data quality hinder a good implementation and positive value of the system. In the same perspective, the research also underlines several theoretical implications by ascertaining that to run a big data analytics project successfully it is important to focus on the challenges and anticipate consequences. Therefore, current",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "is important to focus on the challenges and anticipate consequences. Therefore, current interpretative paths and managerial models require radical rethinking to better catch and depict the interconnections that could be possible between humans and technology. Despite the conceptual and empirical advancements in the knowledge offered by the reflections herein, several limitations can be identified with reference to the proposed research approach because the results offered by the analyses of the case studies are subjective and related to the background in which they have been approached and analysed. In such a vein,the next steps for the research are required to test to what extent the proposed results and observations can be generalised to different cognitive and geographical domains. References Aarnikoivu, M., Nokkala, T., Siekkinen, T., Kuoppala, K. and Pekkola,",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "domains. References Aarnikoivu, M., Nokkala, T., Siekkinen, T., Kuoppala, K. and Pekkola, E. (2019), “Working outside academia? Perceptions of early-career, fixed-term researchers on changing careers”, European Journal of Higher Education , Vol. 9, pp. 172-189. Alter, S. (2006), The Work System Method: Connecting People, Processes, and IT for Business Results , Work System Method. Amendola, C., Calabrese, M. and Caputo, F. (2018), “Fashion companies and customer satisfaction: a relation mediated by information and communication technologies”, Journal of Retailing and Consumer Services , Vol. 43, pp. 251-257. Ardito, L., Scuotto, V., Del Giudice, M. and Petruzzelli, A.M. (2018), “A bibliometric analysis of research on big data analytics for business and management”, Management Decision ,V o l .5 7 No. 8, pp. 1993-2009. Baker, O. and Thien, C.N. (2020), “A",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "7 No. 8, pp. 1993-2009. Baker, O. and Thien, C.N. (2020), “A new approach to use big data tools to substitute unstructured data warehouse”, 2020 IEEE Conference on Big Data and Analytics (ICBDA) , IEEE, pp. 26-31. Bayer, S., Gimpel, H. and Rau, D. (2020), “IoT-commerce-opportunities for customers through an affordance lens”, Electronic Markets , Vol. 31 No. 1, pp. 27-50. Blumberg, R. and Atre, S. (2003), “The problem with unstructured data”, Dm Review , Vol. 13, pp. 42-49. Boonstra, A. (2003), “Structure and analysis of IS decision-making processes”, European Journal of Information Systems , Vol. 12 No. 3, pp. 195-209. Buneman, P., Davidson, S., Fernandez, M. and Suciu, D. (1997), “Adding structure to unstructured data”, International Conference on Database Theory, Springer, pp. 336-350. Caputo, F., Cillo, V.,",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "International Conference on Database Theory, Springer, pp. 336-350. Caputo, F., Cillo, V., Candelo, E. and Liu, Y. (2019a), “Innovating through digital revolution: the role of soft skills and big data in increasing firm performance”, Management Decision , Vol. 57 No. 8, pp. 2032-2051. PAGE 2806jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 11] Caputo, F., Evangelista, F., Perko, I. and Russo, G. (2017), “The role of big data in value co-creation for the knowledge economy”, in Vrontis, S., Weber, T., Tsoukatos, E. (Eds), Global and National Business Theories and Practice: bridging the past with the Future , EuroMed Press, pp. 269-280. Caputo, F., Garcia-Perez, A., Cillo, V. and Giacosa, E. (2019b), “A knowledge-based view of people",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "A., Cillo, V. and Giacosa, E. (2019b), “A knowledge-based view of people and technology: directions for a value co-creation-based learning organisation”, Journal of Knowledge Management , Vol. 3 No. 7, pp. 1314-1334. Caputo, F., Walletzky, L. and S ˇtep /C19anek, P. (2019c), “Towards a systems thinking based view for the governance of a smart city’s ecosystem”, Kybernetes , Vol. 48 No. 1, pp. 108-123. Castells, M. (1999), The Social Implications of Information and Communication Technologies ,UNESCO ’s World Social Science Report . Chaston, I., Badger, B. and Sadler-Smith, E. (2001), “Organizational learning: an empirical assessment of process in small UK manufacturing firms”, Journal of Small Business Management ,V o l .3 9N o .2 , pp. 139-151. Chen, H., Chiang, R.H. and Storey, V.C. (2012), “Business intelligence and",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "139-151. Chen, H., Chiang, R.H. and Storey, V.C. (2012), “Business intelligence and analytics: from big data to big impact”, MIS Quarterly , pp. 1165-1188. Chinnaswamy, A., Papa, A., Dezi, L. and Mattiacci, A. (2018), “Big data visualisation, geographic information systems and decision making in healthcare management”, Management Decision , Vol. 57 No. 8, pp. 1937-1959. Citroen, C.L. (2011), “The role of information in strategic decision-making”, International Journal of Information Management , Vol. 31 No. 6, pp. 493-501. Darke, P., Shanks, G. and Broadbent, M. (1998), “Successfully completing case study research: combining rigour, relevance and pragmatism”, Information Systems Journal , Vol. 8 No. 4, pp. 273-289. Daunt, K.L. and Harris, L.C. (2012), “Motives of dysfunctional customer behavior: an empirical study”, Journal of Services Marketing , Vol. 26 No. 4,",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "an empirical study”, Journal of Services Marketing , Vol. 26 No. 4, pp. 293-308. Davenport, T.H., Barth, P. and Bean, R. (2012), “How big data is different”, MIT Sloan Management Review , Vol. 54 No. 1, pp. 43-46. Davenport, T., Guszcza, J., Smith, T. and Stiller, B. (2021), Analytics and AI-Driven Enterprises Thrive in the Age of With , Deloitte Insights. Del Giudice, M., Scuotto, V., Papa, A., Tarba, S.Y., Bresciani, S. and Warkentin, M. (2021), “A self-tuning model for smart manufacturing SMEs: effects on digital innovation”, Journal of Product Innovation Management , Vol. 38 No. 1, pp. 68-89. Demangeot, C. and Broderick, A.J. (2006), “Exploring the experiential intensity of online shopping environments”, Qualitative Market Research: An International Journal , Vol. 9 No. 4, pp. 325-351. Demchenko, Y.,",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "An International Journal , Vol. 9 No. 4, pp. 325-351. Demchenko, Y., De Laat, C. and Membrey, P. (2014), “Defining architecture components of the big data ecosystem”, 2014 International Conference on Collaboration Technologies and Systems (CTS) ,IEEE , pp. 104-112. Demchenko, Y., Zhao, Z., Grosso, P., Wibisono, A. and De Laat, C. (2012), “Addressing big data challenges for scientific data infrastructure”, 4th IEEE International Conference on Cloud Computing Technology and Science Proceedings ,IEEE , pp. 614-617. Drucker, P.F. (2011), Technology, Management, and Society , Harvard Business Press. Erevelles, S., Fukawa, N. and Swayne, L. (2016), “Big data consumer analytics and the transformation of marketing”, Journal of Business Research , Vol. 69 No. 2, pp. 897-904. European Commission (2014), “Communication from the commission to the European parliament, the council,",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "Commission (2014), “Communication from the commission to the European parliament, the council, the european economic and social committee and the committee of the regions”, available at: https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri=CELEX:52014DC0442andfrom=EN European Commission (2015), “Communication from the commission to the European parliament, thecouncil, the European economic and social committee and the committee of the regions”, A Digital SingleMarket Strategy for Europe, available at: https://eur-lex.europa.eu/legal-content/EN/TXT/PDF/?uri= CELEX:52015DC0192andfrom=EN Forester, T. (1987), High-Tech Society: The Story of the Information Technology Revolution , MIT Press. Gandomi, A. and Haider, M. (2015), “Beyond the hype: big data concepts, methods, and analytics”, International Journal of Information Management , Vol. 35 No. 2, pp. 137-144. VOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2807 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 12] Goulding, C.",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 12] Goulding, C. (1999), “Consumer research, interpretive paradigms and methodological ambiguities”, European Journal of Marketing , Vol. 33 Nos 9/10, pp. 859-873. Grant, A.M. and Mayer, D.M. (2009), “Good soldiers and good actors: prosocial and impression management motives as interactive predictors of affiliative citizenship behaviors”, Journal of Applied Psychology , Vol. 94 No. 4, pp. 900-920. Griffin, M., Babin, B.J. and Modianos, D. (2000), “Shopping values of Russian consumers: the impact of habituation in a developing economy”, Journal of Retailing , Vol. 76 No. 1, pp. 33-52. Guiot, D. and Roux, D. (2010), “A second-hand shoppers’ motivation scale: antecedents, consequences, and implications for retailers”, Journal of Retailing , Vol. 86 No. 4, pp. 355-371. Gumusluoglu, L. and Ilsev, A.",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": ", Vol. 86 No. 4, pp. 355-371. Gumusluoglu, L. and Ilsev, A. (2009), “Transformational leadership, creativity, and organizational innovation”, Journal of Business Research , Vol. 62 No. 4, pp. 461-473. Hajian, S. and Domingo-Ferrer, J. (2012), “A methodology for direct and indirect discrimination prevention in data mining”, IEEE Transactions on Knowledge and Data Engineering ,V o l .2 5N o .7 , pp. 1445-1459. Harbart, T. (2021), “Tapping the power of unstructured data”, MIT Sloan Management School, available at: https://mitsloan.mit.edu/ideas-made-to-matter/tapping-power-unstructured-data Hicks, B.J., Culley, S.J. and McMahon, C.A. (2006), “A study of issues relating to information managementacross engineering SMEs”, International Journal of Information Management , Vol. 26 No. 4, pp. 267-289. Kaur, S., Gupta, S., Singh, S.K. and Perano, M. (2019), “Organizational ambidexterity through global strategic partnerships: a cognitive",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "and Perano, M. (2019), “Organizational ambidexterity through global strategic partnerships: a cognitive computing perspective”, Technological Forecasting and Social Change , Vol. 145, pp. 43-54. Kohlbacher, F. (2016), “The use of qualitative content analysis in case study research”, Forum Qualitative Sozialforschung/Forum: Qualitative Social Research , Vol. 7 No. 1, pp. 1-30. Manogaran, G., Thota, C. and Lopez, D. (2022), “Human-computer interaction with big data analytics”, Research Anthology on Big Data Analytics, Architectures, and Applications, IGI Global, pp. 1578-1596. Markus, M.L. and Topi, H. (2015), Big Data, Big Decisions for Science, Society, and Business , National Science Foundation. Masse, M. (2011), REST API Design Rulebook: designing Consistent RESTful Web Service Interfaces , O’Reilly Media. Mo¨hring, M., Keller, B., Schmidt, R. and Dacko, S. (2020), “Google popular times: towards a better",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "Schmidt, R. and Dacko, S. (2020), “Google popular times: towards a better understanding of tourist customer patronage behavior”, Tourism Review , Vol. 76 No. 3, pp. 553-593. Mo¨hring, M., Walsh, G., Schmidt, R., Koot, C. and Ha ¨rting, R.C. (2013), “Returns management in eCommerce”, HMD , Vol. 50 No. 5, pp. 66-75. Mummalaneni, V. (2005), “An empirical investigation of web site characteristics, consumer emotional states and on-line shopping behaviors”, Journal of Business Research , Vol. 58 No. 4, pp. 526-532. Nutt, P.C. (2008), “Investigating the success of decision making processes”, Journal of Management Studies , Vol. 45 No. 2, pp. 425-455. Pac¸o, A. and Lavrador, T. (2017), “Environmental knowledge and attitudes and behaviours towards energy consumption”, Journal of Environmental Management , Vol. 197, pp. 384-392. Papadakis, V.M., Lioukas,",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "Journal of Environmental Management , Vol. 197, pp. 384-392. Papadakis, V.M., Lioukas, S. and Chambers, D. (1998), “Strategic decision-making processes: the role of management and context”, Strategic Management Journal , Vol. 19 No. 2, pp. 115-147. Pauleen, D.J. and Wang, W.Y. (2017), “Does big data mean big knowledge? KM perspectives on big data and analytics”, Journal of Knowledge Management , Vol. 21 No. 1, pp. 1-6. Polyakova, A., Loginov, M., Serebrennikova, A. and Thalassinos, E. (2019), “Design of a socio-economic processes monitoring system based on network analysis and big data”, International Journal of Economics and Business Administration , Vol. 7 No. 1, pp. 30-139. Rahrovani, Y. and Pinsonneault, A. (2020), “Innovative IT use and innovating with IT: a study of the motivational antecedents of two different types of",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "IT: a study of the motivational antecedents of two different types of innovative behaviors”, Journal of the Association for Information Systems , Vol. 21 No. 4, pp. 5-14. Raisinghani, M.S. (2000), “Knowledge management: a cognitive perspective on business and education”, American Business Review , Vol. 18 No. 2, pp. 105-131. PAGE 2808jJOURNAL OF KNOWLEDGE MANAGEMENT jVOL. 27 NO. 10 2023 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025 [Página 13] Reinsel, D., Gantz, J. and Rydning, J. (2018), The Digitization of the World. From Edge to Core ,A nI D C White Paper, Seagate. Rokka, J. and Uusitalo, L. (2008), “Preference for green packaging in consumer product choices –do consumer’s care?”, International Journal of Consumer Studies , Vol. 32 No. 5, pp. 516-525. Sabherwal, R. and",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "Consumer Studies , Vol. 32 No. 5, pp. 516-525. Sabherwal, R. and King, W.R. (1995), “An empirical taxonomy of the decision-making processes concerning strategic applications of information systems”, Journal of Management Information Systems , Vol. 11 No. 4, pp. 177-214. Sagiroglu, S. and Sinanc, D. (2013), “Big data: a review”, 2013 International Conference on Collaboration Technologies and Systems (CTS) ,IEEE , pp. 42-47. Sapsford, R. and Jupp, V. (Eds) (1996), Data Collection and Analysis , Sage. Schmarzo, B. (2013), Big Data: Understanding How Data Powers Big Business , John Wiley and Sons. Schmidt, R. and Mo ¨hring, M. (2013), “Strategic alignment of cloud-based architectures for big data”, 2013 17th IEEE International Enterprise Distributed Object Computing Conference ,IEEE , pp. 136-143. Sharma, R., Mithas, S. and Kankanhalli, A. (2014),",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": ",IEEE , pp. 136-143. Sharma, R., Mithas, S. and Kankanhalli, A. (2014), “Transforming decision-making processes: a research agenda for understanding the impact of business analytics on organisations”, European Journal of Information Systems , Vol. 23 No. 4, pp. 433-441. Singh, S.K. and Del Giudice, M. (2019), “Big data analytics, dynamic capabilities and firm performance”, Management Decision , Vol. 57 No. 8, pp. 1729-1733. Stake, R.E. (2000), “Case studies”, in Denzin, N.K and Lincoln, Y.S (Eds), Handbook of Qualitative Research , Sage, pp. 435-453. Stonebraker, M. (2010), “SQL databases v. NoSQL databases”, Communications of the ACM ,V o l .5 3 No. 4, pp. 10-11. Strauss, A. and Corbin, J. (1994), “Grounded theory methodology: an overview”, in Denzin, N.K. and Lincoln, Y.S. (Eds), Handbook of Qualitative Research , Sage,",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "Denzin, N.K. and Lincoln, Y.S. (Eds), Handbook of Qualitative Research , Sage, pp. 273-285. Turban, E., McLean, E. and Wetherbe, J. (1998), Information Technology for Management Making Connections for Strategic Advantage , John Wiley and Sons, Inc. Walsham, G. (1995), “Interpretive case studies in IS research: nature and method”, European Journal of Information Systems , Vol. 4 No. 2, pp. 74-81. Yang, Q., Steinfeld, A. and Zimmerman, J. (2019), “Unremarkable AI: fitting intelligent decision support into critical, clinical decision-making processes”, Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems , pp. 1-11. Yin, R.K. (1994), “Designing single-and multiple-case. Improving educational management: through research and consultancy”, in Bennett, N., Glatter, R. and Levacic, R. (Eds), Improving Educational Management: Through Research and Consultancy , Sage, pp. 135-155.",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "(Eds), Improving Educational Management: Through Research and Consultancy , Sage, pp. 135-155. Yin, R.K. (2003), Case Study Research, Design and Methods , 3rd ed., Sage, Vol. 5. Yin, R.K. (2012), “Case study methods”, in Cooper, H., Camic, P.M., Long, D.L., Panter, A.T., Rindskopf, D. and Sher, K.J. (Eds), APA Handbook of Research Methods in Psychology , Vol. 2.Research Designs: Quantitative, Qualitative, Neuropsychological, and Biological , American Psychological Association, pp. 141-155. Zakir, J., Seymour, T. and Berg, K. (2015), “Big data analytics”, Issues in Information Systems ,V o l .1 6 No. 2, pp. 81-90. Corresponding author Francesco Caputo can be contacted at: francesco.caputo2@unina.it For instructions on how to order reprints of this article, please visit our website: www.emeraldgrouppublishing.com/licensing/reprints.htm Or contact us for further details: permissions@emeraldinsight.com VOL. 27 NO.",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "website: www.emeraldgrouppublishing.com/licensing/reprints.htm Or contact us for further details: permissions@emeraldinsight.com VOL. 27 NO. 10 2023 jJOURNAL OF KNOWLEDGE MANAGEMENT jPAGE 2809 Downloaded from http://www.emerald.com/jkm/article-pdf/27/10/2797/1834268/jkm-10-2022-0794.pdf by UECE user on 08 October 2025",
        "start_idx": 8236,
        "end_idx": 8266
      }
    ],
    "38fd80e4-1db5-4f15-a691-9d1e300929d7": [
      {
        "text": "[Página 1] Issues in Big Data Testing and Benchmarking Alexander Alexandrov Technische Universität Berlin Einsteinufer 17 10587 Berlin, Germany +49 30 314 23555 alexander.alexandrov@tu- berlin.de Christoph Brücke Technische Universität Berlin Einsteinufer 17 10587 Berlin, Germany +49 30 314 23555 christoph.bruecke@campus.tu- berlin.de Volker Markl Technische Universität Berlin Einsteinufer 17 10587 Berlin, Germany +49 30 314 23555 volker.markl@tu- berlin.de ABSTRACT The academic community and industry are currently researching and building next generation data management systems. These syste ms are designed to analyze data sets of high volume with high data ingest rates and short response time s executing complex data analysis algorithms on data that does not adher e to relational data model s. As these big data systems differ from standard relational database systems with respect to data and",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "systems differ from standard relational database systems with respect to data and workloads, the traditional benchmarks used by the database community are insufficient. In this paper , we describe initial solutions and challenges wit h respect to big data generation, methods for creating realistic, privacy -aware, and arbitrarily scalable data sets, workloads, and benchmarks from real world data. We will in particular discuss why we feel that workloads currentl y discussed in the testing and benchmarking community do not capture the real complexity of big data and highlight several research challenges with respect to massively -parallel data generation and data characterization. Categories and Subject Descriptors D.2.5 [Testing and Debugging ]: testing tools, data generators General Terms Measurement, Performance, Experimentation Keywords Big Data, Data Generation, Data Profiling, Workloads, Benchmarking",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "Measurement, Performance, Experimentation Keywords Big Data, Data Generation, Data Profiling, Workloads, Benchmarking 1. INTRODUCTION The database systems building community is currently at a peak of new activity, creating novel systems for managing and analyzing what is commonly called “big data.” Big data is usually characterized by the requirement to conduct advanced analytics on large volumes of data of variable format, wh ich is ingested into the system with high -velocity with the need for fast response times. Novel big data analytics systems differ from traditional data analysis systems for varying reasons, they : (a) c an process terabytes or even p etabytes of data due to their scale-o ut abilities, employin g massively parallel processing , (b) support complex data types in addition to relational sets of tuples",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "(b) support complex data types in addition to relational sets of tuples (i.e., data of complex structure, such as text documents, hierarchies, graphs, or even images, audio, or video files) , (c) allow for defining and processing complex analytics tasks that go beyond the traditional operations of the relational algebra ( e.g., user -defined functions, data mining or machine learning algorithms, graph algorithms) , (d) provide fault -tolerance in order to ensure termination even for long-running computations , and (e) compute answers with low - latency, producing r esults in a pipelined fashion. Some examples of systems that showcase several of these features are Google MapReduce [ DG04 ], its open source implementation Hadoop [ Had13] , its ecosystem of languages (e.g ., Hive [TSJ+09 ], JAQL [",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": ", its ecosystem of languages (e.g ., Hive [TSJ+09 ], JAQL [ BEG+11 ], Pig [ ORS+08]) and libraries such as Mahout [ Mah13], and other big data systems such as Asterix [ABG+12 ], GraphLab [ LBG+12 ], Spark [Spa13] and our own Stratosphere system [ ABE+10 , Str13 ]. At the same time, there is a trend to make more traditional relational data analysis sy stems more scalable. Examples of these efforts are SAP Hana [FML +12], Impala [ Imp13 ], Oracle Exadata [ GSA+11 ], or the columnar storage extensions to Microsoft’s and IBM’s database products , to name a few. While all these systems have advanced the capabilit ies of data analysis with respect to the five dimensions above, database testing and benchmarking have not",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "respect to the five dimensions above, database testing and benchmarking have not moved forward to provide data generators, data sets, and workloads. In particular, we see the need to generate large, realistic data sets at scale, as well as the need for well-defined workloads that capture the nature of novel, modern analysis tasks. 2. BIG DATA GENERATION Data generation tools and practices can be principally assigned to one of two classes : (a) reusing existing, well-known data generation tools, or (b ) implementing custom, use -case tailored data generator s. We first review the benefits of each one of these classes and then discuss some implications for the evaluation of big data analytics systems. Since the establishment of standardized benchmarks as a “gold standard” for performance evaluation of",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "establishment of standardized benchmarks as a “gold standard” for performance evaluation of database systems in the early 90’s , experimental results reported in research papers often reuse data sets and queries from well-known benchmarks , like TPC-H, TPC-C [TPC13 ], and XMLGen [ XML13 ]. This practice is justified by two main factors. First, the synthetic data used by standardized or public benchmarks typically adheres to a short textual specification that is well-known in the database community . Reusing data sets from such benchmark s therefore makes the data properties and their impact on the evaluated tasks more comprehensible and increases the trust in the reported experiment result s. Second , well-known benchmarks typically provide open-source tools for data and workload generation , which can be adapted and",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "tools for data and workload generation , which can be adapted and used by third parties relatively easy . This reduces the overall effort required to prepare and execute Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee prov ided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specif ic permission and/or a fee. DBTEST ’13, June 24, 201 3, New York City, NY, USA Copyright 2 013 ACM 1 -58113 -000-0/00/0010 …$15 .00. [Página 2] “proof -of-concept” experiments and",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "ACM 1 -58113 -000-0/00/0010 …$15 .00. [Página 2] “proof -of-concept” experiments and allows researchers to spend more time working on the actual prototype s rather than the tooling to evaluate them . An alternative approach that sometimes is preferred for specialized experimental studies is to define an d implement a custom data generator tailored towards the requirements of the concrete experiment s at hand . If the experiments are recognized as relevant by the database community, the data and tasks described in the original research are often reused by other authors in follow -up work. For example, Pavlo et al. followed this approach in their comparison of approaches for large -scale data analytics [PPR+ 09] and implemented a synthetic generator that generates a collection of linked HTML documents",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "implemented a synthetic generator that generates a collection of linked HTML documents and associated data (e.g., user traffic , PageRank ). The data generator and the tasks have since then been used in several other papers dealing with large - scale data analytics systems [ DQJ+10 , JOS+10 ]. For graph data, the Kronecker multiplication approach suggested by Leskovec et al. [LC K+05] offers a simple algorithm for synthetic generation of unlabeled graphs with real world characteristics ( e.g., shrinking diameter, skewed degree distribution) . Due to the lack of public ally available real-world graph s in the terabyte range , Kronecker graphs are often featured in the evaluation sections of several graph -mining papers over the past few years [ KTF 09, KTA +11]. Principally , the",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "the past few years [ KTF 09, KTA +11]. Principally , the main issue with both classes is the inherent simplicity in the statistical structure of the generated data. In the first case , this simplicity is driven by the need for concise and understandable specification for standardized benchmarks. In the second case, the main hindering factor is the complexity introduced in the data generation programs by the need for correlated data and the amount of resources that researchers are willing to invest in their development . Figure 1: Simplified Retail Database Schema In reference to the characteristic s of new big data analysis systems presented in Section 1 , the use of oversimplified synthetic data creates a subtle pitfall that may impact the relevance of research results",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "creates a subtle pitfall that may impact the relevance of research results for real -world applications . The reason for this is that per definition such systems must work in a distributed execution environment (cluster or cloud), and also must use some form of data-parallelism in order to ensure scale-out . These design decisions are highly sensitive to data skew, which often is present in many target application domains “a priori” and potentially changes over time. To illustrate the problem, consider the retail database schema depicted on Figure 1 and a use -case, where the benchmarks or experimental setup models an application that wants to compute the top -k most purchased items per product category. Since some product categories are n aturally more in demand than others, introducing",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "some product categories are n aturally more in demand than others, introducing a skew over the product category distribution in the joined LINEITEM -PRODUCT view is critical to the relevance of the generated data. As most systems will process each product category g roup in parallel, skew will obviously influence system performance for this particular task. Moreover, for an online computation of the same counts in a streaming setting, the degree of skew will depend on the time of the current window ( e.g., in the U.S. shopping peaks between Thanksgiving Day & Christmas and attains a maximum on “Black Friday ”). In this case, assuming an evenly distributed load across time is an oversimplif ication that can influence the relevance of the experimental results for real -world",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "that can influence the relevance of the experimental results for real -world applications. With the advent of big data co mes the requirement to quickly generate huge data sets. This is particularly a challenge when generating data sets with key/foreign -key relationships or other complex correlations across tables. Using specialized random number generators with seed skipping allows for doing so in parallel without having to communicate data generated on one node of a shared -nothing cluster to another [RFS+10 , FPR12, ASP+1 1, ATM12], resulting in toolkits such as PDGF [PDG13] or Myriad [Myr13] . Both toolkits provide a set of domain-specific primitives for data generation that facilitate the transparent use of seed-skip PRNGs and complementary technique s for scalable generation of complex data . 3. GENERATING REALISTIC",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "technique s for scalable generation of complex data . 3. GENERATING REALISTIC DATA SETS The advances in new methods for scalable generation of realistic data highlight an important practical question: “If the data generator program can be expressed in terms of a small set of special primitives, then to which extent and in which scenarios can the specification process itself be executed automatically ?” A naïve general approach is based on the analysis of empirical observations in the modeled domain and the subsequent synthesis of a data generator specification from these observations . In business scenarios , however, the analysis is often done in the context of a reference dataset that represents a ground truth for the derived data generator. This section sketches our vision for an integrated",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "the derived data generator. This section sketches our vision for an integrated framework for such usage scenarios . We propose an extensible architecture with clean separation between the data generation primitives and the methods and techniques used to extract relevant features from the ground truth data set. A large problem for benchmarking and testing of big data system s is the lack of realistic data sets. Many synthetic data sets follow simplistic assumptions ( e.g., few correlations, most ly uniform distributions, over simplified schema ) that are not re presentative for real-world data . A promising, generalizable, and more effective way is to automatically extract the domain information from a ground truth data set , which is often available in practice. Figure 2 illustrates our envisioned pipeline. The",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "is often available in practice. Figure 2 illustrates our envisioned pipeline. The domain information is first extracted from the reference database in the form of domain constraints , which can be either hard (e.g., foreign keys, unique keys, and other functional dependencies ) or soft (e.g., local statistical models) . The obtained structural, semantic, and statistical information is then unified into a n intermediate model representing the schema information with ann otated constraints . A final synthesis pass transforms the intermediate representation into a data generator specification for a specific target environment like the Myriad . This specification is then used to create a concrete data generator instance that is able to mimic the original data set. [Página 3] Figure 2: A Pipeline for the A nalysis &",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "set. [Página 3] Figure 2: A Pipeline for the A nalysis & Synthesis of D ata Generators We note that in the first step of this process, the circumstances in which the analysis is performed will influence its depth and consequently the quality of the collected domain information. If the reference database cannot be accessed directly and the domain information is available only in a derived form , such as in a database catalog, the analysis must be performed indirectly and can only extract the available catalog information . This information commonly consists of attribute value statistics ( e.g., frequen cy values, histograms, number of distinct va lues, and number of NULLs) , schema information , and integrity constraints (e.g., referential integrity , primary key s, and unique",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "and integrity constraints (e.g., referential integrity , primary key s, and unique constraints as well as other constraints represent ing domain invariants ). Alternativ ely, if the reference database is available directly , advanced profiling methods could be leveraged to obtain information beyond the catalog in order to capture a more accurate domain model . This approach will require us to determine additional characterizations of the dataset to be generated (e.g., advanced multivariate statistics [SHM+06] and soft constraints [IMH+04, BH03, SBH+06]) on the data with scalable methods (see [HIL+09] for an overview of statistical methods, and [Nau13] for an overview of data profiling). Using these techniques will allow for determining the essential characteristics of real -world data set s and correspondingly will enable one to scale up or",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "data set s and correspondingly will enable one to scale up or down synthetic clones . The integration of data profiling and data generation workflows is relevant in the era of big data for a number of reasons . First, many institutions publish their data sets in order to let others perform their experiments on them. However, database sizes are becoming larger and larger . Conseque ntly, it is becom ing increasingly difficult to transfer these huge data sets to the person wishing to use them due to network and bandwidth constraints . Therefore, it is desirable to have a compact specification of the data sets, i.e. , a synopsis or profile from which one can automatically generate a data generator specification and thus the dataset . Second,",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "automatically generate a data generator specification and thus the dataset . Second, data profiling will increase the relevance tests or benchmarks. Huppler [Hup09] describes five key aspects for a good benchmark , namely a good b enchmark has to be relevant, repeatable, fair, verifiable, and economical. Section 2 mainly addressed the latter one, while data profiling will help to improv e the relevance. Currently, w e are develo ping a prototype called Oligos [Oli13] that adheres to our aforementioned vision. The initial version of Oligos can generate data generator specifications for the Myriad Toolkit [Myr13] from the system catalog of a database system. Our long -term vision is provide a modular API that will allow learning advanced statistics and correlation information, in order to ge nerate even more",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "advanced statistics and correlation information, in order to ge nerate even more realistic data sets. 4. AN APPLICATION: REGRESSION TESTING OF BIG DATA SYSTEMS An important part of the maintenance lifecycle of co mmercial big data system s as well as general data management system s is devoted to the diagnosis of performance regressions observed by customers in a production setting. When trying to reproduce the problematic behavior in a test environment, database system developers often face the problem of missing data – even though the database schema and the problematic queries can be provided by the customer as part of the regression report, the actual database instance typically cannot be obtained ( e.g., due to privacy restrictions). T ypically , what is available is the database catalog,",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "privacy restrictions). T ypically , what is available is the database catalog, which contains a statistical approximation of the reference database in the form of value distributions, cardinalities , and histograms on columns or column groups. As a fallback solution, developers currently trick the optimizer of a test database by feeding customer catalog data in order to obtain the query access paths of the actual production system. As the underlying data is missing and the database catalog is usually lacking crucial information (e.g., on multivariate distributions ) synthetic data sets generated in the lab are not representative. Thus, information on how the query access paths perform requires further assistance and feedback from the client. The lack of a complete and representative regression database therefore slows down the maintenance",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "of a complete and representative regression database therefore slows down the maintenance process and causes additional costs . The methods for data generation based on data and workload characterization as envisioned in Oligos and Myriad would offer a remedy to this problem. 5. OPEN ISSUES AND CONCLUSIONS We have given an overview of issues in big data benchmarking and testing, with a strong focus on data generation. We believe that efficiently generating a huge, realistic data set is an important prerequisite for the advancement, evaluation, and fair comparison of big data systems. Myriad [Myr13], PDGF [DPG13] , and Oligos [Oli13 ] are a first step in this direction. However, in the context of big data generation and benchmarking, a large number of challenges remain open. However, in the",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "and benchmarking, a large number of challenges remain open. However, in the context of big data generation and benchmarking, a l arge number of challenges remain open. For realistic data generation from a given reference dataset the challenges exist both in the analysis and the synthesis phase. During the analysis phase, a combination of data characterization and profiling methods can be identified and applied in order to increase the quality of the dom ain information that can be inferred directly from the reference database. Such methods will allow to efficiently determine multi-key dependencies, in particular referential integrity, as well as to profile data with complex structure (e.g., text, graphs, NF² and hierarchical data). In order to preserve privacy when conducting data profiling, data obfuscation methods may as well",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "preserve privacy when conducting data profiling, data obfuscation methods may as well be required. [Nau13] lists further challenges in the area of data profiling. Inferred schema information and constraints must be then unified into an intermediate representation (IR) in the synthesis phase. Two problems exist in this context. First, in order to facilitate the subsequent translation of the IR into a data generator specification, the IR should lend itself to the features and primitives common to the underlying data -generation engines. Second, the unification process should determine and handle [Página 4] inconsistencies in the domain information collected in the analysis phase. Recently, Arasu [AKL11] and Torlak [Tor12] suggested two different constraint -based languages for data generator specification that can serve as a starting point for the development of",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "specification that can serve as a starting point for the development of a suitable IR and synthesis algorithm. For both languages, the authors give sufficiency conditions for the existence of a data set fulfilling the input constraints and provide approximate algorithms to find such an instance. The approach presented in [Tor12] uses a mix of hard (dimension or integrity) and soft (statistical) c onstraints and is restricted to dimension models, whereas [AKL11] works on general relational models and relies solely on soft (cardinality) constraints (hard constraints are represented implicitly as a special form of soft constraints). As the target lang uage in our setting is likely to include primitives that directly enforce certain types of hard constraints (e.g. unique keys, foreign keys), we believe that a distinction between",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "constraints (e.g. unique keys, foreign keys), we believe that a distinction between soft and hard constraints in the IR is a more promising approach. Another big open area is the provisioning of workloads. Traditional benchmarks focus on simple workloads that essentially follow the relational algebra or an NF² algebra/ XQuery. For evaluating and testing big data analytics systems, we will require m ore complex workloads that involve machine learning algorithms, information extraction, and graph analysis/mining. The lack of a standardized data analysis language currently is a big obstacle for arriving at realistic, comparable, and universally useful w orkload specifications. Ideally, u ntil a standardized declarative language is available use-case repositories may be a first step in this direction. 6. ACKNOWLEDGMENTS We thank Berni Schiefer from IBM and Tillmann",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "this direction. 6. ACKNOWLEDGMENTS We thank Berni Schiefer from IBM and Tillmann Rabl from the University of Toronto for interesting discussions. Our investigations were funded by a CAS grant from IBM, the ICT Labs of the European Institute of Technology as well as the DFG (German National Science Foundation) via the Stratosphere Collabor ative Research Unit. 7. REFERENCES [ABE+10] A. Alexandrov, D. Battré, S. Ewen, M. Heimel, F. Hueske, O. Kao, V. Markl, E. Nijkamp, D. Warneke: Massively Parallel Data Analysis with PACTs on Nephele. PVLDB Vol. 3, No. 2, pp. 1625– 1628 (2010) [ABG+12] S. Als ubaiee, A. Behm, R. Grover, R. Vernica, V. Borkar, M. J. Carey, C. Li: ASTERIX: Scalable Warehouse -Style Web Data Integration. In Proceedings of the Ninth International Workshop on Information Integration",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "Data Integration. In Proceedings of the Ninth International Workshop on Information Integration on the Web, Article 2, ACM, (2012) [AKL11] A. Arasu, R. Kaushik, J. Li: Data Generation using Declarative Constraints. Proceeding of the SIGMOD Conference, pp. 685-696 (2011) [ASP+1 1] A. Alexandrov, B. Schiefer, J. Poelman, S. Ewen, T. Bodner, V. Markl: Myriad - Parallel Data Generation on Shared-Not hing Architectures , In Proc. ASBD, pp. 30-33 (2011) [ATM12] A. Ale xandrov, K. Tzoumas, V. Markl: Myriad: Scalable and Expressive Data Generation , In Proc. VLDB(5) pp. 1890- 1893 ( 2012) [BH03] P. Brown, P. Haas: BHUNT: Automatic Discovery of Fuzzy Algebraic Constraints in Relational Data. VLDB 2003: 668-679 [BEG+11] K. S. Beyer, V. Ercegovac, R. Gemulla, A. Balmin, M. Eltabakh, C.-C. Kanne, E. J. Shekita: Jaql:",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "R. Gemulla, A. Balmin, M. Eltabakh, C.-C. Kanne, E. J. Shekita: Jaql: A scripting language for large scale semistructured data analysis. In Proc. of VLDB Conference. (20 11) [DG04] J. Dean, S. Ghemawat: MapReduce: simplified data processing on large clusters , In OSDI, pp. 137 -150 (2004) [FML+ 12] F. Färber, N . May, W . Lehner, P . Große, I . Müller, H . Rauhe, J . Dees: The SAP HANA Database -- An Architecture Overview. IEEE Data Eng. Bull. 35(1): 28-33 (2012) [FPR12] M. Frank, M. Poess, T. Rabl: Efficient update data generation for DBMS benchmarks. ICPE 2012: 169-180 [GSA+11 ] R. Greenwald, R. Stackowiak, M. Alam, M. Bhuller.. Achieving extreme performance with Oracle Exadata. McGraw -Hill Osborne Media (2011) [Had13] http://hadoop.apache.org/ , last accessed 05",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "Exadata. McGraw -Hill Osborne Media (2011) [Had13] http://hadoop.apache.org/ , last accessed 05 -10-2013 [HIL+09] P. J. Haas, I. Ilyas, G. Lohman, V. Markl: Disco vering and Exploiting Statistical Properties for Query Optimization in Relational Databases: A Survey. Statistical Analysis and Data Mining 1(4): 223 -250 (2009) [Hup93 ] K. Huppler: The Art of Building a Good Benchmark. TPCTC 2009: 18- 30 (2009) [IMH+04] I. Ilyas, V . Markl, P . Haas, P . Brown, A . Aboulnaga: CORDS: Automatic Discovery of Cor relations and Soft Functional Dependencies. SIGMOD Conference 2004: 647-658 [Imp13] https://github.com/cloudera/impala , last accessed 05 - 10-2013 [LBG+12] Y. Low, D. Bickson , J. Gonzalez, C. Guestrin , A. Kyrola , J. M. Hellerstein: DistributedGraphLab: A framework for machine learning and data mining in the cloud.",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "DistributedGraphLab: A framework for machine learning and data mining in the cloud. Proceedings of the VLDB Endowment, 5(8), pp. 716-727 (2012) [Mah13] Mahout: http://mahout.apache.org/ , last accessed 04 - 21-2013 [Myr13] https://github.com/TU -Berlin-DIMA/myriad - toolkit/wiki , last accessed 05 -10-2013 [Nau13] http://www.hpi.uni- potsdam.de/naumann/publications/publications_by_ty pe/year/2013/2276/Nau13.html , SIGMOD Record (2013) [Oli13] https://github.com/TU -Berlin-DIMA/myriad - toolkit/wiki/Using -Oligos -Guide , last accessed 05 -10- 2013 [ORS+08] C. Olston, B. Reed, U. Srivastava, R. Kumar, A. Tomkins: Pig Latin: A Not-So -Foreign Language for Data Processing. Proceedings of the SIGMOD Conference (SIGMOD), pp. 1099 -1110, (2008) [PDG13] http://www.paralleldatageneration.org/drupal6/ , last accessed 05 -10-2013 [RFS+10] T. Rabl, M . Frank, H . Sergieh, H . Kosch: A Data Generator for Cloud-Scale Benchmarking. TPCTC 2010: 41- 56 [Página 5] [SBH+06] Y. Sismanis, P. Brown, P.",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "TPCTC 2010: 41- 56 [Página 5] [SBH+06] Y. Sismanis, P. Brown, P. Haas, B. Reinwald: GORDIAN: Efficient and Scalable Discovery of Composite Keys. VLDB 2006: 691-702 [SHM+06] U. Srivastava, P. Haas, V . Markl, M . Kutsch, T .Tran: ISOMER: Consistent Histogram Construction Using Query Feedback. ICDE (2006) [Spa 13] http://spark -project.org/ , last accessed 05 -10-2013 [Str13] http://www.stratosphere.eu/ , last accessed 05 -10-2013 [Tor12] E. Torlak: Scalable test data generation from multidimensional models . Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of S oftware Engineering (2012) [TPC13] http://www.tpc.org , last accessed 05 -10-2013 [TSJ+09] A. Thusoo, J. S.Sarma, N. Jain, Z. Shao, P.Chakka, S. Anthony, H. Liu, P. Wyckoff, R. Murthy: Hive - A Warehousing Solution Over a M ap-Reduce Framework. PVLDB 2(2),",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "Hive - A Warehousing Solution Over a M ap-Reduce Framework. PVLDB 2(2), pp. 1626- 1629 (2009) [XML13] http://www.xml-benchmark.org/ , last accessed 05 -10- 2013 [PPR+09] A. Pavlo, E . Paulson, A . Rasin, D. Abadi, D . DeWitt, S. Madden, M . Stonebraker: A comparison of approaches to large-scale data analysis. SIGMOD Conference 2009: 165 -178 [DQJ+10] J. Dittrich, J . Quiané -Ruiz, A . Jindal, Y . Kargin, V . Setty, J . Schad: Hadoop++: Making a Yellow Elephant Run Like a Cheetah (Without It Even Noticing). PVLDB 3(1): 518- 529 (2010) [JOS+10] D. Jiang, B . C. Ooi, L. Shi, S . Wu: The Performance of MapReduce: An In-depth Study. PVLDB 3(1):472-483 (2010) [LCK+05] J. Leskovec, D . Chakrabarti, J . Kleinberg, C . Faloutsos: Realistic,",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "J. Leskovec, D . Chakrabarti, J . Kleinberg, C . Faloutsos: Realistic, Mathematically Tractable Graph Gene ration and Evolution, Using Kronecker Multiplication. PKDD 2005: 133 -145 [KTF09] U. Kang, C .E. Tsourakakis, C . Faloutsos: PEGASUS: A Peta -Scale Graph Mining System. ICDM 2009: 229-238 [KTA+11] U. Kang, C.E. Tsourakakis, A.P. Appel, C . Faloutsos, J. Leskovec: HADI: Mining Radii of Large Graphs. TKDD 5(2): 8 (2011)",
        "start_idx": 4060,
        "end_idx": 4127
      }
    ],
    "c1e3eb5c-c9e9-4b54-91de-8f4b3dfab991": [
      {
        "text": "[Página 1] Computers in Biology and Medicine 163 (2023) 107166 Available online 9 June 2023 0010-4825/© 2023 Elsevier Ltd. All rights reserved.An enhanced grey wolf optimizer boosted machine learning prediction model for patient-flow prediction Xiang Zhanga, Bin Lub, Lyuzheng Zhangc, Zhifang Pand, Minjie Liaoa, Huihui Shena, Li Zhange, Lei Liuf, Zuxiang Lig,*, YiPao Huh,**, Zhihong Gaoi,*** aWenzhou Data Management and Development Group Co.,Ltd, Wenzhou, Zhejiang, 325000, China bWenzhou City Bureau of Justice, Wenzhou, Zhejiang, 325000, China cB-soft Co.,Ltd., B-soft Wisdom Building, No.92 Yueda Lane, Binjiang District, Hangzhou, 310052, China dThe First Affiliated Hospital of Wenzhou Medical University, Wenzhou, 325000, China eWenzhou Hongsheng Intellectual Property Agency (General Partnership), Wenzhou, Zhejiang, 325000, China fCollege of Computer Science, Sichuan University, Chengdu, Sichuan, 610065, China gOrganization Department of the Party Committee, Wenzhou",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "University, Chengdu, Sichuan, 610065, China gOrganization Department of the Party Committee, Wenzhou University, Wenzhou, 325000, China hWenzhou Health Commission, Wenzhou, Zhejiang, 325000, China iZhejiang Engineering Research Center of Intelligent Medicine, The First Affiliated Hospital of Wenzhou Medical University, Wenzhou, 325000, China ARTICLE INFO Keywords: Patient-flow prediction Support vector regression Machine learning Meta-heuristic Swarm-intelligence ABSTRACT Large and medium-sized general hospitals have adopted artificial intelligence big data systems to optimize the management of medical resources to improve the quality of hospital outpatient services and decrease patient wait times in recent years as a result of the development of medical information technology and the rise of big medical data. However, owing to the impact of several elements, including the physical environment, patient, and physician behaviours, the real optimum treatment effect does",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "physical environment, patient, and physician behaviours, the real optimum treatment effect does not meet expectations. In order to promote orderly patient access, this work provides a patient-flow prediction model that takes into account shifting dy- namics and objective rules of patient-flow to handle this issue and forecast patients ’ medical requirements. First, we propose a high-performance optimization method (SRXGWO) and integrate the Sobol sequence, Cauchy random replacement strategy, and directional mutation mechanism into the grey wolf optimization (GWO) al- gorithm. The patient-flow prediction model (SRXGWO-SVR) is then proposed using SRXGWO to optimize the parameters of support vector regression (SVR). Twelve high-performance algorithms are examined in the benchmark function experiments ’ ablation and peer algorithm comparison tests, which are intended to validate SRXGWO ’s optimization performance. In order to",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "which are intended to validate SRXGWO ’s optimization performance. In order to forecast independently in the patient-flow prediction trials, the data set is split into training and test sets. The findings demonstrated that SRXGWO-SVR outperformed the other seven peer models in terms of prediction accuracy and error. As a result, SRXGWO-SVR is anticipated to be a reliable and efficient patient-flow forecast system that may help hospitals manage medical resources as effec- tively as possible. 1.Introduction Primary medical care is the guarantee of people ’s survival and development. With the continuous development of economic, cultural, and social construction, people ’s demand for medical resources is much higher. Their awareness of medical and health care also increases re- quirements for the current medical industry. Since the medical service system is",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "quirements for the current medical industry. Since the medical service system is complex, it is not only influenced by factors such as local de- mographic characteristics, socio-economic conditions, natural environ - mental conditions, medical hardware, software facilities, and patient *Corresponding author. **Corresponding author. ***Corresponding author. E-mail addresses: zhxan@126.com (X. Zhang), wzlubin@139.com (B. Lu), 66199293@qq.com (L. Zhang), panzhifang@wmu.edu.cn (Z. Pan), 1829820@qq.com (M. Liao), ylvias7@126.com (H. Shen), 101744491@qq.com (L. Zhang), liulei.cx@gmail.com (L. Liu), lizuxiang@wzu.edu.cn (Z. Li), huyipao@outlook.com (Y. Hu), gzh@wzhospital.cn (Z. Gao). Contents lists available at ScienceDirect Computers in Biology and Medicine u{�~zkw! s{yo|kro>! ÐÐÐ1ow�o �to~1m{y2w{m k�o2m{y|lt{ yon! https://doi.org/10.1016/j.compbiomed.2023.107166 Received 10 March 2023; Received in revised form 25 May 2023; Accepted 8 June 2023 [Página 2] Computers in Biology and Medicine 163 (2023) 107166 2and doctor behaviors [1]. But there",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "Biology and Medicine 163 (2023) 107166 2and doctor behaviors [1]. But there are also various interactions and positive and negative feedback between these influencing factors, which may result in the longer the waiting time in the hospital, the more attractive the patients are, or the regular changes in the hospital waiting queue, etc. Self-organized regularities and Emergent behavior make it difficult for hospitals to implement optimal outpatient management measures and cause the actual use of available resources not to match the expected results [2]. Therefore, to improve the efficiency of existing medical resources, improve the quality of hospital outpatient services, shorten patient waiting queues and waiting times, it is crucial to un- derstand the changing dynamics and objective patterns of patient-flow to provide a basis for dynamic adjustment",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "and objective patterns of patient-flow to provide a basis for dynamic adjustment of physician consultation plans and to achieve orderly and effective patient control. In recent years, the advancement of medical informatization and the rise of big medical data has allowed studying patient-flow prediction based on big data mining. Researchers have conducted some research in the analysis of patient-flow change patterns, analysis of patient-flow influencing factors, and patient-flow prediction. Li et al. [3] proposed a time series patient-flow prediction method based on XGBoost, a sup- port vector machine (SVM), to solve the problem of planning and allo- cation of healthcare resources by government and hospital management. Nikakhtar et al. [4] proposed a patient visit prediction model based on eigendistance and mesocentricity that can help healthcare managers and decision-makers",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "based on eigendistance and mesocentricity that can help healthcare managers and decision-makers predict the trend of infectious patient-flow. Sharafat et al. [5] proposed an emergency room patient-flow prediction model (PatientFlowNet) based on a deep learning framework, including pre- dicting arrival, treatment, and discharge rates. The results show that PatientFlowNet has higher accuracy and lower average absolute error than the benchmark algorithm. Tavakoli et al. [6] proposed a seasonal autoregressive integrated moving average (SARIMA) model for patient-flow prediction of the current epidemic of neocrown pneumonia disease, effectively predicting the number of patients’ visits to Thai hospitals in the next 30. According to the current research status, it is easy to find that more and more researchers are using machine learning techniques to predict the number of patient visits in",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "using machine learning techniques to predict the number of patient visits in hospitals. However, since most of the prediction models use a monadic time-series feature prediction method and the changes of patient-flow are affected by a variety of complex factors and do not have obvious linear characteris - tics, resulting in the accuracy of the models is not high. On the other hand, it is limited by the defects of the classification predictor itself, which leads to large prediction bias of prediction models based on SVM and other prediction models. Therefore, how to improve the accuracy and reduce the error of patient-flow prediction models is a major chal- lenge in current medical resource scheduling research. As a novel optimization method with strong robustness and flexi- bility, the swarm",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "a novel optimization method with strong robustness and flexi- bility, the swarm intelligence optimization algorithm is widely used in predictive optimization problems. The swarm intelligence optimization algorithm is a stochastic optimization algorithm abstracted by simu- lating the collaborative behavior of animals, insects, and other organ - isms. The current well-known algorithms are, grey wolf optimization (GWO) [7], bat-inspired algorithm (BA) [8], different evolution (DE) [9], sine cosine algorithm (SCA) [10], salp swarm algorithm (SSA) [11], whale optimizer (WOA) [12], moth-flame optimization (MFO) [13], particle swarm optimization (PSO) [14], hunger games search (HGS) [15], Harris hawks optimization (HHO) [16], rime optimization algo- rithm (RIME) [17], colony predation algorithm (CPA) [18], Runge Kutta optimizer (RUN) [19], weighted mean of vectors (INFO) [20], slime mould algorithm (SMA) [21,22], opposition-based SCA (OBSCA)",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "of vectors (INFO) [20], slime mould algorithm (SMA) [21,22], opposition-based SCA (OBSCA) [23], modified SCA (m_SCA) [24], boosted GWO (OBLGWO) [25], A-C para- metric WOA (ACWOA) [26], fruit fly optimizer (FOA) with multi-population outpost mechanism (MOFOA) [27], SCA with differ - ential evolution (SCADE) [28], and so on. They also have been applied to solve many problems such as bankruptcy prediction [29], feature se- lection [30–34], economic emission dispatch [35], multi-objective optimization [36], global optimization [37,38], dynamic multi-objective optimization [39], numerical optimization [40–42], scheduling optimization [43,44], feed-forward neural networks [45], medical image segmentation [46–48], feature selection [49,50], per- formance optimization [51,52], identification of pulmonary hyperten - sion animal [53], constrained multi-objective optimization [54], and large-scale complex optimization [55]. More and more researchers are considering optimizing models using swarm",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "optimization [55]. More and more researchers are considering optimizing models using swarm intelligence optimization methods to improve the accuracy of prediction methods. Chou et al. [56] proposed a swarm intelligence algorithm-based support vector machine prediction model (SFALSSVM) using the smart firefly algorithm (SFA) to optimize the parameters of the least squares support vector regression (SVR) and successfully applied it to several geotechnical engineering problems. Kaushik et al. [57] pro- posed a binary swarm intelligence algorithm by combining the firefly algorithm and bat algorithm with a wavelet neural network (WNN) and offered a prediction model for software development effort (SDEE), which has high prediction accuracy. Mehraein et al. [58] proposed a CatBoost (CB) prediction model based on a swarm intelligence algorithm for predicting the monthly flow of satellite precipitation",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "a swarm intelligence algorithm for predicting the monthly flow of satellite precipitation data and demonstrated a significant reduction in the root mean square error (RMSE) of the proposed CB compared with an artificial neural network (ANN). Zhu et al. [59] combined the WOA and the simulated annealing algorithm (SA) to optimize the kernel extreme learning machine (KELM). They proposed an enhanced search-based prediction algorithm (EMWS) that effectively addresses defect prediction in software modules. Zhou et al. [60] improved the Firefly algorithm (FA) by incorpo - rating chaotic mapping, adaptive inertia weights, and Levy flight for accurate prediction of reinforcement tensile loads for assessing the in- ternal stability of geosynthetic reinforced soil (GRS) structures. They used the improved FA to optimize the hyperparameters of the least-squares SVR model. The",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "improved FA to optimize the hyperparameters of the least-squares SVR model. The improved SVR model had excellent ac- curacy with an average absolute percentage error of less than 10%. Ma et al. [61] proposed an SVR prediction model integrated with k-fold cross-validation (CV) and used an artificial bee colony (ABC) algorithm and genetic algorithm (GA) to optimize the hyperparameters of the model. The results showed that the hybrid approach can be used to determine the optimal hyperparameters and present statistical signifi - cance. Huang et al. [62] proposed a swarm intelligence algorithm (DFP) integrating floral pollination algorithm (FPA) and differential evolution (DE) and an algorithmic model for predicting the groutability of cement paste in combination with SVR. Luo et al. proposed a hybrid prediction model (LS-SVMR) using a",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "SVR. Luo et al. proposed a hybrid prediction model (LS-SVMR) using a coupled simulated annealing (CSA) algorithm to optimize the hyperparameter selection of SVR, which effectively implemented the lateral strength prediction of reinforced concrete (RC) columns. Based on the above improvement methods for prediction models, it can be found that swarm intelligence optimization algorithms can effectively help prediction models find optimal hyperparameters, and SVR is applied very frequently in many models. However, due to the variety of swarm intelligence algorithms, each algorithm has defects, such as low convergence accuracy, slow search speed, and easy falling into local optimality. Therefore, in this paper, to accurately predict the number of patients and reasonably schedule medical resources, an SVR prediction model based on improved GWO is proposed using the GWO algorithm",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "prediction model based on improved GWO is proposed using the GWO algorithm with high exploitation capability combined with SVR pre- diction methods. First, to give full play to the exploitation advantages of GWO and overcome the shortcomings of GWO in the search process as much as possible, the following three methods are used for improve - ment: (1) To address the problem of narrow coverage of the initialized search agent of GWO, the original random initialization method is used instead of Sobol sequence to expand the distribution of the initial so- lution. (2) To address the problem of too little information exchange among GWO search agents, a directional mutation mechanism is used to increase the interactivity of solutions, improving the algorithm’s search efficiency. (3) To address the problem",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "of solutions, improving the algorithm’s search efficiency. (3) To address the problem of imbalance between GWO search and exploitation, a Cauchy random replacement strategy is added X. Zhang et al. [Página 3] Computers in Biology and Medicine 163 (2023) 107166 3to the core update formula to adjust the weights of search and exploi - tation of the algorithm in the iterative process. Based on the above ideas, Sobol sequence-based population initialization, Cauchy random replacement strategy, and directional mutation mechanism are intro- duced into GWO to propose a high-performance GWO variant (SRXGWO). Then, to verify the optimization performance of SRXGWO, this paper designs comparative simulation experiments based on the classical IEEE CEC2014 test set and compares SRXGWO with other X methods. The experiments show that the proposed SRXGWO method",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "with other X methods. The experiments show that the proposed SRXGWO method significantly improves initialization, search efficiency, and defects of iterative balance. This paper also analyzes the comparative results using the Wilcoxon signed-rank test [63] and the Friedman test [64]. SRXGWO has a higher convergence speed compared with peer algo- rithms and accuracy. Further, this paper proposes a multivariate SRXGWO-SVR prediction model for predicting patient flow by optimizing two hyperparameters of SVR using high-performance SRXGWO. To validate the real prediction ability of the SRXGWO-SVR model, the prediction results of the model are presented in detail using real clinical data sets and divided into training and test sets. Further, the SRXGWO-SVR model based on SRXGWO, the GWO-SVR model based on GWO, and the original SVR model are compared in",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "model based on GWO, and the original SVR model are compared in this paper, and the experimental results also demonstrate that the SRXGWO-SVR can effectively outperform the two original models without improvement. Finally, this paper also compares the SRXGWO-SVR model with well-known prediction models such as Radial basis function networks, convolutional neural networks, etc. R- squared (R2), root mean squared error (RMSE), and mean absolute error (MAE) are used for validation and confirm that SRXGWO-SVR is more advantageous in predicting hospital patient-flow. The data set used in this paper is the attendance statistics of Wenzhou Medical University Hospital in China, which serves a radius of nearly 30 million people and has an annual outpatient volume of 5.3 million. Due to the large volume of data, the latest data",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "5.3 million. Due to the large volume of data, the latest data from January 2022 to September 2022 is selected, with a sample size of 240 items. The main contributions of this paper are as follows. 1. Sobol sequence-based population initialization, Cauchy random replacement strategy, and directional mutation mechanism are introduced into GWO to propose a high-performance algorithm SRXGWO. The strategies and mechanisms employed in this paper can provide a valid reference for the field of evolutionary computation. 2.We designed experiments comparing SRXGWO with 12 similar al- gorithms to verify the algorithm ’s improvement ideas and optimi - zation performance. Experiments can effectively demonstrate the performance of SRXGWO ’s benchmark functions and provide illus- trations for their specific applications. 3. SRXGWO is used to optimize the hyperparameters of",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "their specific applications. 3. SRXGWO is used to optimize the hyperparameters of SVR, and the SRXGWO-SVR multivariate prediction model is proposed and suc- cessfully applied to predict patient flow. The proposed model can effectively predict patient flow and provide useful suggestions for hospital management. 4. We designed a comparison experiment between SRXGWO-SVR and eight similar prediction models to verify the effectiveness of the improvement and the accuracy of the prediction. The experiments illustrate that the proposed model has great potential for predicting other time series problems. The rest of this paper is organized as follows. Section 2 describes the prediction dataset, the original GWO, and SVR. In Section 3, SRXGWO is proposed based on three improvement strategies, and the SRXGWO-SVR model is proposed in conjunction with SVR. In",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "strategies, and the SRXGWO-SVR model is proposed in conjunction with SVR. In Section 4, benchmark function comparison experiments and simulation prediction comparison experiments are designed. Finally, Section 5 summarizes the work of this paper and illustrates further research directions. 2.Materials and methods This section introduces the swarm intelligence optimization algo- rithm GWO and the regression prediction model SVR used in this study. 2.1. Description of GWO algorithm In the GWO algorithm, grey wolf individuals are divided into four classes: α、β、δ and ω. α is mainly responsible for participating in the decision-making and management of the pack; ω is for other grey wolf individuals; β and δ are for grey wolf individuals with the second highest adaptation level to α. The GWO algorithm focuses on three behaviors: encirclement behavior,",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "level to α. The GWO algorithm focuses on three behaviors: encirclement behavior, hunting behavior, and attack behavior. 1. Encirclement behavior The first stage of prey predation by grey wolves is to encircle the prey, and the mathematical model can be described by Eq. (1) and Eq. (2). D↗⃦⃦⃦⃦C↗⋅X↗ p t\u0000X↗ t⃦⃦⃦⃦(1) X↗ t1X↗ v t\u0000A↗⋅D↗(2) where D↗is the distance between the prey and the wolves; A↗2a⋅r2\u0000 a, C↗2⋅r↗ 2; X↗is the current location of the wolves; t is the number of current iterations; X↗ p is the location of the prey; r1 , r2 are random numbers, between 0C1; a∃2C0. 2. Hunting behavior After a wolf pack surrounds a prey, it will hunt the surrounding prey. If α is the global optimal solution, β is the global second",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "If α is the global optimal solution, β is the global second solution, and δ is the global third solution, then the mathematical model of α, β, and δ repositioning can be described by Eqs. (3)–(5). D↗ α⃦⃦⃦⃦C↗ 1⋅X↗ α\u0000X↗⃦⃦⃦⃦(3) D↗ β⃦⃦⃦⃦C↗ 2⋅X↗ β\u0000X↗⃦⃦⃦⃦(4) D↗ δ⃦⃦⃦⃦C↗ 2⋅X↗ δ\u0000X↗⃦⃦⃦⃦(5) where D↗ α, D↗ β and D↗ δ denote the approximate distances of α, βCand δ from X↗, respectively; X↗ α, X↗ β, X↗ δ denote the position information of α, β, and δ, respectively; C↗ 1, C↗ 2 and C↗ 3 denote the random vectors, respectively. The current solution X↗and the updated solution X↗ t1 can be described by Eq. (6)-Eq. (9). X↗ 1X↗ α\u0000A↗ 1⋅( D↗ α) (6) X↗ 2X↗ β\u0000A↗ 2⋅[ D↗ β] (7) X↗ 3X↗ δ\u0000A↗",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "α) (6) X↗ 2X↗ β\u0000A↗ 2⋅[ D↗ β] (7) X↗ 3X↗ δ\u0000A↗ 3⋅( D↗ δ) (8) X↗′ t1[ X↗ 1X↗ 2X↗ 3][ 3 (9) where A↗ 1 , A↗ 2 , and A↗ 3 denote random vectors, respectively. X. Zhang et al. [Página 4] Computers in Biology and Medicine 163 (2023) 107166 43. Attack behavior The final stage of the GWO algorithm is the prey attack phase, which can be achieved by adjusting the parameter A. If †A†≼1, the whole wolf pack approaches the prey X∗CY∗and focuses on the prey; if †A†F1, the whole wolf pack moves away from the prey and looks for new prey again. 2.2. Description of support vector regression Support vector machine (SVM) models are used to classify data by mapping the input metric",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "(SVM) models are used to classify data by mapping the input metric data to a higher dimensional space, then constructing an optimal hyperplane in this higher dimensional space so that the constructed hyperplane has the largest edges to classify the input data. The learning strategy used by the support vector machine is interval maximization, which can be formalized as solving a convex quadratic programming problem. Instead of the traditional statistical induction followed by deduction, the SVR model constructs a regression function to infer a prediction model on the training data and then uses the model to make predictions. The objective of SVR modeling is to build a classification surface that separates the two types of samples as well as possible. SVR modeling aims to minimize the distance between",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "as well as possible. SVR modeling aims to minimize the distance between all the sample data and the classification surface. The accuracy of the SVR model is highly depen - dent on the kernel function ’s quality and the penalty factor ’s accuracy, and the appropriate choice of parameters dramatically improves the accuracy of the regression model. When the parameters of the regression model are not selected appropriately, the regression model will not be applicable to solve the actual problem. For the training data, regression aims to solve the following regression function, as in Eq. (10). f y〈W0y〉b (10) The above equation is 〈w0y〉 is the inner product of w and y. The following equation is the constraint to solve the constrained optimiza - tion problem: Min 1⎡",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "the constraint to solve the constrained optimiza - tion problem: Min 1⎡ 2Dw0wFĈm i1\u0000 ξiξ∗ i) (11) Zi\u0000Dw0yiFb≼εξi (12) Dw0yiF\u0000zib≼εyj\u0000yk (13) where C represents the penalty factor of the model, the value of C is positively related to the complexity of the model, the complexity of the model increases with the value of C, and the value of C is negatively related to the computational error of the model, the error of the model becomes smaller as the value of C increases. The solution of the optimization problem is first transformed into the corresponding pairwise problem and, secondly transformed into the solution of the maximum constraint value by introducing the kernel function. Finally, the regression equation of the model is shown in Eq. (14). f y̂m i1\u0000 aj\u0000aj)0k\u0000",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "of the model is shown in Eq. (14). f y̂m i1\u0000 aj\u0000aj)0k\u0000 yCyj) b (14) 3.The proposed method In this section, three improvement ideas are described, namely, Sobol sequence-based population initialization, Cauchy random replacement strategy, and directional mutation mechanism. Finally, the proposed SRXGWO is used to optimize the hyperparameters of the SVR model, and the patient-flow prediction model SRXGWO-SVR is proposed. 3.1. Proposed GWO variant 3.1.1. Sobol sequence-based population initialization The population initialization of the original GWO algorithm is randomly generated, which primarily affects the algorithm ’s perfor - mance. In contrast, the Sobol sequence can make the spatial points uniformly distributed and generate unlimited samples without pre- determining the number of samples and storing them. Therefore, this paper introduces the Sobol sequence to filter the initialization position",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "Therefore, this paper introduces the Sobol sequence to filter the initialization position of the grey wolf population, improve the uniformity and diversity of the grey wolf population, and improve the performance of the original GWO algorithm. Each dimension of the Sobol sequence is a Radical inversion with base 2, and each dimension has a different generating matrix C. When C is taken as a unit vector, the corresponding Sobol sequence is repre - sented as N i̂M k12\u0000kak i (15) where i is denoted as a binary number, ak ion each bit of the number is arranged as a vector, which is mirrored to the right of the decimal point and converted to decimal, resulting in a one-dimensional Sobol sequence Xi⊔N 1CN 2…CN iCi∃N⊓, and a multi-dimensional Sobol",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "a one-dimensional Sobol sequence Xi⊔N 1CN 2…CN iCi∃N⊓, and a multi-dimensional Sobol sequence is obtained by multiplying the generating matrix C of each dimension. The Sobol sequence is used to uniformly distribute n points within the threshold of the target parameter search as the initialized population space location. The first three solutions are defined as α, β, and δ wolves, respectively. To confirm the effectiveness of Sobol sequence-based population initialization, Ablation experiments of SRXGWO are designed in Section 4.1.2 , where SGWO is the improved GWO using this strategy alone. 3.1.2. Cauchy random replacement strategy In the iterative process, the position update of GWO is conservative. On the one hand, such an update is beneficial to the exploitation of the algorithm. Still, on the other hand, it may",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "the exploitation of the algorithm. Still, on the other hand, it may cause the algorithm to have a poor quality of the search solution and fall into local optimum when dealing with multi-peaked problems. Therefore, in this paper, to solve this problem, the Cauchy replacement search strategy is used to appropriately perturb the dimensionality of the search agent and improve the interaction between individuals. Specifically, firstly, the grey wolf population with the number of individuals N is traversed by the parameter l, and the selected one is the Xl individual. Then, according to the ratio of the remaining runs of the algorithm to the total number of runs compared with the Cauchy random number, if the Cauchy random number is less than the ratio, the h-th dimensional value",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "Cauchy random number is less than the ratio, the h-th dimensional value of Xl is replaced with the hth dimensional value of the optimal solution α wolves. Finally, the fitness value of the updated Xl The evaluation function calculates the optimal solution, and the optimal fitness value are replaced if the fitness value is better than the optimal solution. Otherwise, it remains unchanged. To confirm the effectiveness of the Cauchy replacement search strategy, RGWO in Ablation experiments of SRXGWO is the GWO improved using this strategy alone. 3.1.3. Directional mutation strategy Since the original GWO relies too much on the searchability of the top three ranked wolves to find the optimal solution, it is easy to fall into the local optimal trap and reduce the accuracy of the",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "fall into the local optimal trap and reduce the accuracy of the optimal solution. Therefore, this paper proposes a directional mutation strategy based on genetic algorithms ’ mutation and crossover strategies. The directional mutation strategy consists of two important operations: directional crossover and directional variation. 1. Directional crossover (DM) X. Zhang et al. [Página 5] Computers in Biology and Medicine 163 (2023) 107166 5The when-directed crossover mechanism uses the position informa - tion of the current iteration ’s optimal individual to guide the in- dividual ’s next change trend. There are four main parameters, which are crossover rate (pc), variable crossover probability (pcv), directional probability (pd) and multiplication factor (α). First, the execution of the directed crossover mechanism requires different parent individuals in the current population. The parent",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "crossover mechanism requires different parent individuals in the current population. The parent individuals are generated by random selection from the population, pj 1 and pj 2, j∃1Cd]. pj mean and pj best are the mean value of the parent individuals in the jth dimension and the value of the best individual in the jth dimension, respectively. In the first case, when pj best≽pj mean (c1 and c2 does the directed hybridization mechanism generate the individuals). val1\u00000B5e⌈ †pj 1\u0000pj 2† yj u\u0000yj l⌉ (16) βr3 α2(17) c1val∗\u0000 pj 1\u0000pj 2) αr3∗e 1\u0000β∗ 1\u0000val∗⃦⃦pj 1\u0000pj 2⃦⃦Cifr4≼pd (18) c2 1\u0000val∗\u0000 pj 1\u0000pj 2) \u0000α 1\u0000r3∗e \u0000β∗val∗⃦⃦pj 1\u0000pj 2⃦⃦Cifr4≼pd (19) c1val∗\u0000 pj 1pj 2) \u0000αr3∗e 1\u0000β∗ 1\u0000val∗⃦⃦pj 1\u0000pj 2⃦⃦Cifr4Fpd (20) c2 1\u0000val∗\u0000 pj 1pj 2) α 1\u0000r3∗e \u0000β∗val∗⃦⃦pj 1\u0000pj 2⃦⃦Cifr4Fpd (21) When",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "c2 1\u0000val∗\u0000 pj 1pj 2) α 1\u0000r3∗e \u0000β∗val∗⃦⃦pj 1\u0000pj 2⃦⃦Cifr4Fpd (21) When pj bestDpj mean. c1val∗\u0000 pj 1pj 2) \u0000αr3∗e 1\u0000β∗ 1\u0000val∗⃦⃦pj 1\u0000pj 2⃦⃦Cifr4≼pd (22) c2 1\u0000val∗\u0000 pj 1pj 2) α 1\u0000r3∗e \u0000β∗val∗⃦⃦pj 1\u0000pj 2⃦⃦Cifr4≼pd (23) c1val∗\u0000 pj 1pj 2) αr3∗e 1\u0000β∗ 1\u0000val∗⃦⃦pj 1\u0000pj 2⃦⃦Cifr4Fpd (24) c2 1\u0000val∗\u0000 pj 1pj 2) \u0000α 1\u0000r3∗e \u0000β∗val∗⃦⃦pj 1\u0000pj 2⃦⃦Cifr4Fpd (25) If the parent individuals have the same value, but pj bestℑpj mean. val1\u00000B5e⌈ †pj best\u0000pj mean† yj u\u0000yj l⌉ (26) βr3 α2(27) c1val∗\u0000 pj bestpj mean) αr3∗e 1\u0000β∗ 1\u0000val∗\u0000 pj best\u0000pj mean) Cifr4≼pd (28) c2 1\u0000val∗\u0000 pj bestpj mean) \u0000α 1\u0000r3∗e \u0000β∗val∗\u0000 pj best\u0000pj mean) Cifr4 ≼pd (29) c1val∗\u0000 pj bestpj mean) \u0000αr3∗e 1\u0000β∗ 1\u0000val∗\u0000 pj best\u0000pj mean) Cifr4Fpd (30) c2 1\u0000val∗\u0000 pj bestpj mean) α 1\u0000r3∗e \u0000β∗val∗\u0000 pj best\u0000pj mean)",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "(30) c2 1\u0000val∗\u0000 pj bestpj mean) α 1\u0000r3∗e \u0000β∗val∗\u0000 pj best\u0000pj mean) Cifr4Fpd (31) where r3 and r4 are two different random numbers, r3∃ 0C1and r4∃ 0C1. val and β are two parameters computed in each iteration. yj u and yj l are the upper and lower bounds of the individual in the jth dimension, respectively. А is the multiplicative factor. 2. Directional variation First, assume that the dimensions of population size and objective function are D and d, respectively. Assume that the current iteration individual is y. The guided variation mechanism guides the variation of the current iteration individual y based on the position information of the current optimal individual ybest. When individual y is selected for guided mutation operation, the DM mechanism will compare the size",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "selected for guided mutation operation, the DM mechanism will compare the size of yj i and yj best, if yj best≽yj i. β1e[ 2r\u00002 r] (32) β2e[ r\u00002 r] (33) ym| 〈 ⎜yj iβ1∗\u0000 yj u\u0000yj i) Cifr2≼pd yj i\u0000β2∗\u0000 yj i\u0000yj l) Cotherwise(34) where β1 and β2 are two parameters, which can also be called the weights that determine the change steps of the formula. r and r2 are two random numbers, r∃ 0C1and r2∃ 0C1, rℑ0. yj u and yj l are the upper and lower bounds of the individual in the jth dimension, respectively. pd represents the orientation probability, pd∃ 0B5C1. If yj bestDyj i. ym| 〈 ⎜yj i\u0000β1∗\u0000 yj i\u0000yj l) Cifr2≼pd yj iβ2∗\u0000 yj u\u0000yj i) Cotherwise(35) To illustrate the effectiveness of the",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "yj iβ2∗\u0000 yj u\u0000yj i) Cotherwise(35) To illustrate the effectiveness of the Directional mutation strategy, the XGWO in ablation experiments of SRXGWO is the GWO improved using this strategy alone. 3.1.4. Proposed SRXGWO The analysis shows that GWO is an excellent algorithm with solid exploitation capability, but several aspects still need improvement. First, GWO is randomly generated with strong uncertainty in the initialization of the grey wolf population, which will lead to the initial solution of the whole population cannot effectively cover the solution space of the problem, thus causing problems such as low efficiency in the search phase. Secondly, the lack of information exchange among individuals in the iterative process of GWO tends to make the algorithm suffer from poor-quality of search solutions and fall into local",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "the algorithm suffer from poor-quality of search solutions and fall into local optimum when dealing with multi-peaked problems. In addition, GWO relies too much on the exploitation ability of the top three ranked wolves to find the optimal solution, which cannot effectively search the whole solution space, leading to the inability to find the optimal solution and reducing the quality of understanding. Therefore, this paper addresses the above three problems and makes corresponding improvements to GWO. First, Sobol sequence-based population initialization is used instead of the original random initiali - zation method to generate a low-sequence population of grey wolves, which covers the whole solution space uniformly. Second, the dimen - sional values between search agents are effectively exchanged by Cau- chy’s random replacement strategy to enhance the",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "are effectively exchanged by Cau- chy’s random replacement strategy to enhance the information exchange between individuals and improve the exploitation capability of the algorithm. Third, the directional mutation mechanism is intro- duced to perform crossover and mutation at the level of the search so- lution, and the crossover or mutation operation is performed for the nature of the current individuals, which effectively improves the search ability of the algorithm and the ability to jump out of the local optimum. The algorithm flowchart of SRXGWO as shown in Fig. 1. 3.2. The proposed SRXGWO-SVR model To accurately predict the number of patients and reasonably schedule medical resources, this section combines the high-performance X. Zhang et al. [Página 6] Computers in Biology and Medicine 163 (2023) 107166 6SRXGWO algorithm with",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "6] Computers in Biology and Medicine 163 (2023) 107166 6SRXGWO algorithm with the SVR prediction method and proposes the SRXGWO-SVR, an SVR prediction model based on the improved GWO. According to Section 2.2, SVR is a supervised machine learning method with two key parameters: the penalty parameter C and the kernel function parameter g. The penalty parameter C affects the complexity and stability of the model, the kernel function parameter reflects the distribution of samples in the feature space, and the parameter selection directly impacts the prediction accuracy and generalization ability of the model. Therefore, to address the above is- sues, SRXGWO is introduced to optimize the radial basis kernel function parameters and penalty factors in the SVR patient-flow prediction model to form the best combination of parameters",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "the SVR patient-flow prediction model to form the best combination of parameters to improve the prediction accuracy and reduce the error size. The specific steps for building the SRXGWO-SVR model are as follows. (1) Data pre-processing. Routine data pre-processing is performed on the collected patient-flow data, including data cleaning, missing value processing, outlier processing, etc. (2) Establish the objective function. The sample data are substituted into the mean square error minimization function as shown in Eq. (26), and then the optimal radial basis kernel function parameters C and penalty factor γ are obtained. Qm CCσ1 n̂n k1 yk\u0000}yksBtBC∃CminCCmaxCγ∃γminCγmax (36) where yk denotes the actual size of the patient flow, and √yk denotes the corresponding size value of the patient-flow prediction. (3) Search for hyperparameters using SRXGWO. First, the",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "of the patient-flow prediction. (3) Search for hyperparameters using SRXGWO. First, the parameters involved in the SRXGWO algorithm are set initially. The fitness function RMSE is applied to calculate the fitness values of the population individuals, where m is the number of samples. RMSE⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ 1 m̂m k1 yk\u0000}yk2̅ (37) (4) Determine whether the maximum number of iterations is reached. The iteration is continued if the maximum number of iterations is not reached. Suppose the maximum number of iter- ations is reached. In that case, the C and γ corresponding to the optimal individual location information is output. The best combination of the two parameters is applied to build the SRXGWO-SVR prediction model. Then the patient-flow dataset is predicted. The flow chart of the SRXGWO-SVR prediction model based on",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "is predicted. The flow chart of the SRXGWO-SVR prediction model based on hospital patient-flow proposed in this section is shown in Fig. 2. 4.Experimental results and discussions In this section, ablation and benchmark function experiments are designed to validate the global optimization performance of SRXGWO. Then, the proposed SRXGWO-SVR is used in patient-flow prediction experiments to demonstrate the accuracy and validity of SRXGWO-SVR. 4.1. Benchmark functions comparison experiment 4.1.1. Benchmark test experiment setup First, the running environment of the benchmark function test experiment needs to be described. the software of the experiment is Matlab2017b and the core hardware is Intel(R) Xeon(R) CPUE5-2660v3 (2.60 GHz). The benchmark function test set used in this section is the currently familiar IEEE CEC2014, described in detail in Table 1. The comparison experiments",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "familiar IEEE CEC2014, described in detail in Table 1. The comparison experiments include SRXGWO and GWO and well-known Fig. 1.Algorithm flow chart of SRXGWO The algorithmic complexity of SRXGWO comes mainly from Sobol sequences, core formula updates, Cauchy random replacement strategy, and directional mutation mechanism. The complexity level of Sobol sequence initialization is O N; the computational complexity level of the core formula is O N2 N∗logN; the computational complexity level of Cauchy random replacement strategy is O N∗logN; and the complexity level of directional mutation mechanism is O N2. By comprehensive calculation, the overall complexity level of SRXGWO is O SRXGWOO N2N∗logN. X. Zhang et al. [Página 7] Computers in Biology and Medicine 163 (2023) 107166 7algorithms such as PSO, SCA, etc. Therefore, to ensure the validity",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "107166 7algorithms such as PSO, SCA, etc. Therefore, to ensure the validity and fairness of the experiments, all swarm intelligence algorithms are searched in dimension 30, the population size is 30, the number of evaluations is also uniformly 300,000, and the internal parameters of the algorithms are all default values. Finally, to ensure the correctness and validity of the experimental results, all the algorithms were run independently 30 times, and the results of the experiments were further verified using Wilcoxon signed-rank test and the Friedman test. 4.1.2. Ablation experiments In this section, ablation experiments of SRXGWO were designed to discuss the effects of Sobol sequence-based population initialization, Cauchy random replacement strategy, and directional mutation mech - anism on the effect of GWO. First, the experiments combined the three",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "anism on the effect of GWO. First, the experiments combined the three improved strategies with GWO by permutation, including GWO itself, with a total of eight algorithms, as shown in Table 2. In the table, S stands for Sobol sequence-based population, R stands for Cauchy random replacement strategy, and X stands for directional mutation mechanism. in addition, “1″ indicates that the current strategy is used, and “0″ indicates that no strategy is used. For example, SGWO uses the Sobol sequence but not the other two strategies. Table 3 shows the experimental results of SRXGWO with the other seven algorithms, including the Wilcoxon signed-rank test results and P- value. The number of algorithms that are “better than/equal to/worse than ” other algorithms. “Mean ” indicates the average ranking of",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "to/worse than ” other algorithms. “Mean ” indicates the average ranking of the 30 functions tested, and “rank ” indicates the final overall ranking. In the results of the Wilcoxon test, SRXGWO is 23 better than the unimproved GWO, which indicates that the improvement of GWO by the three improvement strategies is very significant. In addition, SRXGWO has a significant advantage over SGWO, RGWO, and XGWO using a single mechanism, with at least 14 stronger than them. Finally, SRXGWO has an advantage over the two-two combination of SRGWO, SXGWO, and RXGWO, indicating that the three SRXGWO improvement strategies are effective. The table also shows the empirical p-values, and the bolded data indicate that SRXGWO is significantly different from other algo- rithms, and it can be said that the",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "different from other algo- rithms, and it can be said that the advantage of SRXGWO is more prominent compared to other algorithms. In summary, the mechanism employed in SRXGWO is reasonable and effective, and can significantly improve the performance of GWO. 4.1.3. Comparison of SRXGWO with well-known peer algorithms In this subsection, similar algorithm comparison experiments are designed based on 30 benchmark functions to compare SRXGWO with 12 other peer algorithms to demonstrate that the proposed algorithm has more robust optimization performance among the same type of al- gorithms. Among the compared algorithms, six original algorithms are PSO, SCA, MFO, WOA, BA, and FA, all highly cited algorithms. The other six algorithms are new variants proposed recently, including OBSCA, m_SCA, OBLGWO, ACWOA, MOFOA, and SCADE. Table 4 shows",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "recently, including OBSCA, m_SCA, OBLGWO, ACWOA, MOFOA, and SCADE. Table 4 shows the experimental results of the comparison. Where AVG denotes the average optimal fitness value of 30 independent ex- periments, STD denotes the variance of the experiments, and the bolded data are the optimal values of the current function of the algorithm. In the experimental results, SRXGWO finds the optimal solution relative to its peer algorithms in most of the function evaluations, especially in the class of complex functions F23–F30, which indicates that SRXGWO is more advantageous in dealing with complex problems. In addition, the STD fluctuation of SRXGWO is small, which suggests that the algorithm has strong stability. Similarly, to further validate the SRXGWO experimental results, we used the Wilcoxon signed-rank test to compare and validate",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "experimental results, we used the Wilcoxon signed-rank test to compare and validate SRXGWO, and the results are shown in Table 5; the Friedman test was used to verify the average ranking of SRXGWO, and the results are shown in Fig. 3, which can be more intuitive to observe the comparison results. The Wilcoxon signed-rank test results show that SRXGWO ranks first overall when comparing other algorithms and is at least 19 better than other high citation algorithms and 20 better than other variants. The Friedman test shows that the average ranking of SRXGWO is slightly different, but Fig. 2.SRXGWO-SVR prediction model based on hospital patient-flow. X. Zhang et al. [Página 8] Computers in Biology and Medicine 163 (2023) 107166 8it is still better than PSO and MFO algorithms,",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "163 (2023) 107166 8it is still better than PSO and MFO algorithms, and the overall perfor - mance is also the first. In summary, the results of the comparison experiment are valid and reasonable, and SRXGWO does outperform other peer algorithms. To further demonstrate the advantages of SRXGWO over other al- gorithms, this experiment recorded the optimization search process of each algorithm and plotted it as an iterative curve, as shown in Fig. 4. The horizontal coordinate indicates the number of evaluations, and the vertical coordinate indicates the fitness value. Firstly, it can be seen that SRXGWO has good convergence accuracy on F6, F8, F9, F10, F11 and F13 in unimodal and simple multimodal function classification and faster search speed than other similar algorithms. In addition, it can",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "and faster search speed than other similar algorithms. In addition, it can be observed in the hybrid and combinatorial functions F16, F23, F30 that SRXGWO also has excellent results in solving complex optimization problems. Further in the figure, SRXGWO has a clear advantage in the F6, F8, F9, F10, and F16 test functions. Both in the search period of the search process and the exploitation period of the iteration, SRXGWO can quickly find the current optimal solution. At the same time, the other algorithms cannot outperform SRXGWO from the beginning to the end. In addition, SRXGWO has a clear decreasing inflection point in the middle of the algorithm iteration in the function tests of F11 and F16. Few other algorithms can continue the development, which indicates that SRXGWO",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "F16. Few other algorithms can continue the development, which indicates that SRXGWO has a strong ability to jump out of the local optimum. Finally, the nine function tests in the figure demonstrate that SRXGWO has stronger search and exploitation capabilities than other algorithms and is a high-performance optimization algorithm. In future work, it also be applied to more cases, such as optimization of machine learning models [65], MRI reconstruction [66], service ecosystem [67], compu - tational experiments [68,69], power distribution network [70], and medical signals [71,72]. 4.2. Patient-flow prediction The patient-flow dataset is presented in this section, and SRXGWO- SVR training and test experiments are designed. First, the patient flow dataset used is presented. Immediately after, the experimental setup including comparison methods, parameter settings, and evaluation criteria are",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "the experimental setup including comparison methods, parameter settings, and evaluation criteria are described. Finally, SRXGWO-SVR is proposed and applied to the prediction of patient flow. 4.2.1. Patient-flow dataset The data set used in this section is the attendance statistics of Wenzhou Medical University Hospital in China, which serves a radius of nearly 30 million people and has an annual outpatient volume of 5.3 million. Due to the large volume of data, the latest data from January 2022 to September 2022 is selected, with a sample size of 240 items. The data ’s main characteristic attribute is “number of appointments, ” and the label attribute is “number of actuals ”. In addition, to reduce the dependence of the model on a single time series and the error of the",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "the model on a single time series and the error of the prediction results, this paper also selects three independent attribute series, namely, “number of people without pre-deposit system ”, “num- ber of people without ID”, and “number of late arrivals ”. “Three inde- pendent attribute series are selected to describe the trend changes of patient-flow with the influence of multiple factors. Finally, when col- lecting data, there are inevitably null values and outliers, and this paper also preprocesses the data by removing abnormal samples and linear interpolation. Fig. 5 shows a 240-day line graph of actual hospital visits. First of all, according to Fig. 5, we can see that the number of hospital visits as a whole fluctuates a lot, and there is a local repetition, mostly",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "a whole fluctuates a lot, and there is a local repetition, mostly between 14,000 and 4,200 visits. The main reason for this phenomenon is that the 14,000 visits are during the weekdays, i.e., Monday through Friday, when the hospital doctors are in regular attendance and the equipment is functioning normally, and the number of visits is relatively higher. The 4,200 visits are due to the fact that most of the departments and facilities are closed during the weekends, and the number of visits is relatively low. In addition, it can be seen that the average number of hospital visits between 180 and 220 days was very high, reaching 18,000 at one point, and the number of weekend visits did not drop too much. This is because this period",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "weekend visits did not drop too much. This is because this period corresponds to July and August, which is the free time of summer vacation, and most people will concentrate on their visits during this period. In general, this data set shows a cyclical distribution, and the difficulty in building the model is to reduce the error while avoiding the problem of overfitting. 4.2.2. Experimental setup First, the numerical settings of the SRXGWO and GWO algorithms used for hyperparameter optimization are presented. The number of populations is set to 20, the dimension is defined as 2, the maximum number of iterations is 50, the upper and lower bounds for the value of C are 100 and 0.1, and the upper and lower bounds for the value of R",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "0.1, and the upper and lower bounds for the value of R are also 100 and 0.1. Then, to prove the effectiveness of the prediction model SRXGWO-SVR improvement, the SRXGWO-SVR was compared with GWO-SVR and the original SVR in the experiments. Also, to prove the effectiveness of SRXGWO-SVR model, backpropagation (BP), Table 1 Description of the 30 benchmark functions. Class No. Functions F∗ i Fi x∗ Unimodal Functions 1 Rotated High Conditioned Elliptic Function 100 2 Rotated Bent Cigar Function 200 3 Rotated Discus Function 300 Simple Multimodal Functions 4 Shifted and Rotated Rosenbrock ’s Function 400 5 Shifted and Rotated Ackley ’s Function 500 6 Shifted and Rotated Weierstrass Function 600 7 Shifted and Rotated Griewank ’s Function 700 8 Shifted Rastrigin ’s Function 800 9 Shifted",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "Griewank ’s Function 700 8 Shifted Rastrigin ’s Function 800 9 Shifted and Rotated Rastrigin ’s Function 900 10 Shifted Schwefel ’s Function 1000 11 Shifted and Rotated Schwefel ’s Function 1100 12 Shifted and Rotated Katsuura Function 1200 13 Shifted and Rotated HappyCat Function 1300 14 Shifted and Rotated HGBat Function 1400 15 Shifted and Rotated Expanded Griewank ’s plus Rosenbrock ’s Function 1500 16 Shifted and Rotated Expanded Scaffer ’s F6 Function 1600 Hybrid Functions 17 Hybrid Function 1 (N 3) 1700 18 Hybrid Function 2 (N 3) 1800 19 Hybrid Function 3 (N 4) 1900 20 Hybrid Function 4 (N 4) 2000 21 Hybrid Function 5 (N 5) 2100 22 Hybrid Function 6 (N 5) 2200 Composition Functions 23 Composition Function 1 (N 5) 2300",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "(N 5) 2200 Composition Functions 23 Composition Function 1 (N 5) 2300 24 Composition Function 2 (N 3) 2400 25 Composition Function 3 (N 3) 2500 26 Composition Function 4 (N 5) 2600 27 Composition Function 5 (N 5) 2700 28 Composition Function 6 (N 5) 2800 29 Composition Function 7 (N 3) 2900 30 Composition Function 8 (N 3) 3000 Table 2 GWO variants based on three strategies. Algorithms S R X SRXGWO 1 1 1 GWO 0 0 0 SGWO 1 0 0 RGWO 0 1 0 XGWO 0 0 1 SRGWO 1 1 0 SXGWO 1 0 1 RXGWO 0 1 0 X. Zhang et al. [Página 9] Computers in Biology and Medicine 163 (2023) 107166 9random forest (RF), KELM, radial basis function network (RBF),",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "163 (2023) 107166 9random forest (RF), KELM, radial basis function network (RBF), con- volutional neural networks (CNN), and other well-known predictive classifiers are added to the comparison experiments. To verify the pre- diction effectiveness of the proposed patient-flow prediction models, three evaluation metrics are applied to evaluate the performance of various prediction models in this paper. The three-evaluation metrics are the spearman correlation coefficient (R2) of Eq. (38), the mean ab- solute error (MAE) of Eq. (39), and the root mean square error (RMSE) of Eq. (40) for the evaluation analysis. R21\u0000⋃m k1 yk\u0000}yk2 ⋃m k1 yk\u0000}yk2(38) MAE1 m̂m\u00001 i0†yi\u0000}yi† (39) RMSE⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪⎪ 1 n̂m k1 yk\u0000}yk2̅ (40) where m is the number of samples, yk is defined as the actual value size of the test sample, yk is",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "defined as the actual value size of the test sample, yk is the mean size of the test sample, and √yk is the predicted value of the test sample. 4.2.3. Prediction results and analysis To perform regression calculations on the decomposed subsequences using the SVR model, the patient-flow data set needs to meet the input format of the SVR model. For this purpose, the original data samples are processed as follows. First, for the time series y1Cy2C…yn, define the input matrix. X⎫ ⎭y1⋯ yd ⋮ ⋱ ⋮ yn\u0000d⋯ yn\u00001⎩ ⎨ (41) where d is the step size parameter and is the number of sample attri- butes, which in this paper is 4. Then, define the output labels. y⎫ ⎭yd1 ⋮ yn⎩ ⎨ (42) Finally, use X and y",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "labels. y⎫ ⎭yd1 ⋮ yn⎩ ⎨ (42) Finally, use X and y defined above as the input and label of the SVR model, respectively. In practice, X and y are divided into a training set and a test set in the ratio of 1:1. The training set is used to train the model and determine the optimal parameters of the model. Then, the trained model is simulated and tested on the test set to demonstrate the training effect of the prediction model. Finally, the accuracy perfor - mance of the model is verified by evaluating the metrics R2, RMSE, and MAE. The following are the experimental results and training and test sets analysis. 1. Prediction experiments on the training set The patient–flow dataset is divided into 120 sample",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "on the training set The patient–flow dataset is divided into 120 sample sets by 1:1 crossover as the training set for training seven prediction models, including SRXGWO-SVR, GWO-SVR, SVR, BP, RF, KELM, RBF, and CNN. Fig. 6 shows the prediction result plot of SRXGWO-SVR. The original fold represents the training set’s original data distribution and the Pre- dicted fold represents the prediction results given by the SRXGWO-SVR model. The line graph shows that the overall prediction effect of the SRXGWO-SVR model is excellent, especially in the interval of 70–120 days. The Original and Predicted lines nearly overlap, which indicates that the prediction is very accurate. The large deviations between the Table 3 Results of Wilcoxon signed-rank test for ablation experiments and P-value. Item SRXGWO GWO SGWO RGWO XGWO",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "test for ablation experiments and P-value. Item SRXGWO GWO SGWO RGWO XGWO SRGWO SXGWO RXGWO /\u0000/  ~ 23/1/6 15/1/14 14/3/13 18/0/12 6/5/19 9/0/21 9/2/19 Mean 2.57 6.90 5.40 4.47 4.67 2.93 4.13 3.53 Rank 1 8 7 5 6 2 4 3 F1 N/A 1.9209E-06 1.0246E-05 4.0483E-01 4.7162E-02 2.8948E-01 9.7772E-02 1.6503E-01 F2 N/A 1.9209E-06 1.9209E-06 8.3071E-04 1.6394E-05 2.4118E-04 3.7243E-05 3.3269E-02 F3 N/A 1.7344E-06 1.7344E-06 6.0350E-03 8.9364E-01 6.8359E-03 6.2683E-02 3.1849E-01 F4 N/A 2.3704E-05 3.8822E-06 6.2884E-01 3.6094E-03 4.4052E-01 7.8647E-02 5.9994E-01 F5 N/A 1.7344E-06 2.6033E-06 6.8923E-05 1.7344E-06 4.1955E-04 1.7344E-06 8.1302E-01 F6 N/A 4.7162E-02 3.1618E-03 7.0356E-01 4.4052E-01 9.0993E-01 9.0993E-01 9.0993E-01 F7 N/A 1.7344E-06 1.7344E-06 1.1499E-04 1.2453E-02 4.0715E-05 3.1618E-03 6.5833E-01 F8 N/A 1.7344E-06 1.9209E-06 9.3676E-02 1.7344E-06 1.9861E-01 2.3534E-06 7.1889E-01 F9 N/A 3.6004E-01 2.9894E-01 2.4308E-02 8.6121E-01 5.5774E-01 2.2888E-01 7.0356E-01 F10 N/A 1.7344E-06 1.7344E-06",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "N/A 3.6004E-01 2.9894E-01 2.4308E-02 8.6121E-01 5.5774E-01 2.2888E-01 7.0356E-01 F10 N/A 1.7344E-06 1.7344E-06 4.7162E-02 2.1266E-06 4.7162E-02 1.9209E-06 6.2884E-01 F11 N/A 7.3433E-01 3.0861E-01 5.0383E-01 4.1653E-01 1.3591E-01 9.2626E-01 5.5774E-01 F12 N/A 8.2206E-02 5.4401E-01 5.9836E-02 3.3173E-04 7.7309E-03 1.1079E-02 3.6004E-01 F13 N/A 2.2102E-01 3.9333E-01 1.8462E-01 5.5774E-01 2.9894E-01 3.1849E-01 4.1653E-01 F14 N/A 1.3975E-02 1.8326E-03 2.6230E-01 8.5896E-02 1.2544E-01 1.7791E-01 2.3694E-01 F15 N/A 1.4773E-04 6.3391E-06 3.6826E-02 4.9080E-01 2.7653E-03 1.8462E-01 1.0201E-01 F16 N/A 5.3197E-03 2.9575E-03 1.1138E-03 7.5213E-02 2.5637E-02 1.7138E-01 6.5641E-02 F17 N/A 9.8421E-03 3.0861E-01 3.1849E-01 8.7297E-03 3.8723E-02 7.1889E-01 6.5833E-01 F18 N/A 6.8359E-03 9.3157E-06 8.5896E-02 8.9187E-05 6.5641E-02 1.4936E-05 1.4773E-04 F19 N/A 1.4839E-03 8.9443E-04 1.9861E-01 6.4352E-01 1.3591E-01 2.0589E-01 2.1827E-02 F20 N/A 1.9209E-06 1.7344E-06 5.3070E-05 5.3044E-01 5.2165E-06 1.5886E-01 7.3433E-01 F21 N/A 9.0993E-01 4.7795E-01 7.5213E-02 1.0639E-01 2.1827E-02 8.2901E-01 5.0383E-01 F22 N/A 1.6503E-01 6.5641E-02 7.1903E-02 1.6503E-01 3.8203E-01 1.8519E-02 2.4519E-01 F23 N/A 1.7344E-06",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "F22 N/A 1.6503E-01 6.5641E-02 7.1903E-02 1.6503E-01 3.8203E-01 1.8519E-02 2.4519E-01 F23 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 F24 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 F25 N/A 1.2290E-05 1.0000E00 1.7344E-06 5.6061E-06 1.0000E00 1.0000E00 1.7344E-06 F26 N/A 1.9729E-05 1.6566E-02 1.0357E-03 1.3820E-03 1.5286E-01 3.1603E-02 3.1618E-03 F27 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 F28 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 F29 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 F30 N/A 1.7344E-06 1.0000E00 1.7344E-06 1.7344E-06 1.0000E00 1.0000E00 1.7344E-06 X. Zhang et al. [Página 10] Computers in Biology and Medicine 163 (2023) 107166 10Table 4 Comparison results of SRXGWO with other algorithms. Fun F1 F2 F3 Item AVG STD AVG STD AVG STD SRXGWO 1.5817E07 8.9158E06 1.7773E08 1.4681E08 4.5005E03 3.3271E03 PSO 9.0808E06 1.6903E06 1.4837E08 1.5123E07 9.9378E02",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "1.5817E07 8.9158E06 1.7773E08 1.4681E08 4.5005E03 3.3271E03 PSO 9.0808E06 1.6903E06 1.4837E08 1.5123E07 9.9378E02 1.2790E�02 SCA 2.2839E08 6.9799E07 1.6889E10 2.3915E09 3.7046E04 6.6934E03 MFO 8.7549E07 1.0414E08 1.0114E10 5.9855E09 1.0275E05 5.8223E04 WOA 2.7540E07 1.1331E07 5.0637E06 8.0209E06 3.2575E04 2.0632E04 BA 7.7059E�05 3.5272E�05 5.2698E�05 2.7431E�05 4.2251E�02 1.6464E02 FA 2.5269E08 5.1675E07 1.5002E10 1.8122E09 6.4325E04 1.0623E04 OBSCA 4.0160E08 1.2958E08 2.4801E10 4.7138E09 5.0550E04 9.2351E03 m_SCA 6.3874E07 4.1104E07 6.3318E09 3.7149E09 2.6908E04 6.6947E03 OBLGWO 2.2042E07 1.2605E07 1.6887E07 1.2778E07 9.1358E03 3.3451E03 ACWOA 1.3860E08 6.2461E07 7.4290E09 3.9581E09 5.0191E04 9.0562E03 MOFOA 1.2354E09 7.4867E07 7.7038E10 2.4594E09 7.8687E04 3.7238E03 SCADE 4.5429E08 1.1842E08 3.0003E10 4.0210E09 5.6160E04 7.2834E03 Fun F4 F5 F6 Item AVG STD AVG STD AVG STD SRXGWO 5.4006E02 3.2112E01 5.2075E02 7.2959E-02 6.1118E�02 2.5044E00 PSO 4.6707E02 3.2003E�01 5.2095E02 4.0216E-02 6.2317E02 3.2594E00 SCA 1.4150E03 2.7588E02 5.2093E02 6.2064E-02 6.3356E02 2.3449E00 MFO 1.5209E03 1.0125E03 5.2030E�02 1.6938E-01 6.2361E02",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "1.4150E03 2.7588E02 5.2093E02 6.2064E-02 6.3356E02 2.3449E00 MFO 1.5209E03 1.0125E03 5.2030E�02 1.6938E-01 6.2361E02 3.5309E00 WOA 5.9251E02 6.0017E01 5.2034E02 1.6112E-01 6.3494E02 3.5778E00 BA 4.2155E�02 3.2061E01 5.2095E02 6.4791E-02 6.3398E02 3.6948E00 FA 1.5337E03 1.5192E02 5.2096E02 4.5044E-02 6.3359E02 9.2350E-01 OBSCA 2.3121E03 7.5405E02 5.2095E02 5.7443E-02 6.3205E02 1.4049E00 m_SCA 8.0286E02 1.1489E02 5.2056E02 1.4351E-01 6.2212E02 2.8889E00 OBLGWO 5.4647E02 4.7860E01 5.2096E02 5.9910E-02 6.1916E02 3.3318E00 ACWOA 1.1803E03 2.6266E02 5.2085E02 1.7768E-01 6.3363E02 2.7978E00 MOFOA 1.0092E04 6.9816E02 5.2106E02 3.7558E-02 6.4079E02 6.7902E-01 SCADE 2.2480E03 4.6553E02 5.2097E02 4.3335E-02 6.3428E02 2.4021E00 Fun F7 F8 F9 Item AVG STD AVG STD AVG STD SRXGWO 7.0144E02 4.4844E-01 8.3494E�02 6.4659E�00 9.9741E�02 2.4837E01 PSO 7.0229E02 1.4348E-01 9.7268E02 2.6092E01 1.1067E03 2.4938E01 SCA 8.4528E02 2.6369E01 1.0362E03 1.9353E01 1.1756E03 2.4065E01 MFO 7.9627E02 6.3419E01 9.4824E02 3.3320E01 1.1205E03 4.4316E01 WOA 7.0099E02 7.2969E-02 9.9955E02 4.1935E01 1.1246E03 5.0520E01 BA 7.0066E�02 1.6102E-01 1.0275E03 5.2626E01 1.1641E03",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "7.0099E02 7.2969E-02 9.9955E02 4.1935E01 1.1246E03 5.0520E01 BA 7.0066E�02 1.6102E-01 1.0275E03 5.2626E01 1.1641E03 5.6092E01 FA 8.4000E02 1.0997E01 1.0240E03 1.2118E01 1.1595E03 1.3038E01 OBSCA 9.1758E02 4.4244E01 1.0576E03 1.8074E01 1.1960E03 1.9095E01 m_SCA 7.4867E02 2.2125E01 9.3470E02 2.3339E01 1.0491E03 1.9402E01 OBLGWO 7.0119E02 9.2779E-02 9.2058E02 3.4783E01 1.0637E03 2.9684E01 ACWOA 7.3883E02 2.1566E01 9.8681E02 1.5413E01 1.1270E03 1.7226E01 MOFOA 1.4082E03 4.6569E01 1.1760E03 1.1881E01 1.2583E03 9.4200E�00 SCADE 9.1691E02 4.4469E01 1.0684E03 1.0564E01 1.2058E03 1.8217E01 Fun F10 F11 F12 Item AVG STD AVG STD AVG STD SRXGWO 1.7815E�03 2.3016E�02 4.1565E�03 1.0677E03 1.2012E03 3.7913E-01 PSO 5.0248E03 5.6761E02 5.8289E03 4.4923E02 1.2023E03 3.0765E-01 SCA 7.0064E03 5.2529E02 8.0775E03 3.0696E02 1.2025E03 2.1633E-01 MFO 4.6021E03 8.7516E02 5.2295E03 7.7681E02 1.2004E�03 1.9653E-01 WOA 4.9691E03 7.4150E02 5.8744E03 9.0861E02 1.2017E03 4.7579E-01 BA 5.5034E03 5.6881E02 6.0313E03 6.9746E02 1.2011E03 3.5842E-01 FA 7.5532E03 3.1957E02 7.9058E03 2.9315E02 1.2026E03 2.3995E-01 OBSCA 6.3076E03 4.9831E02 7.3709E03 3.6056E02 1.2022E03",
        "start_idx": 8236,
        "end_idx": 8364
      },
      {
        "text": "7.5532E03 3.1957E02 7.9058E03 2.9315E02 1.2026E03 2.3995E-01 OBSCA 6.3076E03 4.9831E02 7.3709E03 3.6056E02 1.2022E03 4.1510E-01 m_SCA 4.0584E03 7.1133E02 4.7823E03 6.5478E02 1.2008E03 3.3864E-01 OBLGWO 3.8703E03 8.9566E02 5.4446E03 1.0838E03 1.2023E03 5.7151E-01 ACWOA 4.7309E03 7.3276E02 6.1655E03 9.3475E02 1.2018E03 4.7511E-01 MOFOA 9.2300E03 3.9968E02 9.0883E03 2.8283E�02 1.2029E03 2.7367E-01 SCADE 7.3914E03 2.4356E02 8.2418E03 2.8346E02 1.2026E03 2.4238E-01 Fun F13 F14 F15 Item AVG STD AVG STD AVG STD SRXGWO 1.3004E�03 7.4709E-02 1.4005E03 2.8662E-01 1.5163E03 6.0201E00 PSO 1.3004E03 7.7571E-02 1.4003E03 1.2817E-01 1.5166E03 1.1804E�00 SCA 1.3030E03 2.6429E-01 1.4439E03 7.6871E00 5.5707E03 5.0710E03 MFO 1.3020E03 1.3201E00 1.4347E03 2.4514E01 2.1529E05 5.9281E05 WOA 1.3006E03 1.4348E-01 1.4003E�03 4.2398E-02 1.5738E03 2.6213E01 BA 1.3005E03 1.5518E-01 1.4003E03 1.3344E-01 1.5296E03 6.4355E00 FA 1.3028E03 1.9987E-01 1.4404E03 4.2258E00 1.4383E04 5.6495E03 (continued on next page) X. Zhang et al. [Página 11] Computers in Biology and Medicine 163 (2023) 107166 11Table 4",
        "start_idx": 8352,
        "end_idx": 8480
      },
      {
        "text": "[Página 11] Computers in Biology and Medicine 163 (2023) 107166 11Table 4 (continued ) Fun F1 F2 F3 OBSCA 1.3037E03 3.6249E-01 1.4731E03 1.1450E01 1.7595E04 1.0828E04 m_SCA 1.3009E03 7.5448E-01 1.4172E03 7.0904E00 2.1370E03 8.9061E02 OBLGWO 1.3005E03 1.1306E-01 1.4004E03 1.7893E-01 1.5162E�03 4.9642E00 ACWOA 1.3015E03 1.0565E00 1.4197E03 1.4944E01 2.0795E03 6.3700E02 MOFOA 1.3081E03 3.0417E-01 1.6411E03 9.7254E00 2.2096E05 3.2757E04 SCADE 1.3040E03 3.7540E-01 1.4874E03 8.7317E00 1.9117E04 6.0793E03 Fun F16 F17 F18 Item AVG STD AVG STD AVG STD SRXGWO 1.6110E�03 4.7850E-01 6.0312E05 7.0192E05 1.0517E04 9.4630E03 PSO 1.6120E03 5.3328E-01 2.9096E05 1.3413E05 1.9795E06 5.9660E05 SCA 1.6127E03 2.3363E-01 6.2791E06 3.3409E06 1.4952E08 6.6811E07 MFO 1.6128E03 4.8942E-01 3.9449E06 7.4952E06 1.2984E08 4.9905E08 WOA 1.6124E03 4.0816E-01 4.2933E06 3.4224E06 7.9323E�03 5.7540E�03 BA 1.6133E03 3.0344E-01 1.0170E�05 9.2079E�04 9.5662E04 4.7410E04 FA 1.6129E03 2.1659E-01 6.7984E06 1.7537E06 3.0346E08 8.6628E07 OBSCA 1.6130E03 2.5196E-01 9.4571E06 3.4434E06 1.8689E08 1.1951E08 m_SCA",
        "start_idx": 8468,
        "end_idx": 8596
      },
      {
        "text": "6.7984E06 1.7537E06 3.0346E08 8.6628E07 OBSCA 1.6130E03 2.5196E-01 9.4571E06 3.4434E06 1.8689E08 1.1951E08 m_SCA 1.6114E03 7.5109E-01 1.7735E06 1.3758E06 2.3094E07 3.3825E07 OBLGWO 1.6120E03 4.3556E-01 1.2085E06 9.3079E05 3.4535E04 3.1262E04 ACWOA 1.6122E03 4.8843E-01 1.5272E07 1.2808E07 5.6959E07 4.4496E07 MOFOA 1.6134E03 2.3187E-01 8.7256E07 2.6488E07 5.7808E09 1.0374E09 SCADE 1.6127E03 2.0380E-01 1.5384E07 5.7531E06 1.6460E08 8.4537E07 Fun F19 F20 F21 Item AVG STD AVG STD AVG STD SRXGWO 1.9173E03 1.3658E01 2.9079E03 1.1572E03 3.7294E05 3.3804E05 PSO 1.9172E03 1.9835E�00 2.2959E�03 6.5024E�01 1.1324E05 6.7643E04 SCA 1.9893E03 2.5079E01 1.5103E04 3.7270E03 1.4348E06 6.6889E05 MFO 1.9738E03 5.5538E01 5.2933E04 4.0442E04 1.0824E06 2.5030E06 WOA 1.9384E03 2.7102E01 3.2328E04 2.0050E04 1.1294E06 1.7119E06 BA 1.9335E03 3.4019E01 2.4023E03 1.1992E02 6.4514E�04 3.2131E�04 FA 2.0050E03 1.2211E01 1.8924E04 7.2016E03 1.6271E06 7.2788E05 OBSCA 2.0080E03 9.9922E00 2.9021E04 1.1924E04 1.8445E06 8.5563E05 m_SCA 1.9502E03 2.9621E01 1.0791E04 4.6890E03 3.7774E05 5.2345E05 OBLGWO 1.9170E�03 1.6997E01 5.6962E03 2.3328E03 5.2217E05 3.6592E05 ACWOA",
        "start_idx": 8584,
        "end_idx": 8712
      },
      {
        "text": "1.0791E04 4.6890E03 3.7774E05 5.2345E05 OBLGWO 1.9170E�03 1.6997E01 5.6962E03 2.3328E03 5.2217E05 3.6592E05 ACWOA 2.0080E03 2.5161E01 3.9788E04 1.9571E04 6.9036E06 5.4050E06 MOFOA 2.2412E03 1.8281E01 1.4788E05 5.3365E04 3.9619E07 1.4979E07 SCADE 2.0087E03 1.1766E01 2.8049E04 9.6164E03 2.3498E06 1.0171E06 Fun F22 F23 F24 Item AVG STD AVG STD AVG STD SRXGWO 2.6550E03 1.8361E02 2.5000E�03 0.0000E�00 2.6000E�03 0.0000E�00 PSO 2.9439E03 1.8435E02 2.6161E03 5.8346E-01 2.6261E03 5.6750E00 SCA 2.9644E03 1.3112E02 2.6653E03 1.3746E01 2.6001E03 6.9049E-02 MFO 3.0695E03 2.1885E02 2.6708E03 3.4130E01 2.6722E03 2.7522E01 WOA 3.0538E03 2.9728E02 2.6334E03 1.0652E01 2.6118E03 3.7279E01 BA 3.3420E03 4.1760E02 2.6152E03 3.0962E-03 2.6654E03 2.6008E01 FA 3.0002E03 1.1217E�02 2.7329E03 1.7512E01 2.7050E03 4.5757E00 OBSCA 3.1226E03 1.6474E02 2.6858E03 1.7839E01 2.6000E03 3.0468E-04 m_SCA 2.6046E�03 2.1219E02 2.6370E03 6.7666E00 2.6000E03 6.8563E-04 OBLGWO 2.7106E03 1.7350E02 2.6181E03 1.4048E00 2.6009E03 5.0249E00 ACWOA 3.1046E03 2.2793E02 2.5122E03 4.6578E01 2.6000E03 5.0998E-06 MOFOA 1.8112E04 1.1960E04 2.5000E03 0.0000E00 2.6000E03 0.0000E00 SCADE",
        "start_idx": 8700,
        "end_idx": 8828
      },
      {
        "text": "2.5122E03 4.6578E01 2.6000E03 5.0998E-06 MOFOA 1.8112E04 1.1960E04 2.5000E03 0.0000E00 2.6000E03 0.0000E00 SCADE 3.1435E03 1.3870E02 2.5000E03 0.0000E00 2.6000E03 1.9769E-07 Fun F25 F26 F27 Item AVG STD AVG STD AVG STD SRXGWO 2.7000E�03 0.0000E�00 2.7004E�03 8.2706E-02 2.9000E�03 0.0000E�00 PSO 2.7118E03 7.4419E00 2.7871E03 3.4604E01 3.4367E03 2.8726E02 SCA 2.7269E03 8.2372E00 2.7023E03 6.7894E-01 3.4443E03 3.2075E02 MFO 2.7194E03 1.1345E01 2.7024E03 1.2575E00 3.6640E03 1.4731E02 WOA 2.7153E03 1.6594E01 2.7005E03 1.3903E-01 3.8579E03 2.9527E02 BA 2.7314E03 1.2072E01 2.7005E03 1.5158E-01 3.8975E03 3.7586E02 FA 2.7336E03 3.7833E00 2.7024E03 3.2727E-01 3.7997E03 2.1775E01 OBSCA 2.7000E03 1.4243E-08 2.7040E03 4.1439E-01 3.2568E03 4.0280E01 m_SCA 2.7124E03 4.1923E00 2.7008E03 2.1587E-01 3.1851E03 1.2821E02 OBLGWO 2.7000E03 0.0000E00 2.7006E03 1.2740E-01 3.1171E03 3.1805E02 ACWOA 2.7000E03 0.0000E00 2.7636E03 4.8645E01 3.7129E03 3.5075E02 MOFOA 2.7000E03 0.0000E00 2.7925E03 2.3425E01 2.9000E03 0.0000E00 SCADE 2.7000E03 0.0000E00 2.7070E03 1.7566E01 3.2989E03 1.9042E02 Fun F28 F29 F30 Item AVG STD AVG",
        "start_idx": 8816,
        "end_idx": 8944
      },
      {
        "text": "2.7070E03 1.7566E01 3.2989E03 1.9042E02 Fun F28 F29 F30 Item AVG STD AVG STD AVG STD (continued on next page) X. Zhang et al. [Página 12] Computers in Biology and Medicine 163 (2023) 107166 12real and predicted series appear on the 3rd day, around the 32nd day, etc., due to the large fluctuations of the real series, which are difficult to predict and lead to the deviation of the model. To illustrate the improvement of SRXGWO-SVR compared to GWO- SVR, the iteration curves when SRXGWO and GWO optimized SVR are recorded in this paper, as shown in Fig. 7. The vertical axis represents the fitness value of the swarm intelligence algorithm, i.e., the deviation in the model, and the horizontal axis represents the number of iterations. The blue curve",
        "start_idx": 8932,
        "end_idx": 9060
      },
      {
        "text": "and the horizontal axis represents the number of iterations. The blue curve represents the iteration curve of SRXGWO-SVR, and the brown curve represents the iteration curve of GWO-SVR. The iterations also confirm that the two hyperparameters of the SRXGWO-SVR pre- diction model are C 76.2569 and R 0.0101. The hyperparameters of the GWO-SVR are C 2.3654 and R 0.0309. Since the overall de- viations of both SRXGWO-SVR and GWO-SVR are small, and the process of iteration spans an extensive numerical range, we have enlarged the key parts were enlarged. First, in terms of initialization, SRXGWO-SVR has a smaller fitness value than GWO-SVR, which indicates that the Sobol sequence initialization method enhances the pre-search capability of SRXGWO. Then, it can be seen by the magnified image that both SRXGWO",
        "start_idx": 9048,
        "end_idx": 9176
      },
      {
        "text": "Then, it can be seen by the magnified image that both SRXGWO and GWO find the near-optimal solution at the iteration number of 2, but it is evident that SRXGWO has a better fitness value for the near-optimal solution. Finally, during the iterations, SRXGWO also keeps searching for the optimal solution, and the fitness value of SRXGWO is optimized from 0.0003285 at the beginning to 0.0003271. The fitness value of GWO does not change significantly, and the algo- rithm falls into a local optimum. Therefore, it can be said that SRXGWO can improve SVR’s prediction performance more effectively than GWO. This work compares SRXGWO-SVR with well-known classification prediction models including GWO-SVR, SVR, BP, RF, KELM, RBF, and Table 4 (continued ) Fun F1 F2 F3 SRXGWO 3.0000E�03 0.0000E�00",
        "start_idx": 9164,
        "end_idx": 9292
      },
      {
        "text": "and Table 4 (continued ) Fun F1 F2 F3 SRXGWO 3.0000E�03 0.0000E�00 3.1000E�03 0.0000E�00 3.2000E�03 0.0000E�00 PSO 6.8849E03 8.7157E02 7.4382E04 1.3763E05 1.1678E04 6.2526E03 SCA 4.7736E03 2.6752E02 1.2836E07 7.6163E06 2.3980E05 7.9328E04 MFO 3.9703E03 2.4525E02 3.6610E06 3.9023E06 6.3694E04 5.2942E04 WOA 5.0223E03 6.7902E02 6.3246E06 4.5803E06 7.5080E04 4.8586E04 BA 5.1296E03 5.6070E02 3.6448E07 2.6098E07 1.3731E04 1.2024E04 FA 4.2282E03 1.4435E02 3.1490E06 8.4923E05 1.7420E05 3.9597E04 OBSCA 5.3567E03 2.9466E02 2.0712E07 9.7835E06 3.7443E05 1.9299E05 m_SCA 3.8890E03 1.2875E02 1.9729E06 4.4218E06 5.5540E04 2.8810E04 OBLGWO 3.4266E03 5.0458E02 4.9452E06 4.3781E06 1.9074E04 1.4566E04 ACWOA 4.3232E03 1.2224E03 1.8950E07 1.5200E07 3.7383E05 2.2958E05 MOFOA 3.0000E03 0.0000E00 3.1000E03 0.0000E00 3.2000E03 0.0000E00 SCADE 4.9933E03 8.5262E02 1.5512E07 9.5368E06 4.8922E05 1.6393E05 Table 5 Wilcoxon signed-rank test results of SRXGWO versus other peers. Algorithm /\u0000/  Mean Rank SRXGWO ~ 2.13 1 PSO 19/8/3 4.80 4 SCA 30/0/0 8.57 9",
        "start_idx": 9280,
        "end_idx": 9408
      },
      {
        "text": "SRXGWO ~ 2.13 1 PSO 19/8/3 4.80 4 SCA 30/0/0 8.57 9 MFO 26/2/2 7.33 7 WOA 25/4/1 6.13 6 BA 20/7/3 5.93 5 FA 30/0/0 9.47 10 OBSCA 29/0/1 9.70 11 m_SCA 26/2/2 4.73 3 OBLGWO 20/2/8 4.00 2 ACWOA 28/0/2 7.57 8 MOFOA 23/0/7 10.17 13 SCADE 27/0/3 9.87 12 Fig. 3.Friedman test results of SRXGWO versus other peers. X. Zhang et al. [Página 13] Computers in Biology and Medicine 163 (2023) 107166 13 Fig. 4.Convergence curves of SRXGWO and peer algorithms. Fig. 5.240-day folding graph of the number of actual hospital visits. X. Zhang et al. [Página 14] Computers in Biology and Medicine 163 (2023) 107166 14CNN to further highlight the benefits of SRXGWO-SVR. It uses R2, RMSE, and MAE to assess the accuracy of",
        "start_idx": 9396,
        "end_idx": 9524
      },
      {
        "text": "SRXGWO-SVR. It uses R2, RMSE, and MAE to assess the accuracy of the predictions. In order to guarantee the stability of the prediction results and prevent chance mistakes, the 10-fold cross-validation is also utilised in the model training process. Table 6 displays the evaluation findings for each model, and it is clear that SRXGWO-SVR performs the best in terms of R2, RMSE, and MAE assessment indices. The correlation coefficient, R2, is 0.99879, which shows that there is a strong connection between the prediction results of the SRXGWO-SVR model and the actual value. It is clear that SRXGWO- SVR performs best in R2, RMSE, and MAE evaluation indices. RMSE and MAE are used to evaluate errors. The two forms of SVR errors are the least, with corresponding values of",
        "start_idx": 9512,
        "end_idx": 9640
      },
      {
        "text": "two forms of SVR errors are the least, with corresponding values of 159.5753 and 100.0009. Following line graph analysis, iterative graph analysis, and evaluation result analysis, it can be shown that the SRXGWO-SVR model has a very high prediction accuracy and also has more advantages than other Fig. 6.Prediction results of SRXGWO-SVR. Fig. 7.Iteration curves of SRXGWO and GWO when optimizing SVR. Table 6 Evaluation results of each prediction model. Model R2 RMSE MAE SRXGWO-SVR 0.99879 159.5753 100.0009 GWO-SVR 0.99869 159.5886 100.0069 SVR 0.99861 166.1568 105.0999 BP 0.99820 584.2596 119.5581 RF 0.98379 176.6171 335.1838 KELM 0.99819 195.6333 144.1484 RBF 0.99865 168.8734 110.3226 CNN 0.99744 228.9898 110.3226 X. Zhang et al. [Página 15] Computers in Biology and Medicine 163 (2023) 107166 15algorithms. 2. Prediction experiments on the test set",
        "start_idx": 9628,
        "end_idx": 9756
      },
      {
        "text": "Medicine 163 (2023) 107166 15algorithms. 2. Prediction experiments on the test set The model trained by the real sequence must be closer to the training set itself, and there may be problems of false accuracy of the prediction results and overfitting of the prediction model. Moreover, the prediction problem, in reality, will not be the same as the real sequence of the training set, so it is necessary to simulate and test the completed trained model by the test set. Fig. 8 shows the prediction fold of SRXGWO-SVR for the test set. Again, the Original fold represents the data distribution of the test set, and the Predicted fold represents the prediction results given by the SRXGWO-SVR model. It can be seen that SRXGWO-SVR also predicts very well in",
        "start_idx": 9744,
        "end_idx": 9872
      },
      {
        "text": "model. It can be seen that SRXGWO-SVR also predicts very well in the test set prediction with high correlation. However, the deviation of SRXGWO-SVR in predicting the test set is more significant than the training set, e.g., the deviation of the dashboard on days 7,10,13,36 is larger. Therefore, overall, SRXGWO-SVR still has a highly accurate prediction performance and does not fall into the overfitting problem when faced with brand-new patient-flow data. However, it cannot achieve the results in training. To further explore the performance of SRXGWO-SVR in the face of new sample sequences and to show the advantages of SRXGWO-SVR over other algorithms, the test set experiments also compare SRXGWO-SVR with well-known classification prediction models such as GWO-SVR, SVR, and BP, and evaluate the prediction results using R2,",
        "start_idx": 9860,
        "end_idx": 9988
      },
      {
        "text": "as GWO-SVR, SVR, and BP, and evaluate the prediction results using R2, RMSE, and MAE. The evaluation results of each model are shown in Table 7. It can be seen that SRXGWO-SVR has higher Spearman corre - lation and lower error in RMSE, MAE for prediction results compared with GWO-SVR, SVR, which indicates that SRXGWO-SVR still has an advantage over the unimproved GWO-SVR and SVR in the face of new data sets. In addition, it can be seen that SRXGWO-SVR still has a greater advantage over BP, RF, KELM, RBF, and CNN classical models, and performs better in terms of R2, RMSE, and MAE. Finally, this paper combines the prediction results of the training set and the test set for statistical comparisons in order to further highlight the",
        "start_idx": 9976,
        "end_idx": 10104
      },
      {
        "text": "the test set for statistical comparisons in order to further highlight the significance of the training set experiments and the test set experi - ments, as well as to demonstrate the prediction effectiveness of SRXGWO-SVR for various data sets and the advantages of SRXGWO-SVR over other algorithms. The comparison findings are shown in Figs. 9–11, where the horizontal axis represents each comparison model and the vertical axis the assessment standards. Fig. 9 shows that when SRXGWO- SVR is moved from the training set to the test set, the prediction rele- vance of the model diminishes and that KELM fluctuates the least. However, SRXGWO-SVR still outperforms KELM in terms of accuracy, suggesting that it may continue to hold the top spot in future patient- flow prediction. The assessment findings",
        "start_idx": 10092,
        "end_idx": 10220
      },
      {
        "text": "hold the top spot in future patient- flow prediction. The assessment findings were normalized in this research and then shown once more since RMSE and MAE are prediction errors and the difference between the data is too great. Figs. 10 and 11 show intuitively how much more accurate SRXGWO-SVR is than other models like BP, RF, CNN, and others. Additionally, even after switching datasets, there is little error variation in the SRXGWO-SVR prediction results, demonstrating the model ’s great stability. It can be shown that SRXGWO-SVR is a very accurate, highly generalizable, and highly stable prediction model based on the experimental findings of the training and test sets. 5.Conclusions and future works This paper proposes a high-performance optimization algorithm SRXGWO and an effective patient-flow prediction model SRXGWO-SVR, aiming",
        "start_idx": 10208,
        "end_idx": 10336
      },
      {
        "text": "high-performance optimization algorithm SRXGWO and an effective patient-flow prediction model SRXGWO-SVR, aiming to predict patients ’ medical needs and achieve orderly patient access by analyzing the changing dynamics and objective laws of Patient-flow. First, this paper introduces the current research status of artificial intelligence technology for predicting patient-flow and finds that the existing prediction models are not strong in prediction accuracy and generalization. Therefore, to improve the accuracy and general - ization of the prediction model, SRXGWO is proposed based on three improvement strategies and GWO, in which the Sobol sequence im- proves the solution space coverage of population initialization, Cauchy random replacement strategy enhances the information exchange be- tween individuals, directional mutation mechanism improves the search Fig. 8.SRXGWO-SVR predictions for the test set. Table 7 Evaluation results",
        "start_idx": 10324,
        "end_idx": 10452
      },
      {
        "text": "search Fig. 8.SRXGWO-SVR predictions for the test set. Table 7 Evaluation results of each model based on the test set. Model R2 RMSE MAE SRXGWO-SVR 0.99835 199.0553 125.6847 GWO-SVR 0.99802 199.0954 125.7070 SVM 0.99783 218.1971 136.1934 BP 0.99738 232.2147 150.2261 RF 0.97952 701.2146 427.7865 KELM 0.99819 291.1310 185.8860 RBF 0.99831 201.5883 129.3960 CNN 0.98132 628.8679 363.9654 X. Zhang et al. [Página 16] Computers in Biology and Medicine 163 (2023) 107166 16ability of the algorithm and the ability to jump out of the local optimum. Then, the SRXGWO-SVR prediction model is proposed by combining the high-performance SRXGWO algorithm with the SVR prediction method to accurately predict the number of patients and reasonably schedule medical resources. In the experimental part, ablation experiments are first conducted to compare SRXGWO with GWO",
        "start_idx": 10440,
        "end_idx": 10568
      },
      {
        "text": "experimental part, ablation experiments are first conducted to compare SRXGWO with GWO combined with different mechanisms. It is verified that SRXGWO, with three improved strategies, simultaneously is the strongest performance. Then, SRXGWO is compared with 12 highly cited algorithms, such as PSO, SCA, etc., by 30 benchmark functions to demonstrate that SRXGWO is also superior in the search ability and exploitation ability of peer algorithms. Finally, a real patient-flow dataset is used to validate the prediction ability of the SRXGWO-SVR model. Comparing with the other seven prediction models, such as BP, CNN, etc., and evaluating R2, RMSE, and MAE, it is proved that the prediction results of SRXGWO-SVR are more accurate, effective and stronger than other models. Of course, the research in this paper also has some limitations.",
        "start_idx": 10556,
        "end_idx": 10684
      },
      {
        "text": "models. Of course, the research in this paper also has some limitations. For example, three improvement mechanisms were added to GWO, which increased the algorithm ’s complexity. In the future, we will try to solve this problem using parallel techniques and high-performance com- puters. In addition, in future work, we will further enhance SRXGWO and SRXGWO-SVR and apply them to more fields. Fig. 9.R2 comparison results based on two dataset models. Fig. 10.Comparison results of RMSE based on two dataset models. Fig. 11.Comparison results of MAE based on two dataset models. X. Zhang et al. [Página 17] Computers in Biology and Medicine 163 (2023) 107166 17Declaration of competing interest The authors declare that there is no conflict of interests regarding the publication of article. References [1]L. Zhang, L.",
        "start_idx": 10672,
        "end_idx": 10800
      },
      {
        "text": "conflict of interests regarding the publication of article. References [1]L. Zhang, L. Li, Study on the Equilibrium of Spatial Allocation of Medical Resources at Different Levels in Shanghai, Urban Studies, 2019, p. 26. [2]D.Y. Zhou, L.Y. Gao, Q.H. Pan, M.F. He, The Impacts of Medical Resources on Emerging Self-Limiting Infectious Diseases, vol. 12, Applied Sciences-Basel, 2022 . [3]H. Li, D.M. Mu, P. Wang, Y. Li, D.X. Wang, Prediction of obstetric patient flow and horizontal allocation of medical resources based on time series analysis, Front. Public Health 9 (2021) . [4]A. Nikakhtar, S.A. Abbasian-Hosseini, H. Gazula, S.M. Hsiang, Social Network based sensitivity analysis for patient flow using computer simulation, Comput. Ind. Eng. 88 (2015) 264–272. [5]A.R. Sharafat, M. Bayati, PatientFlowNet: a deep learning approach to patient flow prediction in",
        "start_idx": 10788,
        "end_idx": 10916
      },
      {
        "text": "M. Bayati, PatientFlowNet: a deep learning approach to patient flow prediction in emergency departments, IEEE Access 9 (2021) 45552 –45561 . [6]M. Tavakoli, R. Tavakkoli-Moghaddam, R. Mesbahi, M. Ghanavati-Nejad, A. Tajally, Simulation of the COVID-19 patient flow and investigation of the future patient arrival using a time-series prediction model: a real-case study, Med. Biol. Eng. Comput. 60 (2022) 969–990. [7]S. Mirjalili, S.M. Mirjalili, A. Lewis, Grey wolf optimizer, Adv. Eng. Software 69 (2014) 46–61. [8]X.-S. Yang, A new metaheuristic bat-inspired algorithm, in: J.R. Gonz ˘alez, D. A. Pelta, C. Cruz, G. Terrazas, N. Krasnogor (Eds.), Nature Inspired Cooperative Strategies for Optimization (NICSO 2010), Springer Berlin Heidelberg, Berlin, Heidelberg, 2010, pp. 65–74. [9]R. Storn, K.J.J.o.G.O. Price, Differential evolution – a simple and efficient heuristic for global, Optimization over Continuous",
        "start_idx": 10904,
        "end_idx": 11032
      },
      {
        "text": "evolution – a simple and efficient heuristic for global, Optimization over Continuous Spaces 11 (1997) 341–359. [10] S. Mirjalili, SCA, A Sine Cosine Algorithm for solving optimization problems, Knowl. Base Syst. 96 (2016) 120–133. [11] S. Mirjalili, A.H. Gandomi, S.Z. Mirjalili, S. Saremi, H. Faris, S.M. Mirjalili, Salp Swarm Algorithm: a bio-inspired optimizer for engineering design problems, Adv. Eng. Software 114 (2017) 163–191. [12] S. Mirjalili, A. Lewis, The whale optimization algorithm, Adv. Eng. Software 95 (2016) 51–67. [13] S. Mirjalili, Moth-flame optimization algorithm: a novel nature-inspired heuristic paradigm, Knowl. Base Syst. 89 (2015) 228–249. [14] J. Kennedy, R. Eberhart, Particle swarm optimization, in: Proceedings of ICNN ’95 vol. 1944, International Conference on Neural Networks, 1995, pp. 1942 –1948 . [15] Y. Yang, H. Chen, A.A. Heidari, A.H.",
        "start_idx": 11020,
        "end_idx": 11148
      },
      {
        "text": "pp. 1942 –1948 . [15] Y. Yang, H. Chen, A.A. Heidari, A.H. Gandomi, Hunger games search: visions, conception, implementation, deep analysis, perspectives, and towards performance shifts, Expert Syst. Appl. 177 (2021), 114864 . [16] A.A. Heidari, S. Mirjalili, H. Faris, I. Aljarah, M. Mafarja, H. Chen, Harris hawks optimization: algorithm and applications, Future Generation Computer Systems- the International Journal of Escience 97 (2019) 849–872. [17] H. Su, D. Zhao, A. Asghar Heidari, L. Liu, X. Zhang, M. Mafarja, H. Chen, RIME: A Physics-Based Optimization, Neurocomputing, 2023 . [18] J. Tu, H. Chen, M. Wang, A.H. Gandomi, The colony predation algorithm, Journal of Bionic Engineering 18 (2021) 674–710. [19] I. Ahmadianfar, A. Asghar Heidari, A.H. Gandomi, X. Chu, H. Chen, RUN beyond the metaphor: an efficient optimization algorithm based",
        "start_idx": 11136,
        "end_idx": 11264
      },
      {
        "text": "Chu, H. Chen, RUN beyond the metaphor: an efficient optimization algorithm based on Runge Kutta method, Expert Syst. Appl. (2021), 115079 . [20] I. Ahmadianfar, A. Asghar Heidari, S. Noshadian, H. Chen, A.H. Gandomi, INFO: an efficient optimization algorithm based on weighted mean of vectors, Expert Syst. Appl. (2022), 116516 . [21] H. Chen, C. Li, M. Mafarja, A.A. Heidari, Y. Chen, Z. Cai, Slime mould algorithm: a comprehensive review of recent variants and applications, Int. J. Syst. Sci. (2022) 1–32. [22] S. Li, H. Chen, M. Wang, A.A. Heidari, S. Mirjalili, Slime mould algorithm: a new method for stochastic optimization, Future Generat. Comput. Syst. 111 (2020) 300–323. [23] M. Abd Elaziz, D. Oliva, S. Xiong, An improved opposition-based sine cosine algorithm for global optimization, Expert Syst. Appl.",
        "start_idx": 11252,
        "end_idx": 11380
      },
      {
        "text": "An improved opposition-based sine cosine algorithm for global optimization, Expert Syst. Appl. 90 (2017) 484–500. [24] C. Qu, Z. Zeng, J. Dai, Z. Yi, W. He, A modified sine-cosine algorithm based on neighborhood search and greedy Levy mutation, Comput. Intell. Neurosci. (2018), 2018) 4231647-4231647 . [25] A.A. Heidari, R. Ali Abbaspour, H. Chen, Efficient boosted grey wolf optimizers for global search and kernel extreme learning machine training, Appl. Soft Comput. 81 (2019), 105521 . [26] M.A. Elhosseini, A.Y. Haikal, M. Badawy, N. Khashan, Biped robot stability based on an A–C parametric Whale Optimization Algorithm, Journal of Computational Science 31 (2019) 17–32. [27] H. Chen, S. Li, A.A. Heidari, P. Wang, J. Li, Y. Yang, M. Wang, C. Huang, Efficient multi-population outpost fruit fly-driven optimizers: framework and advances in",
        "start_idx": 11368,
        "end_idx": 11496
      },
      {
        "text": "C. Huang, Efficient multi-population outpost fruit fly-driven optimizers: framework and advances in support vector machines, Expert Syst. Appl. (2020) 142. [28] H. Nenavath, R.K. Jatoth, Hybridizing sine cosine algorithm with differential evolution for global optimization and object tracking, Appl. Soft Comput. 62 (2018) 1019 –1043 . [29] Y. Zhang, R. Liu, A.A. Heidari, X. Wang, Y. Chen, M. Wang, H. Chen, Towards augmented kernel extreme learning models for bankruptcy prediction: algorithmic behavior and comprehensive analysis, Neurocomputing 430 (2021) 185–212. [30] Y. Liu, A.A. Heidari, Z. Cai, G. Liang, H. Chen, Z. Pan, A. Alsufyani, S. Bourouis, Simulated annealing-based dynamic step shuffled frog leaping algorithm: optimal performance design and feature selection, Neurocomputing 503 (2022) 325–362. [31] Y. Xue, B. Xue, M. Zhang, Self-adaptive particle swarm optimization for large-scale feature",
        "start_idx": 11484,
        "end_idx": 11612
      },
      {
        "text": "Xue, B. Xue, M. Zhang, Self-adaptive particle swarm optimization for large-scale feature selection in classification, ACM Trans. Knowl. Discov. Data 13 (2019) 1–27. [32] Y. Xue, X. Cai, F. Neri, A multi-objective evolutionary algorithm with interval based initialization and self-adaptive crossover operator for large-scale feature selection in classification, Appl. Soft Comput. 127 (2022), 109420 . [33] X. Wang, X. Dong, Y. Zhang, H. Chen, Crisscross Harris hawks optimizer for global tasks and feature selection, Journal of Bionic Engineering (2022) 1–22. [34] W. Shan, H. Hu, Z. Cai, H. Chen, H. Liu, M. Wang, Y. Teng, Multi-strategies boosted mutative crow search algorithm for global tasks: cases of continuous and discrete optimization, Journal of Bionic Engineering 19 (2022) 1830 –1849 . [35] R. Dong, H. Chen, A.A. Heidari, H. Turabieh,",
        "start_idx": 11600,
        "end_idx": 11728
      },
      {
        "text": "1830 –1849 . [35] R. Dong, H. Chen, A.A. Heidari, H. Turabieh, M. Mafarja, S. Wang, Boosted kernel search: framework, analysis and case studies on the economic emission dispatch problem, Knowl. Base Syst. 233 (2021), 107529 . [36] C. Zhao, Y. Zhou, X. Lai, An integrated framework with evolutionary algorithm for multi-scenario multi-objective optimization problems, Inf. Sci. 600 (2022) 342–361. [37] W. Deng, J. Xu, X.Z. Gao, H. Zhao, An enhanced MSIQDE algorithm with novel multiple strategies for global optimization problems, IEEE Transactions on Systems, Man, and Cybernetics: Systems 52 (2022) 1578 –1587 . [38] G. Sun, R. Han, L. Deng, C. Li, G. Yang, Hierarchical Structure-Based Joint Operations Algorithm for Global Optimization, Swarm and Evolutionary Computation, 2023, 101311 . [39] K. Yu, D. Zhang, J. Liang, K.",
        "start_idx": 11716,
        "end_idx": 11844
      },
      {
        "text": "Computation, 2023, 101311 . [39] K. Yu, D. Zhang, J. Liang, K. Chen, C. Yue, K. Qiao, L. Wang, A correlation-guided layered prediction approach for evolutionary dynamic multiobjective optimization, IEEE Trans. Evol. Comput. (2022), 1-1. [40] G. Sun, G. Yang, G. Zhang, Two-level parameter cooperation-based population regeneration framework for differential evolution, Swarm Evol. Comput. 75 (2022), 101122 . [41] C. Li, G. Sun, L. Deng, L. Qiao, G. Yang, A population state evaluation-based improvement framework for differential evolution, Inf. Sci. 629 (2023) 15–38. [42] G. Sun, C. Li, L. Deng, An adaptive regeneration framework based on search space adjustment for differential evolution, Neural Comput. Appl. 33 (2021) 9503 –9519 . [43] X. Wen, K. Wang, H. Li, H. Sun, H. Wang, L. Jin, A two-stage solution method based",
        "start_idx": 11832,
        "end_idx": 11960
      },
      {
        "text": "Li, H. Sun, H. Wang, L. Jin, A two-stage solution method based on NSGA-II for Green Multi-Objective integrated process planning and scheduling in a battery packaging machinery workshop, Swarm Evol. Comput. 61 (2021), 100820 . [44] G. Wang, E. Fan, G. Zheng, K. Li, H. Huang, Research on Vessel Speed Heading and Collision Detection Method Based on AIS Data, Mobile Information Systems, 2022 . [45] Y. Xue, Y. Tong, F. Neri, An ensemble of differential evolution and Adam for training feed-forward neural networks, Inf. Sci. 608 (2022) 453–471. [46] J. Chen, Z. Cai, H. Chen, X. Chen, J. Escorcia-Gutierrez, R.F. Mansour, M. Ragab, Renal pathology images segmentation based on improved cuckoo search with diffusion mechanism and adaptive beta-hill climbing, Journal of Bionic Engineering (2023) . [47] Y. Han,",
        "start_idx": 11948,
        "end_idx": 12076
      },
      {
        "text": "adaptive beta-hill climbing, Journal of Bionic Engineering (2023) . [47] Y. Han, W. Chen, A.A. Heidari, H. Chen, Multi-verse optimizer with rosenbrock and diffusion mechanisms for multilevel threshold image segmentation from COVID-19 chest X-ray images, Journal of Bionic Engineering 20 (2023) 1198 –1262 . [48] J. Xing, H. Zhao, H. Chen, R. Deng, L. Xiao, Boosting whale optimizer with quasi- oppositional learning and Gaussian barebone for feature selection and COVID-19 image segmentation, Journal of Bionic Engineering 20 (2023) 797–818. [49] H. Hu, W. Shan, J. Chen, L. Xing, A.A. Heidari, H. Chen, X. He, M. Wang, Dynamic individual selection and crossover boosted forensic-based investigation algorithm for global optimization and feature selection, Journal of Bionic Engineering (2023) . [50] X. Wang, X. Dong, Y. Zhang, H. Chen, Crisscross Harris",
        "start_idx": 12064,
        "end_idx": 12192
      },
      {
        "text": ". [50] X. Wang, X. Dong, Y. Zhang, H. Chen, Crisscross Harris hawks optimizer for global tasks and feature selection, Journal of Bionic Engineering 20 (2023) 1153 –1174 . [51] C. Lin, P. Wang, A.A. Heidari, X. Zhao, H. Chen, A boosted communicational salp swarm algorithm: performance optimization and comprehensive analysis, Journal of Bionic Engineering 20 (2023) 1296 –1332 . [52] C. Lin, P. Wang, X. Zhao, H. Chen, Double mutational salp swarm algorithm: from optimal performance design to analysis, Journal of Bionic Engineering 20 (2023) 184–211. [53] J. Hu, S. Lv, T. Zhou, H. Chen, L. Xiao, X. Huang, L. Wang, P. Wu, Identification of pulmonary hypertension animal models using a new evolutionary machine learning framework based on blood routine indicators, Journal of Bionic Engineering 20 (2023)",
        "start_idx": 12180,
        "end_idx": 12308
      },
      {
        "text": "framework based on blood routine indicators, Journal of Bionic Engineering 20 (2023) 762–781. [54] J. Liang, K. Qiao, K. Yu, B. Qu, C. Yue, W. Guo, L. Wang, Utilizing the relationship between unconstrained and constrained pareto fronts for constrained multiobjective optimization, IEEE Trans. Cybern. (2022) 1–14. [55] C. Huang, X. Zhou, X. Ran, Y. Liu, W. Deng, W. Deng, Co-evolutionary competitive swarm optimizer with three-phase for large-scale complex optimization problem, Inf. Sci. 619 (2023) 2–18. [56] J.S. Chou, J.P.P. Thedja, Metaheuristic optimization within machine learning- based classification system for early warnings related to geotechnical problems, Autom. ConStruct. 68 (2016) 65–80. [57] A. Kaushik, N. Singal, A hybrid model of wavelet neural network and metaheuristic algorithm for software development effort estimation, Int. J. Inf. Technol. 14 (2022) 1689 –1698",
        "start_idx": 12296,
        "end_idx": 12424
      },
      {
        "text": "software development effort estimation, Int. J. Inf. Technol. 14 (2022) 1689 –1698 . [58] M. Mehraein, A. Mohanavelu, S.R. Naganna, C. Kulls, O. Kisi, Monthly Streamflow Prediction by Metaheuristic Regression Approaches Considering Satellite Precipitation Data, vol. 14, Water, 2022 . X. Zhang et al. [Página 18] Computers in Biology and Medicine 163 (2023) 107166 18[59] K. Zhu, S. Ying, N.N. Zhang, D.D. Zhu, Software defect prediction based on enhanced metaheuristic feature selection optimization and a hybrid deep neural network, J. Syst. Software 180 (2021) . [60] J.S. Chou, K.H. Yang, J.P. Pampang, P. Anh-Duc, Evolutionary metaheuristic intelligence to simulate tensile loads in reinforcement for geosynthetic-reinforced soil structures, Comput. Geotech. 66 (2015) 1–15. [61] J.W. Ma, D. Xia, H.X. Guo, Y.K. Wang, X.X. Niu, Z.Y. Liu, S. Jiang, Metaheuristic-",
        "start_idx": 12412,
        "end_idx": 12540
      },
      {
        "text": "Xia, H.X. Guo, Y.K. Wang, X.X. Niu, Z.Y. Liu, S. Jiang, Metaheuristic- based support vector regression for landslide displacement prediction: a comparative study, Landslides 19 (2022) 2489 –2511 . [62] N.D. Hoang, D.T. Bui, L. Kuo-Wei, Groutability estimation of grouting processes with cement grouts using differential flower pollination optimized support vector machine, Appl. Soft Comput. 45 (2016) 173–186. [63] S. García, A. Fern˘andez, J. Luengo, F. Herrera, Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: experimental analysis of power, Inf. Sci. 180 (2010) 2044 –2064 . [64] J. Derrac, S. García, D. Molina, F. Herrera, A practical tutorial on the use of nonparametric statistical tests as a methodology for comparing evolutionary and swarm intelligence algorithms, Swarm Evol. Comput. 1",
        "start_idx": 12528,
        "end_idx": 12656
      },
      {
        "text": "methodology for comparing evolutionary and swarm intelligence algorithms, Swarm Evol. Comput. 1 (2011) 3–18. [65] C. Zhao, H. Wang, H. Chen, W. Shi, Y. Feng, JAMSNet: a remote pulse extraction network based on joint attention and multi-scale fusion, IEEE Trans. Circ. Syst. Video Technol. (2022), 1-1. [66] J. Lv, G. Li, X. Tong, W. Chen, J. Huang, C. Wang, G. Yang, Transfer learning enhanced generative adversarial networks for multi-channel MRI reconstruction, Comput. Biol. Med. 134 (2021), 104504 . [67] X. Xue, G. Li, D. Zhou, Y. Zhang, L. Zhang, Y. Zhao, Z. Feng, L. Cui, Z. Zhou, X. Sun, Research roadmap of service ecosystems: a crowd intelligence perspective, International Journal of Crowd Science 6 (2022) 195–222. [68] X. Xue, X.-N. Yu, D.-Y. Zhou, X. Wang, Z.-B. Zhou, F.-Y.",
        "start_idx": 12644,
        "end_idx": 12772
      },
      {
        "text": "[68] X. Xue, X.-N. Yu, D.-Y. Zhou, X. Wang, Z.-B. Zhou, F.-Y. Wang, Computational Experiments: Past, Present and Future, 2022 arXiv preprint arXiv:2202.13690 . [69] X. Xue, X. Yu, D. Zhou, C. Peng, X. Wang, D. Liu, F.-Y. Wang, Computational experiments for complex social systems —Part III: the docking of domain models, IEEE Transactions on Computational Social Systems (2023) . [70] X. Cao, T. Cao, Z. Xu, B. Zeng, F. Gao, X. Guan, Resilience constrained scheduling of mobile emergency resources in electricity-hydrogen distribution network, IEEE Trans. Sustain. Energy (2022) 1–15. [71] Y. Dai, J. Wu, Y. Fan, J. Wang, J. Niu, F. Gu, S. Shen, MSEva: a musculoskeletal rehabilitation evaluation system based on EMG signals, ACM Trans. Sens. Netw. 19 (2022) 1–23. [72] J. Zhou, X. Zhang, Z.",
        "start_idx": 12760,
        "end_idx": 12888
      },
      {
        "text": "Trans. Sens. Netw. 19 (2022) 1–23. [72] J. Zhou, X. Zhang, Z. Jiang, Recognition of imbalanced epileptic EEG signals by a graph-based extreme learning machine, Wireless Commun. Mobile Comput. 2021 (2021), 5871684 . X. Zhang et al.",
        "start_idx": 12876,
        "end_idx": 12913
      }
    ],
    "d27acfd1-c16a-4c63-8631-72a61182680e": [
      {
        "text": "[Página 1] Acta Astronautica 192 (2022) 276–290 Available online 28 December 2021 0094-5765/© 2021 IAA. Published by Elsevier Ltd. All rights reserved.An industry 4.0 approach to large scale production of satellite constellations. The case study of composite sandwich panel manufacturing M. Eugenia,*, T. Querciaa, M. Bernabeia, A. Boschettoa, F. Costantinoa, L. Lampania, A. Marchetti Spaccamelab, A. Lombardob, M. Mecellab, L. Querzonib, R. Usingerc, M. Aliprandic, A. Stancuc, M.M. Ivagnesd, G. Morabitod, A. Simonid, A. Brand ~aoe, P. Gaudenzia aDepartment of Mechanical and Aerospace Engineering, University of Rome “La Sapienza ”, Via Eudossiana 18, Rome, 00184, Italy bDepartment of Computer, Control, and Management Engineering Antonio Ruberti, University of Rome “La Sapienza ”, Via Ariosto 25, Rome, 00185, Italy cRUAG Schweiz AG, RUAG Space, Schaffhauserstrasse 580, 8052, Zürich, Switzerland dThales",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "Italy cRUAG Schweiz AG, RUAG Space, Schaffhauserstrasse 580, 8052, Zürich, Switzerland dThales Alenia Space Italy, Via Saccomuro, 24, Rome, 00131, Italy eEuropean Space Agency, ESTEC: European Space Research and Technology Centre Keplerlaan 1, 2201, AZ Noordwijk, Netherlands ARTICLE INFO Keywords: Industry 4.0 Space 4.0 Smart manufacturing Cyber-physical systems Internet of things Digital twin Artificial intelligence Space Systems MAIT Mega constellations ABSTRACT In recent years the so-called New Space Economy or Space 4.0 paradigm has seen a number of new commercial players entering the satellite industry and creating completely new business models, most of which based on very large constellations consisting of several hundreds or even thousands of satellites. The production of the high number of satellites involved in modern mega-constellations is bringing in the space industry the necessity",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "involved in modern mega-constellations is bringing in the space industry the necessity of improved and optimized manufacturing approaches suitable for serial production , i.e., standard environment/ high number of platforms. In this framework, the adoption of Industry 4.0 methodologies within the space in- dustry will lead to a significant improvement and optimization of the whole Manufacturing Assembly Integration and Testing (MAIT) cycle. The main aim of Industry 4.0 is the creation of intelligent factories where manufacturing technologies are upgraded and transformed by Cyber-Physical Systems (CPSs), the Internet of Things (IoT), Cloud Computing and Big Data Analytics with predictive monitoring features. Main element of the Industry 4.0 approach is the synergic use of embedded sensing technologies in the frame of intelligent production processes, fostering a radical evolution of the",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "the frame of intelligent production processes, fostering a radical evolution of the industrial values chains, production value chains, and business models. In the present work, a possible application of the Industry 4.0 concepts to space industry is presented and discussed in terms of applicability and obtainable advantages. As a case study, the composite sandwich panel manufacturing line of RUAG Space is considered. Particular focus will be given to the development of a CPS, by establishing a control network of sensors (e.g. temperature, location, load) over a targeted MAIT process. 1.Introduction Nowadays, the terms “Industry 4.0” and “Smart Manufacturing ” have become extremely popular to address the so-called Fourth Indus - trial Revolution (4IR) [1] where the evolution of connectivity and computational calculus permit to create a bridge between",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "evolution of connectivity and computational calculus permit to create a bridge between physical and virtual worlds. This connection is represented by Cyber-Physical Sys- tems, which will be the core of the present study. The same revolution applies indeed to the space sector. In 2016, while the European indus - trial context was rushing into the innovation of factories to take advantage of this new concept, the space industry – namely, the Euro- pean Space Agency as its main promoter - followed through and launched the so-called “New Space ” or “Space 4.0” era [2]. The main problem these ambitious initiatives aim to solve can be synthesized as manufacturing inefficiency in a globalized competitive environment, i.e. the slow operational response to customers ’ complex demand driven by increasing availability",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "slow operational response to customers ’ complex demand driven by increasing availability of open information. The background scenario to take into consideration to understand this revolution is the worldwide rise of ICTs as a disruptive force of change in any context, even society itself [3]. The faster and easier availability of data, as much as the greater reachability of people and places all over the world both physically and remotely, kicked-off an unstoppable globalization driver, increasing competitiveness and unlocking new opportunities of sharing knowledge to advance research or make a profit. For this reason, the rising demand for greater *Corresponding author. E-mail address: marco.eugeni@uniroma1.it (M. Eugeni). Contents lists available at ScienceDirect Acta Astronautica u{�~zkw! s{yo|kr o>!ÐÐÐ1ow�o �to~1m{y2w{m k�o2km�kk��~{! https://doi.org/10.1016/j.actaastro.2021.12.039 Received 25 November 2021; Accepted 23 December 2021 [Página",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "�to~1m{y2w{m k�o2km�kk��~{! https://doi.org/10.1016/j.actaastro.2021.12.039 Received 25 November 2021; Accepted 23 December 2021 [Página 2] Acta Astronautica 192 (2022) 276–290 277connectivity pushed new commercial players to risk large investments in the space industry with completely new business models [4]. American ventures and start-ups led the way and invented a new concept of exploitation of already in vogue small satellites, putting them in large constellations and in LEO to give high-bandwidth, low latency internet access to remote areas or to gather new data from more frequent or higher quality observations. The market segment grew exponentially in the last decade, with constellations of even thousands of satellites being already in the launch phase. Nowadays 2500 satellites are actively orbiting around Earth and are expected to be 50 k in ten years [4].",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "Earth and are expected to be 50 k in ten years [4]. Fig. 1 shows the present status of the largest constellations: Space X’s Starlink is the most ambitious one, followed by Amazon ’s Project Kuiper. Both projects aim at operating more than 1000 satellites at a time. In the top ten also Airbus One Web can be found, whose bankruptcy had worried investors and shareholders, up to its recent rescue by UK government to convert it into a navigation system after the loss of participation in Galileo project because of Brexit [5]. Among the well-known English companies, SatRevolution is an example of a company mainly based in Europe. However, all these companies have made international coop- eration with ventures, billionaires, or big space players to allow the",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "coop- eration with ventures, billionaires, or big space players to allow the realization of their projects. A significant reduction of costs is therefore necessary for the industry to take advantage of such a promising new market segment and open it to smaller businesses or more traditional manufacturing players. Producing thousands of satellites of high quality and with tighter deadlines then becomes the top priority, thus requiring an innovative approach to manufacturing processes, which made Smart Manufacturing and its related technologies the best available solutions. The biggest challenges in trying to reduce costs while leveraging capa- bility are the following: ≡the increased diversification of requirements asked by customers or users. ≡the short lead-time to market from product development to product delivery, reduced by global competitiveness. ≡the higher quality assurance",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "development to product delivery, reduced by global competitiveness. ≡the higher quality assurance needed by more complex new tech- nologies [3]. ≡products reliability, stability, and longevity [6]. The main challenge related to cost reduction is linked with the limited-in-time capital investments, especially concerning the cost of launch. Developments are being made to make smaller, more flexible launchers at better prices. Analytics, computing power and AI (Artificial Intelligence) algorithms can improve the operations management of large constellations, reducing response times and operating costs. The goal is to reach the autonomous or semiautonomous spacecraft control and management [4]. However, in the space industry the scale of vol- ume product does not allow for the introduction of automation [7] as much as it does in mass-market sectors, thus generating the need for",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "much as it does in mass-market sectors, thus generating the need for alternative concepts of product and process optimization, relying more on IT (Information Technologies) than OT (Operational Technologies) or, better, on the integration of both. Even when more easily applicable, a new wave of automation would require the conversion of blue-collar jobs to white-collar jobs, with a fast reskilling and new training of human resources toward greater horizontal connectivity and interop - erability [8]. In any case, the space industry needs to become “smarter ” and its smartness level will be measured by the degree of reflection of its products and processes in the new digital world, also called the “cy- berspace ”. Competition is not between products or processes anymore, but rather between the information services",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "not between products or processes anymore, but rather between the information services and analytics algorithms behind them. The solution proposed to convey “smartness ” to the real- ization of large constellations of small satellites can be borrowed by the newest frontiers of Smart Manufacturing, especially in the framework of “Industry 4.0” initiatives spread all over the world [9]. Thanks to the principles of Smart Manufacturing, it is possible to translate a conven - tional in-line dedicated manufacturing process into a fully integrated digitalized process using the latest information technologies. The space industry has not a long experience in serial process optimization, therefore it must take advantage of the state of the art in other industries to win the challenges previously mentioned and meet the need for a Acronyms/abbreviations",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "win the challenges previously mentioned and meet the need for a Acronyms/abbreviations 4IR Fourth Industrial Revolution AGV Automated Guidance Vehicle ALM Application Lifecycle Management AM Additive Manufacturing APM Automated Insert Potting Machine ASIC Application Specific Integrated Circuit CPS Cyber-Physical Systems CPPS Cyber-Physical Production Systems DT Digital Twin ERP Enterprise Resource Planning ICT Information and Communication Technology IoT Internet of Things KET Key Enabling Technology KPI Key Performance Indicator LEO Low Earth Orbit MAIT Manufacturing Assembly Integration and Testing MEMS Micro Electro-Mechanical Systems MES Manufacturing Executive System MOM Manufacturing Operations Management NDI Non-destructive Inspection technique OT Operational Technology UT Ultrasonic Testing Fig. 1.Pareto chart of planned and launched small satellite constellations per number of satellites as of March 2020. [© newspace.im ]. M. Eugeni et al. [Página 3] Acta",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "March 2020. [© newspace.im ]. M. Eugeni et al. [Página 3] Acta Astronautica 192 (2022) 276–290 278better capability over cost ratio. It will need to master the latest de- velopments in other industries in the field of Smart Manufacturing and take them to the next level for the first time. Opportunities for stan- dardization, modularization and serialization are evident, especially benchmarking with the Do-It-Yourself philosophy that the Chinese Aerospace industry is trying to pursue with micro and nanosatellites. For example, from a strategic point of view, with respect to German-born “Industry 4.0”, Chinese plan (called “Made in China 2025”) has star- ted with a pilot and then will be extended step by step [10,11]. Their goal is to introduce a comprehensive innovation system on a small scale to",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "is to introduce a comprehensive innovation system on a small scale to increase the manufacturing capability index and thus the convenience of the product at macro level both for producers and customers [6]. Besides the fundamental trend of lowering costs, satellite manufacturing moves toward the concept of universality, that is the possibility to leverage international competitiveness for supplies and raw materials to set factories for enhanced rapidity, easier maintenance, and better upgradability [6]. Following this reasoning, the UK space sector in 2019 explored the interesting concept of a “Global Production Network”, focused on dynamics and thus on the importance of heritage to manage associated risks. In particular, the UK sector is trying to mitigate risks by counting on well-proven technology and structuring solid relationships with national and international",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "counting on well-proven technology and structuring solid relationships with national and international regulators [12]. The Brexit and covid-19 pandemic, however, will put the success of this viewpoint in doubt for two reasons: first of all, after Brexit the UK will be less and less protected by the European Community, being more of a competitor in the European market, and thus incurring in higher taxa- tion for import/export, thus losing the advantage of lowering process and product costs [5]; secondly, having disrupted logistics, travel and transportation, the covid-19 and its safety regulations will force Euro- pean space companies to rapidly invest in new home-made technologies in order to keep their workforce and avoid tensions coming from the risk of increased unemployment and difficulty to expatriate. The paper is organized",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "risk of increased unemployment and difficulty to expatriate. The paper is organized as follows: Section 2 explains the theoretical background necessary to understand the use of cyber-physical systems in a space factory; Section 3 introduces the approach to its implementation, reviewing hardware technology, software technology, sensors systems and Non Destructive Inspection (NDI) techniques; in Section 4 the approach is applied to the case study on the real process of RUAG’s sandwich composite panel manufacturing; eventually, Section 5 pre- sents the conclusions of the study. 2.Theoretical background: cyber-physical systems In this Section, an overview of Smart Manufacturing concepts, tools and strategies is presented together with the most adopted SM frame - work, RAMI 4.0, are illustrated. Among SM concepts, the theory of Cyber-Physical-Systems is highlighted as the foundation for",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "SM concepts, the theory of Cyber-Physical-Systems is highlighted as the foundation for the inte- gration of IT and OT enabling the improvement of an MAIT process in the space industry. Concepts, characteristics, and contextualization in a production environment are given. 2.1. Smart Manufacturing concepts, tools and strategies Smart Manufacturing focuses on establishing intelligent and communicative systems based on interoperability, i.e. machine-to- machine and human-to-machine interconnections, dealing with a digi- talized data flow from intelligent and distributed system interaction [13]. Products, machines, and company processes acquire a higher level of knowledge by data acquisition of parameters, e.g. product charac - teristics, localization, process parameters (temperature, pressure, speed, etc.), and also information from the other stakeholders (e.g. customers, suppliers). This data collection is transferred through internal or external communication networks,",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "suppliers). This data collection is transferred through internal or external communication networks, to be shared and to enable self-control capacities of products, machines, processes. Thus, these el- ements become “smart”: capable to measure, recognize, communicate, carry out decision-making processes (mostly without man intervention), to activate actions and operations in production [14]. Smart manufacturing in short is “a data intensive application of information technology at the shop floor level and above to enable intelligent, effi- cient, and responsive operations” [15]. To consider a process “smart”, it is necessary to satisfy the following characteristics [16]: (I) computeri - zation, or the ability to control or monitor operations through pro- grammable logics such as PLC, microcontroller, or microcomputer; (II) connectivity, achieved through communication networks such as 4G, 5G, Wi-Fi or specialized",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "connectivity, achieved through communication networks such as 4G, 5G, Wi-Fi or specialized protocols; (III) visibility; (IV) transparency, building an operating history and allowing problem solving based on real data; (V) predictive capacity, adopting models based on algorithms that correlate past operations with the measured real-time parameters; (VI) adaptability, allowing the system to adapt its operations. Smart Manufacturing strategic action lines are focused to reach im- provements on autonomous interoperability, agility, flexibility, decision-making, efficiency or cost reductions, mass customization, servitization [3,17–19]. It enables companies to cope with the chal- lenges of producing individualised products as expected by customers with a short lead-time to market and at the cost of mass production [20]. Smart Manufacturing relies on the interdisciplinary and complex implementation of several different technologies, such as Cyber-Physical-Systems, Artificial",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "interdisciplinary and complex implementation of several different technologies, such as Cyber-Physical-Systems, Artificial Intelligence [21], Cloud Computing [22], Big Data analytics [23], Machine Learning [24], Internet of Things [25], Augmented Reality and Virtual Reality [26], etc. This paper will focus on those selected to the implementation of a CPS architecture in a complex MAIT process in the space industry. However, a common standard infrastructure is shared among all these technologies, helping to contextualize them in the overall product life-cycle value chain: the so-called RAMI 4.0 [27]. RAMI 4.0 ensures intercommunication and understanding across all business units and functions with a service-oriented architecture, starting from physical things and arriving to the most digital business processes through a bi-dimensional hori- zontal and vertical expansion, following respectively the increase of value and",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "hori- zontal and vertical expansion, following respectively the increase of value and the increase of authority, see Fig. 2. RAMI 4.0 well represents Industry 4.0 concepts of holistic integration as well as easy interopera - bility, modularity and reconfigurability, bringing them directly in the structure of the business, sometimes called enterprise, for its compre - hensive service-oriented goals. Being RAMI 4.0 such a complex archi - tecture, a hybrid model with the upper layers substituted with traditional MES and/or ERP is under study to fasten its implementation [28]. Among all SM tools, the CPS has the best potential to reproduce this framework, being the only one able to also integrate all other technologies. 2.2. Cyber-physical systems applied to a manufacturing environment Recently, there has been an explosive growth",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "applied to a manufacturing environment Recently, there has been an explosive growth in the development and implementation of various Cyber-Physical Systems (CPS) [29]. CPS (cyber-physical systems) are physical systems that incorporate in- tegrations of computation, networking-communication, and physical processes control, see Fig. 3. They are made of heterogeneous cooper - ating components interacting through a complex, coupled physical environment operating over many spatial and temporal scales [30]. Embedded computers and networks monitor and control the physical processes, with feedback loops where physical processes affect compu - tations and vice-versa. CPS are defined as transformative technologies for managing interconnected systems between their physical assets and computational capabilities [31]. CPS are systems of integrated compu - tational entities which are in intensive connection with the surrounding physical world and its",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "which are in intensive connection with the surrounding physical world and its on-going processes, providing and using, at the same time, data-accessing and data-processing services available on the Internet [32]. In other words, CPS can be generally characterized as ‘‘physical and engineered systems whose operations are monitored, controlled, coordinated, and integrated by a computing and communi - cating core’’ [33]. To this end, CPSs are able to Ref. [34]: M. Eugeni et al. [Página 4] Acta Astronautica 192 (2022) 276–290 279(i) collect data referred to themselves and their environment (ii) process and evaluate these data (iii) connect and communicate with other systems (iv) initiate actions. A CPS is defined as a system in which physical objects are required to be accompanied by their representation in the digital world,",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "are required to be accompanied by their representation in the digital world, to be in- tegrated with elements with computing, storage, and communication capabilities, and to be networked between them. They are considered one of the key technological innovations (Key Enabling Technology - KET) of the Fourth Industrial Revolution, a transformative technology that can be placed in the foreground for the potential promised for the creation of value along with the three dimensions of the digitalization of manufacturing: the smart product, Smart Manufacturing, and changes in the business models of companies [35]. Smart manufacturing systems use CPS predominantly as a tool to monitor the physical world and make decentralized decisions in the virtual world, often referring to Cyber-Physical Production Systems (CPPS). The growing availability, affordability and adaptability of",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "to Cyber-Physical Production Systems (CPPS). The growing availability, affordability and adaptability of sensors and connection systems are increasing the widespread adoption of CPS and CPPS. Production data are easier to be collected and transferred to cloud platforms, where analytics and AI tools permit to analyse and predict the production be- haviours, and consequently act (manually or automatically) to increase performance. A complete CPS should be able to get information from the physical world and act on it, usually after data computations suggested the action to be implemented. CPS should not be confused with IoT, because IoT is part of a CPS system, that for example could also include AI technology. Some insights on these technologies can be found in Ref. [36]. Fig. 4 shows how CPPS connect a",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "be found in Ref. [36]. Fig. 4 shows how CPPS connect a system in the physical world and its Digital Twin (in the cyber world), with an important remark about the human-centred vision of these systems. Indeed, the oper- ator/manager is always needed to check the process reliability and often to validate the analysis and the actuating decisions. In the design of a CPS it is recommended by Ref. [30] to pay attention to issues of Fig. 2.RAMI 4.0 architecture is the most common standard framework for the application of Smart Manufacturing to a whole enterprise value chain. The archi - tecture is structured on a bi-directional and multi-layer way, with developments going both horizontally, following product life cycle value (procurement to sales) and hierarchical levels of complexity",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "product life cycle value (procurement to sales) and hierarchical levels of complexity (product to connected world) and vertically, expanding from the simple asset (e.g. shop floor equipment) to the entire busi- ness [27]. Fig. 3.The figure shows a layout of the Cyber- Physical System of a sensorized MAIT process plant. It illustrates the cycle from physical to cyber domains, passing by control, communication and computation functions. In the computational layer, data records and analysis are performed. The Digital Twin re- produces the process plant in the Cyber Domain, while the Internet of Things allows its communication with the physical domain through the interconnection of sensors in an online platform. Eventually, intelli - gent analytics can be performed by AI algorithms introduced in the computation phase and aimed at",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "performed by AI algorithms introduced in the computation phase and aimed at improving the data reports, allowing faster decision- making, possibly made autonomously or semi- autonomously by the process machines themselves. M. Eugeni et al. [Página 5] Acta Astronautica 192 (2022) 276–290 280reliability and security, level of abstraction and architecture styles for modular design and development, new frameworks and algorithms, concepts of dependability, reconfigurability, certifiability and trust- worthiness. More research on this topic can be found in Refs. [37,38]. 3.Approach: implementing a CPS architecture in space manufacturing The main problem a space factory nowadays faces is related to the high costs of keeping the pace of a competitive technological market, leading companies worldwide with the help of new business models to lower entry barriers to the segment. Technological",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "of new business models to lower entry barriers to the segment. Technological innovation inte- grating the newest IT solutions is requested to traditional manufacturing shop floors to leverage space long-term heritage while keeping the business sustainable. The CPS was chosen among all SM tools, according to the features described in the previous chapter, as the best candidate to give a measurable and reliable improvement to a space manufacturing process. Introducing a CPS into a space manufacturing facility requires a two-level approach: 1. Monitoring the product to be manufactured. 2. Monitoring the production, integration and test means necessary to deliver the product. To fully realize this approach, three main areas of technical competence have been considered: ● Hardware technology, to identify the critical operations of a complex MAIT process;",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "Hardware technology, to identify the critical operations of a complex MAIT process; ● Software technology, to identify the most performant solutions to digitalize the process; ● Sensor systems and Non-Destructive Inspection (NDI) techniques, to identify types of sensors and related techniques to enhance product and process control and monitoring. 3.1. Hardware technology In this paragraph an overview of the applications of SM tools from the point of view of Hardware Technology is given. First, the illustration of typical production systems will explain the convergence toward the cellular system. Then, traditional production characteristics in the space industry will be mentioned and their evolution following SM principles from the point of view of HW technology will be presented. Concerning production systems, the aerospace industry is mainly characterized by intermittent production",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "Concerning production systems, the aerospace industry is mainly characterized by intermittent production and the management of the production is typically based on job-shop criteria [39]. This system type is characterized by low volume and high variety with relatively low production rate and high flexibility. It is also noteworthy that the planning, routing, and scheduling function is typically done for each part independently. The efficiency of the machines is low and, to reduce cost, they are general purpose machines. The machines and the move - ments are reduced, and few setup operations are required. On the con- trary, in large-scale productions machines are dedicated, and processing parameters are optimized for few types of parts. A continuous flow must be maintained. In this case, high costs and highly specialized machines",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "must be maintained. In this case, high costs and highly specialized machines are affordable thanks to the large production volume. This type of production system is referred to as ‘process-based ’. The addressing of resources is completely dedicated to the optimization of specific pro- cesses and the routing of the single part reflects the sequence of the operations over the selected machine. As a result, the movements are many and the mean lead time is affected. Between these two extremes, a ‘combination layout ’ is usually proposed in industrial manufacturing. It is the so-called ‘cellular production ’ that requires a systematic approach in the design methodology that incorporates all the previous benefits and can easily move between the extremes, see Fig. 5 [39]. The benefits of the cellular",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "between the extremes, see Fig. 5 [39]. The benefits of the cellular production system are widely accepted in industrial production for the so-called mass customization, but many items must be considered in the space industry. It is particularly important to maintain the quality assurance of the fabricated components and it is difficult to allow the automation of labour-intensive operations and combinations between process options. Traditionally, space production systems, besides being of “job-shop ” type, were mainly designed for single units. In Ref. [24] the example of Boeing is presented: the focus was on single unit delivery models and unique parts were supplied by customized contracts with suppliers coming only from the space industry, with prototypes being qualified on demand. Other traditional features included: (I) most of the documen",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "qualified on demand. Other traditional features included: (I) most of the documen - tation produced and archived in paper; (II) a low presence of automation or robotization; (III) single shift/5 days schedule; (IV) long life-cycle products of typically 10 years; (V) siloed structures for the different departments; (VI) “push ” approach with large stock of finite product [24,40–42]. Most of these characteristics evolved in the framework of Industry 4.0 and Space 4.0 initiatives. The following interesting SM concepts have been applied to HW technology, specifically in the context of small satellites ’ constellations [43]: ● Automated Guidance Vehicles (AGVs) Fig. 4.An example of the implementation of a cyber-physical system in the production department. The job flows from production orders to machines, while the decisions rise from machines back",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "from production orders to machines, while the decisions rise from machines back up to customer ’s orders. At every stage of data gathering and processing, human intervention is always necessary to provide advanced monitoring functions and interpreting results [91]. Fig. 5.Types of production systems in terms of volume & variety and flexibility & efficiency. At the extremes, job-shop system qualifies as high variety and high flexibility and process-based system as high efficiency & high volume. The hybrid type cellular system lies in between. M. Eugeni et al. [Página 6] Acta Astronautica 192 (2022) 276–290 281Equipped with cameras and navigation software, these vehicles allow the transportation of heavy components or the final assembly through the factory. Well known in the automotive industry, this level of automation was used by",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "known in the automotive industry, this level of automation was used by OneWeb facility in Florida. ● Spring-based loading machines Specific machines equipped with springs are used to load satellites to avoid human non ergonomic operations. In general, flexibility of orientation and vertical movement is required by satellite platforms to allow the last operations, when most subassemblies are completed and reaching parts is more difficult. ● Additive Manufacturing (AM) AM is based on a layer-by-layer addition of material instead of traditional machining ’s material-removing approach, thus allowing the rapid prototyping of even complex geometries thanks to advanced 3D software design (for this reason, the technique is also called 3D printing [44]). A 3D printing machine was used by Telesat ’s facility in Ottawa, Canada, to realize the apertures",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "used by Telesat ’s facility in Ottawa, Canada, to realize the apertures of the phased-array antennas. This allowed the reduction of multiple part numbers into a single standard one, besides a significant acceleration of times and reduction of costs. The main limitation of the AM manufactured part is of comparable low strength and associated quality, coupled with a high cost of the printing machine system [45]. ● Robots & cobots Multiple robotic solutions were applied for example by Telesat to make repetitive and heavy operations easier, from manipulation of parts to cutting. However, these were used only to make prototypes, as the mass production is yet to come. The new frontier of robotization con- cerns “cobots ”: interconnected and easily programmable; autonomous, flexible, and collaborative; able to avoid",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "”: interconnected and easily programmable; autonomous, flexible, and collaborative; able to avoid collisions based on pre-set up 360•visualizations of the environment; easily programmable [46]. An example of learning cobots for painting can be found in Bombardier [47]. 3.2. Software technology Starting from the traditional manufacturing data management sys- tems and passing by the concepts of interoperability, given by the In- dustrial Internet of Things (IIoT), and digitalization, given by the Digital Twin (DT), in this paragraph the most used CPS architecture will be presented. Many industries adopt the HMI-SCADA System, a comprehensive real-time data control hardware and software architecture for Manufacturing plants [48]. The Supervisory Control And Data Acquisi - tion system (SCADA) represents the overall control system, gathering and analysing data in real-time, while the Human Machine",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "control system, gathering and analysing data in real-time, while the Human Machine Interface (HMI) is the software showing data in a digestible format for humans through computing systems, allowing the interoperability of workers and machines. Interacting with equipment through user-friendly SW interfaces, humans can reduce repetitive, unsafe, and heavy work or facilitate their day-to-day process monitoring activities. The HMI-SCADA System architecture is based on executive functions and communicating functions. The executive functions are represented by the field instrumentation (in-house instruments monitoring and con- trolling automation processes) and Remote Terminal Units (RTU) or Programmable Logic Controllers (PLC), whose concepts are mostly overlapped and represent the interface between plant equipment and their computing control units. The communication functions, on the other hand, are represented by a data communication layer, transferring",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "on the other hand, are represented by a data communication layer, transferring data from the plant to the server; a telemetry layer, transmitting and receiving data from external sources (e.g. Earth telecommunication stations or satellite ground stations); the SCADA host or supervisory system, including the HMI software, representing the data receiving server. In Fig. 6 the system is vertically contextualized as a level of the overall complex enterprise system standardized by the ISA95 model [49], including the device level at the bottom and the management (MOM or MES) and enterprise (ERP) interfaces at the top, the last two representing the data analytics and integration platforms. Enterprise systems (ES) or enterprise information systems (EIS) concepts have been researched and utilized for decades, with applications in the aerospace industry being",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "researched and utilized for decades, with applications in the aerospace industry being studied at [50]. In some industries, like the pharmaceu - tical one, the application of this standard from day one allowed the transition to paperless processes [51]. Over the next ten years, the number of connected devices will exceed the number of inhabitants of the world [33]. The IIoT represents a possible evolution or integration of the HMI-SCADA System in the new industrial landscape. The IIoT is defined as a network of physical sys- tems that can interact with each other thanks to standard communica - tion protocols, to achieve a common goal. Physical systems, and therefore ’things ’, are represented by sensors, actuators, communication modules and devices that can collaborate with each other, through intelligent",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "communication modules and devices that can collaborate with each other, through intelligent components and applied software, and therefore achieve objectives that strongly depend on their ability to transmit and process information. It is a multi-directional communication between processes, including the machinery used, the components and the products. The main form of communication allowed by IIoT technology with respect to SCADA/HMI is machine-to-machine communication: the devices communicate directly using programmable electronic devices and wireless technologies. This form of interoperability among machines could extensively contribute to the implementation of a CPS architec - ture. Other recommendable IIoT characteristics are self-optimization, self-healing, self-configuration, and self-protection [52]. A use-case of IIoT-based architecture applied in aerospace manufacturing can be found in Ref. [53]. To implement a CPS, process physical entities must also have",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "Ref. [53]. To implement a CPS, process physical entities must also have a faithful representation in the digital world. This representation is defined as ‘digital twin ’ (DT). DTs are commonly known as a key enabler for the digital transformation in manufacturing. Different definitions agree on features such as (i) connectivity, i.e., the ability to communi - cate with other entities, (ii) autonomy, i.e., the possibility to live inde- pendently from other entities, (iii) homogeneity, i.e., the capability, strictly connected to autonomy, that allows using the same DT regard - less of the specific production environment, (iv) easiness of custom - ization, i.e., the possibility to modify the behaviour of a physical entity by using the functionalities exposed by its DT, and (v) traceability, i.e., the ability to",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "functionalities exposed by its DT, and (v) traceability, i.e., the ability to trace the activity of the corresponding physical entity. To allow traceability, systems based on barcodes, QR codes or RFIDs [54] are applied or incorporated in the product. Finally, DTs monitor and control the physical entities, where physical entities send data to update what are commonly referred to as the virtual models [55,56]. Many are the advantages of this concept. First, it is easily useable for small series of customized products. Secondly, the DT allows modular simulation: being able to reproduce the operating system, it allows to modify products in a flexible way and to speed up innovation processes. The possibility of minimizing the time between design and product delivery through a DT is a good alternative",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "between design and product delivery through a DT is a good alternative not to change the process itself, which is often more complicated and more expensive. What a DT facilitates that other technologies are not able to is the real time reproduction of the system. Real time is a key concept in process monitoring, as the evolution of industrial trends follows speed, with dynamic systems handling high volumes of data [57], also thanks to the introduction of new semiconductor materials which can fasten electronical connections of process equipment and information systems. A challenge to consider, especially when scaling the concept to a whole process, is the risk to design closed cycles, with monitoring functions heavy dependant on the digital reproduction itself. Simulation models made of DTs are able",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "on the digital reproduction itself. Simulation models made of DTs are able to embrace the entire value chain and the entire life cycle of the products, M. Eugeni et al. [Página 7] Acta Astronautica 192 (2022) 276–290 282thus providing the necessary parameters not only to make fast and short-term decisions, but also to allow more sustainable decisions in the long-term, using the permanent collection of data through historical series, which become rich material for statistical models to build more accurate correlation coefficients and to show more complete predictive graphical instruments for trends ’ interpretation. The integration of the SCADA/HMI level with the machine-to- machine communication characteristic of the IIoT, linked to a 3D real- time throughout the process virtual representation of all sensors and machinery using DTs, allows",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "the process virtual representation of all sensors and machinery using DTs, allows the implementation of a fully compre - hensive CPS architecture. The so-called CPS 5C level architecture [58] clearly defines, through sequential activity flows, the architecture of a CPS starting from the initial data acquisition, up to the creation of final value. The architecture is characterized by five levels: 1. Smart Connection Level: guarantees the timely and reliable acquisition of data from sensors, controllers or company production systems (e.g. ERP, Fig. 6.Pyramidal architecture of an overall enterprise SM standards ’ system, showing the incorporation of HMI/SCADA level [53]. Fig. 7.CPS 5-levels architecture is the most used. Levels of connection, conversion, cyber, cognition and configuration are shown. Related assets, users, and functions are displayed. [58]. M. Eugeni et",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "shown. Related assets, users, and functions are displayed. [58]. M. Eugeni et al. [Página 8] Acta Astronautica 192 (2022) 276–290 283MOM, MES). It is central, considering the heterogeneity of the data, to select appropriate data acquisition methods and sensors (in terms of types and specifications). 2. Data-to-Information Conversion Level: converts the data collected into significant information through specific algorithms and analysis. 3. Cyber-Level: acts as a central hub, where all the information deriving from the various machines and components, arrives and creates an intelligent network. They are then analysed to understand specific or collective information about the state of the system and evaluated to predict future events. 4. Cognition-Level: the implementation of the CPS at this level generates an in-depth knowledge of the monitored system, a valuable support",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "level generates an in-depth knowledge of the monitored system, a valuable support in the decision-making process. This knowledge allows operators to manage the system opti- mally. To ensure visibility, clarity, and immediacy in the understanding of the system by the operators, it is often necessary to implement graphic analysis and representations. 5. Configuration-Level: the configuration level constitutes the feedback of the cyberspace in the physical space and acts as a supervisory control to make the machines self-configuring and self-adaptable. It acts as a resilience control system (RCS) and allows to monitor, prevent, and correct the systems. This 5-level framework is by far the main reference for CPS. Fig. 7 represents its levels and functions. Future developments of CPS use the 5G protocol network, aiming to low latency (ms)",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "of CPS use the 5G protocol network, aiming to low latency (ms) and high data rates (Gbps) [59]. With 6G, already started to be studied, even Tbps could be reached [60]. However, it is estimated that 6G will not be implemented until 2030 [61]. 3.3. Sensor systems and non-destructive inspection techniques Sensor systems and Non-Destructive Inspection techniques are reviewed as suitable to be integrated in the framework of a CPS to improve the process by faster and more qualitative structural health monitoring. Specific references to advantages in the space industry are made. 3.3.1. Fiber optics sensors The principle of fiber optics sensors is that of an input light reflected on a fiber and showing an interference pattern passing by a light de- tector. Fiber optics sensors can measure",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "pattern passing by a light de- tector. Fiber optics sensors can measure all traditional sensed parame - ters in structural health monitoring (e.g. strain, temperature, crack propagation, leakage, corrosion). Fiber optic sensors have numerous advantages for application in aerospace. In fact, they are lightweight, can be easily embedded into composite structures and are immune to electromagnetic interference. Furthermore, considering that a huge number of sensors will be necessary to completely cover the structural elements of a space structure, the multiplexing capability of optical fi- bers, that is the possibility of writing several sensors into one single fiber, results in a notable advantage both in terms of low complexity and low weight. The fact that optical fibers do not involve any electric signal is a clear advantage from a",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "do not involve any electric signal is a clear advantage from a safety point of view. Other advantages are the long-term stability, low signal losses, the ability to operate in a wide range of temperatures. Drawbacks using these sensors are the difficulty in replacing or repairing the fiber if it fails and some technological dif- ficulties at cryogenic conditions such as low response time for hydrogen sensing and low sensitivity for temperature measurements. One of the most interesting applications for small satellite manufacturing is that it is possible to monitor the degree of cure by simply measuring the refractive index changes in isothermal conditions [62]. Optical fibers can also be employed for chemical sensing during the cure of composite materials. At NASA-Langley chemical spectra were obtained using single",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "cure of composite materials. At NASA-Langley chemical spectra were obtained using single mode optical fibers [63]. 3.3.2. Acoustic emission sensors Acoustic emission (AE) sensors resort to the analysis of emissions from active defects and are sensitive to defect activity when a structure is loaded either during service or a proof test. AE analysis is a useful method for the investigation of local damage in materials. It is also possible to observe damage processes during the entire load history without any disturbance to the specimen. Acoustic emission sensors are used for monitoring a wide number of defects in materials such as dy- namic strain, crack growth, leakage, corrosion, delamination, fiber breakage. They are particularly suitable for monitoring the material fatigue behaviour since dynamic strain is measured. Conventional technologies used",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "the material fatigue behaviour since dynamic strain is measured. Conventional technologies used for AE monitoring are piezoelectric transducers, but fiber optic-based AE sensing technology is gaining more and more consideration for the already mentioned advantages related to the use of optical fibers [64]. In-flight AE sensors have been successfully demon - strated on the DC-XA in-flight experimentation vehicle. The AE moni - toring system was conceived to have information on temperature limits, vibrations, noise characterization and to provide in-flight data from the LH2 cryogenic tank. The control unit AEFIS [65] (Acoustic Emission Flight Instrumentation System) was able to monitor and send informa - tion to the on-board computer for real-time monitoring. The system was also conceived for active monitoring through excitation of the acoustic emission sensors. A health",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "for active monitoring through excitation of the acoustic emission sensors. A health monitoring system with 48 sensors for strain and hydrogen monitoring was used on the composite hydrogen tank for the X-33 experimental vehicle during on-ground tests. In addition, AE sensors for high temperatures have been developed for the structural monitoring of the nose TPS on the X-38, now cancelled. Acoustic emission sensors have also been successfully applied during static tests of the X34 composite wing. 3.3.3. Piezoelectric materials Piezoelectric materials [79] are composite materials with incorpo - rated electrical connections. Under the application of stress, their elec- trodes are excited and the material is charged. Moreover, it manifests a linear change in shape. Charge and linear change represent the char- acteristics of such materials in terms of",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "linear change represent the char- acteristics of such materials in terms of their dual use as actuators (transforming electrical energy into mechanical energy) and sensors (detecting possible defects measuring structural variations). The most common family of piezoelectric materials is the so-called PZT (zirconate titanate family). Used as actuators, PZTs sensors can actively monitor the structure. Functioning as both transmitters and receivers, they can be part of a flexible structural health monitoring system capable of performing several evaluation functions. Presently, they can be used for active damage detection with high-frequency electro-mechanical impedance method, or active damage detection with the pulse-echo and pitch-catch techniques using Lamb-waves, or as passive sensors for low-impact damage detection and acoustic emission detection [66]. Furthermore, they can be used in a phased array of sensors",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "[66]. Furthermore, they can be used in a phased array of sensors that allows, through the superposition of the generated waves, to focus or steer the beam in a specific direction. Several studies have demonstrated the capability of piezoelectric sensors for damage detection in composite materials. Studies at ONERA have demonstrated that Lamb waves are sensitive to debonding caused by low impact in a sandwich structure [67]. 3.3.4. Micro-Electromechanical Systems (MEMS) Micro-Electromechanical Systems (MEMS) are thin-film devices produced through photolithography and chemical etching. Sensors for temperature and pressure measurements are already available as com- mercial off-the-shelf products, but other MEMS sensors exist such as accelerometers, gyros, acoustic emission, and chemical sensors. The advantage of using MEMS sensors is their small size and potentially low cost. They can be",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "sensors is their small size and potentially low cost. They can be easily embedded or surface bonded. Furthermore, with an ASIC (Application Specific Integrated Circuit) technology, it is possible to create a microsystem of different sensors in one single chip [68]. Some issues for structural health monitoring of aerospace struc- tures are the temperature range that goes to the best from \u000050 •C to 175 •C. Furthermore, the temperature dependency of some sensors may affect the measurements, thus limiting the performance [69]. Development is required to attain space qualification, and most of all, these devices should be tested in real environment conditions. Another issue is the packaging of MEMS sensors. As an example, a smart layer M. Eugeni et al. [Página 9] Acta Astronautica 192 (2022) 276–290 284composed",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "M. Eugeni et al. [Página 9] Acta Astronautica 192 (2022) 276–290 284composed by PZT sensors developed by Acellent Tech. Inc. has been embedded into a composite laminate that was also equipped with an electromagnetic layer to measure electrical resistance properties. 3.3.5. Self-monitoring materials Some structural materials can be used as self-monitoring materials, which means they can sense their own strain and damage by measuring their electrical resistance [70]. Carbon fiber-reinforced polymers are very suitable as self-monitoring materials since the fibers are electrically conducting and the electrical properties of the material are sensitive to damage. Self-monitoring materials are intrinsically smart, which means they don’t need embedded or attached sensors, so they have some ad- vantages like low cost, simple design, great durability, large sensing volume and absence of mechanical",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "cost, simple design, great durability, large sensing volume and absence of mechanical property degradation due to embedding of sensors. A problem of concern for electrical measurements is the electrostatic disturbance due to the electrical charging of the structure when flying through charged atmosphere at high speeds or in orbit due to encounter with ionized molecules. Another issue to be addressed is the ability to locate the damage in large composite struc- tures. As an alternative, CFRP self-healing materials are under study [71], having the advantage of using a new ISOX (iso- cyanurate-oxazolidone) thermosetting matrix able to restructure itself in case of delamination or debonding (fiber breakages are not detectable though). 3.3.6. Thermocouples, strain gauges and accelerometers Together with new sensing technologies such as optical fibers, pie- zoelectrics and",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "Together with new sensing technologies such as optical fibers, pie- zoelectrics and so on, conventional sensors are also used for structural health monitoring in the space industry. The major issues for sensors such as thermocouples, strain gauges and accelerometers are the weight penalty from the sensor itself, but also from the wires required to pro- vide power and data communication. Wireless transceivers can be used to overcome this penalty. These have been flight tested at NASA in the frame of ARIES experiment as part of an Integrated Vehicle Health Monitoring architecture [72]. The transceivers radio frequency emis- sions have been demonstrated to not have interference with communi - cation and navigation antennas. 3.3.7. Thermography Thermographic methods are non-destructive inspection methods in which the presence of flaws is determined",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "are non-destructive inspection methods in which the presence of flaws is determined by monitoring the flow of heat over the surface of a structure after some external introduction of a temperature gradient [73]. The presence of flaws disrupts the normal pattern of heat flow that would be expected in a sound structure. The method is more sensitive to flaws near the surface. Modern thermo - graphic systems commonly use infrared (IR) cameras to detect radiated heat and are controlled by TV video electronics which sample the field of view at a typical rate of 50 Hz, allowing temperature variations on a 20 ms timescale to be resolved [74]. The camera is sensitive to temperature changes of about 0.005 •C and covers a chosen range of temperature, 4 •C",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "about 0.005 •C and covers a chosen range of temperature, 4 •C and 8 •C being commonly suitable, although operation is feasible between \u000050 •C and 100 •C. Liquid crystal coatings and pyroelectric detectors have also been used to detect IR radiation. Thermographic methods fall broadly into two groups: active methods, and passive methods. Active methods are those in which the thermal gradient is produced and continuously maintained by the application of cyclic stress. An interesting application of IR thermographic technique is the installation of a thermo-camera to an unmanned aerial vehicle for the monitoring of defects at the distance of 2 m and 6 m [75]. Passive methods are those in which the thermal gradient results from a transient change. Passive methods are the most widely applied",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "results from a transient change. Passive methods are the most widely applied NDI techniques in composites inspection. Also, non-IR conductive thermography has been applied to aerospace applications, such as in the field of Maintenance, Repair and Overhaul (MRO), being able to identify defects in a laminate composite at low temperature [76]. 3.3.8. Ultrasonic testing Ultrasonic testing (UT) is the most widely used non-destructive in- spection method for the examination of composites [77]. On micro - scopically homogenous materials (i.e. non-composite) it is commonly used in the frequency range 20 kHz to 20 MHz. With composite mate- rials the testing range is significantly reduced because of the increased attenuation, so the operating frequency limit is usually 5 MHz or less. However, the ability to resolve small flaws will also",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "MHz or less. However, the ability to resolve small flaws will also be reduced. In most techniques, short pulses of ultrasound (typically a few microseconds) are passed into the composite material and detected after having interro - gated the structure. The techniques include pulse-echo, through- - transmission, back-scattering, acoustic-ultrasonics, and ultrasonic spectroscopy. In these methods, it is important to avoid frequencies at which resonance occurs between ply interfaces. For unidirectional plies spaced at 8 plies/mm this frequency is usually about 12 Mhz. There may be an additional resonance for woven fabrics at approximately 6 Mhz for 0.25 mm plies, although resonance at other frequencies has been seen in practice. Different approaches can be used: manual, immersion, and laser testing. Moreover, an example of combination of UT and conven",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "and laser testing. Moreover, an example of combination of UT and conven - tional IR thermography techniques is presented in Ref. [78], using car- bon/epoxy patches bonded on an aluminium plate and producing fusion algorithms correlating both inspection results. Sensors and NDI techniques above mentioned can be object of trade- off analyses to improve space manufacturing processes according to customers ’ requests, mainly to increase factories ’ KPIs like Quality of Service (QoS) and defects rate. In the following chapter the case study of a real space manufacturing process includes the assessment on the use of some of these technologies. 4.Case study: RUAG ’s composite sandwich panel manufacturing As a case study, RUAG ’s composite sandwich panel manufacturing process was taken in consideration. Panel manufacturing today is still",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "panel manufacturing process was taken in consideration. Panel manufacturing today is still a largely manual process. This is especially valid for large, non-serial spacecrafts for scientific missions. With the establishment of constella - tions during the last years, considerable effort was made to industrialise the overall manufacturing process. Still, the state-of-the-art manufacturing process is distant from an Industry 4.0 philosophy. Here follow the main process areas (according to the job-shop pro- duction system), each of which is made of stations, operations and phases: - Parts preparation: procured and stored parts (cut-to-shape aluminium face sheets, already expanded aluminium honeycomb core, adhesives, foams, inserts and heat pipes) are machined and pre- assembled. Parts whose surface is destined to external exposure are treated under galvanic bath to prevent corrosion. - Panel",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "external exposure are treated under galvanic bath to prevent corrosion. - Panel assembly: the pre-assembly is bonded under hot press. - Panel inspection and testing: Non-Destructive Inspections (NDI) techniques (e.g., Ultrasonic Inspection - UT) and testing (e.g., flat- wise tensile strength) are performed. - Panel equipment: hot-bonded inserts are automatically potted, and cold-bonded inserts are machined; thermal equipment (e.g., paint and heat pipes) is integrated. For a summary of existing sensors or automated equipment deliv- ering process data, with related measurement properties and units the reader can refer to Table 1 The general approach to industrial panel manufacturing varies at different points compared to the more traditional solution. With industrial-based manufacturing, materials and processes are tailored to the product itself. In the case of the sandwich panels, this",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "to the product itself. In the case of the sandwich panels, this means that face sheets are already procured cut to shape. Furthermore, time- consuming processes are being automized, as for instance the bonding of inserts. A two-level approach has been considered to improve the process as shown in Fig. 9. M. Eugeni et al. [Página 10] Acta Astronautica 192 (2022) 276–290 285First, existing data must be collected, categorized, and interpreted, so that bottlenecks or shortcomings can be more easily identified. Different types of sensors and non-destructive sensing techniques were identified, see Fig. 10(a) and Fig. 10(b) showing how a sensing network is deployed over the observed process. The collection and categorization of data will be possible thanks to the improvement of the built-in traceability sys- tem, extending",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "possible thanks to the improvement of the built-in traceability sys- tem, extending it to the whole process and introducing an IoT infra- structure based on a sensors network and a data processing and analytics platform. Process parameters like pressure, humidity and temperature are tracked, as well as product part numbers; optical imagery aids quality control, sound alarms on thresholds helps day-by-day opera - tions; all these functions (and more programmable ones according to production needs) are so interconnected and easily monitorable by a dashboard by means the software architecture shown in Fig. 10 (c)). Observations are used to perform an AS-IS analysis about data collection within the case process. The Acatech maturity model is chosen as a foundation for the development of a new assessment model to represent",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "a foundation for the development of a new assessment model to represent the current smartness level of the process. The new assessment model is based both on the evaluation of single activities, which is crucial to thoroughly verify every operating step of the process, and on the assessment of the whole process, which allows to identify transversal integration elements, which would be otherwise scarcely visible. The first assessment focuses on the single activities. This process is based upon a customization of the Acatech model, which ensures a digital maturity level assessment comprising six maturity stages: from a not- digitalized company to a company with all the features of Industry 4.0. This model was adjusted to the objective of the assessment, i.e. to measure the smartness of the process",
        "start_idx": 8236,
        "end_idx": 8364
      },
      {
        "text": "objective of the assessment, i.e. to measure the smartness of the process in terms of data collection, and to assess single activities. In particular, a qualitative assessment was per- formed, and the achievement of a smart level was evaluated according to the maturity model ’s features of computerization, connectivity, visibil - ity, transparency, predictive capacity and adaptability, see Sec. 2.1 and Fig. 8. An analysis of possible gatherings of new useful information by new technologies or new stations can be conducted once the already avail- able are collected and analysed by means a suitable software and computing infrastructure. In case the interpretation of data executed at step 1 needed a deeper insight or critical points in the process were identified, some new technologies should be added accordingly. One",
        "start_idx": 8352,
        "end_idx": 8480
      },
      {
        "text": "the process were identified, some new technologies should be added accordingly. One of Table 1 As-is process: sensors and automated equipment with related measurement pa- rameters and units. © RUAG Space. PROCESS STATION/ OPERATION/ PHASE SENSOR/ EQUIPMENT MEASUREMENT PROPERTY MEASUREMENT UNIT Parts preparation/ Panel milling Laser External dimensions (lenght, width, pocket positions) Mm Parts preparation/ Facesheet bonding surface preparation/ Galvanic bath Timer Time of bath S Parts preparation/ Facesheet bonding surface preparation/ Galvanic bath Sensor Chemical composition pH Parts preparation/ Facesheet bonding surface preparation/ Galvanic bath Sensor Chemical composition Concentration Parts preparation/ Insert bonding surface preparation/ Galvanic bath Timer Time of bath S Parts preparation/ Insert bonding surface preparation/ Galvanic bath Sensor Chemical composition pH Parts preparation/ Insert bonding surface preparation/ Galvanic bath Sensor Chemical composition Concentration Parts",
        "start_idx": 8468,
        "end_idx": 8596
      },
      {
        "text": "preparation/ Insert bonding surface preparation/ Galvanic bath Sensor Chemical composition Concentration Parts preparation/ Adhesives/ Incoming inspection Tensile testing machine Lap shear strenght Mpa Parts preparation/ Adhesives/ Storing Timer Storage time S Sandwich assembly/ Sandwich layup Laser Alignment Mm Sandwich assembly/Panel bonding Hot press Pressure Bar Sandwich assembly/Panel bonding Hot press Temperature •C Sandwich assembly/Panel bonding Hot press Time S Panel inspection and testing/ Ultrasonic inspection Sensor Panel defects (delamination, inhomogeneity, bonding defects, etc.) dB Panel inspection and testing/ Flatwise tensile test Tensile testing machine Tensile strenght Mpa Panel inspection and testing/3- point and 4- point bending test Tensile testing machine Bending strenght Mpa Panel inspection and testing/ Thermal cycling Thermal chamber Outgassing % Panel equipment/ Insert potting APM Insert-injected adhesive mass G Table 1 (continued ) PROCESS STATION/",
        "start_idx": 8584,
        "end_idx": 8712
      },
      {
        "text": "potting APM Insert-injected adhesive mass G Table 1 (continued ) PROCESS STATION/ OPERATION/ PHASE SENSOR/ EQUIPMENT MEASUREMENT PROPERTY MEASUREMENT UNIT Panel equipment/ Insert potting APM Adhesive mixing ratio % Panel equipment/ Insert potting APM Insert height w.r.t. facesheet Mm Panel equipment/ Insert potting APM Insert angle w.r.t. facesheet Rad Panel equipment/ Insert potting APM Insert position Mm Panel equipment/ Adhesive curing Sensor oven Curing temperature •C Panel equipment/ Adhesive curing Sensor oven Curing time S Panel equipment/ Insert proof-load test Sensor Load-displacement diagram N/mm Panel equipment/ Insert pull-out test Sensor Pull-out load N Panel equipment/ Heater bonding Laser Position Mm Panel equipment/ MLI bonding Testing machine Bonding strenght Mpa Panel equipment/ Tie-base bonding Testing machine Bonding strenght Mpa M. Eugeni et al. [Página 11] Acta Astronautica 192 (2022)",
        "start_idx": 8700,
        "end_idx": 8828
      },
      {
        "text": "strenght Mpa M. Eugeni et al. [Página 11] Acta Astronautica 192 (2022) 276–290 286the main process phases to concentrate on to add information is quality testing. Quality testing usually requires long times and heavily impacts both the technical and economic aspects of the process. Making it more agile and automating it would fasten the process and make the testing itself more accurate thanks to incorporated statistical models. In our case study, testing stations J4 and J5 right after panel bonding (process step 11) and J2 just after panel machining and inserts installation by means of RUAG ’s fully automated APM technology (process step 13) is of particular interest for future developments. The method proposed to assess the AS-IS process status will be applied to understand the steps required",
        "start_idx": 8816,
        "end_idx": 8944
      },
      {
        "text": "the AS-IS process status will be applied to understand the steps required to reach the desired “smart ” level, in terms of individual activities, and to understand how to generate a greater level of interconnection and be able to monitor a greater number of performances. If, for instance, the intention is to guarantee that the available data generates an “Enterprise ” level of interconnection throughout the entire process, see Fig. 6, it would be necessary to guarantee a circulation of data that goes beyond the com- pany ’s internal borders, in an extensive and transversal manner between the various constituent areas. The aim of this study ’s CPS is to reach the Manufacturing Operations Management (MOM) or Manufacturing Execution System (MES) level. However, its inherent feature of scal-",
        "start_idx": 8932,
        "end_idx": 9060
      },
      {
        "text": "or Manufacturing Execution System (MES) level. However, its inherent feature of scal- ability allows the extension from the single process to the overall factory to the overall plant. An example of process improvement through the application of the Fig. 8.An example of process performance assessment using the AS-IS model. Fig. 9.The study ’s approach has two levels: data collection and interpretation, aimed to gather data from the process, and CPS architecture and implementation, to digitalize the existing data and possibly add new information. Measurement of KPIs is then applied to both industrial and digital aspects of the study to verify improvements. M. Eugeni et al. [Página 12] Acta Astronautica 192 (2022) 276–290 287CPS model was realized through a preliminary simulation of raw data coming from the Automated Potting",
        "start_idx": 9048,
        "end_idx": 9176
      },
      {
        "text": "through a preliminary simulation of raw data coming from the Automated Potting Machine and connected to the software architecture described in Fig. 11. First of all, the APM data (represented by a list of measurements and their timestamp) is included in a database. Once the database is collected, data is normalized and aggregated online according to the different timeframes and stored in the data lake. In batches, such data is clustered and displayed in a dashboard. The data collection and visualization allow the monitoring, control, and use of data analysis to detect process deviations for example to stop the line or alert operators. A possible dashboard and an example Fig. 10.RUAG ’s sandwich composite panel manufacturing process is shown before (a) and after (b) the integration of existing",
        "start_idx": 9164,
        "end_idx": 9292
      },
      {
        "text": "process is shown before (a) and after (b) the integration of existing sensors with an IoT network com- manded by a computing infrastructure. Sensors measure temperature, pressure and humidity and scan panel ’s surface through optical and laser systems. Traceability is also performed through barcodes. The whole process is included in a tree-shaped system. The computing infrastructure is then represented in detail (c). Online processing of sensors ’ data inputs is performed through actions including preprocessing, normalization, thresholds ’ check, and monitoring. Processed data is then stored in a data lake, where users are able to have continuous open access, while data are interpreted by a statistical model-based closed-loop of KPIs ’ prediction and forecast and are displayed through a user-friendly visual dashboard. Some of the many SW",
        "start_idx": 9280,
        "end_idx": 9408
      },
      {
        "text": "are displayed through a user-friendly visual dashboard. Some of the many SW platforms available in the market to realize such concept are mentioned [81–86,88–90,92]. Fig. 11.The CPS∕layers as a flux of data from input to output. In the first layer data from interconnected sensors (IoT) is simulated or collected from historical archives, so that the process is reconstructed (DT). In the cyber layer, i.e. the core of the CPS, data collec - tion, storage and analytics is done with the help of statistical predictive models, allowing data correla - tion (AI). In the final layer, data can be visualized through reports and insights and interpreted with human touch, allowing to understand causation effects. M. Eugeni et al. [Página 13] Acta Astronautica 192 (2022) 276–290 288of graphs displayable as",
        "start_idx": 9396,
        "end_idx": 9524
      },
      {
        "text": "al. [Página 13] Acta Astronautica 192 (2022) 276–290 288of graphs displayable as output are shown respectively in Fig. 12 and Fig. 13. The approach was extended to the whole process thanks to its layout reconstruction in the cyber space, see Fig. 10. In Fig. 14 a representation of the Sandwich Panel Manufacturing process using BPMN and a simu- lation through Bizagi Modeler allows the performance of a «what if » analysis. This tool is useful to investigate costs and times needed to execute the entire process. A top-down approach was applied: starting from a model of the macro-tasks, and then defining each task following the most left representation to define each block as an independent process. This allows a detailed analysis, gaining a more realistic repre - sentation",
        "start_idx": 9512,
        "end_idx": 9640
      },
      {
        "text": "This allows a detailed analysis, gaining a more realistic repre - sentation on the timing of the macro-block. Finally, the model reaches automation level and is upgraded with a Markov-chain-based AI algo- rithm able to show probabilities of failure for sample properties of in- terest. The system upgrade can be categorized in three levels: 1. Level 1 – “Process monitoring ” This level is characterized by the ability of the CPS to process the collected data automatically generating reports and sending alarms, based on inputs pregiven manually. In case of failures being signalized, the information provided allows the operators and/or process engineers to intervene and adjust the process parameters to address the issue. Reports can assist in the identification of trends by displaying data over a longer period.",
        "start_idx": 9628,
        "end_idx": 9756
      },
      {
        "text": "in the identification of trends by displaying data over a longer period. 2. Level 2 – “Small-scale process control ” At this level, further analysis and interpretation is performed auto- matically by the AI algorithms to predict the outcome of the process. For instance, the CPS can stop and restart the potting process with a new insert if the probability of negative process outcome is high. Based on identified trends, the CPS can signal potential failures before they occur. However, the system is incapable of adjusting any of the process pa- rameters to keep the process running and avoid the identified threats. 3. Level 3 – “Large-scale process control ” At level 3, the AI-assisted CPS can optimize the process parameters to achieve optimal process result – delivering",
        "start_idx": 9744,
        "end_idx": 9872
      },
      {
        "text": "can optimize the process parameters to achieve optimal process result – delivering the right product quality in the shortest production time. It can perform continuous predictive analysis on all production system components using the data fed in real- time by the sensor network. Based on the performance forecast, the CPS can predict the completion time for each panel, tool exchange rates, and equipment maintenance intervals, thereby being able plan the entire material flow through the station. At this stage, multiple production stations can be interconnected using the same CPS. To reach these levels, capital investment in upgrading the production system is necessary. Table 2 shows estimated investment figures needed to support the CPS implementation. 5.Conclusions The paper contextualized Smart Manufacturing technologies in the fast-evolving market of large constellations",
        "start_idx": 9860,
        "end_idx": 9988
      },
      {
        "text": "paper contextualized Smart Manufacturing technologies in the fast-evolving market of large constellations of small satellites and related new production paradigms. A review of fundamental theoretical concepts behind Industry 4.0 disruptive change was presented, focused on Cyber-Physical Systems and their 5C-level standard architecture. Possible Smart Manufacturing solutions, in terms of hardware and software technologies, were reviewed to contribute to a future signifi - cant improvement and optimization of a whole MAIT cycle. CPS, DT and IoT were selected as the most promising technologies to be adopted and RUAG ’s composite sandwich panel manufacturing process was taken as case study. The process was reconstructed so that each sensor could be simulated in the cyber space as a flux of data. In parallel, an assessment of the SM level of the",
        "start_idx": 9976,
        "end_idx": 10104
      },
      {
        "text": "of data. In parallel, an assessment of the SM level of the process according to the Acatech maturity model was carried on unlocking the process improvement potential. The flux of data flowing from the sensing layer into the cyber layer of the CPS through an interconnected IoT network is represented by unit blocks related to each process step. The use of AI upgrades the model, giving it the ability to also reach some level of process control and optimization. Three different levels of process improvement are identified each of which is linked to its economic estimation of the necessary computing infrastructure. By this model equipment data can be interpreted through its pre-processing, normalization, storage and distribution to a user- friendly visual dashboard, according to a new logical analysis",
        "start_idx": 10092,
        "end_idx": 10220
      },
      {
        "text": "to a user- friendly visual dashboard, according to a new logical analysis of the industrial process, delivering the final improvement, represented by the opportunity of reconfiguring the production line to reach the goals measured by traditional Key Performance Indicators (KPIs), among which panel production rate and Overall Equipment Efficiency (OEE), and optimize specific parameters related to SM, such as process agility and flexibility and the CPS scalability. Fig. 12.An example of the CPS dashboard. M. Eugeni et al. [Página 14] Acta Astronautica 192 (2022) 276–290 289Declaration of competing interest The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper. Acknowledgment The present paper results from the project “Smart Manufacturing for future constellations",
        "start_idx": 10208,
        "end_idx": 10336
      },
      {
        "text": "The present paper results from the project “Smart Manufacturing for future constellations ” funded by the European Space Agency (ESA ITT AO/1 –10002/19/NL/AR for technology development) and developed in collaboration by Sapienza University of Rome, Thales Alenia Space Italy and RUAG Space. References [1]M. Blanchet, THINK ACT. INDUSTRY 4.0. The New Industrial Revolution. How Europe Will Succeed, Roland Berger, March 2014 . [2]E. S. Agency, What Is Space 4.0? [Online]. Available, November 2021. November 2021, https://www.esa.int/About_Us/Ministerial_Council_2016/What_is_space_4. 0. [3]R.Y. Zhong, X. Xua, E. Klotz, S.T. Newmanc, Intelligent manufacturing in the context of industry 4.0: a review, Engineering 3 (2017) 616–630. [4]C. Daehnick, I. Klinghoffer, B. Maritz, B. Wiseman, “Large LEO Satellite Constellations: Will it Be Different This Time?, ” McKinsey &Co, Aerospace and Defence Practice, May 2020 . [5]UK",
        "start_idx": 10324,
        "end_idx": 10452
      },
      {
        "text": "Time?, ” McKinsey &Co, Aerospace and Defence Practice, May 2020 . [5]UK saves OneWeb, Spaceflight 62 (September) (2020) . [6]J. Hou, Y. Zhao, Y. Zhou, X. Du and Z. Li, “The creative application of DIY manufacturing technology in remote sensing satellite, ” Aero. China. Vol. 17. N.2, Summer 2016. [7]K. Jackson, K. Efthymioua, J. Borton, “Digital Manufacturing and Flexible Assembly Technologies for Reconfigurable Aerospace Production Systems, ” Changeable, Agile, Reconfigurable & Virtual Production Conference, 2016 . [8]A. Kusiak, Smart manufacturing, Int. J. Prod. Res. 56 (2018) 508–517. [9]S. Marigonda, “Smart Manufacturing: sfide e opportunit ˇa.,” Digital Tools 4.0. [10] L. Li, China ’s manufacturing locus in 2025: with a comparison of “Made-in-China 2025 ” and “Industry 4.0, Technol. Forecast. Soc. Change 135 (2018) 66–74. [11] L.D. Xu, Industry",
        "start_idx": 10440,
        "end_idx": 10568
      },
      {
        "text": "4.0, Technol. Forecast. Soc. Change 135 (2018) 66–74. [11] L.D. Xu, Industry 4.0: state of the art and future trends, Int. J. Prod. Res. 56 (8) (2018) . [12] C. Bryson, Heritage and Satellite Manufacturing: Firm-Level Competitiveness and the Management of Risk in Global Production Networks, Economic Geography, 2019, pp. 423–441. [13] C. Salkin, M. Oner, A. Ustundag, E. Cevikcan, A Conceptual Framework for Industry 4.0, 2018 . [14] K. Nakamoto, K. Shirase, Simulation technologies for the development of an autonomous and intelligent machine tool, Int. J. Autom. Technol. (2013), https:// doi.org/10.20965/ijat.2013.p0006 . [15] K.D. Thoben, S. Wiesner, T. Wuest, Industrie 4.0’ and smart manufacturing – a review of research issues and application examples, Int. J. Autom. Technol. 11 (1) (January 2017) 4–16. [16] G.G. Schuh, Industrie 4.0",
        "start_idx": 10556,
        "end_idx": 10684
      },
      {
        "text": "Autom. Technol. 11 (1) (January 2017) 4–16. [16] G.G. Schuh, Industrie 4.0 Maturity Index. Managing the Digital Transformation of Companies [Online]. Available:, 2017. February 2021, https://hal.archives-ouver tes.fr/hal-02455705 . [17] V. Cruz-Machado, Scanning the industry 4.0: a literature review on technologies for manufacturing systems, Engineering Science and Technology, an International Journal 22 (3) (June 2019) 899–919. [18] D.P. Perales, F.A. Valero, A.B. García, Industry 4.0, A Classification Scheme, 2018 . [19] O. Cardin, Classification of cyber-physical production systems applications: proposition of an analysis framework, Comput. Ind. 104 (January 2019) 11–21, https://doi.org/10.1016/j.compind.2018.10.002 . [20] A. Rojko, Industry 4.0 concept: background and overview, International Journal of Interactive Mobile Technologies 11 (5) (2017) . Fig. 13.Examples of graphs showable by the dashboard: the first graph represents the single operation ’s timing vs",
        "start_idx": 10672,
        "end_idx": 10800
      },
      {
        "text": "the dashboard: the first graph represents the single operation ’s timing vs time, the second one the production efficiency vs time and the last one a map of discarded APM inserts for adhesive quantity. Scales are not shown for confidential reasons. Fig. 14.The process layout represented in the cyber space and its focus at APM. Table 2 Production volume requirements - rough order of magnitude estimates. CPS Upgrade Level Level Description Estimated Machine Procurement Cost Increase [%] Estimated CPS Implementation and Operation Cost [EUR] Minimum Production Volume [inserts] Level 1 Process monitoring 3–5 42∕000 €/5 years 20.000 Level 2 Small-scale process control 10–15 55∕500 €/5 years 200.000 Level 3 Large-scale process control 40–60 82∕500 €/5 years 1.000.000 M. Eugeni et al. [Página 15] Acta Astronautica 192 (2022) 276–290",
        "start_idx": 10788,
        "end_idx": 10916
      },
      {
        "text": "1.000.000 M. Eugeni et al. [Página 15] Acta Astronautica 192 (2022) 276–290 290[21] B.-h. Li, H. Bao-cun, L. Xiao-bing, Y. Chun-wei, Y. Wen-tao, Applications of artificial intelligence in intelligent manufacturing: a review, Frontiers of Information Technology & Electronic Engineering 18 (1) (2017) 86–96. [22] J. Jadaan, K.S. Siderska, Cloud manufacturing: a service-oriented manufacturing, Engineering Management in Production and Services 10 (1) (2018) 22–31. [23] N. Khan, I. Yaqoob, I. Abaker, T. Hashem, Z. Inayat, W. Kamaleldin, A. Mahmoud, M. Alam, M. Shiraz, A. Gani, Big Data: Survey, Technologies, Opportunities, and Challenges, ” The Scientific World Journal, July 2014 . [24] C. Duke, G. Sadlier, D. Herr, Industry 4.0 and the Future of UK Space, ” London Economics, 2019 . [25] E. Sisinni, A. Saifullah, S. Han, U. Jennehag,",
        "start_idx": 10904,
        "end_idx": 11032
      },
      {
        "text": "Economics, 2019 . [25] E. Sisinni, A. Saifullah, S. Han, U. Jennehag, M. Gidlung, Industrial internet of things: challenges, opportunities, and directions, IEEE Trans. Ind. Inf. 10 (10) (2018) . [26] H. Li, Application research of virtual reality and augmented reality, Advances in Intelligent Systems and Computing 1233 (2021) 494–499. [27] Federal Ministry for Economic Affairs and Energy, Plattform Industrie 4.0 - RAMI4.0 – a reference framework for digitalisation, Plattf. Ind. 4 (2019), 0. [28] M. Yli-Ojanper aa, S. Sierla, N. Papakonstantinou, V. Vyatkin, Adapting an agile manufacturing concept to the reference architecture model industry 4.0: a survey and case study, Journal of Industrial Information Integration 15 (2019) 147–160. [29] J.H. Kim, A review of cyber-physical system research relevant to the emerging IT trends: industry 4.0, IoT, big",
        "start_idx": 11020,
        "end_idx": 11148
      },
      {
        "text": "system research relevant to the emerging IT trends: industry 4.0, IoT, big data, and cloud computing, Journal of Industrial Integration and Management 2 (3) (2017) . [30] H. Gill, R. Baheti, Cyber-physical systems, in: T. Samad, A.M. Annaswamy (Eds.), The Impact of Control Technology, 2011 . [31] H. Gill, R. Baheti, Cyber-physical Systems: from Theory to Practice, 2011 . [32] L. Monostori, Cyber-physical systems in manufacturing, CIRP Ann 65 (2) (2016) 621–641. [33] R. Rajkumar, I. Lee, L. Sha, J. Stankovic, Cyber-physical systems: the next computing revolution, Des. Autom. Conf. (2010) 731–736. [34] A. Napoleone, M. Macchi, A. Pozzetti, A review on the characteristics of cyber- physical systems for the future smart factories, J. Manuf. Syst. 54 (December) (2019) . [35] S. Thiede, M. Juraschek, C. Herrmann, Implementing",
        "start_idx": 11136,
        "end_idx": 11264
      },
      {
        "text": "54 (December) (2019) . [35] S. Thiede, M. Juraschek, C. Herrmann, Implementing cyber-physical production systems in learning factories, Procedia CIRP 54 (2016) 7–12. [36] C. Zhan, Y. Chen, A review of research relevant to the emerging industry trends: industry 4.0, IoT, blockchain, and business analytics, Journal of Industrial Integration and Management 5 (1) (2020) 165–180. [37] H. Chen, Theoretical foundations for cyber-physical systems: a literature review, Journal of Industrial Integration and Management 2 (3) (2017) . [38] Y. Lu, Cyber physical system (CPS)-based industry 4.0: a survey. Journal of Industrial Integration and Management, Journal of Industrial Integration and Management 2 (3) (2017) . [39] G.K. Rand, N. Singh, D. Rajamani, Cellular manufacturing systems design, planning and control, J. Oper. Res. Soc. (1997) . [40] T. Pultarova, “Satellite Manufacturing",
        "start_idx": 11252,
        "end_idx": 11380
      },
      {
        "text": "control, J. Oper. Res. Soc. (1997) . [40] T. Pultarova, “Satellite Manufacturing in a State of Transition, ” [Online]. Available: http://interactive.satellitetoday.com/via/march-2019/satellite-manu facturing-in-a-state-of-transition/_fragment.html . [Accessed October 2020]. [41] P.M. Laurent Jaffarta, Constellations: The satellite serial production challenge, in: 71st International Astronautical Congress (IAC) – the CyberSpace Edition, October 2020, pp. 12–14. [42] e. directory, “WorldView legion constellation, ” European Space Agency. [Online]. [Accessed February 2021]. [43] C. Hofacker, How to Make a Megaconstellation, March 2020 [Online]. Available: https://aerospaceamerica.aiaa.org . [44] T. Gornet, T. Wohlers, History of Additive Manufacturing, ” Wohlers, 2014 . [45] A. Javaid, M. Haleem, Additive manufacturing applications in industry 4.0: a review, Journal of Industrial Integration and Management 4 (4) (2019) . [46] K. Schwab, The Fourth Industrial Revolution, Portfolio Penguin, 2017 . [47] A. B˘ecue,",
        "start_idx": 11368,
        "end_idx": 11496
      },
      {
        "text": "Schwab, The Fourth Industrial Revolution, Portfolio Penguin, 2017 . [47] A. B˘ecue, CyberFactory#1 – securing the Industry 4.0 with cyber-ranges and digital twins, in: IEEE, 2018 . [48] HMI/SCADA software in the age of Industrial IoT and evolving human machine interfaces, ” I-Scoop, [Online]. Available: https://www.i-scoop.eu/industry-4-0/h mi-scada-software/ . [Accessed February 2021]. [49] Y. Lu, Current Standards Landscape for Smart Manufacturing Systems, ” National Institute of Standards and Technology - US Department of Commerce, February 2016 . [50] H. Wang, Enterprise system and its application in aerospace industry, Journal of Industrial Integration and Management 2 (2) (2017) . [51] I.C. Reinhardt, Current perspectives on the development of industry 4.0 in the pharmaceutical sector, Journal of Industrial Information Integration 18 (3) (2020) . [52] H. Wu, S. Li, L.D. Xu,",
        "start_idx": 11484,
        "end_idx": 11612
      },
      {
        "text": "Integration 18 (3) (2020) . [52] H. Wu, S. Li, L.D. Xu, Internet of things in industries: a survey, IEEE Trans. Ind. Inf. 10 (4) (2014) 2233 –2243 . [53] A. B˘ecue, A new concept of digital twin supporting optimization and resilience of factories of the future, Appl. Sci. 10 (2020) 4482 . [54] T. Fei, Z. Meng, Digital twin shop-floor: a new shop-floor paradigm towards smart manufacturing, IEEE Access 5 (2017) . [55] H. Gill, R. Baheti, Cyber-physical systems. The impact of control technology, IEEE Control Systems Society 1 (2011) . [56] E.A. Lee, Cyber physical systems: design challenges, in: 11th IEEE. International Symposium on Object and Component-Oriented Real-Time Distributed Computing, ISORC)., 2008, pp. 363–369. [57] M. Abdirad, A two-stage metaheuristic algorithm for the dynamic vehicle routing",
        "start_idx": 11600,
        "end_idx": 11728
      },
      {
        "text": "[57] M. Abdirad, A two-stage metaheuristic algorithm for the dynamic vehicle routing problem in industry 4.0 approach, J. Manag. Anal. 1 (15) (2020) . [58] J. Lee, B. Bagheri, H.A. Kao, A Cyber-Physical Systems architecture for Industry 4.0-based manufacturing systems, Manufacturing Letters 3 (2015) 18–23. [59] G. Aceto, V. Persico, A. Pescap ˘e, Industry 4.0 and health: internet of things, big data, and cloud computing for healthcare 4.0, Journal of Industrial Information Integration 18 (2020) . [60] X. You, Towards 6G Wireless Communication Networks: Vision, Enabling Technologies, and New Paradigm Shifts, vol. 64, Science China - Information Sciences, 2021 . [61] Y. Lu, Security in 6G: the prospects and the relevant technologies, Journal of Industrial Integration and Management 5 (3) (2020) 271–289. [62] A. Cusano, P. Salvarezza, G.",
        "start_idx": 11716,
        "end_idx": 11844
      },
      {
        "text": "and Management 5 (3) (2020) 271–289. [62] A. Cusano, P. Salvarezza, G. Breglio, A. Cutolo, A. Calabr ˇo, M. Giordano, S. De Nicola, An integrated fiber optic sensing system for in situ characterization of the curing, Proc. SPIE 4328 (2001) 275–284. [63] K.H. Wood, T.L. Brown, M.C. Wu, C.B. Gause, Fiber Optic Sensors for Cure-Health, ” Proceeding 3rd Intern. Workshop on Structural Health, 2001, pp. 1149 –1157 . [64] K. Saddik, M. Alam, A. El, C2ps: a digital twin architecture reference model for the cloud-based cyber-physical systems, IEEE Access 5 (2017) 2050 –2062 . [65] R.D. Finlayson, M. Friesel, M. Carlos, P. Cole, Health monitoring of aerospace structures with acoustic emission and acousto-ultrasonics, in: 15th World Conference on Non-destructive Testing, 2000 . [66] V. Giurgiutiu, A. Zagrai, J.J.",
        "start_idx": 11832,
        "end_idx": 11960
      },
      {
        "text": "Conference on Non-destructive Testing, 2000 . [66] V. Giurgiutiu, A. Zagrai, J.J. Bao, Piezoelectric wafer embedded active sensors for aging aircraft structural health monitoring, Int. J. Struct. Health Monitor. November (2001) . [67] D. Devillers, F. Taillade, D. Osmont, D. Balageas, D. Royer, Interaction of Lamb waves with defects in composite sandwich structures, in: European COST F3 Conference on System, 2000 . [68] J.S. Kim, K.J. Vinoy, V.K. Varadan, Wireless health monitoring of cracks in structures with MEMS-IDT sensors, Proc. SPIE 4700 (2002) 342–353. [69] S.J. Burgett, M. Kranz, MEMS sensor systems developments at AMCOM for environmental conditions monitoring, in: Proc. 3 Rd Intern. Workshop on Structural Health Monitoring, 2001, pp. 1134 –1141 . [70] D. Chung, Structural health monitoring by electrical resistance measurement, Journal of smart materials",
        "start_idx": 11948,
        "end_idx": 12076
      },
      {
        "text": "Chung, Structural health monitoring by electrical resistance measurement, Journal of smart materials and structures 10 (2001) 624–636. [71] L. Zhang, Novel self-healing CFRP composites with high glass transition temperatures, Compos. Sci. Technol. 168 (2018) 96–103. [72] W.H. Prosser, T.L. Brown, S.E. Woodard, G.A. Fleming, E.G. Cooper, Sensor technology for integrated vehicle health management of aerospace vehicles, in: AIP Conference Proceedings, vol. 657, 2003, p. 1582 . [73] P. Gaudenzi, M. Bernabei, E. Dati, G. De Angelis, M. Marrone, L. Lampani, On the evaluation of impact damage on composite materials by comparing different NDI techniques, Compos. Struct. 118 (2014) 257–266. [74] X. Maldague, Theory and Practice of Infrared Thermography for Non Destructive Testing, John Wiley & Sons, Canada, 2001 . [75] S. Deane, Application of NDT thermographic imaging of",
        "start_idx": 12064,
        "end_idx": 12192
      },
      {
        "text": "Canada, 2001 . [75] S. Deane, Application of NDT thermographic imaging of aerospace structures, Infrared Phys. Technol. 97 (2019) 456–466. [76] D.I. Gillespie, Defect detection in aerospace sandwich composite panels using conductive thermography and contact sensors, Sensors 20 (2020) . [77] R.D. Finlayson, M. Friesel, M. Carlos, P. Cole, Health monitoring of aerospace structures with acoustic emissions and acousto-ultrasonics, in: 15th World Conference on Non-destructive Testing, October 2020 . [78] P. Daryabor, M.S. Safizadeh, Image fusion of ultrasonic and thermographic inspection of carbon/epoxy patches bonded to an aluminum plate, NDT E Int. 90 (2017) 1–10. [79] P. Gaudenzi, Smart Structures: Physical Behaviour, Mathematical Modelling and Applications, John Wiley Sons, 2009 . [81] [Online]. Available:, Elastic, November 2021. Accessed November 2021, http s://www.elastic.co/ . [82] Grafana [Online]. Available: Accessed",
        "start_idx": 12180,
        "end_idx": 12308
      },
      {
        "text": "2021. Accessed November 2021, http s://www.elastic.co/ . [82] Grafana [Online]. Available: Accessed November 2021, https://grafana.com/ , November 2021. [83] Ignite [Online]. Available: Accessed November 2021, https://ignite.apache.org/ , November 2021. [84] Kafka [Online]. Available: Accessed November 2021, https://kafka.apache.org/ , November 2021. [85] Kibana [Online]. Available: Accessed November 2021, https://www.elastic. co/kibana/ , November 2021. [86] Pytorch [Online]. Available: Accessed November 2021, https://pytorch.org/ , November 2021. [88] [Online]. Available:, Scikit Learn, November 2021. Accessed November 2021, https://scikit-learn.org/ . [89] Tensorflow [Online]. Available: Accessed November 2021, https://www.tensor flow.org/ , November 2021. [90] Redis [Online]. Available: Accessed November 2021, https://redis.io , November 2021. [91] M. Li, “Spatial-Temporal Finite Element Analytics for CPS-Enabled Smart Factory: Application in Hybrid Flow Shop, ” Procedia Manufacturing, 2020, pp. 1229 –1236 . [92] “Flink Flink [Online]. Available: Accessed",
        "start_idx": 12296,
        "end_idx": 12424
      },
      {
        "text": "Manufacturing, 2020, pp. 1229 –1236 . [92] “Flink Flink [Online]. Available: Accessed November 2021, https://flink.apache. org/, November 2021. M. Eugeni et al.",
        "start_idx": 12412,
        "end_idx": 12434
      }
    ],
    "b9f85718-1117-4a9a-ad4a-1eade3ffcda1": [
      {
        "text": "[Página 1] Assessing business value of Big Data Analytics in European ﬁrms☆ Nadine Côrte-Real ⁎, Tiago Oliveira, Pedro Ruivo NOVA IMS, Universidade Nova de Lisboa, 1070-312, Lisboa, Portugal abstract article info Available online 9 August 2016 In the strategic management ﬁeld, dynamic capabilities (DC) such as organizational agility are considered to be paramount in the search for competitive advantage. Recent research claims that IT business value research needs a more dynamic perspective. In particular, the Big Data Analytics (BDA) value chain remains unexplored. To assess BDA value, a conceptual model is proposed based on a knowledge-based view and DC theories. Toempirically test this model, the study addresses a survey to a wide range of 500 European ﬁrms and their IT and business executives. Results show that BDA can",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "ﬁrms and their IT and business executives. Results show that BDA can provide business value to several stages of the value chain. BDA can create organizational agility through knowledge management and its impact on process and competitive advantage. Also, this paper demonstrates that agility can partially mediate the effect betweenknowledge assets and performance (process level and competitive advantage). The model explains 77.8% of the variation in competitive advantage. The current paper also presents theoretical and practical implications of this study, and the study's limitations. © 2016 Elsevier Inc. All rights reserved.Keywords: Big Data Analytics (BDA) IT business value Knowledge Based View (KBV)Dynamic capabilities (DC)Organizational agilityCompetitive advantage 1. Introduction In the era of Big Data, ﬁrms in every sector are required to deal with a huge amount of data.",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "every sector are required to deal with a huge amount of data. Data in vast amounts can offer invaluable insights and competitive advantage if the right technological and organizational resources support them ( Morabito, 2015 ). Recently, several academics and practitioners have stressed the need to understand how, why, and when Big Data Analytics (BDA) applications can be a valuable resource for companies to gain competitive advantage ( Abbasi, Sarker, & Chiang, 2016; Agarwal & Dhar, 2014; Corte Real, Oliveira, & Ruivo, 2014; LaValle et al., 2011 ). Although BDA technologies have been recognized as the “next big thing for innovation ”(i.e., a potential source of business value and competitive advantage), the BDA value chain remains relatively unexplored and needs further investigation. No empirical research exists assessing how",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "relatively unexplored and needs further investigation. No empirical research exists assessing how BDA can bring business value (Abbasi et al., 2016 ), establishing a linkage between knowledge assets, organizational agility, and performance (process-level and competitive advantage) ( Corte Real et al., 2014 ). Firms that inject BDA in their business operations can surpass their peers by 5% in productivity and 6% in pro ﬁtability ( Barton, 2012 ). For that reason, European ﬁrms are investing heavily in BDA technologies ( SAS, 2013; Sharma, Mithas, & Kankanhalli, 2014 ). Nevertheless, this investment can only be valuableif organizations use the appropriate technology and organizational resources to achieve competitive advantage ( Manyika et al., 2011a ). In response to the scarcity of research on this subject, this study examines the impact",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "the scarcity of research on this subject, this study examines the impact of BDA on the business value chain in a European context by empirically testing a new theoretical frame- work that merges two strategic management theories (Knowledge B a s e dV i e w( K B V )a n dd y n a m ic capabilities (DC)) at ﬁrm-level. Not only does this paper extend BDA research by transposing, merging, and examining hypotheses in IT innovations and management ﬁelds, but also contributes to DC research by empirically assessing the ante- cedents and impacts of a speci ﬁc dynamic capability (organizational agility), when using BDA technologies. This is the ﬁrst paper that studies the entire BDA value chain at ﬁrm-level, linking concepts of knowledge management, agility, and",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "BDA value chain at ﬁrm-level, linking concepts of knowledge management, agility, and performance (process-level and competitive advantage). To clarify the role of agility on perfor- mance, this papers tests if agility is a mediator of knowledge assets on performance (process-level performance and competitive advantage). The study explores the following three research ques- tions (RQs): RQ1 –What are the BDA enablers for the creation of organizational agility?RQ2 –What are the impacts of this dynamic capability created by BDA on sustainable competitive advantage? RQ3 –Is agility a mediator of knowledge assets on performance (process-level performance and competitive advantage)?Journal of Business Research 70 (2017) 379 –390 ☆The author is grateful for the comments by anonymous reviewers, on earlier drafts of this article. ⁎Corresponding author. E-mail address: nreal@novaims.unl.pt (N. Côrte-Real). http://dx.doi.org/10.1016/j.jbusres.2016.08.011 0148-2963/©",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "of this article. ⁎Corresponding author. E-mail address: nreal@novaims.unl.pt (N. Côrte-Real). http://dx.doi.org/10.1016/j.jbusres.2016.08.011 0148-2963/© 2016 Elsevier Inc. All rights reserved. Contents lists available at ScienceDirect Journal of Business Research [Página 2] This study offers guidance for executives and managers to assess the conditions under which BDA can add business value to organizations. Managers and IT executives can bene ﬁt from an evaluation instrument to assess the impact of BDA. Also, this paper provides valuable support to justify BDA investments and initiatives. Firms that have not yet decided to adopt these technologies can obtain a view of potential gains from adopting and effectively using BDA. This research demon-strates how best to leverage the knowledge embedded in BDA systems, acquiring organizational agility capabilities that lead toward competi- tive advantage. The remainder of",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "organizational agility capabilities that lead toward competi- tive advantage. The remainder of this paper has the following structure: Section 2 provides an introduction to the BDA concept and a theoretical background to assess BDA initiatives; Section 3 presents the conceptual model and the hypotheses; Section 4 outlines the methodology; and Section 5 shows the empirical results. Finally, the paper presents a discussion and the conclusions from the ﬁndings. 2. Background2.1. Big Data Analytics Chen, Chiang ( Chen, Chiang, & Storey, 2012 ) coined the term Big Data Analytics (BDA) as a related ﬁeld of business intelligence & analytics (BI&A), referring to the BI&A technologies that mostly concern data mining and statistical analysis. Authors de ﬁne BDA as “an e w generation of technologies and architectures, designed to economically",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "as “an e w generation of technologies and architectures, designed to economically extract value from very large volumes of a wide variety of data, by enabling high velocity capture, discovery and/or analysis. ”(IDC, 2011 ). BDA tech- nologies allow ﬁrms to improve existing applications by offering business-centric practices and methodologies that provide a competi- tive advantage ( Chen et al., 2012; Davenport, 2006 ). The latest literature indicates that there is much room for further BDA research ( Abbasi et al., 2016; Agarwal & Dhar, 2014; Erevelles, Fukawa, & Swayne, 2016 ). There are already academic studies that re ﬂect the adoption and use of BDA (e.g., ( Malladi, 2013; Xu, Frankwick, & Ramirez, 2016; Kwon, Lee, & Shin, 2014 )). Regarding value, most BDA academic studies focus",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "Lee, & Shin, 2014 )). Regarding value, most BDA academic studies focus on analyzing business value from a data or system perspective (e.g., ( LaValle et al., 2011; Kwon et al., 2014 )). From the strategic management perspective only one conceptual paper explores how BDA affects several marketing activities ( Erevelles et al., 2016 ). The remaining literature addresses industry primarily ( LaValle et al., 2011; Russom, 2011 ). As ﬁrms do not know how to capture business value (Barton, 2012; LaValle et al., 2011 ), some scholars ( Corte Real et al., 2014; Malladi, 2013 ) argue that BDA value research is scarce and needs to extend beyond post-adoption stages toward competitiveness (Erevelles et al., 2016; Xu et al., 2016 ). Although numerous approaches assess IT Value",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "2016; Xu et al., 2016 ). Although numerous approaches assess IT Value at the process and ﬁrm levels (see Schryen ( Schryen, 2013 ) for a review), this study extends IT business value research from the strategic management perspective, by empirically assessing the BDA business value chain in European ﬁrms. 2.2. Theoretical foundation Many studies in recent decades investigate IT business value and competitive advantage using the resource-based view (RBV) ( Barua, Kriebel, & Mukhopadhyay, 1995; Bharadwaj, 2000; Mata, Fuerst, & Barney, 1995; Melville, Kraemer, & Gurbaxani, 2004; Ruivo, Oliveira, & Neto, 2015; Soh & Markus, 1995; Zhu & Kraemer, 2005 ). The limitations of RBV encourage the use of other theories such as DC and KBV ( Arend & Bromiley, 2009; Wang & Ahmed, 2007 ). As",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "KBV ( Arend & Bromiley, 2009; Wang & Ahmed, 2007 ). As DC theory constitutes the second foundation that supports knowledge-based thinking ( Pettigrew, Thomas, & Whittington, 2001 ), this study combines these theories. KBV explores a ﬁrm's potential to acquire competitiveness in a dynamic market context, but only DC theory can solve the problem of sustaining competitive advantage in turbulent environments ( Grant, 1996; Volberda, 1996 ).2.2.1. Knowledge Based View theory KBV states that a ﬁrm's knowledge resources are unique and inimitable and that the ﬁrm's primary function is to leverage them into productive outcomes ( Grant, 1996; Nonaka, 1995 ). The possession of knowledge resources gives the ﬁrm basic foundations to renew or re- conﬁgure its resource base and to build dynamic capabilities ( Wu, 2006",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "conﬁgure its resource base and to build dynamic capabilities ( Wu, 2006 ), such as organizational agility. Companies that have high levels of staff knowledge and involvement can more skillfully identify the need to make changes to existing resources and decide about the ac- tions necessary to implement these changes ( Nieves & Haller, 2014 ). KBV theory can help to conceptualize the performance effects of IT in- vestments ( Pavlou et al., 2005 ). Management studies use this theory (e.g., ( Nieves & Haller, 2014 )), as do studies in IT ﬁelds (e.g., ( Sher & Lee, 2004 )) to understand the role of knowledge management in the creation of DC. In BDA technologies, Xu, Frankwick ( Xu et al., 2016 ) seek to understand the relationships",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "Frankwick ( Xu et al., 2016 ) seek to understand the relationships among traditional marketing analytics, BDA, and new product success. The current paper is the ﬁrst that empirically tests KBV to understand the role of BDA in the creation of agility. 2.2.2. Dynamic capability theory In the past decade the DC perspective arose as one of the most effective theoretical lenses for the strategic management ﬁeld (Schilke, 2014 ), attracting the interest of scholars not only in business, but also in the IT management ﬁeld ( Helfat et al., 2009; Protogerou, Caloghirou, & Lioukas, 2012 ). Rooted in RBV and KBV, DC argues that the dynamic capabilities enable ﬁrms to modify their resource to adapt rapidly to changing conditions, helping them to sustain their competitive advantage over",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "rapidly to changing conditions, helping them to sustain their competitive advantage over time ( Helfat & Peteraf, 2009; Teece, Pisano, & Shuen, 1997 ). Although the literature has a broad range of deﬁnitions for DC, one of the seminal papers de ﬁnes DC as “the ability to integrate, build, and recon ﬁgure internal and external competencies to address rapidly-changing environments ”(Teece et al., 1997 ). DC disaggregates into “the capacity (1) to sense and shape opportunities and threats, (2) to seize opportunities, and (3) to maintain competitive-ness through enhancing, combining, protecting, and, when necessary, recon ﬁguring the business enterprise's intangible and tangible assets ”. Some authors argue that agility is an organizational dynamic capability ( Blome, Schoenherr, & Rexhausen, 2013; Sambamurthy et al., 2007; Zhou & Wu, 2010",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "Schoenherr, & Rexhausen, 2013; Sambamurthy et al., 2007; Zhou & Wu, 2010 ). Teece ( Teece, 2007 )d eﬁnes agility as a higher-order dynamic capability that emerges over time, generally deﬁning agility as a capability with which ﬁrms can identify and re- spond to environmental threats and opportunities and quickly adjust their behaviors ( Goldman, Nagel, & Preiss, 1995; Sambamurthy, Bharadwaj, & Grover, 2003 ). This concept also relates to the operational ﬂexibility of organizational processes and IT systems to support structured or unstructured changes ( Chen et al., 2014 ). Achieving agility demands processing a large and varied amount of information (Goldman et al., 1995 ). This process is possible with BDA applications. However, like IT applications ( Sambamurthy et al., 2003; Weill, Subramani, & Broadbent, 2002",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "IT applications ( Sambamurthy et al., 2003; Weill, Subramani, & Broadbent, 2002 ), BDA tools cannot automatically improve agility. In fact, under certain conditions BDA tools can impede agility ( Chen et al., 2014 ). For this reason, the need exists to understand how BDA applications can create agility. Several recent studies in the business management ﬁeld apply DC theory to measure the in ﬂuence of DC in the creation of competitive ad- vantages (e.g., Schilke, 2014; Zott, 2003; Drnevich & Kriauciunas, 2011 ). In the IT management ﬁeld, few empirical studies use this theory. Analyzing the IT in ﬂuence on DC generically, ( Chen et al., 2014; Sher & Lee, 2004 ), researchers conclude that IT is an enabler of DC in organizations. Regarding agility, several studies",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "IT is an enabler of DC in organizations. Regarding agility, several studies assess the impact of IT on organizational agility (e.g., Sambamurthy et al., 2007; Chen et al., 2014; Cai et al., 2013; Tallon & Pinsonneault, 2011; Liu et al., 2013; Lu & Ramamurthy, 2011 ). These studies demonstrate a positive relation- ship between IT and agility. Chen ( Chen et al., 2014 ) recently concludes that the IT business value essentially depends on how agile a ﬁrm is380 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390 [Página 3] with regard to managing business processes. Although the literature addresses the impact of IT on the creation of organizational agility, no study links BDA with this speci ﬁc DC. Apart from some qualitative stud-",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "links BDA with this speci ﬁc DC. Apart from some qualitative stud- ies in the area of business analytics (BA) ( Shanks & Bekmamedova, 2013; Shanks & Sharma, 2011 ), only conceptual papers use DC theory to study BDA value ( Corte Real et al., 2014; Erevelles et al., 2016 ). Firms that do not develop the resources and capabilities to use BDA applications will struggle to develop a sustainable competitive advan- tage ( Erevelles et al., 2016 ). Given that agility is vital for companies´ survival, and that BDA can support organizational business processes, this study ﬁlls this academic gap and links the two concepts empirically. 3. Conceptual model With recourse to the two strategic management theories (KBV and DC) discussed above, this section explains the conceptual",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "management theories (KBV and DC) discussed above, this section explains the conceptual model and the speci ﬁc hypotheses ( Fig. 1 ). Rooted in an earlier conceptual model ( Corte Real et al., 2014 ), this research model empirically tests 12 propositions. The study assesses the entire value chain starting with how BDA can leverage different forms of knowledge to create organizational agility ( H1,H2,H3). BDA technologies can provide organizational agility to the ﬁrm by using effective knowledge management. Firms owning this type of dynamic capability can achieve competitive advantage directly ( H4a)o ri n d i r e c t - ly through business processes ( H4b). Results obtained by using business processes will impact the overall organization ( H5). Agility can also mediate the relationship between",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "the overall organization ( H5). Agility can also mediate the relationship between knowledge assets and performance (H6a,b,c-H7a,b,c). BDA uses some controls such as country, industry, technological turbulence, and time. 3.1. Hypothesis3.1.1. Knowledge assets Organizational knowledge such as operational routines, skills, and know-how constitutes a key source of competitiveness ( Grant, 1996 ). Knowledge management plays a critical role in pro ﬁciently managing data and delivering it to the end users to support business processes (Rajpathak & Narsingpurkar, 2013 ). Knowledge management repre- sents a dimension supported by KBV ( Ruggles, 1998 ) and enables dynamic capabilities by offering speci ﬁc functional competences that can improve business performance ( Teece et al., 1997 ). A naturalrelationship exists between KM and BDA. Both deal with intangible assets such as data,",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "between KM and BDA. Both deal with intangible assets such as data, knowledge, and intelligence ( Erickson & Rothberg, 2015 ). BDA is a source of knowledge management, allowing ﬁrms to add value primarily at the beginning of the information value chain and helping knowledge to ﬂow to achieve business excellence ( Chau & Xu, 2012; Popovi čet al., 2012 ). Big data is a potential knowledge asset, contingent upon the proper use of that knowledge ( Erickson & Rothberg, 2015 ). BDA represents technologies drivers of a strategic knowledge asset (big data). BDA applications have the potential to add value by providing more transparent and accurate results to support decision-making in several business areas ( Manyika et al., 2011a ). BDA strategy requires the capacity to sense,",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "Manyika et al., 2011a ). BDA strategy requires the capacity to sense, acquire, process, store, and analyze the data and convert that data into knowledge ( Rajpathak & Narsingpurkar, 2013 ). Several empirical studies state that the knowl- edge processes are antecedent dimensions of successful DC, by allowing ﬁrms to continually renew their knowledge base and deliver business performance ( Ambrosini & Bowman, 2009; Sher & Lee, 2004; Zheng, Zhang, & Du, 2011 ). As DC are information-intensive ( Pavlou & El Sawy, 2011 ), BDA may help in the creation of DC and organizational agility speci ﬁcally. Using BDA technologies helps to store and share knowledge, thereby allowing for an improvement of organizational knowledge by promoting ef ﬁciency within an organization, particularly by data integration and the",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "promoting ef ﬁciency within an organization, particularly by data integration and the use of analytical tools ( Russom, 2011 ). Some authors argue that ﬁrms must combine endogenous and exogenous knowledge to achieve DC ( Sher & Lee, 2004 ). Zhao ( Cai et al., 2013 ) argues that IT capability and KM capability are important in fostering organizational agility. Agility is promoted through knowledge manage- ment by improving innovative responses, and can improve through the use of IT and automated business processes ( Cai et al., 2013 ). In the same way, organizations should be able to use BDA technologies to convert knowledge into new routines and enhance organizational agility. Based on these ﬁndings, the hypotheses are: H1. BDA technologies allow an effective endogenous knowledge management that",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "hypotheses are: H1. BDA technologies allow an effective endogenous knowledge management that positively in ﬂuences dynamic capabilities such as organizational agility. H2. BDA technologies allow an effective exogenous knowledge management that positively in ﬂuences dynamic capabilities such as or- ganizational agility. Fig. 1. Proposed conceptual model.381 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390 [Página 4] Knowledge sharing with key channel partners refers to the extent to which a ﬁrm shares insights and know-how about its business context with its partners ( Saraf, Langdon, & Gosain, 2007 ). Channel partners are considered to be tactically and strategically important for companies. They can help to collect crucial market-related information with which to ﬁne tune the strategy to meet customer needs, resulting in long-term ﬁnancial",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "ﬁne tune the strategy to meet customer needs, resulting in long-term ﬁnancial performance ( Lorenzoni & Lipparini, 1999 ). Literature points out that the collaborative knowledge sharing capacity provides an opportunity to increase value (e.g.,( Saraf et al., 2007 )) and enable DC (e.g., ( Della Corte & Del Gaudio, 2012 )). Considering that DC theory encompasses several levels of analysis, it is important to consider the relational view, including the ability to collaborate with channel partners ( Teece, 2007 ). Literature shows that agility needs the support of effective knowledge sharing ( Liu, Song, & Cai, 2014 ). Some studies link the knowledge sharing capability through IT with agility (e.g., ( Cai et al., 2013; Liu et al., 2014 )). Such interactions can also beneﬁt from the",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "Liu et al., 2014 )). Such interactions can also beneﬁt from the use of BDA technologies, consequently enhancing organizational agility by in ﬂuencing the capabilities to sense opportuni- ties and threats, shape them, and seize them ( Della Corte & Del Gaudio, 2012 ). Therefore, another hypothesis is: H3. BDA technologies allow an effective knowledge sharing with partners that positively in ﬂuences organizational dynamic capabilities such as organizational agility. 3.1.2. Organizational agility DC can play a key role in determining a ﬁrm's competitive advantage (Teece et al., 1997; Zott, 2003 ). Agility is the “capacity of an organization to efﬁciently and effectively redeploy/redirect its resources to value cre- ating and value protecting (and capturing) higher-yield activities as in- ternal and external circumstances warrant ”(Teece, Peteraf, & Leih, 2016",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "as in- ternal and external circumstances warrant ”(Teece, Peteraf, & Leih, 2016 ). In the management ﬁeld several researchers recognize that DC does not lead directly to sustainable competitiveness, and that this value derives from improved business processes (e.g., ( Schilke, 2014; Drnevich & Kriauciunas, 2011 )). Some authors conclude that agility can in ﬂuence organizational performance ( Cai et al., 2013; Liu et al., 2013; Tallon & Pinsonneault, 2011 ). Hence, additional hypotheses are: H4a. Organizational agility is a dynamic capability leveraged by BDA that positively affects the creation of competitive advantages. H4b. Organizational agility is a dynamic capability leveraged by BDA that positively in ﬂuences the process-level performance. By engaging the business activities (e.g., sense customer needs, mar- ket research, R&D) companies can increase the possibility",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "sense customer needs, mar- ket research, R&D) companies can increase the possibility of achieving process innovation success ( Zollo & Winter, 2002 ). In the IT ﬁeld some authors focus on the importance of assessing how business processes can bring value to ﬁrms (e.g., ( Chen et al., 2014; Tallon, 2007 )). Recent conceptual considerations are that BDA is a source of DC (organizational agility, speci ﬁcally) and that BDA are a way to provide business value to ﬁrms ( Erevelles et al., 2016 ). Therefore, the hypothesis is: H5. Process-level performance has a positive effect on competitive advantage. 3.1.3. The mediating role of agility on the relationship between knowledge assets and performance Earlier IT literature considers that dynamic capabilities can establish a link between knowledge assets and",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "considers that dynamic capabilities can establish a link between knowledge assets and ﬁrm performance ( Sher & Lee, 2004; Wang, Klein, & Jiang, 2007 ). In the management ﬁeld some authors examine agility as a mediator between the management of knowledge assets and performance ( Chung, 2010; Liu et al., 2014 ). Also, the proposed model suggests a potential mediating role of agility in the relationship between knowledge assets and two types ofperformance (process-level performance and competitive advantage). Thus, additional hypotheses are: H6a. Agility positively mediates the relationship between endogenous knowledge management and competitive advantage. H6b. Agility positively mediates the relationship between exogenous knowledge management and competitive advantage. H6c. Agility positively mediates the relationship between knowledge sharing with partners and competitive advantage. H7a. Agility positively mediates the relationship",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "sharing with partners and competitive advantage. H7a. Agility positively mediates the relationship between endogenous knowledge management and process-level performance. H7b. Agility positively mediates the relationship between exogenous knowledge management and process-level performance. H7c. Agility positively mediates the relationship between knowledge sharing with partners and process-level performance. 3.1.4. Competitive advantage Competitive advantage exists when a ﬁrm reveals having greater success compared with its current or potential competitors ( Peteraf & Barney, 2003 ). To be consistent with this conceptualization, superior ﬁrm performance relative to that of competitors constitutes an empiri- cal and common indicator of competitive advantage. ( Barnett, Greve, & Park, 1994; Schilke, 2014 ). Based on Schilke's construct ( Schilke, 2014 ), competitive advantage was operationalized as re ﬂective-re ﬂective type (Ringle, Sarstedt, & Straub, 2012 ),",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "operationalized as re ﬂective-re ﬂective type (Ringle, Sarstedt, & Straub, 2012 ), with the ﬁrst-order dimensions of: (1) strategic performance (qualitative dimension) and (2) ﬁnancial per- formance (quantitative dimension), both in comparison to competition. 3.1.5. Controls As literature widely supports, this study uses the industry and the country in which a ﬁrm competes as predictors of competitiveness (Schilke, 2014 ). BDA may be particularly useful to ﬁrms operating in turbulent technological environments ( Wade & Hulland, 2004 ), and consequently, following the approach of Menguc and Auh ( Menguc & Auh, 2006 ) and Drnevich and Kriauciunas ( Drnevich & Kriauciunas, 2011 ), the study includes turbulent technological environment as a con- trol. A turbulent technological environment makes current technology obsolete and requires the development of new advances",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "environment makes current technology obsolete and requires the development of new advances ( Menguc & Auh, 2006 ). Finally, we use the variable “time since adoption of BDA ” to control for the knowledge and experience that organizations gain by using BDA over time ( Elbashir et al., 2013 ). These controls explain all dependent variables (agility, process-level performance, and competitive advantage). 4. Research design 4.1. Measurement To test the model ( Fig. 1 ) and the related hypotheses, the study per- forms a multi-country survey of European organizations from several industries. Following the recommendations of Moore and Benbasat (Moore & Benbasat, 1991 ), the study uses a survey instrument drawing upon a comprehensive literature review. Regarding content validity, ﬁve established academic IS researchers and two language experts",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "Regarding content validity, ﬁve established academic IS researchers and two language experts review each item on the questionnaire, assessing its content, scope, and purpose ( Brislin, 1970 ). To test the dif ﬁculty of the questions, to- gether with the reliability and validity of the scales, a pilot study uses a sample of 30 executives from ﬁrms not part of the main survey. Removal of some items reduces ambiguity and simpli ﬁes interpretation. The survey instrument and measurement items are in Appendix A.382 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390 [Página 5] 4.2. Data The survey was conducted in 2015 using an online survey tool. To guarantee the quality of the data, the respondent pro ﬁle uses the following three criteria: deep",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "the data, the respondent pro ﬁle uses the following three criteria: deep knowledge of the organization strategy, more than ﬁve years of experience in BI&A/BDA initiatives, and holding an IT/business executive or management position in the company. Themailing database comes from Dun & Bradstreet, one of the world's lead- ingﬁrms for commercial information and business insight. The initial sample of 500 ﬁrm executives from European ﬁrms receives an email to participate in the survey. Ninety-two valid responses were received in the ﬁrst month. To increase the response rate a follow-up email was sent. During the following months 83 additional valid responses were received from late responders, totaling 175 usable responses (overall response rate of 35%). As seen in Table 1 , the sample comprises different industries of which",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "seen in Table 1 , the sample comprises different industries of which almost half are ﬁnancial ﬁrms (40.5%). Regarding ﬁrm size, the sample is equally distributed between mid-size and large compa- nies. Business (41.4%) and IT executives (58.6%) are well represented. Non-response bias was assessed using the sample distributions of the early and late respondent groups compared with the Kolmogorov- Smirnov test ( Ryans, 1974 )( s e e Table 2 ). The early respondents were identi ﬁed by selecting the respondents in the ﬁrst month. The test shows that the two groups do not differ statistically (5% signi ﬁcance level, pN0.05), demonstrating the absence of non-response bias (Ryans, 1974 ). Due to the fact that the study collects data simultaneous- ly from a single source, for the",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "the study collects data simultaneous- ly from a single source, for the sake of validity, common method bias needs to be assessed. The study uses Harman's post hoc single-factor analysis for this purpose. A factorial analysis of all indicators was con- ducted and the ﬁrst extracted factors explain 36.9% of variance. This means that common method bias is unlikely to be an issue in the data Podsakoff et al., 2003 . 5. Results To estimate the conceptual model, the study uses the partial least squares (PLS) method ( Hair, Ringle, & Sarstedt, 2011 ). PLS ful ﬁlls theresearch purpose by examining the validity of the constructs, without requiring normal distributions for the variables. PLS requires a sample size of ten times the number of the largest number of",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "sample size of ten times the number of the largest number of structural paths directed at a particular construct ( Gefen & Straub, 2005 ). In the conceptual model the largest number of structural paths directed to a particular construct is three, which means that the minimum sample size should be 30. The sample is larger ( n=1 7 5 ) ,m e a n i n gt h a ti ti sa d - equate for PLS. Before testing the structural model, the study analyzes the measurement model in order to assess reliability and validity. 5.1. Measurement model The study examines indicator reliability, construct reliability, con- vergent validity, and discriminant validity in order to assess the mea- surement model. Tables 3 and 4 show the results of",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "the mea- surement model. Tables 3 and 4 show the results of the measurement model. Regarding indicator reliability, only loadings above 0.7 were considered. Hence, four items (ENKM5, DC1, PLP3-4) were eliminated. AsTable 3 reveals, the instrument presents good indicator reliability, as the loadings are above 0.70. The composite reliability coef ﬁ cient as- sesses the construct reliability because construct reliability takes into consideration indicators having different loadings ( Hair et al., 2011; Henseler, Ringle, & Sinkovics, 2009 ).Table 4 shows that all constructs have composite reliability above 0.7, which suggests that the constructs are reliable. To test convergent validity, the study uses average variance extracted (AVE). The AVE should be higher than 0.5, (i.e., the latent var- iable explains more than half of the variance of its",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "latent var- iable explains more than half of the variance of its indicators ( Henseler et al., 2009; Fornell & Larcker, 1981 )).Table 4 shows that all constructs meet this criterion. Regarding discriminant validity, the study uses two measures: the Fornell-Larcker criterion and cross-loadings. First, ac- cording to Fornell and Larcker ( Fornell & Larcker, 1981 ), the square root of AVE should be greater than the correlations with other latent variables. Table 4 shows that the square roots of AVEs (in bold) are higher than the correlation between constructs. All the constructs show evidence of acceptable discrimination. Second, the loading of each indicator should be greater than all cross-loadings ( Chin, 1998a ) (see Table 3 ). Overall, the model has good indicator reliability, construct reliability, convergent",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "3 ). Overall, the model has good indicator reliability, construct reliability, convergent validity, and discriminant validity. As these criteria are met, the constructs can test the structural model. 5.2. Structured model To evaluate the structured model, we followed Hair's ﬁve-step approach ( Hair et al., 2013 ): (1) collinearity assessment, (2) structural model path coef ﬁcients, (3) coef ﬁcient of determination (R2value), (4) effect size f2,a n d( 5 )p r e d i c t i v er e l e v a n c eQ2and blindfolding. Regarding collinearity (1), the results suggest minimal collinearity among the constructs (the highest VIF among the explanatory variables is 2.95), which means the predictors in the structural model do not suffer from this issue. To empirically assess the hypotheses postulated",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "do not suffer from this issue. To empirically assess the hypotheses postulated inSection 3 , the study examines the level of signi ﬁcance in pathTable 1 Sample pro ﬁle. Sample characteristics (n = 175) Obs. (%) Respondent position IT executive Chief Information Of ﬁcer (CIO) 22 12.5% IT Director 26 14.8%IT Manager 32 18.2%Other IT executive 23 13.1% Business executive Chief Financial Of ﬁcer (CFO) 19 10.9% Business Manager - Strategic Planning 18 10.3% Central Operations Of ﬁcer (COO) 14 8.0% Other Business executive 21 12.0% No. of employees b50 14 8.0% 50–250 76 43.4% N250 85 48.5% Industry Manufacturing 23 13.1%Electricity, gas and water supply activities 11 6.2%Wholesale and retail trade 19 10.8%Transports and telecommunications 18 10.2%Financial intermediation 71 40.5%Others 33 18.8% Notes: (1) The ﬁrm size",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "18 10.2%Financial intermediation 71 40.5%Others 33 18.8% Notes: (1) The ﬁrm size is categorised based on European enterprises size classi ﬁcation [104]; (2) The industries of activity are in accordance with NACE (European standard clas-siﬁcation of productive economic activities).Table 2 Testing possible response bias: early vs. late respondents. Constructs Full sample N = 175Early respondentsN=9 2Late respondentsN=8 3Kolmogorov- Smirnov test Mean S.D. Mean S.D. Mean S.D. p-Value ENKM 5.9 0.71 5.9 0.67 5.9 0.75 0.65 EXKM 5.8 0.86 5.9 0.85 5.7 0.86 0.07 KSP 4.8 0.89 4.8 0.80 4.7 0.98 0.30AG 6.1 0.93 6.1 0.78 6.0 1.07 0.72PLP 6.1 0.81 6.1 0.78 6.0 0.83 0.23CA 5.9 0.82 6.0 0.72 5.8 0.92 0.34SP 6.0 0.81 6.0 0.72 6.0 0.89 0.76FP 5.9 0.96 6.0 0.81 5.7 1.09 0.16383 N. Côrte-Real",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "6.0 0.89 0.76FP 5.9 0.96 6.0 0.81 5.7 1.09 0.16383 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390 [Página 6] coefﬁcients (2) by means of a bootstrapping technique ( Hair et al., 2011; Henseler et al., 2009 ) with 5000 iterations of re-sampling, with each bootstrap sample constituted by the number of observations (i.e., 175 cases). To have more conservative outcomes, the study uses theno sign change option ( Hair et al., 2013 ).Fig. 2 shows the estimated model (path coef ﬁcients, R2and Q2), and Table 5 summarizes the results. Concerning R2values (3), all dependent variables present rea- sonable values. In addition, this study calculates the f2and q2effect sizes (4). Most of the values of f2effect size are small, with the exception of",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "of the values of f2effect size are small, with the exception of agility in process-level-performance and exogenous knowledge management in agility (moderate effects). Last, based on a blindfolding procedure, all Q2values are above zero, which means the model has predictive power concerning the dependent variables (see Fig. 2 ). Fig. 2 summarizes the analysis results as follows: the conceptual model explains 61.8% of the variation in organizational agility. Endoge- nous Knowledge Management (EnKM) ( ^β= 0.155; pb0.01) and Exog- enous Knowledge Management (ExKM) ( ^β= 0.248; pb0.001) are statistically signi ﬁcant in explaining organizational agility (AG). Thus, H1 and H2 are con ﬁrmed, whereas knowledge sharing partners (KSP) (H3) is not con ﬁrmed. Organizational agility (AG) ( ^β= 0.371; pb0.001) is statistically signi ﬁcant in explaining Process-level Perfor-",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "( ^β= 0.371; pb0.001) is statistically signi ﬁcant in explaining Process-level Perfor- mance (PLP), and consequently H4b is supported. The conceptual model explains 57.8% of the variation in Process-level Performance (PLP). Agility (AG) contributes signi ﬁcantly to explain performance attwo levels: Process-level Performance (PLP) ( ^β= 0.371; p b0.001) and Competitive Advantage (CA) ( ^β=0 . 2 0 4 ;p b0.01), which con ﬁrms H4a and H4b .H5is not supported, as the effect is statistically not signif- icant (PLP- NCA). The conceptual model explains 77.8% of the variation in Competitive Advantage (CA). The conceptual model substantially ex- plains the variation of all three dependent variables ( Chin, 1998b; Henseler et al., 2009 ). 5.3. Mediating effect testing Based on the guidelines of Hair ( Hair et al., 2013",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "testing Based on the guidelines of Hair ( Hair et al., 2013 ), Preacher (Preacher & Hayes, 2008 ), and Nitzl ( Nitzl, Roldán, & Cepeda, 2016 ), the study evaluates the signi ﬁcance of the mediating effects of organizational agility. Mediation analysis is eligible if the indirect effect is signi ﬁcant. Table 6 presents the results, which ful ﬁll the nec- essary conditions to perform the mediator assessment. Also, the study calculates variance accounted for (VAF) to determine the size of the indirect effect in relation to the total effect ( Hair et al., 2013 ). The results show that agility can partially mediate the relationship between knowledge assets (endogenous and exogenous knowledge) and performance (process-level performance andcompetitive advantage), thereby supporting H6a,b and H7a,b. No mediating effects",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "(process-level performance andcompetitive advantage), thereby supporting H6a,b and H7a,b. No mediating effects were found between knowledge sharing withTable 3 Loadings and cross-loadings for the measurement model. Construct Item ENKM EXKM KSP AG PLP FP SP Endogenous knowledge management ENKM1 0.715 0.171 0.270 0.264 0.240 0.266 0.180 ENKM2 0.796 0.092 0.393 0.184 0.094 0.331 0.190 ENKM3 0.915 0.317 0.294 0.450 0.322 0.476 0.371 ENKM4 0.826 0.313 0.135 0.374 0.331 0.508 0.365 Exogenous knowledge management EXKM1 0.086 0.797 -0.183 0.390 0.365 0.328 0.345 EXKM2 0.214 0.899 -0.136 0.495 0.477 0.446 0.403 EXKM3 0.397 0.775 0.057 0.444 0.636 0.515 0.434 Knowledge sharing partners KSP1 0.383 −0.012 0.873 −0.125 −0.140 −0.167 −0.156 KSP2 0.324 −0.058 0.939 −0.145 −0.185 −0.116 −0.192 KSP3 0.210 −0.140 0.960 −0.245 −0.276 −0.199 −0.300 Agility AG2 0.395 0.453",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "KSP3 0.210 −0.140 0.960 −0.245 −0.276 −0.199 −0.300 Agility AG2 0.395 0.453 −0.182 0.860 0.576 0.586 0.729 AG3 0.397 0.482 −0.189 0.931 0.604 0.619 0.665 AG4 0.402 0.538 −0.085 0.905 0.608 0.607 0.627 AG5 0.327 0.494 −0.263 0.928 0.590 0.640 0.682 Performance at process level PLP1 0.315 0.629 −0.231 0.676 0.951 0.571 0.563 PLP2 0.308 0.533 −0.204 0.558 0.939 0.525 0.552 Competitive advantage Financial performance FP1 0.445 0.501 −0.238 0.675 0.571 0.950 0.728 FP2 0.531 0.496 −0.071 0.594 0.487 0.949 0.665 FP3 0.477 0.518 −0.199 0.657 0.594 0.950 0.704 Strategic performance SP1 0.343 0.363 −0.134 0.615 0.507 0.584 0.840 SP2 0.327 0.445 −0.298 0.683 0.499 0.719 0.932 SP3 0.321 0.485 −0.230 0.715 0.590 0.681 0.927 Theﬁgures in bold represents the cross-loadings for the measurement model. Table 4 Correlation",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "in bold represents the cross-loadings for the measurement model. Table 4 Correlation matrix, composite reliability (CR), and square root of AVEs. CR ENKM EXKM KSP AG PLP FP SP Endogenous knowledge management (ENKM) 0.89 0.82 Exogenous knowledge management (EXKM) 0.87 0.30 0.83 Knowledge Sharing with Partners (KSP) 0.95 0.31 −0.09 0.93 Agility (AG) 0.95 0.42 0.54 −0.20 0.91 Process level performance (PLP) 0.94 0.33 0.62 −0.23 0.66 0.95 Financial performance (FP) 0.97 0.51 0.54 −0.18 0.68 0.58 0.95 Strategic performance (SP) 0.93 0.37 0.49 −0.25 0.75 0.59 0.74 0.90 (1) First column are CR (composite reliability). (2) Diagonal elements are square root of average variance extracted (AVE).(3) Off-diagonal elements are correlations.The bold ﬁgures represent the square roots of AVEs.384 N. Côrte-Real et al. / Journal of Business Research",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "roots of AVEs.384 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390 [Página 7] partners and performance (process-level performance and competi- tive advantage), which means H6c and H7c are not con ﬁrmed. 6. Discussion As BDA can generate value in several ways, the need exists to under- stand the entire chain. This study ﬁlls the research gap by assessing not only the antecedents but also the effects of BDA initiatives in European ﬁrms. The results strongly support the claim that BDA applications can allow an effective internal and external knowledge management which can help ﬁrms to create organizational agility. This agility exists in several ways: (1) by sensing opportunities and threats (e.g., reacting to new products or services of competitors); (2) by seizing possible",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "reacting to new products or services of competitors); (2) by seizing possible chances (e.g., expanding into new regional or international markets), and (3) by adjusting to the technological environment to attain competitive advantage (e.g., adopting new technologies to produce products andservices more ef ﬁciently). This ﬁnding is consistent with earlier literature (Chen et al., 2014; Liu et al., 2014; Sher & Lee, 2004 ). Regarding the antecedents, the results demonstrate that BDA can support organizational knowledge management, allowing the crea- tion/enhancement of dynamic capabilities such as organizational agility. Thisﬁnding is consistent with earlier studies applied to IT innovations and organizational management (e.g., ( Nieves & Haller, 2014; Sher & Lee, 2004; Cai et al., 2013; Liu et al., 2014; Cepeda & Vera, 2007 )). The results suggest that",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "et al., 2014; Cepeda & Vera, 2007 )). The results suggest that exogenous knowledge management deserves more attention, which was considered more important than endoge- nous knowledge management. This outcome suggests that BDA technologies can provide business value by facilitating the acquisition of supply chain and marketing knowledge. While knowledge manage- ment is important to explain BDA value creation, the way of sharing this strategic asset among business partners is not statistically signi ﬁ- cant in this study. Although the hypothesis related to the knowledge shared with partners ( H3) seems plausible and consistent with earlier studies for other IT innovations (e.g., ( Zhu & Kraemer, 2005; Zheng Fig. 2. Estimated model. Note: ns = non-signi ﬁcant. ** |t| N=1.96 at p = 0.05; *** |t| N=2 .",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "ﬁcant. ** |t| N=1.96 at p = 0.05; *** |t| N=2 . 5 7a tp=0 . 0 1l e v e l ;* * * *| t | N=3.29 at p = 0.001 level. Table 5 Signiﬁcant testing results of the structural model path coef ﬁcients. Structural path Path coef ﬁcient (t-value) Effect size (f2) Effect size (q2) 95% con ﬁdence interval Conclusion EndKM →AG 0.155⁎⁎ (2.562)0.038 0.024 [0.032; 0.268] H1supported ExKM→AG 0.248 ⁎⁎⁎⁎ (4.556)0.120 0.074 [0.149; 0.364] H2supported KSP→AG 0.010 ns (0.121)0.000 0.000 [ −0.145; 0.169] H3not supported AG→CA 0.204 ⁎⁎⁎ (2.786)0.064 0.021 [0.065; 0.351] H4a supported AG→PLP 0.371⁎⁎⁎⁎ (3.969)0.125 0.080 [0.173; 0.544] H4b supported PLP→CA 0.106 ns (1.579)0.021 0.007 [ −0.030; 0.234] H5not supported Note: ns = non-signi ﬁcant. The values of f2and q2effects can be",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "Note: ns = non-signi ﬁcant. The values of f2and q2effects can be considered weak (0.02). moderate (0.15) and strong (0.35). Conﬁdence level: ⁎⁎|t|N=1.96 at p= 0.05 level. ⁎⁎⁎|t|N=2 . 5 7a t p=0 . 0 1l e v e l . ⁎⁎⁎⁎ |t|N=3.29 at p= 0.001 level.385 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390 [Página 8] et al., 2011; Ruivo, Oliveira, & Neto, 2014 )), this construct does not con- tribute to creating valuable organizational agility. An earlier study con- cludes that using this type of knowledge is not always useful and can harm speci ﬁc business processes in some situations. Moreover, this study shows that agility can partially mediate the positive effect of some knowledge assets (exogenous and endogenous) and performance",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "the positive effect of some knowledge assets (exogenous and endogenous) and performance (process-level performance and competitive advantage) ( H6a,H6b and H7a,H6b). This ﬁnding is consistent with earlier studies ( Liu et al., 2013; Liu et al., 2014; Pavlou & El Sawy, 2006 ). Competitive performance is not only about how much ﬁrms know, but how they use what they know ( Haas & Hansen, 2005 ). A possible explanation for this result is that ﬁrms are reluctant to share sensitive information that might compromise their competitive advantage. In fact, synergies with business partners can be bene ﬁcial (e.g.,( Setia, Richardson, & Smith, 2015 )), but careful attention is needed regarding the shared information. The study shows that knowledge sharing with partners can be truly compromising in the areas",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "that knowledge sharing with partners can be truly compromising in the areas of Production and Op- erations or Product and Service enhancement, which represent the core business practices of a ﬁrm. An information sharing agreement might be a solution to overcome this constraint. Concerning the effects of agility leveraged by BDA, the results indi- cate that this dynamic capability can positively impact competitive ad- vantage in different ways (via processes or organizationally), which is in line with the ﬁndings of other authors ( Drnevich & Kriauciunas, 2011; Protogerou et al., 2012 )(H4a,b). Agility can also be more effective in improving speci ﬁc business processes than organizational perfor- mance, which is consistent with Drnevich and Kriauciunas ( Drnevich & Kriauciunas, 2011 ).The results demonstrate that no signi ﬁcant link",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "Drnevich & Kriauciunas, 2011 ).The results demonstrate that no signi ﬁcant link exists between process-level performance and competitive advantage (H5). In this sense, Drnevich and Kriauciunas ( Drnevich & Kriauciunas, 2011 ) argue that a ﬁrm's performance depends on a set of elements that might fail due to miscommunication between the business areas and the top management. Although some business areas can behave in an ef ﬁcient way, this ef ﬁciency does not necessarily have a signi ﬁcant effect on the overall performance. Although BDA technologies are generaly associated with customer management or marketing areas, results indicate that, in general, European ﬁrms focus more on internally improving their assets (products and services) and the way that these are being produced to optimize costs. With Europe still showing signs",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "these are being produced to optimize costs. With Europe still showing signs of ﬁnancial crisis, this ﬁnding might point the way to a change of survival strategy in compet- itive markets. 6.1. Limitations and further research Certain limitations apply to the interpretation of the results of this study. First, the antecedents of agility do not extend beyond the speci ﬁc knowledge resources included in the model. Other factors can also determine the development of this dynamic capability in European ﬁrms. Future studies may include these resources as variables of themodel or by moderating existing variables. Second, although the study considers constructs in the model embedding the impact of BDA at process-level, the model is ﬁrm-level. Before generalization is possible, researchers should perform a longitudinal study based on the",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "generalization is possible, researchers should perform a longitudinal study based on the process approach. Future research should use speci ﬁcp r o c e s sc o n s t r u c t st o assess the impact of BDA on several business areas in detail. Third, due to the perceptual nature of the measures used, future studies should identify the issues associated with cross-sectional research design. Although the use of objective measures to assess ﬁrm performance is important, in this study companies were reluctant to provide them. Fourth, although the sample size is statistically adequate, a larger sample could be useful to reinforce the conclusions of this study. As researchers generally accept that BDA can provide bene ﬁts to all European ﬁrms ( European_Commission, 2015 )",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "can provide bene ﬁts to all European ﬁrms ( European_Commission, 2015 ) across several indus- tries, reinforced on a McKinsey survey ( Manyika et al., 2011b )r e p o r t s that most industries in Europe have the capacity to store and manipu- late big data, and consequently the potential value of using big data resides mainly in developed countries. Therefore, data from ﬁve European developed countries were collected. By conducting future studies in more countries and industries, which may have different per- ceptions of BDA and diverse external contexts, the understanding of BDA business value could likely improve. Due to their different cultures, research to perform a comparative study among European regions (e.g., Northern and Southern Europe) could be interesting. 6.2. Theoretical implications This study",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "Northern and Southern Europe) could be interesting. 6.2. Theoretical implications This study offers two key contributions that extend theory on BDA in technology and organizational management research: (1)BDA value chain understanding - Despite the potential bene ﬁts, some ﬁrms fail to capture value from BDA initiatives ( Kaisler et al., 2013 ). Recent papers focus on BDA research opportunities (Abbasi et al., 2016; Agarwal & Dhar, 2014 ), claiming that there is a need to conduct assessments of the actual impact of BDA investments and use, and to understand how to achieve the beneﬁts for performance. The BDA value chain remains relatively unexplored and requires further investigation. The current paper responds to the calls of scholars by empirically assessing the value that BDA can bring to European ﬁrms.",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "by empirically assessing the value that BDA can bring to European ﬁrms. This study theoreti- cally proposes and empirically validates a conceptual model based on strategic management theories (KBV and DC), never before combined for this purpose, to explain the full BDA value chain. Liu ( L i ue ta l . ,2 0 1 4 ) argues that literature about the relationship among knowledge management, organizational agility, and ﬁrm performance is still limited. This is the ﬁrst study that empirically demonstrates that BDA applications based on an effective knowledge management can help ﬁrms to create organizational agility leading to competitive advantage. Further studies could bene ﬁcially use this theoretical framework to assess the business value in other IT innovations at a process-Table 6 Mediation test by bootstrapping",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "in other IT innovations at a process-Table 6 Mediation test by bootstrapping approach. Effect of Direct effect (t-value) Indirect effect (t-value) Total effect VAF (%) Interpretation Conclusion EnKM→AG→CA 0.137 ⁎⁎(2.317) 0.053 ⁎⁎(2.156) 0.190 ⁎⁎⁎⁎(3.577) 27.89% Partial mediation H6a supported ExKM→AG→CA 0.081 ns (1.506) 0.097 ⁎⁎⁎(2.617) 0.178 ⁎⁎⁎⁎(4.037) 54.49% Partial mediation H6b supported KSP→AG→CA 0.026 ns (0.464) −0.014 ns (0.607) 0.012 ns (0.199) na No mediation H6cnot supported EnKM→AG→PLP 0.141 ⁎⁎(1.988) 0.057 ⁎⁎(2.212) 0.198 ⁎⁎⁎(2.813) 28.79% Partial mediation H7a supported ExKM→AG→PLP 0.344⁎⁎⁎⁎(5.412) 0.092⁎⁎⁎(3.041) 0.436⁎⁎⁎(7.219) 21.10% Partial mediation H7b supported KSP→AG→PLP −0.157⁎⁎(2.408) 0.003 ns (0.119) −0.154⁎⁎(2.172) na No mediation H7cnot supported Note: VAF = variance accounted for. The VAF N80% indicates full mediation. 20% ≤VAF≥80% show partial mediation. VAF b20% indicates no mediation. ns = non-signi ﬁcant. na = not",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "VAF b20% indicates no mediation. ns = non-signi ﬁcant. na = not applicable. ⁎⁎|t|N=1.96 at p = 0.05 level. ⁎⁎⁎|t|N= 2.57 at p = 0.01 level. ⁎⁎⁎⁎ |t|N=3.29 at p = 0.001 level.386 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390 [Página 9] level and ﬁrm-level. Academics can make use of this paper for pedagogical support for teaching about BDA value chain. (2)DC literature –This paper contributes to DC research by empir- ically testing agility business value in a BDA context ( Drnevich & Kriauciunas, 2011 ). The results strongly support the belief that BDA technologies can trigger agility and that agility can af- fect competiveness in two ways (via processes or globally). AsBDA can signi ﬁcantly improve business processes ( Davenport, 2006",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "or globally). AsBDA can signi ﬁcantly improve business processes ( Davenport, 2006 ), business process enhancement driven by BDA is an im- portant research area ( Abbasi et al., 2016 ). Earlier studies focus only on the link between agility and ﬁrm performance ( Chen et al., 2014; Liu et al., 2014; Tallon & Pinsonneault, 2011 ), while this study empirically demonstrates that an effect of agility exists at the process-level, too. In addition, despite an increasing use of mediation testing, most of the studies in PLS-SEM do not analyze mediation effects ( Hair et al., 2013; Nitzl et al., 2016 ). Under- standing mediation issues can be crucial for researchers because they can better explain or hinder the in ﬂuence of a third variable in the relationship",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "or hinder the in ﬂuence of a third variable in the relationship between two variables in a model ( Cepeda & Vera, 2007 ). This study demonstrates that agility can be a mediator between external and internal knowledge assets and performance (process-level performance and competitive advantage). 6.3. Managerial implications For practitioners (including executives and IT managers) this study demonstrates how best to leverage the knowledge embedded in BDA systems and initiatives and achieve capabilities that will help to main- tain competitive advantages. The paper provides support to justify BDA investments and initiatives. The results indicate that although BDA technologies call for substantial investment in implementation and maintenance, European ﬁrms are aware of BDA's potential value and bene ﬁts. Executives should apply these guidelines to their organiza- tional IT",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "bene ﬁts. Executives should apply these guidelines to their organiza- tional IT strategy. BDA can provide value at several stages: (1) knowledge; (2) dynamic capability (organizational agility); (3) business process; and (4) com- petitive performance. To initiate the value creation process, ﬁrms should invest in an effective BDA program. First, the value that BDA can provide derives ﬁrst from the way ﬁrms use the technologies available to manage knowledge. An effective training program can help to leverage the way users extract and manage knowledge. Second, by effectively using BDA, ﬁrms can acquire capabilities to innovate and rapidly adjust to external demands (e.g., optimize business processes). Third, these capabilities will encourage speci ﬁc business areas to involve the whole organization, when an effective bottom-up strategy is followed, supported by",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "the whole organization, when an effective bottom-up strategy is followed, supported by good communication practices. By applying this framework to BDA speci ﬁcally, managers and IT executives can beneﬁt from a performance metric that uniquely speci ﬁes the impact of BDA. By evaluating the organizational knowledge conversion into process and ﬁrm-level capabilities, practitioners can increase their productivity. Software vendors of BDA can also gain a better under- standing of how European ﬁrms can invest and experience the value created through BDA. They can natively embed BDA capabilities in their solutions as a way for their customers to achieve superior ﬁnancial and strategic performance. Finally, ﬁrms that have not yet decided to adopt these technologies can gain a perception of what is possible by adopting and effectively using BDA.",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "a perception of what is possible by adopting and effectively using BDA. 6.4. Business research implications The business community now sees big data as a potential tool of business value for achieving competitive advantage. This value can only be real if companies know how to effectively manage Big Data An- alytics (BDA) initiatives. This paper establishes a ﬁrst link between BDAprocess-level performance and competitive advantage, by merging the ﬁeld of information systems and strategic management. By presenting and discussing strategic and organizational drivers and impacts of BDA, guidance to business researchers, practitioners, and scholars is provided. As such, this paper extends knowledge by directly evaluating the effect of BDA on the decision-making process to support an effective IT resource management, focusing on challenges for adoption, gover-nance, and evaluation.",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "effective IT resource management, focusing on challenges for adoption, gover-nance, and evaluation. The outcomes of this paper indicate that BDA can be an effective aid to survival in competitive markets, particularly by supporting Production and Operations or P roduct and Service enhancement. Striving to overcome damages of the ﬁnancial crisis, European ﬁrms are using BDA tools to internally improve their assets (products and services) and the way that these are being produced to optimize costs. European ﬁrms tend to attribute greater value to external knowledge provided by BDA applications than to internal knowledge management. Sharing knowledge with business partners is poten- tially harmful to organizational productivity, so careful attention is in order when exchanging this type of core data between companies. Also, this study concludes that organizational agility",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "of core data between companies. Also, this study concludes that organizational agility leads directly to a better performance (process-level and competitive advantage) but can mediate effects from knowledge assets on performance. This means that ﬁr m sm u s tb e a ri nm i n dt h a ts e v e r a lp a t h sc a nl e a dt o competitive advantage. First, managers should consider investing in BDA technologies to take advantage of internal and external knowledge resources. Second, by governing the knowledge extract- ed by BDA, agility becomes the “ultimate ”organizational capability that leads to sustainable compet itive advantages. Firms should conﬁdently invest in the development of agility supported by BDA tools. 7. Conclusions As Big Data Analytics (BDA) can",
        "start_idx": 8236,
        "end_idx": 8364
      },
      {
        "text": "supported by BDA tools. 7. Conclusions As Big Data Analytics (BDA) can offer value to companies in several ways, many scholars highlight the need to understand the path to competitive advantage. The main outcome emerging from this paper has to do with understanding the value chain of BDA. Grounded on knowledge-based view (KBV) and dynamic capabilities (DC), this study ﬁlls a research gap from the strategic management perspective, by perceiving the antecedents (knowledge assets) and the impacts (on process-level performance and competitive advan- tage) of BDA initiatives in European ﬁrms. The results show that the model signi ﬁcantly explains all dependent variables (61.8% of agility variation, 57.8% of process-level performance variation, and 77.8% of competitive advantage variation). The major conclusions of this study are: a) BDA can be",
        "start_idx": 8352,
        "end_idx": 8480
      },
      {
        "text": "variation). The major conclusions of this study are: a) BDA can be a strategic investment for European ﬁrms to enhance or- ganizational agility and survive in competitive markets. Firms should invest in the development of organizational agility supported by effective BDA applications. b) To create agility, European ﬁrms tend to believe that the external knowledge deriving from BDA applications can be more effective in the creation of agility than internal knowledge. Sharing knowl- edge with business partners is problematic, as sharing, is a potential barrier for process-level performance. c) Regarding the impacts of agility, this capability leads directly to a better performance (process-level and competitive advantage) but can mediate effects from knowledge assets on performance. This means that BDA initiatives can lead to better operational ef ﬁciency, but",
        "start_idx": 8468,
        "end_idx": 8596
      },
      {
        "text": "means that BDA initiatives can lead to better operational ef ﬁciency, but several paths can lead to competitive advantage. Thus, a crucial need exists for ﬁrms to have an integrated view of the BDA chain in order to be able to fully leverage the innovative power of BDA capabilities to achieve competitive advantage.387 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390 [Página 10] Appendix A. Survey questionnaire References Abbasi, A., Sarker, S., & Chiang, R. H. (2016). Big Data research in information systems: To- ward an inclusive research agenda. Journal of the Association for Information Systems , 17(2), 3. Agarwal, R., & Dhar, V. (2014). Editorial —Big Data, data science, and analytics: The oppor- tunity and challenge for IS research. Information Systems Research",
        "start_idx": 8584,
        "end_idx": 8712
      },
      {
        "text": "analytics: The oppor- tunity and challenge for IS research. Information Systems Research ,25(3), 443 –448. Ambrosini, V., & Bowman, C. (2009). What are dynamic capabilities and are they a useful construct in strategic management? International Journal of Management Reviews , 11(1), 29 –49. Arend, R., & Bromiley, P. (2009). Assessing the dynamic capabilities view: spare change, everyone? Strategic Organization ,7(1), 75. Barnett, W. P., Greve, H. R., & Park, D. Y. (1994). An evolutionary model of organizational performance. Strategic Management Journal ,15(S1), 11 –28. Barton, D. (2012). Making advanced analytics work for you. Harvard Business Review ,90, 78–83. Barua, A., Kriebel, C. H., & Mukhopadhyay, T. (1995). Information technologies and busi- ness value: An analytic and empirical investigation. Information Systems Research , 6(1), 3 –23. Bharadwaj, A. S.",
        "start_idx": 8700,
        "end_idx": 8828
      },
      {
        "text": "empirical investigation. Information Systems Research , 6(1), 3 –23. Bharadwaj, A. S. (2000). A resource-based perspective on information technology capability and ﬁrm performance: An empirical investigation. MIS Quarterly ,24(1), 169–196.Blome, C., Schoenherr, T., & Rexhausen, D. (2013). Antecedents and enablers of supply chain agility and its effect on performance: A dynamic capabilities perspective. International Journal of Production Research ,51(4), 1295 –1318. Brislin, R. W. (1970). Back-translation for cross-cultural research. Journal of Cross-Cultural Psychology ,1(3), 185 –216. Cai, Z., et al. (2013). Developing organizational agility through IT capability and KM capability. The moderating effects of organizational climate .P A C I S . Cepeda, G., & Vera, D. (2007). Dynamic capabilities and operational capabilities: A knowledge management perspective. Journal of Business Research ,60(5), 426–437. Chau, M., & Xu, J.",
        "start_idx": 8816,
        "end_idx": 8944
      },
      {
        "text": "perspective. Journal of Business Research ,60(5), 426–437. Chau, M., & Xu, J. (2012). Business intelligence in blogs: Understanding consumer interac- tions and communities. MIS Quarterly ,36(4), 1189 –1216. Chen, H., Chiang, R., & Storey, V. (2012). Business intelligence and analytics: From Big Data to big impact. MIS Quarterly ,36(4), 1165 –1188. Chen, Y., et al. (2014). IT capability and organizational performance: The roles of business process agility and environmental factors. European Journal of Information Systems , 23(3), 326 –342. Chin, W. W. (1998a). Commentary: Issues and opinion on structural equation modeling. JSTOR, 7 –16. Chin, W. W. (1998b). The partial least squares approach for structural equation modeling. Chung, T. R. (2010). Knowledge creation and ﬁrm performance. In e. (Ed.), Mediating processes from an organizational agility perspective .A M",
        "start_idx": 8932,
        "end_idx": 9060
      },
      {
        "text": "In e. (Ed.), Mediating processes from an organizational agility perspective .A M C I S .Constructs Items Source Knowledge assets Please indicate the extent to which these forms of knowledge are used in your organization. BDA technologies: Endogenous knowledge ManagementENKM1. Reduce uncertainties of knowledge loss ENKM2. Reduce dependence on speci ﬁc personnel ENKM3. Are comprehensively utilized by members in organizationENKM4. Are comprehensively constructed in organization*(Sher & Lee, 2004 ) Exogenous knowledge ManagementEXKM1. Facilitate acquisition of supply chain knowledge EXKM2. Facilitate processing of supply chain knowledge EXKM3. Facilitate processing of marketing knowledge(Sher & Lee, 2004 ) Knowledge sharing with channel partnersKSP1. We frequently share knowledge about our business environment (e.g., other business relationships) with our channel partners.KSP2. Knowledge about all of our channel partners, competitors, etc., is shared with",
        "start_idx": 9048,
        "end_idx": 9176
      },
      {
        "text": "Knowledge about all of our channel partners, competitors, etc., is shared with ourother channel partners.KSP3. Business insights are exchanged between us and our other channel partners.(Liu et al., 2014 ) Organizational agility (dynamic capability)Please indicate the degree to which the use of BDA tools in the last three years has helped to: AG1. Respond to changes in aggregate consumer demand.* AG2. React to new product or service launches by competitors. AG3. Expand into new regional or international markets.AG4. Change (i.e., expand or reduce) the variety of products/services available for sale.AG5. Adopt new technologies to produce better, faster, and cheaper products and services.(Lu & Ramamurthy, 2011 ) Process-level performance To what extent has BDA been used to support critical business activities in each of the following processes in the",
        "start_idx": 9164,
        "end_idx": 9292
      },
      {
        "text": "support critical business activities in each of the following processes in the last three years. A sampling of critical activities in each process is shown below.PLP1. Production and operations: improve throughout, boost labour productivity, improve ﬂexibility and equipment utilisation, and streamline operations. PLP2. Product and service enhancement: embed IT in products, increase pace of development/R&D, monitor design cost, improve quality, support innovation.PLP3. Marketing and sales: spot market trends, anticipate customer needs, build market share,improve forecast accuracy, and evaluate pricing options.*PLP4. Customer relations: respond to customer needs, provide after-sales service and support, improvedistribution, create customer loyalty*(Peteraf & Barney, 2003 ) Competitive advantage Please indicate the degree to which you agree with the following statements. Strategic Performance SP1. We have gained strategic advantages over our competitorsSP2. We have a large",
        "start_idx": 9280,
        "end_idx": 9408
      },
      {
        "text": "We have gained strategic advantages over our competitorsSP2. We have a large market share.SP3. Overall, we are more successful than our major competitors.Financial performanceFP1. Our EBIT (earnings before interest and taxes) is continuously above industry average.FP2. Our ROI (return on investment) is continuously above industry average.FP3. Our ROS (return on sales) is continuously above industry average.(Schilke, 2014 ) Control variables Time since BDA adoption Number of years since adoption (#)Country CountryIndustry Type of industryTechnological turbulence Please indicate the degree to which you agree with the following statements. TT1. Extent of technological turbulence in the environment.TT2. Leadership in product/process innovation. TT3. Impact of new technology on operations.(Brislin, 1970 ) Notes: (1) * items eliminated due low loading. (2) Items were measured using a 7-point numerical scale (1 is Strongly",
        "start_idx": 9396,
        "end_idx": 9524
      },
      {
        "text": "(2) Items were measured using a 7-point numerical scale (1 is Strongly Disagree and 7 is Strongly Agree).388 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390 [Página 11] Corte Real, N., Oliveira, T., & Ruivo, P. (2014). Understanding the hidden value of business intelligence and analytics (BI&A). Twentieth American Conference of Information Systems . Savannah, Georgia: Association of Information Systems. Davenport, T. H. (2006). Competing on analytics. Harvard Business Review ,84,1–12. Della Corte, V., & Del Gaudio, G. (2012). Dynamic capabilities: A still unexplored issue with growing complexity. Corporate Ownership and Control ,9,3 2 7 –338. Drnevich, P. L., & Kriauciunas, A. P. (2011). Clarifying the conditions and limits of the con- tributions of ordinary and dynamic capabilities to relative ﬁrm performance. Strategic",
        "start_idx": 9512,
        "end_idx": 9640
      },
      {
        "text": "con- tributions of ordinary and dynamic capabilities to relative ﬁrm performance. Strategic Management Journal ,32(3), 254 –279. Elbashir, M. Z., et al. (2013). Enhancing the business value of business intelligence: The role of shared knowledge and assimilation. Journal of Information Systems ,27(2), 87 –105. Erevelles, S., Fukawa, N., & Swayne, L. (2016). Big Data consumer analytics and the trans- formation of marketing. Journal of Business Research ,69(2), 897 –904. Erickson, S., & Rothberg, H. (2015). Big Data and knowledge management: Establishing a conceptual foundation. Leading issues in knowledge management. Vol. Two . (pp. 204) 2. European_Commission (2015). Towards a thriving data-driven economy. Accessed on: 30th December 2015]; Available from http://ec.europa.eu/digital-agenda/en/ towards-thriving-data-driven-economy#Article Fornell, C., & Larcker, D. F. (1981). Evaluating structural equation models with unobserv- able variables and measurement",
        "start_idx": 9628,
        "end_idx": 9756
      },
      {
        "text": "F. (1981). Evaluating structural equation models with unobserv- able variables and measurement error. Journal of Marketing Research ,18,3 7 5 –381. Gefen, D., & Straub, D. (2005). A practical guide to factorial validity using PLS-Graph: Tu- torial and annotated example. Communications of the Association for Information Systems ,16(1), 5. Goldman, S. L., Nagel, R. N., & Preiss, K. (1995). Agile competitors and virtual organizations: Strategies for enriching the customer. Van Nostrand Reinhold. Grant, R. M. (1996). Prospering in dynamically-competitive environments: Organization- al capability as knowledge integration. Organization Science ,7(4), 375 –387. Haas, M. R., & Hansen, M. T. (2005). When using knowledge can hurt performance: The value of organizational capabilities in a management consulting company. Strategic Management Journal ,26(1), 1 –24. Hair, J. F., Ringle, C. M., &",
        "start_idx": 9744,
        "end_idx": 9872
      },
      {
        "text": "Management Journal ,26(1), 1 –24. Hair, J. F., Ringle, C. M., & Sarstedt, M. (2011). PLS-SEM: Indeed a silver bullet. Journal of Marketing Theory and Practice ,19(2), 139 –152. Hair, J. F., Jr., et al. (2013). A primer on partial least squares structural equation modeling (PLS-SEM). Sage Publications. Helfat, C., & Peteraf, M. (2009). Understanding dynamic capabilities: Progress along a de- velopmental path. Strategic Organization ,7(1), 91. Helfat, C. E., et al. (2009). Dynamic capabilities: Understanding strategic change in organiza- tions. John Wiley & Sons. Henseler, J., Ringle, C. M., & Sinkovics, R. R. (2009). The use of partial least squares path modeling in international marketing. Advances in International Marketing (AIM) ,20, 277–320. IDC (2011). Big Data analytics. Future architectures, skills and roadmaps for the CIO . Kaisler,",
        "start_idx": 9860,
        "end_idx": 9988
      },
      {
        "text": "Data analytics. Future architectures, skills and roadmaps for the CIO . Kaisler, S., et al. (2013). Big Data: Issues and challenges moving forward. In system sci- ences (HICSS). 2013 46th Hawaii International Conference on System Sciences .I E E E . Kwon, O., Lee, N., & Shin, B. (2014). Data quality management, data usage experience and acquisition intention of Big Data analytics. International Journal of Information Management ,34(3), 387 –394. LaValle, S., et al. (2011). Big Data, analytics and the path from insights to value. MIT Sloan Management Review ,52(2), 21 –31. Liu, H., Song, D., & Cai, Z. (2014). Knowledge management capability and ﬁrm performance: The mediating role of organizational agility. PACIS. Liu, H., et al. (2013). The impact of IT capabilities on ﬁrm performance: The mediating",
        "start_idx": 9976,
        "end_idx": 10104
      },
      {
        "text": "al. (2013). The impact of IT capabilities on ﬁrm performance: The mediating roles of absorptive capacity and supply chain agility. Decision Support Systems , 54(3), 1452 –1462. Lorenzoni, G., & Lipparini, A. (1999). The leveraging of inter ﬁrm relationships as a distinc- tive organizational capability: A longitudinal study. Strategic Management Journal , 20(4), 317 –338. Lu, Y., & Ramamurthy, K. (2011). Understanding the link between information technology capability and organizational agility: An empirical examination. MIS Quarterly ,35(4), 931–954. Malladi, S. (2013). Adoption of business intelligence & analytics in organizations –An em- pirical study of antecedents. 19th American Conference on Information Systems (AMCIS) Chicago, Illinois. Manyika, J., et al. (2011a). In M.G. Institute (Ed.), Big Data: The next frontier for innovation, competition and productivity .M c K i n",
        "start_idx": 10092,
        "end_idx": 10220
      },
      {
        "text": "next frontier for innovation, competition and productivity .M c K i n s e yG l o b a lI n s t i t u t e . Manyika, J., et al. (2011b). Big Data: The next frontier for innovation competition and productivity. McKinsey Global Institute. Mata, F. J., Fuerst, W. L., & Barney, J. B. (1995). Information technology and sustained competitive advantage: A resource-based analysis. MIS Quarterly ,19(4), 487 –505. Melville, N., Kraemer, K., & Gurbaxani, V. (2004). Information technology and organiza- tional performance: An integrative model of IT business value. MIS Quarterly ,28(2), 283–322. Menguc, B., & Auh, S. (2006). Creating a ﬁrm-level dynamic capability through capitaliz- ing on market orientation and innovativeness. Journal of the Academy of Marketing Science ,34(1), 63 –73. Moore, G.",
        "start_idx": 10208,
        "end_idx": 10336
      },
      {
        "text": "Journal of the Academy of Marketing Science ,34(1), 63 –73. Moore, G. C., & Benbasat, I. (1991). Development of an instrument to measure the percep- tions of adopting an information technology innovation. Information Systems Research ,2(3), 192 –222. Morabito, V. (2015). Big Data and analytics: Strategic and organizational impacts. Springer. Nieves, J., & Haller, S. (2014). Building dynamic capabilities through knowledge resources. Tourism Management ,40,2 2 4 –232. Nitzl, C., Roldán, J. L., & Cepeda, G. (2016). Mediation analyses in partial least squares structural equation modeling. Helping researchers discuss more sophisticated models (pp. 3 –21). Nonaka, I. (1995). The knowledge-creating company: How Japanese companies create the dynamics of innovation. Oxford University Press.Pavlou, P. A., & El Sawy, O. A. (2006). From IT leveraging competence to competitive ad- vantage",
        "start_idx": 10324,
        "end_idx": 10452
      },
      {
        "text": "Sawy, O. A. (2006). From IT leveraging competence to competitive ad- vantage in turbulent environments: The case of new product development. Information Systems Research ,17(3), 198 –227. Pavlou, P. A., & El Sawy, O. A. (2011). Understanding the elusive black box of dynamic ca- pabilities. Decision Sciences ,42(1), 239 –273. Pavlou, P. A., et al. (2005). Measuring the return on information technology: A knowledge-based approach for revenue allocation at the process and ﬁrm level. Journal of the Association for Information Systems ,6(7), 199 –226. Peteraf, M. A., & Barney, J. B. (2003). Unraveling the resource-based tangle. Managerial and Decision Economics ,24(4), 309 –323. Pettigrew, A. M., Thomas, H., & Whittington, R. (2001). Handbook of strategy and manage- ment. Sage. Podsakoff, P. M., et al. (2003). Common method biases",
        "start_idx": 10440,
        "end_idx": 10568
      },
      {
        "text": "manage- ment. Sage. Podsakoff, P. M., et al. (2003). Common method biases in behavioral research: A critical review of the literature and recommended remedies. Journal of Applied Psychology , 88(5), 879. Popovi č, A., et al. (2012). Towards business intelligence systems success: Effects of maturity and culture on analytical decision making. Decision Support Systems ,54, 729–739. Preacher, K. J., & Hayes, A. F. (2008). Asymptotic and resampling strategies for assessing and comparing indirect effects in multiple mediator models. Behavior Research Methods ,40(3), 879 –891. Protogerou, A., Caloghirou, Y., & Lioukas, S. (2012). Dynamic capabilities and their indirect impact on ﬁrm performance. Industrial and Corporate Change ,21(3), 615 –647. Rajpathak, T., & Narsingpurkar, A. (2013). Managing knowledge from Big Data analytics in product development. Tata Consulting, 11. Ringle, C.",
        "start_idx": 10556,
        "end_idx": 10684
      },
      {
        "text": "from Big Data analytics in product development. Tata Consulting, 11. Ringle, C. M., Sarstedt, M., & Straub, D. (2012). A critical look at the use of PLS-SEM in MIS quarterly. MIS Quarterly (MISQ) ,3 6 ( 1 ) . Ruggles, R. (1998). The state of the notion: Knowledge management in practice. California Management Review ,40(3), 80 –89. R u i v o ,P . ,O l i v e i r a ,T . ,&N e t o ,M .( 2 0 1 4 ) . Examine ERP post-implementation stages of use and value: Empirical evidence from Portuguese SMEs. International Journal of Accounting Information Systems ,15(2), 166 –184. Ruivo, P., Oliveira, T., & Neto, M. (2015). Using resource-based view theory to assess the value of ERP commercial-packages in",
        "start_idx": 10672,
        "end_idx": 10800
      },
      {
        "text": "Using resource-based view theory to assess the value of ERP commercial-packages in SMEs. Computers in Industry ,73,1 0 5 –116. Russom, P. (2011). Big Data analytics. Fourth Quarter: TDWI Best Practices Report. Ryans, A. B. (1974). Estimating consumer preferences for a new durable brand in an established product class. Journal of Marketing Research ,4 3 4 –443. Sambamurthy, V., Bharadwaj, A., & Grover, V. (2003). Shaping agility through digital op- tions: Reconceptualizing the role of information technology in contemporary ﬁrms. MIS Quarterly ,2 3 7 –263. Sambamurthy, V., et al. (2007). IT-enabled organizational agility and ﬁrms' sustainable competitive advantage. ICIS 2007 proceedings (pp. 91). Saraf, N., Langdon, C. S., & Gosain, S. (2007). IS application capabilities and relational value in inter ﬁrm partnerships. Information Systems Research ,18(3), 320",
        "start_idx": 10788,
        "end_idx": 10916
      },
      {
        "text": "and relational value in inter ﬁrm partnerships. Information Systems Research ,18(3), 320 –339. SAS (2013). Big Data analytics. An assessment of demand for labour and skills, 2012 –2017 . Schilke, O. (2014). On the contingent value of dynamic capabilities for competitive advan- tage: The nonlinear moderating effect of environmental dynamism. Strategic Management Journal ,35(2), 179 –203. Schryen, G. (2013). Revisiting IS business value research: What we already know, what we still need to know, and how we can get there. European Journal of Information Sys- tems,22(2), 139 –169. Setia, P., Richardson, V., & Smith, R. J. (2015). Business value of partner's IT intensity: Value co-creation and appropriation between customers and suppliers. Electronic Markets ,1–16. Shanks, G., & Bekmamedova, N. (2013). Creating value with business analytics in the sup-",
        "start_idx": 10904,
        "end_idx": 11032
      },
      {
        "text": "& Bekmamedova, N. (2013). Creating value with business analytics in the sup- ply chain. European Conference of Information Systems. Utrecht: European Conference on Information Systems . Shanks, G., & Sharma, R. (2011). Creating value from business analytics systems: The im- pact of strategy. 15th Paci ﬁc Asia Conference on Information Systems: Quality Research in Paci ﬁc, PACIS 2011 (pp. 1 –12). Queensland: Queensland University of Technology. Sharma, R., Mithas, S., & Kankanhalli, A. (2014). Transforming decision-making processes: A research agenda for understanding the impact of business analytics on organisa- tions. European Journal of Information Systems ,23(4), 433 –441. S h e r ,P .J . ,&L e e ,V .C .( 2 0 0 4 ) . Information technology as a facilitator for enhancing dynamic capabilities through knowledge",
        "start_idx": 11020,
        "end_idx": 11148
      },
      {
        "text": ". Information technology as a facilitator for enhancing dynamic capabilities through knowledge management. Information & Management , 41(8), 933 –945. Soh, C., & Markus, M. L. (1995). How IT creates business value: A process theory synthesis. International Conference of Information Systems . ICIS Proceedings. Tallon, P. P. (2007). A process-oriented perspective on the alignment of information technology and business strategy. Journal of Management Information Systems , 24(3), 227 –268. Tallon, P. P., & Pinsonneault, A. (2011). Competing perspectives on the link between stra- tegic information technology alignment and organizational agility: Insights from a mediation model. MIS Quarterly , 35(2). Teece, D. J. (2007). Explicating dynamic capabilities: The nature and microfoundations of (sustainable) enterprise performance. Strategic Management Journal ,28(13), 1319 –1350. Teece, D., Peteraf, M. A., & Leih, S.",
        "start_idx": 11136,
        "end_idx": 11264
      },
      {
        "text": "Journal ,28(13), 1319 –1350. Teece, D., Peteraf, M. A., & Leih, S. (2016). Dynamic capabilities and organizational agility: Risk, uncertainty and entrepreneurial management in the innovation economy. Un- certainty and Entrepreneurial Management in the Innovation Economy (April 7, 2016) . Teece, D. J., Pisano, G., & Shuen, A. (1997). Dynamic capabilities and strategic manage- ment. Strategic Management Journal ,18(7), 509 –533. Volberda, H. W. (1996). Toward the ﬂexible form: How to remain vital in hypercompet- itive environments. Organization Science ,7(4), 359 –374. Wade, M., & Hulland, J. (2004). Review: The resource-based view and information sys- tems research: Review, extension, and suggestions for future research. MIS Quarterly ,28(1), 107 –142.389 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390 [Página 12] Wang, C. L., &",
        "start_idx": 11252,
        "end_idx": 11380
      },
      {
        "text": "Business Research 70 (2017) 379 –390 [Página 12] Wang, C. L., & Ahmed, P. K. (2007). Dynamic capabilities: A review and research agenda. International Journal of Management Reviews ,9(1), 31 –51. Wang, E., Klein, G., & Jiang, J. J. (2007). IT support in manufacturing ﬁrms for a knowledge management dynamic capability link to performance. International Journal of Production Research ,45(11), 2419 –2434. Weill, P., Subramani, M., & Broadbent, M. (2002). Building IT infrastructure for strategic agility. MIT Sloan Management Review ,44(1), 57. Wu, L. -Y. (2006). Resources, dynamic capabilities and performance in a dynamic envi- ronment: Perceptions in Taiwanese IT enterprises. Information & Management , 43(4), 447 –454. Xu, Z., Frankwick, G. L., & Ramirez, E. (2016). Effects of big data analytics and traditional marketing analytics on new",
        "start_idx": 11368,
        "end_idx": 11496
      },
      {
        "text": "(2016). Effects of big data analytics and traditional marketing analytics on new product success: A knowledge fusion perspective. Journal of Business Research ,69(5), 1562 –1566.Zheng, S., Zhang, W., & Du, J. (2011). Knowledge-based dynamic capabilities and innova- tion in networked environments. Journal of Knowledge Management ,15(6), 1035 –1051. Zhou, K. Z., & Wu, F. (2010). Technological capability, strategic ﬂexibility, and product in- novation. Strategic Management Journal ,31(5), 547 –561. Zhu, K., & Kraemer, K. (2005). Post-adoption variations in usage and value of e-business by organizations: Cross-country evidence from the retail industry. Information Systems Research ,16(1), 61 –84. Zollo, M., & Winter, S. G. (2002). Deliberate learning and the evolution of dynamic capabilities. Organization Science ,13(3), 339 –351. Zott, C. (2003). Dynamic capabilities and the emergence of intraindustry differential",
        "start_idx": 11484,
        "end_idx": 11612
      },
      {
        "text": "–351. Zott, C. (2003). Dynamic capabilities and the emergence of intraindustry differential ﬁrm performance: Insights from a simulation study. Strategic Management Journal ,24(2), 97–125.390 N. Côrte-Real et al. / Journal of Business Research 70 (2017) 379 –390",
        "start_idx": 11600,
        "end_idx": 11637
      }
    ],
    "fc9e80fa-1776-4165-b47f-a112395ab0c0": [
      {
        "text": "[Página 1] Annals of Operations Research (2023) 328:1073–1103 https://doi.org/10.1007/s10479-022-04955-2 ORIGINAL RESEARCH Big data analytics and the effects of government restrictions and prohibitions in the COVID-19 pandemic on emergency department sustainable operations Görkem Sariyer1·Mustafa Gokalp Ataman2·Sachin Kumar Mangla3· Yigit Kazancoglu4·Manoj Dora5 Accepted: 29 August 2022 / Published online: 15 September 2022 © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2022 Abstract Grounded in dynamic capabilities, this study mainly aims to model emergency departments’(EDs) sustainable operations in the current situation caused by the COVID-19 pandemic byusing emerging big data analytics (BDA) technologies. Since government may impose somerestrictions and prohibitions in coping with emergencies to protect the functioning of EDs,it also aims to investigate how such policies affect ED operations. The proposed model isdesigned",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "to investigate how such policies affect ED operations. The proposed model isdesigned by collecting big data from multiple sources and implementing BDA to transformit into action for providing efﬁcient responses to emergencies. The model is validated inmodeling the daily number of patients, the average daily length of stay (LOS), and dailynumbers of laboratory tests and radiologic imaging tests ordered. It is applied in a case studyrepresenting a large-scale ED. The data set covers a seven-month period which collectivelymeans the periods before COVID-19 and during COVID-19, and includes data from 238,152patients. Comparing statistics on daily patient volumes, average LOS, and resource usage,both before and during the COVID-19 pandemic, we found that patient characteristics anddemographics changed in COVID-19. While 18.92% and 27.22% of the patients requiredlaboratory and radiologic imaging tests",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "While 18.92% and 27.22% of the patients requiredlaboratory and radiologic imaging tests before-COVID-19 study period, these percentageswere increased to 31.52% and 39.46% during-COVID-19 study period. By analyzing theeffects of policy-based variables in the model, we concluded that policies might cause sharpdecreases in patient volumes. While the total number of patients arriving before-COVID-19was 158,347, it decreased to 79,805 during-COVID-19. On the other hand, while the averagedaily LOS was 117.53 min before-COVID-19, this value was calculated to be 165,03 min B Yigit Kazancoglu yigit.kazancoglu@yasar.edu.tr 1Yasar University, Department of Business Administration, ˙Izmir, Turkey 2Bakırçay University Çi˘ gli Region Training and Research Hospital, Department of Emergency Medicine, ˙Izmir, Turkey 3Digital Circular Economy for Sustainbale Development Goals (DCE-SDG), Jindal Global Business School, O P Jindal Global University, Haryana, India 4Yasar University, Department of",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "School, O P Jindal Global University, Haryana, India 4Yasar University, Department of Logistics Management, ˙Izmir, Turkey 5Sustainable Production and Consumption School of Management Anglia Ruskin University, Cambridge, UK 123 [Página 2] 1074 Annals of Operations Research (2023) 328:1073–1103 during-COVID-19 study period. We ﬁnally showed that the model had a prediction accuracy of between 80 to 95%. While proposing an efﬁcient model for sustainable operations manage-ment in EDs for dynamically changing environments caused by emergencies, it empiricallyinvestigates the impact of different policies on ED operations. Keywords Big data analytics ·Emergency department ·COVID-19 ·Machine learning · Sustainable operations 1 Introduction Medical scientists and sociologists have widely researched the effects of the COVID-19pandemic on human physical and psychological health. Its impacts on operations and supplychain management have gained signiﬁcant attention from",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "Its impacts on operations and supplychain management have gained signiﬁcant attention from scholars (Choi, 2021 ; Queiroz et al., 2020 ;S a r k i s , 2021 ) and industry experts (Deloitte, 2020 ; Harvard Business Review, 2020 ). However, although the COVID-19 pandemic has affected operations and supply chains ona large scale and most the companies have faced disruptions (Fortune, 2020 ) since it has also created emergency situations in many countries, its impact on health services is a highpriority and needs to be addressed. Efﬁcient and timely service delivery is a signiﬁcant burden for health services, and the importance of providing rapid responses increases in emergencies. However, as experiencedduring the COVID-19 pandemic, this is very challenging, particularly for EDs, which areincreasingly used as gateways to",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "is very challenging, particularly for EDs, which areincreasingly used as gateways to hospital admissions and have been identiﬁed as one ofthe most overcrowded health services units. Besides, since most countries provide a 7/24ED service, non-urgent patients frequently occupy them, which has also been identiﬁed asan essential issue leading to increased overcrowding (Ataman & Sariyer, 2021 ). While the problem of overcrowding in EDs is a major challenge for the service providers even in regulartimes (Sariyer & Ataman, 2020 ), pandemic environments push these services into bottlenecks since the number of patients being infected increases uncontrollably. In addition to this sharpincrease in patient volumes, the proﬁles and demographics of patient admissions to hospitalEDs also vary signiﬁcantly. Under these circumstances, to protect the functioning of healthservices and EDs, governments are",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "these circumstances, to protect the functioning of healthservices and EDs, governments are forced to impose widespread restrictions and prohibitions.To cope with the COVID-19 pandemic, the leaders of many countries declared sudden orphased lockdowns and quarantines and the closure of physical shops and businesses, transportbans, etc. Although these may help the functioning of EDs under emergencies and cause asudden decrease in patient volumes, it is crucial for ED service providers to rapidly adapt thesystem in response to such changes and be able to manage operations efﬁciently in highlydynamic conditions (Alinaghian & Goli, 2017 ; Hossain et al., 2021 ; Mondal & Roy, 2021 ; Thakur et al., 2021 ). Thus, not only but especially under emergencies, EDs must have strong dynamic capabilities to manage these uncertain and dynamically changing",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "must have strong dynamic capabilities to manage these uncertain and dynamically changing environments. These huge patient volumes and the extensive range of patient characteristics also create large volumes of data for EDs. Thus, these health services are additionally challenged bya ubiquitous context of big data, which has appeared as an exciting frontier of productivityand opportunity (Sanders & Ganeshan, 2018 ). In this era, data is also identiﬁed as a valuable asset of EDs, enabling insights and decision making (Feng & Shanthikumar, 2018 ). However, big data requires the ability to process and arrange it to be used in decision-making. Thus,although the collected data is precious for EDs, unless they can analyze it and transform itinto useful information that can be turned into rapid action, it cannot go beyond",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "information that can be turned into rapid action, it cannot go beyond useless data 123 [Página 3] Annals of Operations Research (2023) 328:1073–1103 1075 recording that simply takes up storage capacity. At this point, BDA becomes increasingly crucial for EDs in making efﬁcient and timely decisions in emergency situations. The term ’BDA’ is used to refer to the techniques, technologies, systems, practices, methodologies, and applications for analyzing big data sets and is deﬁned as a holistic processof collecting, managing, and investigating the ﬁve major dimensions of data: volume, variety,velocity, veracity, and value (Wamba et al., 2017 ). BDA can support operational and strategic decision-making and turn to action in value creation for all organizational levels and enhanceoperational performance. BDA technologies have been implemented for various operationsand supply chain",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "enhanceoperational performance. BDA technologies have been implemented for various operationsand supply chain practices based on their superior performances (Gupta et al., 2021 ;K u m a r et al., 2016 ,2020 ; Mari´ ce ta l . , 2021 ;M i s h r ae ta l . , 2018 ). In the big data era, BDA can be viewed as an organizational capability for EDs to cope with dynamically changing situa-tions. Thus, besides having strong dynamic capabilities, if an ED holds BDA capabilities tomanage big data, it should respond more actively to emergencies, increasing its efﬁciencyand performance in managing operations. Moreover, big data and BDA implementations inreal-time systems will have great importance in providing sustainable ED operations (Daset al., 2021 ;G o l ie ta l .",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "ED operations (Daset al., 2021 ;G o l ie ta l . , 2019 ,2021 ; Midya et al., 2021 ; Mondal & Roy, 2022 ). Having such capabilities and advantages, BDA has attracted researchers, decision, and policymakers incoping with COVID-19 as a current global emergency (Abdel-Basset et al., 2021 ; Bag et al., 2021 ; Huang et al., 2020 ; Kapoor et al., 2021 ; Lee & Trimi, 2021 ; Mondal & Roy, 2021 ; Papadopoulos et al., 2020 ; Sharma et al., 2020 ; Sözen et al., 2022 ; Tirkolaee et al., 2022 ). Although these technologies are popular in the COVID-19 context, they have little use in the ED operations decision-making processes in this pandemic period. On the other hand,since EDs are the main actors",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "this pandemic period. On the other hand,since EDs are the main actors of health services in managing emergency environments,taking advantage of these technologies to improve EDs’ operations is critical in effectivelymanaging emergencies. Besides, since governmental reactions in ﬁghting COVID-19 havecaused sharp and signiﬁcant changes in the demand for EDs, investigating the effects of theseactions in EDs operations and putting these effects into account in decision-making modelsis another unique point. Therefore, this study aims to present a model implementing BDAtechnologies for managing four primary ED operations in COVID-19. By conducting inter-views with ED service providers and searching the related literature, the primary operationsthat are challenging for ED services in emergencies and even in regular times are deter-mined as managing daily patient volumes, average stay lengths of patients, and utilizationof",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "as managing daily patient volumes, average stay lengths of patients, and utilizationof laboratory radiologic imaging services. Besides proposing a generic model for managingED operations under emergencies and validating this model for different processes of EDs,taking the governmental actions as the main factors of this model and thus showing how theyaffect these operations is the novelty of this paper. Hence, we aim to answer the followingresearch questions in this paper: RQ1. How does BDA assist in making effective decisions for predicting daily patient volumes, average stay lengths of patients, and resource utilization of EDs under dynam-ically changing conditions caused by emergencies?RQ2. How do government-imposed restrictions and prohibitions affect daily patientvolumes, average stay lengths, and ED resource utilization of EDs in emergencies? Since the current emergency having worldwide effects is",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "of EDs in emergencies? Since the current emergency having worldwide effects is the COVID-19 pandemic, we focus on modeling ED operations during COVID-19 and identify the restrictions and prohi-bitions imposed to cope with this pandemic. To address these research questions, we proposea BDA-driven model and implement machine learning techniques as one of the most potentsub-set of BDA. More speciﬁcally, we implement neural networks-based techniques and mul-tilayer perceptron (MLP) algorithms to develop required predictions on daily patient volumes, 123 [Página 4] 1076 Annals of Operations Research (2023) 328:1073–1103 average stay lengths, and daily utilization of laboratory and imaging services of EDs. In vali- dating this model in different ED operations, we deﬁne the output variables for each operationas previously stated and identify two sets of factors (input variables). While",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "operationas previously stated and identify two sets of factors (input variables). While in the ﬁrst set, weidentify possible operation-speciﬁc factors that may affect the output variable of this oper-ation. We deﬁne additional elements representing different types of government restrictionsand prohibitions in the second set. These factors are similarly used for each operation. Withthe proposed model and implemented MLP algorithm by obtaining 80% to 95% accuraciesfor predicting the output values of four ED operations, we answered the RQ1 of this studysince such accurate predictions play a crucial role in making efﬁcient decisions EDs underemergencies. By investigating the signiﬁcance of the relations between the output variablesand the set of input factors representing the government-imposed restrictions and prohibitionsand analyzing the directions of these relations, we answered the RQ2 of this study.",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "the directions of these relations, we answered the RQ2 of this study. The organization of this paper is as follows. In Sect. 2, we discuss the theoretical back- ground of this paper. We present the proposed model in Sect. 3and introduce the case study, and data set characteristics, data pre-processing steps, and results of the proposed model inSect. 4. Section 5discusses the ﬁndings of this study. We present the theoretical, managerial, and policy implications in Sect. 6. Section 7offers concluding remarks, limitations of this study, and the future research directions. 2 Theoretical background 2.1 The dynamic capabilities view Dynamic capabilities deﬁne an organization’s ability to innovate, adapt to change, andimprove in a good way for its customers (Teece et al., 2016 ). Zollo and Winter ( 2002 ,",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "customers (Teece et al., 2016 ). Zollo and Winter ( 2002 , p. 340) deﬁned dynamic capability as a \"learned and stable pattern of collective activitythrough which the organization systematically generates and modiﬁes its operating routinesto pursue improved effectiveness.\" The dynamic capabilities utilize an organization’s internal and external resources in the best possible manner to respond appropriately to environmental uncertainties (Teece et al.,1997 ). Emergencies cause environmental or external uncertainties, and managing opera- tions in EDs, particularly under emergencies, requires real-time information whereby serviceproviders can arrive at critical decisions. The dynamic capabilities help integrate primaryresources through the availability of this information and then further help to modify ED oper-ating routines and procedures appropriately. Therefore, we based our research on the dynamiccapability view. Positioning the resources correctly is the",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "our research on the dynamiccapability view. Positioning the resources correctly is the prime requisite for coping with theseuncertainties and the chaotic environments related to emergencies. Dynamic capabilities arethe main processes for sensing, integrating, learning, and reconﬁguring resources and capa-bilities (Birkinshaw et al., 2016 ) and stress an organization’s capacity to create, extend or modify its resources purposefully. These are also crucial in managing ED operations, par-ticularly in emergencies, since aligning the capabilities and resources and reconﬁguring theprocesses may help dynamically deal with changing patient volumes and proﬁles. To dealwith unexpected increases in patient volumes in COVID-19, many countries reconﬁguredtheir health systems, so pandemic services were opened to provide patients. The resourcesand capacities of these services, such as doctors, nurses, and other health staff, required med-ical equipment (medicines, beds,",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "as doctors, nurses, and other health staff, required med-ical equipment (medicines, beds, intensive care units, respiratory devices), were provided bymany different hospital departments and mainly from the EDs. In some countries where pan-demic services were not opened, EDs served as these services and encountered COVID-19 123 [Página 5] Annals of Operations Research (2023) 328:1073–1103 1077 patients. For such countries, the increased need for medical staff and resources was satisﬁed by reconﬁguring the hospital’s other services and aligning them with the pandemic services. In the health services operations and supply chain management literature, many stud- ies base their theoretical backgrounds on the dynamic capability perspective (Rubbio et al.,2020 ). In the era of big data, health systems are one of the primary services that deal with big data sets",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "are one of the primary services that deal with big data sets of the high volume, variety, and velocity of patient data. Thus, we move furthertowards BDA capability (BDAC), which has evolved from the dynamic capability perspec-tive. We, therefore, highlight the importance of having BDAC for managing health servicesoperations, particularly in emergencies. 2.2 Big data analytics capability During the COVID-19 pandemic, BDA has been used to detect surface indicators related tothe pandemic (Guo et al., 2020 ). Real-time big data-driven insights have helped scholars and decision-makers to comprehend the impact of this pandemic. COVID-19 trackers provide anessential source of data to help scholars research and make more informed decisions on copingwith this pandemic by collecting and aggregating big data (Verma & Gustafsson, 2020 ). Such situations increase the",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "aggregating big data (Verma & Gustafsson, 2020 ). Such situations increase the volume and the variety of patients’ characteristics in health services.Besides, many external factors may come into play, changing the system dynamics. Undersuch circumstances, it is necessary for health services providers to rapidly adapt the systemto the changing conditions to provide timely and effective services to patients. Thus, the roleof BDAC in healthcare operations gained increased attention (Yu et al., 2021 ). We propose a system for managing ED operations, such as forecasting patient volumes, analyzing patient LOS, and modeling the use of primary resources in emergencies. Even inregular times, the main challenge faced by ED service providers is the overcrowded environ-ment of these services, which creates vast volumes and varieties of patients. An emergencyis an external",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "which creates vast volumes and varieties of patients. An emergencyis an external challenge that may cause an unexpected and sharp increase in patient volumesand varieties, thus straining the system and making managing operations much more difﬁcult.Government is a prominent actor as a system enabler in this era. To protect the functioningof these services and respond to emergencies, governments impose some policies, such asrestrictions and prohibitions, which may cause a sudden decrease in patient volumes but stillchange the characteristics and increase the system’s randomness. All these create dynami-cally changing environments, and the service providers must adopt the system appropriatelyand effectively in response to these rapidly changing conditions. Since by their nature anddue to all these sudden changes, ED services include a huge volume, variety, velocity, andveracity of data, these",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "ED services include a huge volume, variety, velocity, andveracity of data, these services may take advantage of BDA to help operations cope withsuch rapid changes in the system. We summarise the theoretical framework of our researchin Fig. 1. As seen in Fig. 1, based on huge volumes, velocities, and varieties of patients, the data inherent in the EDs exhibits a dynamic feature. Since emergencies are also featuredwith rapidly changing conditions, these increase the randomness in the EDs and, therefore,stalemate decision-making processes in EDs. This study attempts to contribute to dynamiccapability theory and BDAC by extending their usage for the decision-making processes ofone of the most important actors of health services, EDs, under emergencies. By presentingthe rapidly changing features of the EDs in emergencies and presenting a model highlightinga",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "changing features of the EDs in emergencies and presenting a model highlightinga need for BDAC, this study aims to contribute to the context of these theories. 123 [Página 6] 1078 Annals of Operations Research (2023) 328:1073–1103 Fig. 1 Theoretical framework of this research 3 Proposed models In this paper, we propose models for managing the primary operations of EDs, particu- larly in emergencies. These models include ﬁve main sequential steps: Data Collection,Pre-processing, Modelling, Testing & Model Evaluation, and Providing Managerial & Pol-icy Implications. As discussed earlier, ED environments contain big data sets that can beprocessed with BDA, and valuable information can be obtained in decision-making. Thus,an essential initial step for adapting these emerging technologies into proposed models andsystems is bringing data sets related to the context. A data",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "models andsystems is bringing data sets related to the context. A data set can be obtained using differentsources within this research framework. To get the related data of the proposed models, werequired data triangulation. Valuable data sets for the proposed models are secondary datareceived from a case ED covering the period before and during COVID-19; governmentreports; documentary analysis; and interviews with ED service providers. Case study datamay include relevant information about patients arriving at this ED during the study period.Government reports and documentary analyses should be checked to identify the types ofrestrictions and prohibitions imposed by the government to cope with the emergency. Finally,interviews and documents should be used to decide on the main challenges to ED operations,making planning and managing operations more difﬁcult in emergencies. Related metrics",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "ED operations,making planning and managing operations more difﬁcult in emergencies. Related metrics andtargeted values of these metrics can also be identiﬁed by collecting data through interviewsand a literature search. Since the collected data is raw data, which in its current form is not suitable for analyz- ing and modeling, different data pre-processing tasks must be performed. It is necessary todeﬁne the input and output variables of the model, deﬁne the periodicity (hourly, daily, weekly,monthly, etc.) of the analysis, and determine ways to measure the values of the variables. Datatransformation may also involve measuring the values of the variables. One of the main pre-processing tasks in big data studies is cleaning the data set to remove redundant or inappropri-ate data, missing values, and outliers. After all these tasks have",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "or inappropri-ate data, missing values, and outliers. After all these tasks have been performed, the structureddata set, which can further be processed with BDA tools and techniques, is obtained. Once the structured data set of the model is ready, the modeling step comes next. The obtained data set is split into two train and test sets. Train data sets include the values ofall the input and output variables, whereas since the test data set will be used to evaluatethe model’s prediction accuracies, it does not include the values of the output variables. The 123 [Página 7] Annals of Operations Research (2023) 328:1073–1103 1079 train data set is further processed with machine learning as one of the most widely used BDA techniques. Machine learning presents algorithms to extract knowledge",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "most widely used BDA techniques. Machine learning presents algorithms to extract knowledge and make efﬁcientdecisions by learning from given data sets. Researchers widely prefer these algorithms basedon their ﬂexibility in using data to capture complex and non-linear behaviors (Choi et al.,2018 ). Among various machine learning algorithms, MLP neural networks have received signiﬁcant attention since these are appropriate and efﬁcient for function approximation,pattern classiﬁcation, and prediction. Incorporating hidden layers between input and outputlayers is one of the other parser properties of these algorithms. When required by extendingthe number of hidden layers, MLP neural networks can expand the number of input featurecombinations to improve the model’s learning ability, ﬁnally increasing the prediction power.Although many other BDA techniques have been widely implemented in the literature, themachine learning-based MLP neural network",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "have been widely implemented in the literature, themachine learning-based MLP neural network algorithm is integrated into the proposed modelbased on these properties and superiorities. The testing and model evaluation step comes next in the proposed model. The obtained MLP algorithm with the optimized parameters is applied to the test data set to get the predictedvalues of the output variables of interest. The predicted values are then compared with theactual values, and the mean errors and accuracies of the prediction should be calculated. Theseperformances should then be compared with the target values. If the targets are achieved orthe model performance goes beyond the targeted one, the model can be proposed for real-life applications. The results on the signiﬁcance and impacts of government restrictions andprohibitions may also be discussed in",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "signiﬁcance and impacts of government restrictions andprohibitions may also be discussed in detail, and implications should be recommended topolicymakers. Suppose the model performance cannot achieve the targets. In that case, it isnecessary to go back to the data pre-processing step and re-deﬁne the model input and outputvariables. The modeling, testing, and evaluation steps must be repeated until proper modelshave been obtained. The proposed model is shown in Fig. 2. Fig. 2 Flowchart of the proposed model 123 [Página 8] 1080 Annals of Operations Research (2023) 328:1073–1103 4 Case study 4.1 Case study specification We collected the data set of this study from an ED of a research and training hospital located in a metropolitan region in Izmir, Turkey. The daily number of patients or visits to this EDis",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "Izmir, Turkey. The daily number of patients or visits to this EDis more than 1,000. This huge patient volume is due to several reasons. First, as mentionedpreviously, overcrowding is a common problem in EDs. Second, due to the vast volumes ofnon-urgent patient visits, this problem can be more severe in some countries, such as Turkey,compared to many other countries. Third, many patients may choose to be treated in thishospital due to its type. Fourth, since this is a public hospital, receiving service from EDs isfree of charge. Fifth, since it is located in a metropolitan region and is very close to publictransport stations and the city center, it is also easily accessible for ambulances. Sixth butnot least, since this ED provides uninterrupted service (7 days and 24 h)",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "least, since this ED provides uninterrupted service (7 days and 24 h) while many of theother departments of this hospital provide service only within working hours on weekdays,this causes additional visits of patients of different departments to EDs out of the workinghours. These characteristics created huge volumes, velocities, and varieties in the data set. In Turkey, the ﬁrst COVID-19 case was reported on March 10, 2020, in Istanbul city, and the virus then spread quickly to the whole country. In Turkey, the COVID-19 was encounteredlater than in many other countries. Thus, public awareness had already been created aboutthis virus and the pandemic. Public awareness was a crucial initial step in coping with thisvirus. Since it ﬁrst appeared in Turkey, the government started announcing policies like\"social distancing,\" \"hygiene,\" and",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "appeared in Turkey, the government started announcing policies like\"social distancing,\" \"hygiene,\" and \"stay at home.\" However, raising public awareness fromthe outset and making announcements was not enough to prevent the spread of the virus.Then, the government imposed other types of restrictions and prohibitions. Restrictions forthe elderly, inter-city transport bans and restrictions for the young were imposed startingfrom the end of March. In addition, starting from the middle of April, total curfews wereimposed at weekends (for two days) and for extended weekends in some of the weeks, whichcould last up to three or four days. The number of cases and deaths started to fall by May.Then the period of normalization began at the beginning of June. Although restrictions andprohibitions were still in use during this month, they were more",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "restrictions andprohibitions were still in use during this month, they were more relaxed. Having high volumes, velocities, and varieties in patient sizes and characteristics, the selected ED was identiﬁed as proper for this study’s theoretical framework and methodology.Besides, since in different periods (such as before March and during April) and days (suchas weekdays and weekends), government-imposed actions were highly changing during thestudy period, the case ED allowed to investigate the impact of these actions on ED operations. 4.2 Data set characteristics The data set covers seven months, from December 2019 to June 2020, and includes 238,152patients. Data from between March 10 to the end of June 2020 represents data collected duringthe period of COVID-19’s ﬁrst peak in Turkey. To have a similar number of days before theCOVID-19 period,",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "in Turkey. To have a similar number of days before theCOVID-19 period, the related data set was started in December 2019. Thus, before COVID-19 and during COVID-19 periods cover around 3.5 months of data. For each arriving patient,records of the ED case include the following information: patient ID, gender, age, arrivaltype, triage level, date of arrival, time of arrival, diagnostic tests for treatment-if required,related times for diagnostic tests, assigned diagnosis type by a doctor after treatment, andtime of departure. The patient ID is unique for each patient arrival. Gender is recorded as 123 [Página 9] Annals of Operations Research (2023) 328:1073–1103 1081 male and female. Age is recorded as it is in a continuous form. The arrival type represents if a patient arrived by themselves or by ambulance,",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "arrival type represents if a patient arrived by themselves or by ambulance, so it is recorded as one of two options:\"walk-in\" or \"by ambulance.\" When a patient comes to this ED, they are ﬁrst met by a triagenurse, who triages the patient based on his complaints and clinical acuities. This ED uses the3-level Emergency Severity Index for patient triage. Furthermore, trauma patients are treated in a different zone. Thus, arriving patients are assigned to one of four zones labeled green, yellow, red, and trauma zones. The arrivaldate represents the full date of the patient’s arrival in a day, month, and year form. Time ofarrival shows the exact time of arrival in an hour, minute, and second form. Many diagnostictests can be ordered in EDs for patient diagnosis. The",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "form. Many diagnostictests can be ordered in EDs for patient diagnosis. The label of the requested test, and therelated ordering time, approval time, and result time are recorded in the next three rows inan hour, minute, and second form. When doctors diagnose the patients, they assign the typeof diagnosis based on the International Classiﬁcation of Diagnosis 10th version (ICD-10).Thus, the diagnosis cell includes the diagnosis based on the ICD-10 codes, which can have22 different categories. The last cell consists of the departure time of the patient in an hour,minute, and second form. The data set includes additional attributes to represent government restrictions and prohi- bitions. The four main restrictions and prohibitions imposed in Izmir city are considered inthe proposed models. During the COVID-19 study period, total curfew (lockdowns),",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "considered inthe proposed models. During the COVID-19 study period, total curfew (lockdowns), curfewfor the young (age ≤20), curfew for the elderly (age ≥65), and transport bans were imposed. These are also adopted in the proposed models as model input variables, as discussed in thenext section on data pre-processing. As presented in Fig. 2, selecting the study variables is an important initial step of the proposed model. However, it should be kept in mind that these variables are not ﬁxed andrigid and may depend on the selected case studies. Different variables may deﬁne the system’sinternal and external dynamics for other cases. 4.3 Data pre-processing We implement the proposed model with four different ED operations to investigate how theimposed policies have changed and affected the primary operations and resource usage.",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "theimposed policies have changed and affected the primary operations and resource usage. Theﬁrst and second operations, Operation 1 and Operation 2, respectively predict the daily num-ber of patients arriving and the average LOS of these patients (LOS is deﬁned as the timebetween the patient’s arrival and their departure) for each day during-COVID-19 period.Different diagnostic tests can be mainly grouped into either laboratory tests or radiologicimaging tests. Thus, we also implement the model for two other operations to analyze theprimary resource usage. Operation 3 and Operation 4 predict daily numbers of ordered labo-ratory tests and radiologic imaging tests for diagnosing patients. Regarding output variablesor attributes of the model for each operation, these are deﬁned adequately as the daily numberof patients, average daily LOS of patients, the daily number of",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "daily numberof patients, average daily LOS of patients, the daily number of laboratory tests ordered, andthe daily number of radiologic imaging tests ordered during-COVID-19 period. Since the aim is to model and manage related daily values, the data set was initially trans- formed. In this process, we eliminated the repetitive values from the data set. More than oneICD-10 encoded diagnosis can be assigned to a patient. Different laboratory tests (hemogram,biochemistry, enzyme, hormone, etc.) or radiologic imaging tests (X-ray, tomography, ultra-sound, magnetic resonance imaging, etc.) can also be ordered for a patient with a unique ID.While obtaining the corresponding daily value of the models, we eliminated these repetitiveor redundant values. 123 [Página 10] 1082 Annals of Operations Research (2023) 328:1073–1103 Besides the policy-based attributes, some other input variables were",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "Research (2023) 328:1073–1103 Besides the policy-based attributes, some other input variables were also deﬁned to adopt the system characteristics in the proposed models. These variables were used to represent thesystem dynamics in normal circumstances. Previous studies showed that the day of the weekhas a signiﬁcant effect on patient volume and LOS (Sarıyer et al., 2020 ). Existing literature also presented that the patient volume, LOS, and numbers of diagnostic tests ordered differedsigniﬁcantly between categories of demographic variables (Sarıyer & Ataman, 2020 ). We, therefore, identiﬁed these factors as internal factors to represent the ED environment in normalcircumstances. To measure the values of these inputs, we used the study’s data set coveringthe before-COVID-19 period. As in output variables, we made the required transformationsto obtain the daily values of these",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "variables, we made the required transformationsto obtain the daily values of these input variables. The data set is described in Table 1. We performed data pre-processing by dropping missing values in the dataset by using the dropna() function of the pandas module in Python. After this, based on standardization, we removed the outliers from the data set by using the zscore() function of the pandas module Table 1 Deﬁnitions and measurement scales of the model variables Operation Deﬁned output variables (symbol, deﬁnition, scale)Operation-speciﬁc input variables representingsystem dynamics(symbol, deﬁnition,scale)Common input vari- ables(symbol, deﬁnition,scale) 1: Managing daily numbers of patientY1: The daily number of patients arriving eachd a yi nt h eduring-COVID-19study period(numerical)X1: The average daily number of patientsarriving for each day ofthe week—Mondaythrough to Sunday(numerical)Representing govern- ment restrictions andprohibitions",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "patientsarriving for each day ofthe week—Mondaythrough to Sunday(numerical)Representing govern- ment restrictions andprohibitions X2: The whole curfew exists in the day to bepredicted or not (cat-egorical) X3: Curfew for young exists in the day to bepredicted or not (cat-egorical) X4: Curfew for the elderly exists in theday to be predictedor not (categorical) X5: Transport ban exists in the day tobe predicted or not(binary) 2: Managing daily average LOS ofpatientsY2: Average daily LOS of patients arriving eachd a yi nt h eduring-COVID-19study period(numerical)X7-X8: average daily LOS of female-malepatients for each day ofthe week (numerical) X9-X10-X11: Average daily LOS of agegroups—[0–14],[15–64], ≥65—for each day of the week(numerical) 123 [Página 11] Annals of Operations Research (2023) 328:1073–1103 1083 Table 1 (continued) Operation Deﬁned output variables (symbol, deﬁnition, scale)Operation-speciﬁc input variables representingsystem dynamics(symbol,",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "(continued) Operation Deﬁned output variables (symbol, deﬁnition, scale)Operation-speciﬁc input variables representingsystem dynamics(symbol, deﬁnition,scale)Common input vari- ables(symbol, deﬁnition,scale) X12 through X15: Average daily LOS oftriage groups—red,yellow, green, traumazones—for each day of the week (numerical) X16 through X37: Average daily LOS ofICD-10 encodeddiagnosis, for 21 groups *, for each day of the week(numerical) 3: Managing daily numbers of ordered laboratory testsY3: The daily number of laboratory tests ordered in the during-COVID-19study period(numerical)X38-X39: Average daily numbers of laboratory tests ordered for female-male patientsfor each day of theweek (numerical) X40-X41-X42: Average daily numbers of laboratory testsordered for agegroups—[0–14],[15–64], ≥65—for each day of the week(numerical) X43-X44: Average daily numbers of laboratorytests ordered for arrivaltype groups—byambulance orwalk-in—for each dayof the week(numerical) X45 through X48: Average daily numbersof laboratory testsordered for triagegroups; red, yellow,green, trauma",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "through X48: Average daily numbersof laboratory testsordered for triagegroups; red, yellow,green, trauma zones,for each day of theweek (numerical) 123 [Página 12] 1084 Annals of Operations Research (2023) 328:1073–1103 Table 1 (continued) Operation Deﬁned output variables (symbol, deﬁnition, scale)Operation-speciﬁc input variables representingsystem dynamics(symbol, deﬁnition,scale)Common input vari- ables(symbol, deﬁnition,scale) X49 through X69: Average daily numbersof laboratory testsordered for ICD-10encoded diagnosis, for 21 groups *, for each day of the week(numerical)Representing system dynamics X1-fcast: Predicted daily number ofpatients with Model 1 on each day during-COVID-19study period(numerical) –used in2 nd,3rd,a n d4th operations modeling 4: Managing daily numbers of orderedradiologic imagingtestsY4: The daily number of radiologic imaging testsordered in theduring-COVID-19study period(numerical)X70-X71: Average daily numbers of radiologicimaging tests orderedfor female-malepatients for each day ofthe week (numerical) X72-X73-X74: Average daily numbers ofradiologic imagingtests ordered",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "each day ofthe week (numerical) X72-X73-X74: Average daily numbers ofradiologic imagingtests ordered for agegroups—[0–14],[15–64], ≥65—for each day of the week(numerical) X75-X76: Average daily numbers of radiologicimaging tests orderedfor arrival typegroups—by ambulanceor walk-in—for eachday of the week(numerical) X77 through X80: Average daily numbersof radiologic imagingtests ordered for triagegroups—red, yellow,green, traumazones—for each day ofthe week (numerical) X81 through X101: Average daily numbersof radiologic imagingtests ordered forICD-10 encoded diagnosis, for 21 groups *, for each day of the week(numerical) 123 [Página 13] Annals of Operations Research (2023) 328:1073–1103 1085 in Python. We initiated the categorical conversion of the input variables with the Categorical class initializer of the pandas module in Python. We used the Categorical class to encode numerical values as categorized by the capability of initializing the corresponding variableswith categorical",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "values as categorized by the capability of initializing the corresponding variableswith categorical values. After these pre-processing steps, we obtained the structured data setfor further modeling with the MLP neural network. As seen in Table 1, we identiﬁed the government policies as common input variables in each operation to analyze their effects on each of the deﬁned output variables for the correspondingoperations. However, once we predicted the daily number of patients in Operation 1, we usedthese predictions to describe system characteristics in all other models. The daily number ofpatients may affect the average daily LOS, and the number of each diagnostic test ordered. 5 Results 5.1 Descriptive results The study period covering the before-COVID-19 period included 100 days of data, andthe total number of patients arriving during these days",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "days of data, andthe total number of patients arriving during these days was 158,347. Laboratory tests wereordered for 29,953 of these patients and 43,106 radiologic imaging tests. On the other hand,the study period covering the during-COVID-19 period included 113 days of data, and thetotal number of patients arriving during these days was 79,805. The number of laboratoryand radiologic imaging tests ordered during this period was 25,154 and 31,488. The averagedaily LOS was 117.53 min in the before-COVID-19 period and 165,03 min in the during-COVID-19 period. Daily values for the number of patients, average LOS, and numbers ofeach type of diagnostic test ordered in the whole study period are depicted in Fig. 3. These results show that while daily and total numbers of patients and diagnostic tests ordered sharply",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "while daily and total numbers of patients and diagnostic tests ordered sharply decreased, average LOS values increased during the during-COVID-19period compared to before-COVID-19. However, although decreases are seen in three ofthe operations’ output variables (1, 3, 4), the sharpest decline was seen in Operation 1’s out-put, the daily number of patients. The decrease in patient numbers may have also caused thedecline in the number of tests ordered. On the other hand, it should be noted that, althoughpatient and diagnostic test numbers decreased, average LOS values increased. All these criti-cal numerical ﬁndings could be due to the change in the system dynamics, which were mainlycaused by patients who occupied EDs unnecessarily and did not need an emergency service. We categorized the patients into three groups to support this idea",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "service. We categorized the patients into three groups to support this idea by numerical ﬁndings consistent with our model boundaries and comparatively presented the related statistics for Fig. 3 Daily values of the models’ output variables in the study period 123 [Página 14] 1086 Annals of Operations Research (2023) 328:1073–1103 each of these. These categories were: patients requiring no diagnostic tests, laboratory tests, and radiologic imaging tests. Since diagnostic tests are one of the most critical resources fordiagnosing patients, we believe most patients for whom no tests are ordered can representthe cases that occupy EDs for non-urgent conditions. For these categories, the average daily numbers of patients and their average LOS are shown for each day of the week before-COVID-19 and during-COVID-19 periods in Fig. 4. Figure 4shows",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "of the week before-COVID-19 and during-COVID-19 periods in Fig. 4. Figure 4shows that while average daily values for patient numbers decreased in each of the three categories in the during-COVID-19 period compared to the before-COVID-19 period,the majority of the decrease is related to the category of patients requiring no diagnostic test.Although it is worth noting that reductions were seen in the number of patients requiring nodiagnostic test, some increases were seen in their average LOS values in the during-COVID-19 period. This ﬁnding mainly supports our hypothesis. On the other hand, at least some Fig. 4 Daily average patient numbers and LOS values for each day of the week 123 [Página 15] Annals of Operations Research (2023) 328:1073–1103 1087 decreased levels were observed in the average LOS values of",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "328:1073–1103 1087 decreased levels were observed in the average LOS values of patients requiring diagnostic tests during the pandemic period. This could be due to the decreases in resource utilization.When resource utilization decreases, it accelerates access to resources and enables moreefﬁcient use. Based on the daily distributions of patient numbers, one other ﬁnding should benoted. In the patients requiring no diagnostic test category, while Saturdays and Sundays, thatis, the weekend, had the highest daily patient numbers compared to weekdays in the before-COVID-19 period, daily numbers were the highest on Mondays in the during-COVID-19period. The impact of government restrictions and prohibitions on ED operations is directlyseen in this ﬁnding. Since most of the weekends, total curfews were imposed during thisperiod, patient volume, particularly in the patients requiring no diagnostic",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "imposed during thisperiod, patient volume, particularly in the patients requiring no diagnostic test category,sharply decreased at weekends. Table 2shows the total number of patients arriving at this ED based on the categories of the considered demographics (gender, age, triage, arrival types, diagnosis) for the before-and during-COVID-19 study periods comparatively. From the values of Table 2, it should be seen that the distribution of patient numbers based on gender changed in the during-COVID-19 period compared to the before-COVID-19period, as the number of male patients increased. Differences were also depicted based on agedistributions. For each of the three categories, in the young group, age:[0–14], patient numbersand distributions sharply decreased in the during-COVID-19 period, and in the elderly group,age≥65. In contrast, distributions fell in the patients requiring diagnostic tests category overall.",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "In contrast, distributions fell in the patients requiring diagnostic tests category overall. There was some increase in this age category. Additionally, for all three types,the distribution of patients arriving by ambulance increased in the during-COVID-19 studyperiod. Another important ﬁnding showed that, while distributions of green zone patientssigniﬁcantly decreased in the patients requiring no diagnostic test category, the distributionof green zone patients increased in some other categories. Finally, signiﬁcant differenceswere observed between 22 different ICD-10 encoded diagnosis types on the distributions ofthe four main groups. These ICD-10 codes were J00-J99 (disease of the respiratory system),M00-M99 (disease of musculoskeletal system and connective tissue), R00-R99 (symptoms,signs, and abnormal clinical and laboratory ﬁndings, not elsewhere classiﬁed), and U00-U85(codes for special purposes, COVID-19 here). The signiﬁcant differences in the distributionsof these diagnosis types",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "purposes, COVID-19 here). The signiﬁcant differences in the distributionsof these diagnosis types are associated with the COVID-19 pandemic and the season. 5.2 Model results The proposed model was implemented in the obtained data sets of the corresponding casestudy. Since we focus on four primary ED operations, the model was tested repetitively fourtimes for Operations 1 through 4, which increased the model’s validity. In this section, the relation between the identiﬁed input variables and the corresponding output variables for each ED operation of interest will be presented based on the results ofthe Pearson correlation analysis. The statistical association between the model variables ispresented in a heat-map structure in the Appendix for each operation. In Table 3,w es h o w e d the direction, magnitude, and signiﬁcance level of",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "h o w e d the direction, magnitude, and signiﬁcance level of the relationships, notably the signiﬁcantinput variables of the model for each operation. From the values of Table 3, it is observed that the deﬁned input variables of Operation 1, X1 through X5, were all signiﬁcantly related to the output variable Y1. Besides, therelations were in a negative direction. This demonstrates how policy-based restrictions andprohibitions reduce the predicted number of daily patients in the during-COVID-19 period.Nonetheless, while it is observed that the system dynamics related to input variable X1 had a 123 [Página 16] 1088 Annals of Operations Research (2023) 328:1073–1103 Table 2 Distributions of each patient demographic variable for three categories in the before- and during- COVID-19 periods Variable Levels Patients requiring no diagnostic testPatients requiring",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "and during- COVID-19 periods Variable Levels Patients requiring no diagnostic testPatients requiring laboratory testsPatients requiring radiology tests Before During Before During Before During n (%) n (%) n (%) n (%) n (%) n (%) Gender Female 47,670 (47.742)14,784 (42.444)16,636 (55.540)12,407 (49.324)21,894 (51.177)14,899 (47.316) Male 52,179 (52.258)20,048 (57.556)13,317 (44.460)12,747 (50.676)20,887 (48.823)16,589 (52.684) Age age: [0–14] 20,722 (20.753)3,683 (10.574)4,726 (15.778)1,715 (6.818)7,991 (18.679)2,951 (9.372) age: (15–64)70,980 (71.087)27,538 (79.059)17,730 (59.193)18,310 (72.792)26,814 (62.677)23,309 (74.025) age≥65 8,147 (8.159)3,611 (10.367)7,497 (25.029)5,129 (20.390)7,976 (18.644)5,228 (16.603) Triage levelgreen room 68,335 (68.438)12,122 (34.801)2,624 (8.760)6,490 (25.801)7,746 (18.106)7,279 (23.117) yellow room23,212 (23.247)14,888 (42.742)20,542 (68.581)12,037 (47.853)21,406 (50.036)12,280 (38.999) red room 2,313 (2.316)2,076 (5.960)5,904 (19.711)5,737 (22.808)4,833 (11.297)4,950 (15.720) trauma room5,989 (5.998)4,904 (14.079)883 (2.948)890 (3.538)8,796 (20.561)6,979 (22.164) Arrival typewalk in 98,553 (98.702)33,148 (95.165)24,508 (81.822)19,224 (76.425)37,374 (87.361)25,642 (81.434) by ambu- lance1,296 (1.298)1,684 (4.835)5,445",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "98,553 (98.702)33,148 (95.165)24,508 (81.822)19,224 (76.425)37,374 (87.361)25,642 (81.434) by ambu- lance1,296 (1.298)1,684 (4.835)5,445 (18.178)5,930 (23.575)5,407 (12.639)5,846 (18.566) ICD-10 encodeddiagno-sisA00-B99 3,095 (3.100)755 (2.168)241 (0.805)193 (0.767)156 (0.365)96 (0.305) C00-D49 32 (0.032)24 (0.069)49 (0.164)31 (0.123)43 (0.101)24 (0.076) D50-D89 135 (0.135)139 (0.399)75 (0.250)88 (0.350)37 (0.086)51 (0.162) E00-E89 108 (0.108)122 (0.350)131 (0.437)117 (0.465)74 (0.173)81 (0.257) F01-F99 696 (0.697)515 (1.479)223 (0.744)183 (0.728)132 (0.309) 124 (0.394) G00-G99 1,211 (1.213)540 (1.550)335 (1.118)221 (0.879)415 (0.970)277 (0.880) H00-H59 646 (0.647)453 (1.301)10 (0.033)7 (0.028) 12 (0.028)6 (0.019) 123 [Página 17] Annals of Operations Research (2023) 328:1073–1103 1089 Table 2 (continued) Variable Levels Patients requiring no diagnostic testPatients requiring laboratory testsPatients requiring radiology tests Before During Before During Before During n (%) n (%) n (%) n (%) n (%) n (%) H60-H95 1,541 (1.543)576 (1.654)61 (0.204)36 (0.143)63 (0.147)46 (0.146) I00-I99",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "(%) n (%) H60-H95 1,541 (1.543)576 (1.654)61 (0.204)36 (0.143)63 (0.147)46 (0.146) I00-I99 1,113 (1.115)730 (2.096)1,192 (3.980)857 (3.407)959 (2.242)715 (2.271) J00-J99 36,073 (36.128)5,368 (15.411)3,174 (10.597)5,223 (20.764)4,427 (10.348)4,913 (15.603) K00-K95 3,925 (3.931)1,789 (5.136)1,580 (5.275)935 (3.717)1,184 (2.768)753 (2.391) L00-L99 1,384 (1.386)1,154 (3.313)69 (0.230)67 (0.266)38 (0.089)47 (0.149) M00-M99 13,190 (13.210)7,625 (21.891)2,459 (8.210)1,933 (7.685)14,039 (32.816)8,924 (28.341) N00-N99 2,050 (2.053)1,206 (3.462)2,434 (8.126)1,562 (6.210)1,673 (3.911)1,195 (3.795) O00-O9A 28 (0.028)25 (0.072)17 (0.057)18 (0.072)54 (0.126)28 (0.089) P00-P96 49 (0.049)51 (0.146)50 (0.167)39 (0.155)5 (0.012) 4 (0.013) Q00-Q99 3 (0.003) 5 (0.014) 4 (0.013) 5 (0.020) 5 (0.012) 6 (0.019) R00-R99 11,797 (11.815)3,544 (10.175)13,110 (43.769)7,599 (30.210)12,321 (28.800)6,957 (22.094) S00-T88 2,556 (2.560)1,790 (5.139)193 (0.644)179 (0.712)632 (1.477)537 (1.705) U00-U85 0 (0.000) 644 (1.849)0 (0.000) 2,106 (8.372)0 (0.000) 1,971 (6.260) V00-Y99 1,448 (1.450)1,286 (3.692)517 (1.726)426 (1.694)1,509 (3.527)801 (2.544) Z00-Z99 18,769 (18.797)6,491 (18.635)4,029",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "V00-Y99 1,448 (1.450)1,286 (3.692)517 (1.726)426 (1.694)1,509 (3.527)801 (2.544) Z00-Z99 18,769 (18.797)6,491 (18.635)4,029 (13.451)3,329 (13.234)5,003 (11.694)3,932 (12.487) signiﬁcant relation with the model output variable, the relations of the policy-based variables, particularly X5, X2, and X3, were more substantial. However, for Operation 2, we observedthat most of the selected input variables were not signiﬁcantly related to Y2. We observedthat only X1-fcast and X5 were related considerably to Y2. As also seen in Table 3,m o s to f the selected input variables of the model were signiﬁcant while modeling Operations 3 and4. We also observed that some of the selected policy-based variables had signiﬁcant negativerelations with Y3 and Y4. This result demonstrated that such policies caused substantialdecreases in resource usage of EDs during-COVID-19 period. After analyzing the effects of the",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "resource usage of EDs during-COVID-19 period. After analyzing the effects of the identiﬁed input variables on the operations, we further processed the obtained data sets using the MLP neural networks. MLPRegressor in the neuralnetwork package of the sklearn module in Python was initialized to process the data sets of the models. The solver function of the algorithm chosen was adam() and the activation function 123 [Página 18] 1090 Annals of Operations Research (2023) 328:1073–1103 Table 3 Correlation results for signiﬁcant input parameters of the model for each of the operations Modeling daily patient numbers: Operation 1Modeling average daily LOS: Operation2Modeling daily numbers of ordered laboratorytests: Operation 3Modeling daily numbers of orderedradiologic imagingtests: Operation 4 rY1−X1=-0.25** rY1−X2=-0.40** rY1−X3=-0.43** rY1−X4=-0.22* rY1−X5=-0.76**rY2−X1−fc a s t = 0.18* rY2−X5=− 0.29**rY3−X1−fc a s",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "rY1−X3=-0.43** rY1−X4=-0.22* rY1−X5=-0.76**rY2−X1−fc a s t = 0.18* rY2−X5=− 0.29**rY3−X1−fc a s t = 0.39** rY3−X2=-0.36** rY3−X38=0.24** rY3−X39=0.33** rY3−X40=0.19* rY3−X41=0.27** rY3−X43=0.29** rY3−X46=0.33** rY3−X49=0.28** rY3−X51=0.39** rY3−X53=0.19* rY3−X55=− 0.28** rY3−X58=0.37** rY3−X59=0.22 rY3−X62=− 0.24** rY3−X63=− 0.28** rY3−X64=− 0.38** rY3−X66=0.27** rY3−X67=0.39**rY4−X1−fc a s t = 0.87** rY1−X2=-0.42** rY1−X3=-0.31** rY1−X5=-0.66** rY1−X70=0.34** rY1−X71=0.36** rY1−X72=0.42** rY1−X73=0.38** rY1−X74=0.30** rY1−X75=0.42** rY1−X77=0.26** rY1−X78=0.40** rY1−X79=0.32** rY1−X80=0.28** rY1−X82=0.32** rY1−X83=0.30** rY1−X87=0.25** rY1−X88=0.19* rY1−X90=0.36** rY1−X91=0.32** rY1−X92=-0.30** rY1−X93=0.37** rY1−X95=− 0.20* rY1−X98=0.30** rY1−X99=0.24** *Correlation is signiﬁcant in 95%CI **Correlation is signiﬁcant in 99%CI selected was relu() . The train test split was used for experimentation, and the separation was applied randomly. The train/test split value of 0.8 was applied. The experiment was repeatedseveral times to obtain the optimal model parameters for learning rate, momentum, and thenumber of hidden layers. The prediction performances of the",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "rate, momentum, and thenumber of hidden layers. The prediction performances of the models were tested on the testdata sets based on the mean absolute percentage error (MAPE), and the root mean squareerror (RMSE) statistics. The optimal model parameters speciﬁc to each model and modelperformances are represented in Table 4. Table 4shows that the proposed model performs well for managing ED operations in the COVID-19 periods. The model, tested in four different operations, achieved around 90%accuracy in two of these operations and 95% accuracy in one. On the other hand, in one of the 123 [Página 19] Annals of Operations Research (2023) 328:1073–1103 1091 Table 4 MLP neural network performances on ED operations predictions during-COVID-19 ED operations during-COVID-19 and related modelOptimized parameters (learning rate-LR, momentum-M, number ofhidden layers-HLModel performance",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "during-COVID-19 and related modelOptimized parameters (learning rate-LR, momentum-M, number ofhidden layers-HLModel performance MAPE RMSE Modelling daily patient numbers: Operation 1LR=0.01, M =0.01, HL =2 10.573 88.624 Modelling daily average LOS: Operation 2LR=0.5, M =0.2, HL =3 19.309 40.473 Modelling daily numbers of ordered laboratory tests: Operation 3LR=0.001, M =0.125, HL =4 9.884 28.325 Modelling daily numbers of ordered radiologic imaging tests: Operation 4LR=0.019, M =0.19, HL =3 5.924 20.324 operations modeling average daily LOS, the model performance was lower, having around 80% accuracy. The model results are also consistent with the ﬁndings on the relationshipbetween model attributes. Since lower relations were observed between variables on LOSmodeling, prediction performance could not achieve the modeling performances on otheroperations with higher correlation levels between the variables. Nonetheless, the achievedaccuracies were still",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "with higher correlation levels between the variables. Nonetheless, the achievedaccuracies were still acceptable and practically implementable compared with related studiesand targeted levels. 6 Discussion This study emphasizes implementing emerging technologies, particularly BDA, in manag-ing health services’ operations. As noted in the literature (Akter & Wamba, 2019 ; Donthu & Gustaffson, 2020 ), we believe that the challenges posed by COVID-19 can be tackled using these technologies. Grounded in dynamic capabilities and the related context of BDAC, weproposed a model for the management of ED operations in emergencies. To show the valid-ity of the proposed model, we tested it in four different primary operations of EDs. Whiledeﬁning the model variables, besides using the system dynamics-related factors, we imple-mented additional variables to represent the effect of government restrictions and prohibitionsimposed",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "imple-mented additional variables to represent the effect of government restrictions and prohibitionsimposed to cope with emergencies. Thus, we contribute to the literature by proposing an efﬁ-cient system for managing ED operations in emergencies by implementing emerging BDAtechnologies and investigating the effects of these policy-based factors on ED operations. The model has been validated using real-life data from a large-scale ED operating in ˙Izmir city, Turkey. Although the overcrowded environments of EDs are a global problem, this problem is worse in some countries, such as Turkey, in which EDs are frequently occupiedunnecessarily by non-emergent patients. By comparing the daily and total patient volumes inthe before- and during-COVID-19 study periods, the descriptive ﬁndings on the case data setmainly represent the signiﬁcance of this problem in this ED since patient volumes",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "represent the signiﬁcance of this problem in this ED since patient volumes sharplydecreased during-COVID-19 period. By classifying patients into three categories—patientsrequiring no diagnostic tests, laboratory tests, and radiologic imaging tests—and identifyingthat the reduction in patient volume was mainly caused by the ﬁrst category (patients requir-ing no diagnostic tests), we also provide evidence to support this ﬁnding. We additionallysupport this ﬁnding by observing increases in the average LOS values of patients who do not 123 [Página 20] 1092 Annals of Operations Research (2023) 328:1073–1103 require any diagnostic tests. Contrarily, the average LOS values were observed to decrease for patients requiring diagnostic tests during-COVID-19 period. All these ﬁndings demon-strate that most patients make unnecessary visits to this ED. This result supports the existingstudies reporting a substantial decrease in ED visits",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "This result supports the existingstudies reporting a substantial decrease in ED visits during the COVID-19 (Jeffery et al.,2020 ; Schereyer et al., 2020 ). We also contribute to the literature by linking this result to one of the biggest operational challenges of EDs and demonstrating that unnecessary visits arethe leading cause of overcrowded ED environments. Besides, from the practical viewpoint,the decrease in patient numbers and diagnostic test orders during COVID-19 may be usedfor hospital managers’ better scheduling and allocation of ED resources. Although a sharpdecline was observed in these values, a signiﬁcant increase was observed in patients’ averageLOS values, meaning that arriving patients to EDs during-COVID-19 required more andlonger interventions and treatments. Thus, better planning and allocation of ED resourceswill be essential for functioning these services during emergencies.",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "allocation of ED resourceswill be essential for functioning these services during emergencies. Signiﬁcant decreases in patient volume during-COVID-19 period may be related to two main factors. First, the pandemic created stress in patients. To protect themselves from beinginfected, they may have avoided visiting EDs if they did not have emergent or urgent sit-uations. Second, due to the government restrictions and prohibitions imposed, people werepartially obliged to stay at home if they did not need an emergent or urgent health service.Since the ﬁrst factor is more behavioral, it is beyond the scope of this study. However, weaimed to identify the impacts of policy-based factors on ED operations by adopting our modelinto a case study representing the overcrowding of ED environments and frequently unneces-sary ED visits. This result supports the",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "of ED environments and frequently unneces-sary ED visits. This result supports the existing studies reporting decreased patient volumesdue to the governmental actions taken in ﬁghting COVID-19 (Kendzerska et al., 2021 ; Sözen et al., 2022 ). It also enhances literature by considering this effect in developing prediction models for patient volumes, average stay lengths of patients, and resource utilization of EDsduring this pandemic period. The depicted decreases in the average LOS values of patients requiring laboratory or radiologic imaging tests in the during-COVID-19 period compared to the before-COVID-19period highlights another essential ﬁnding of this study. While this ﬁnding has been widelypresented in the literature (Houshyar et al., 2020 ; Jeffery et al., 2020 ), by proposing an efﬁcient data-driven model for predicting the daily utilization of these services",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "an efﬁcient data-driven model for predicting the daily utilization of these services during thispandemic, once again, this study differs from the existing studies. As an interpretation, itshould be noted that the decrease in the utilization of EDs’ resources accelerates the accessto resources and enables more efﬁcient use of them, and solves another challenge of longwaiting times in EDs. A critical step in devising the proposed model was determining the model inputs appro- priately. In the case study implementation, input variables are deﬁned in two categories as(i) variables representing system dynamics and (ii) government restrictions and prohibitions.While policy-based variables are deﬁned commonly in implementing the proposed modelfor considered ED operations, system dynamics-based variables are explicitly deﬁned foreach operation. The primary demographics, such as gender, age, triage level, arrival type,and",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "operation. The primary demographics, such as gender, age, triage level, arrival type,and ICD-10 encoded diagnosis in the ED patients’ database, were used and appropriatelytransformed to identify operation-speciﬁc input variables. The values of these variables weremeasured based on the data set for the before-COVID-19 study period. After forming data sets in this manner, the proposed model was tested for the considered ED operations of managing the daily number of patients, average daily LOS, daily numbersof laboratory tests ordered, and daily numbers of radiologic imaging tests ordered. Whenthe relations between the speciﬁed input variables and the daily number of patients during-COVID-19 period were analyzed, it was concluded that policy-based attributes have more 123 [Página 21] Annals of Operations Research (2023) 328:1073–1103 1093 signiﬁcant effects on the daily number of patients",
        "start_idx": 8236,
        "end_idx": 8364
      },
      {
        "text": "Research (2023) 328:1073–1103 1093 signiﬁcant effects on the daily number of patients compared to the identiﬁed system dynamics- related input variables. Some relations were observed between the deﬁned input variables,such as transport bans and restrictions on the elderly, and the daily average LOS during-COVID-19. While policy-based variables, such as total curfew, are related to the daily numberof laboratory tests ordered during-COVID-19 period, some other system dynamics-relatedinput variables also have relations with the corresponding output variable. Finally, bothpolicy-based attributes, namely, curfews and restrictions and transport bans, and most systemdynamics-related variables seemed to relate to the daily number of radiologic imaging testsordered. It is also noted that the depicted correlations between policy-based input variablesand the corresponding output variables had negative signs showing that such policies maydecrease patient volume and the",
        "start_idx": 8352,
        "end_idx": 8480
      },
      {
        "text": "had negative signs showing that such policies maydecrease patient volume and the utilization of primary ED resources. From these ﬁndings, itis concluded that the restrictions and prohibitions imposed by the government in coping withCOVID-19 have had signiﬁcant impacts on the management of ED operations. This resultis in line with the existing studies (Akter & Wamba, 2019 ; Haldane & Morgan, 2021 ; Sözen et al., 2022 ). Our ﬁndings contribute to the literature by investigating the effects of system dynamics-related and government-imposed actions together and comparatively for differentoperations of EDs. The obtained data sets were then used to implement the proposed model in the four primary ED operations using MLP neural networks. Neural network algorithms have been presentedin the literature for automatic COVID-19 detection (Qayyum et al., 2021",
        "start_idx": 8468,
        "end_idx": 8596
      },
      {
        "text": "been presentedin the literature for automatic COVID-19 detection (Qayyum et al., 2021 ) and infection rate predictions (Wieczorek et al., 2020 ; Sozen, Sariyer & Ataman, 2021). By implementing this algorithm in multi real-life operations of EDs, the used contexts of this BDA technique havebeen extended in this paper. The model has high prediction accuracies for managing dailypatient numbers and daily use of resources during a pandemic. Besides achieving or exceedingthe prediction performances of models in the literature in this context (Whitt & Zhang, 2019 ), these results achieved the targeted value (85%) set by this ED’s service providers. Althoughthe model’s performance is lower in predicting daily average LOS values, it can still matchthe performance of previous studies (Ataman & Sariyer, 2021) and achieve the targeted valueof 75%",
        "start_idx": 8584,
        "end_idx": 8712
      },
      {
        "text": "previous studies (Ataman & Sariyer, 2021) and achieve the targeted valueof 75% accuracy. This operation’s targeted value is smaller than others since modeling LOS ismore complex. Thus, with the proposed model, which utilizes BDA, we believe that even themost challenging health services operations may be managed efﬁciently, and the difﬁcultiesposed by emergencies can be handled. 7 Implications 7.1 Theoretical implications The study underpins the dynamic capability theory in two folds. The emergencies are featuredwith the rapidly changing conditions and parameters. Hence, the data inherent in the crisesexhibits a dynamic feature. Eventually, the properties of the data set are subject to change.Therefore, DC theory arises as an ideal theoretical structure to embrace dynamically changingenvironments caused by emergencies. While such situations cause rapid changes in patientvolumes, varieties, and characteristics, from",
        "start_idx": 8700,
        "end_idx": 8828
      },
      {
        "text": "While such situations cause rapid changes in patientvolumes, varieties, and characteristics, from different viewpoints, the government’s policies,such as restrictions and prohibitions in ﬁghting these situations, create additional modiﬁ-cations in the system environment. For instance, during emergencies caused by pandemicillnesses, volumes of infected patients may signiﬁcantly increase. The total patient volumein health services may also be decreased due to panic and stress factors created by being 123 [Página 22] 1094 Annals of Operations Research (2023) 328:1073–1103 infected and based on governmental policies such as stay-home warnings and curfews. All of this support how emergencies create dynamically changing environments. This implicationis strengthened by comparing the main features of the health system data before-COVID19and during-COVID19 periods. Hence, the study’s ﬁndings state that DC is applicable inemergencies. The second fold of the",
        "start_idx": 8816,
        "end_idx": 8944
      },
      {
        "text": "ﬁndings state that DC is applicable inemergencies. The second fold of the theoretical implication can be asserted that dynamically changing environments caused by emergencies affect decision-making processes. As the propertiesof the data set act in a dynamic manner, it forces the decision-making process to be in linewith this rapid change. Even though the big data nature of the data sets stays the same,the time pressure on the decision-makers is higher due to the fast and dynamic change ofdata. Thus, the need for rapid decision-making increases the need for the capabilities relatedto data analytics. Therefore, BDAC is a crucial structure for building the decision-makingmechanism within emergencies. Once again, the study’s ﬁndings support this implication byhighlighting the signiﬁcant changes in patient volumes, demographics (such as distributionson gender, age, triage, arrival",
        "start_idx": 8932,
        "end_idx": 9060
      },
      {
        "text": "changes in patient volumes, demographics (such as distributionson gender, age, triage, arrival type, and diagnosis categories), and diagnostic test requirements(resource usage) between the before and during pandemic periods. Being aware of changes insuch parameters and having capabilities of shaping ED services rapidly in response to thesechanges provide signiﬁcant advantages in ﬁghting emergencies. Thus, it can be depicted thatBDAC is applicable in emergencies. Thus, although dynamic capability theory and the recent view of BDAC have been well presented in management literature, this study attempts to extend their usage in the healthcontext, particularly under emergencies. By discussing the rapidly changing parameters andfeatures of the health system environments in emergencies, proposing a model highlightinga need for BDAC, and implementing this model in a real-life big data study, this study aimsto contribute",
        "start_idx": 9048,
        "end_idx": 9176
      },
      {
        "text": "this model in a real-life big data study, this study aimsto contribute to the context of these theories. 7.2 Managerial implications Our main suggestion is that the decision-makers of health services have BDAC and use bigdata sets of their system environments effectively to create meaningful knowledge, whichshould then be turned rapidly into actions. Adopting the system to dynamically changing con-ditions caused by emergencies quickly and efﬁciently should be achieved by taking advantageof the emerging technologies and by being able to implement these technologies in practicefor planning and managing operations. Based on the results of this study, we showed howthe current emergency, COVID-19, and the government policies change the patient volumes,varieties, and characteristics. Since such changes may signiﬁcantly affect ED operations, andbecause it is essential to provide rapid responses",
        "start_idx": 9164,
        "end_idx": 9292
      },
      {
        "text": "signiﬁcantly affect ED operations, andbecause it is essential to provide rapid responses to these changing situations, it should alsobe noted that understanding and identifying the main factors that impact their operations iscritical. Suppose system-related factors are characterized and appropriately measured, andexternal factors that may arise from the emergencies are carefully followed and identiﬁed.All these factors can be collectively used in modeling ED operations by taking advantageof BDA technologies. Hence, the system may function efﬁciently even in emergencies. Thechallenges arising in the ED environment and posed by emergencies can be easily managed insuch conditions. Based on such models, the managers will be able to make rapid and correctdecisions and adapt the system efﬁciently to dynamically changing conditions. We also highlight the importance of data recording in health services. Although",
        "start_idx": 9280,
        "end_idx": 9408
      },
      {
        "text": "We also highlight the importance of data recording in health services. Although BDA and BDAC are signiﬁcant technologies and capabilities for health services and particularly emer-gency departments, all these do not make any sense if there exist no data sets to analyze, create 123 [Página 23] Annals of Operations Research (2023) 328:1073–1103 1095 knowledge, and use in decision making. Therefore, we suggest that the ED decision-makers focus on electronic recording and data storage processes and should not avoid investing inthese processes and systems. Since the quantity and quality of the data allow meaningful andactionable knowledge, the decision-makers should spend time and effort testing the quality ofrecording processes. Assuring the existence of valid and reliable big data sets is the primaryprior condition for an ED decision-maker to take advantage",
        "start_idx": 9396,
        "end_idx": 9524
      },
      {
        "text": "sets is the primaryprior condition for an ED decision-maker to take advantage of BDA in ﬁghting against thechallenges and uncertainties posed by emergencies. This is also very important for satisfyingthe sustainable monitoring in ED processes and real-time emergency response applications. 7.3 Policy implications This study mainly emphasized the overcrowded ED environments and the signiﬁcance of thisproblem in our ED, even regularly. Based on the ﬁndings, we noted that this overcrowdingmight be primarily associated with the redundant use of these services, particularly for patientswho occupy them for non-urgent situations. These types of patients generally perceive EDsas gateways to hospitals. To not make an appointment and wait in line for polyclinic servicesor receive a health service at weekends or nights, as EDs provide a 7/24 service, patientsmay choose to visit",
        "start_idx": 9512,
        "end_idx": 9640
      },
      {
        "text": "or nights, as EDs provide a 7/24 service, patientsmay choose to visit EDs. However, providing a timely and efﬁcient service becomes morechallenging in these crowded environments based on limited resources. If ED operationscannot be appropriately managed, patients even in emergent and urgent situations may have towait to be treated, which may have signiﬁcant consequences. To cope with this overcrowdingproblem, different government actions should be taken. This study also analyzes the effects of government restrictions and prohibitions in coping with emergencies, particularly COVID-19. It should be highlighted that imposing these poli-cies is crucial in emergencies to protect the functioning of EDs. Government policies, suchas curfews (lock-downs), transport bans, and partial restrictions on the elderly or the young,may decrease patient volumes, redundant ED visits, and resource utilization. In today’s era",
        "start_idx": 9628,
        "end_idx": 9756
      },
      {
        "text": "decrease patient volumes, redundant ED visits, and resource utilization. In today’s era that requires awareness of big data and the related contexts of BDA and BDAC, we also advise policymakers to invest in data storage and analysis in governmentagencies. Governments must create awareness of these emerging concepts and technologiesin public institutions. Governments should pay time, effort, and budget to regularly controlthe agencies based on their data storage capabilities, qualities, quantities, and reliabilities.It may be necessary to impose sanctions on institutions deﬁcient in these concepts duringthese controls. Creating high-quality, reliable, and robust data sets in government institutionswill improve more accurate and timely decision-making processes in emergency and routinesituations. This may also help governments integrate sustainability orientation in health careoperations and ﬂexibility for managing emergencies. 8 Conclusion While emergencies precisely",
        "start_idx": 9744,
        "end_idx": 9872
      },
      {
        "text": "health careoperations and ﬂexibility for managing emergencies. 8 Conclusion While emergencies precisely demonstrate dynamically changing environments, health ser-vices are the main actors in coping with those situations. Governments are another leadingactor; they are the enablers of the system and may impose restrictions and prohibitions toprotect the functioning of health services. We, therefore, propose a model, which is groundedin the dynamic capabilities and related context of BDAC, for managing operations of one ofthe most crucial health services units, namely, EDs, during emergencies. With this model,we aim not only to manage ED operations sustainably but also to investigate the effects 123 [Página 24] 1096 Annals of Operations Research (2023) 328:1073–1103 of imposed restrictions and prohibitions on these operations. Besides proposing a generic machine learning integrated model for managing ED operations",
        "start_idx": 9860,
        "end_idx": 9988
      },
      {
        "text": "Besides proposing a generic machine learning integrated model for managing ED operations under emergencies and vali-dating this model for different operations of EDs, taking the governmental actions as the mainfactors of this model and thus showing how they affect these operations is the main contri-bution of this paper. This study also contributes to dynamic capability theory and BDAC byextending their usage for the decision-making processes of one of the most important actorsof health services, EDs, under emergencies. We also believe that the proposed BDA-drivenmodel or more general big data and BDA implementations in real-life operations may helpsatisfy sustainable operations in EDs. The proposed model adopts one of the most popular BDA techniques: multilayer per- ceptron neural networks. The model is implemented in a real-life data set representing alarge-scale",
        "start_idx": 9976,
        "end_idx": 10104
      },
      {
        "text": "networks. The model is implemented in a real-life data set representing alarge-scale ED with daily patient volumes of more than 1,000. The current COVID-19 pan-demic represents a focused emergency. The model is validated in four different primaryoperations of EDs: managing daily numbers of patients, daily average stays of patients anddaily usage of resources (laboratory services and radiologic imaging services). The predic-tion performance of the proposed model varies between 80 to 95% for the correspondingoperations. This study also showed that policy-based factors might signiﬁcantly affect EDoperations. Such restrictions and prohibitions may cause sharp decreases in patient volumesand resource utilisations in EDs, which are challenged by overcrowding. Thus, imposingsuch policies is crucial to protect ED functioning in emergencies. The main limitation of this study was that its experimental evaluation was",
        "start_idx": 10092,
        "end_idx": 10220
      },
      {
        "text": "The main limitation of this study was that its experimental evaluation was based on data collected from a single case study, and its ﬁndings may, therefore, not generalize to emer-gency departments with signiﬁcantly different patient populations, characteristics, volumes,and varieties. Generalizing these results to other emergency departments with different oper-ational processes, guidelines, and dynamics may also be impossible. Operationally, to ensurerobustness, it is critical to check for variations in patient and system dynamics patternsobserved in this case study to transfer the proposed model to other emergency departments.Future studies should include a broader set of operations, measurements, internal and exter-nal variables, and outcomes from multiple emergency departments to support the robustnessof the proposed model. Finally, we expect that the implementation of deep learning tech-niques can potentially further improve the predictive",
        "start_idx": 10208,
        "end_idx": 10336
      },
      {
        "text": "the implementation of deep learning tech-niques can potentially further improve the predictive performance of the proposed model forconsidered operations of EDs. Appendix 1 Correlation matrices of the identiﬁed variables of the models for corresponding ED opera-tions. See Fig. 5. 123 [Página 25] Annals of Operations Research (2023) 328:1073–1103 1097 Fig. 5 Operation 1: Modelling daily numbers of ED patients during COVID-19 123 [Página 26] 1098 Annals of Operations Research (2023) 328:1073–1103 Appendix 2 See Fig. 6. Fig. 6 Operation 2: Modelling daily average LOS of ED patients during COVID-19 123 [Página 27] Annals of Operations Research (2023) 328:1073–1103 1099 Appendix 3 See Fig. 7. Fig. 7 Operation 3: Modelling daily numbers of laboratory tests ordered 123 [Página 28] 1100 Annals of Operations Research (2023) 328:1073–1103 Appendix 4 See",
        "start_idx": 10324,
        "end_idx": 10452
      },
      {
        "text": "[Página 28] 1100 Annals of Operations Research (2023) 328:1073–1103 Appendix 4 See Fig. 8. Fig. 8 Operation 4: Modelling daily numbers of radiologic imaging tests ordered References Abdel-Basset, M., Chang, V ., & Nabeeh, N. A. (2021). An intelligent framework using disruptive technologies for COVID-19 analysis. T echnological F orecasting and Social Change, 163 , 120431. Akter, S., & Wamba, S. F. (2019). Big data and disaster management: A systematic review and agenda for future research. Annals of Operations Research, 283 (1), 939–959. Alinaghian, M., & Goli, A. (2017). Location, allocation and routing of temporary health centers in rural areas in crisis, solved by improved harmony search algorithm. International Journal of Computational Intelligence Systems, 10 (1), 894–913. Ataman, M. G., & Sarıyer, G. (2021). Predicting waiting and treatment",
        "start_idx": 10440,
        "end_idx": 10568
      },
      {
        "text": "894–913. Ataman, M. G., & Sarıyer, G. (2021). Predicting waiting and treatment times in emergency departments using ordinal logistic regression models. The American Journal of Emergency Medicine, 46 , 45–50. Bag, S., Gupta, S., Choi, T. M., & Kumar, A. (2021). Roles of innovation leadership on using big data analytics to establish resilient healthcare supply chains to combat the COVID-19 pandemic: A multimethodologicalstudy. IEEE Transactions on Engineering Management .https://doi.org/10.1109/TEM.2021.3101590 123 [Página 29] Annals of Operations Research (2023) 328:1073–1103 1101 Birkinshaw, J., Zimmermann, A., & Raisch, S. (2016). How do ﬁrms adapt to discontinuous change? Bridging the dynamic capabilities and ambidexterity perspectives. California Management Review, 58 (4), 36–58. Choi, T. M. (2021). Fighting against COVID-19: What operations research can help and the sense-and-respond framework. Annals of Operations Research .https://doi.org/10.1007/s10479-021-03973-w",
        "start_idx": 10556,
        "end_idx": 10684
      },
      {
        "text": "research can help and the sense-and-respond framework. Annals of Operations Research .https://doi.org/10.1007/s10479-021-03973-w Choi, T. M., Wallace, S. W., & Wang, Y . (2018). Big data analytics in operations management. Production and Operations Management, 27 (10), 1868–1883. Das, S. K., Pervin, M., Roy, S. K., & Weber, G. W. (2021). Multi-objective solid transportation-location problem with variable carbon emission in inventory management: A hybrid approach. Annals of Operations Research .https://doi.org/10.1007/s10479-020-03809-z Deloitte. (2020). COVID -19: Managing supply chain risk and disruption . Retrieved November 10, 2020, from https://www2.deloitte.com/global/en/pages/risk/articles/covid-19-managing-supply-chain- risk-anddisruption.html . Donthu, N., & Gustafsson, A. (2020). Effects of COVID-19 on business and research. Journal of Business Research, 117 , 284. Feng, Q., & Shanthikumar, J. G. (2018). How research in production and operations management may evolve in the era of big",
        "start_idx": 10672,
        "end_idx": 10800
      },
      {
        "text": "in production and operations management may evolve in the era of big data. Production and Operations Management, 27 (9), 1670–1684. Fortune. (2020). 94% of the F ortune 1000 are seeing coronavirus supply chain disruptions: Report . Retrieved November 10, 2020, from https://fortune.com/2020/02/21/fortune-1000-coronavirus-china- supply-chain-impact/ . Goli, A., Zare, H. K., Tavakkoli-Moghaddam, R., & Sadeghieh, A. (2019). Hybrid artiﬁcial intelligence and robust optimization for a multi-objective product portfolio problem Case study: The dairy productsindustry. Computers and Industrial Engineering, 137 , 106090. Goli, A., Khademi-Zare, H., Tavakkoli-Moghaddam, R., Sadeghieh, A., Sasanian, M., & Malekalipour Kordestanizadeh, R. (2021). An integrated approach based on artiﬁcial intelligence and novel meta- heuristic algorithms to predict demand for dairy products: a case study. Network Computation in Neural Systems, 32 (1), 1–35. Guo, M., Zhang, Q., Liao,",
        "start_idx": 10788,
        "end_idx": 10916
      },
      {
        "text": "Computation in Neural Systems, 32 (1), 1–35. Guo, M., Zhang, Q., Liao, X., Chen, F. Y ., & Zeng, D. D. (2020). A hybrid machine learning framework for analyzing human decision-making through learning preferences. Omega, 101 , 102263. Gupta, S., Justy, T., Kamboj, S., Kumar, A., & Kristoffersen, E. (2021). Big data and ﬁrm marketing per- formance: Findings from knowledge-based view. T echnological F orecasting and Social Change, 171 , 120986. Haldane, V ., & Morgan, G. T. (2021). From resilient to transilient health systems: The deep transformation of health systems in response to the COVID-19 pandemic. Health Policy and Planning, 36 (1), 134–135. Harvard Business Review. (2020). Coronavirus is proving we need more resilient supply chains . Retrieved November 5, 2020, from https://hbr.org/2020/03/coronavirus-is-proving-that-we-need-moreresilient- supply-chains . Hossain, M.",
        "start_idx": 10904,
        "end_idx": 11032
      },
      {
        "text": "chains . Retrieved November 5, 2020, from https://hbr.org/2020/03/coronavirus-is-proving-that-we-need-moreresilient- supply-chains . Hossain, M. K., Thakur, V ., & Mangla, S. K. (2021). Modeling the emergency healthcare supply chains: Responding to the COVID-19 pandemic. Journal of Business and Industrial Marketing .https://doi.org/ 10.1108/JBIM-07-2020-0315 Houshyar, R., Tran-Harding, K., Glavis-Bloom, J., Nguyentat, M., Mongan, J., Chahine, C., Loehfelm, T. W., Kohli, M. D., Zaragoza, E. J., Murphy, P. M., & Kampalath, R. (2020). Effect of shelter-in-place on emergency department radiology volumes during the COVID-19 pandemic. Emergency radiology, 27(6), 781–784. Huang, H., Peng, Z., Wu, H., & Xie, Q. (2020). A big data analysis on the ﬁve dimensions of emergency management information in the early stage of COVID-19 in China. Journal of Chinese Governance, 5(2), 213–233. Jeffery, M. M., D’onofrio, G., Paek, H., Platts-Mills,",
        "start_idx": 11020,
        "end_idx": 11148
      },
      {
        "text": "Chinese Governance, 5(2), 213–233. Jeffery, M. M., D’onofrio, G., Paek, H., Platts-Mills, T. F., Soares, W. E., Hoppe, J. A., Genes, N., Nath, B., & Melnick, E. R. (2020). Trends in emergency department visits and hospital admissions in health caresystems in 5 states in the ﬁrst months of the COVID-19 pandemic in the US. JAMA internal medicine, 180(10), 1328–1333. Kapoor, K., Bigdeli, A. Z., Dwivedi, Y . K., & Raman, R. (2021). How is COVID-19 altering the manufac- turing landscape? A literature review of imminent challenges and management interventions. Annals of Operations Research . https://doi.org/10.1007/s10479-021-04397-2 Kendzerska, T., Zhu, D. T., Gershon, A. S., Edwards, J. D., Peixoto, C., Robillard, R., & Kendall, C. E. (2021). The effects of the health system response to the COVID-19 pandemic on chronic",
        "start_idx": 11136,
        "end_idx": 11264
      },
      {
        "text": "effects of the health system response to the COVID-19 pandemic on chronic disease management:A narrative review. Risk Management and Healthcare Policy, 14 , 575. Kumar, A., Shankar, R., Choudhary, A., & Thakur, L. S. (2016). A big data MapReduce framework for fault diagnosis in cloud-based manufacturing. International Journal of Production Research, 54 (23), 7060–7073. 123 [Página 30] 1102 Annals of Operations Research (2023) 328:1073–1103 Kumar, A., Shankar, R., & Aljohani, N. R. (2020). A big data driven framework for demand-driven forecasting with effects of marketing-mix variables. Industrial Marketing Management, 90 , 493–507. Lee, S. M., & Trimi, S. (2021). Convergence innovation in the digital age and in the COVID-19 pandemic crisis. Journal of Business Research, 123 , 14–22. Mari´ c, J., Galera-Zarco, C., & Opazo-Basáez, M. (2021). The",
        "start_idx": 11252,
        "end_idx": 11380
      },
      {
        "text": ", 14–22. Mari´ c, J., Galera-Zarco, C., & Opazo-Basáez, M. (2021). The emergent role of digital technologies in the context of humanitarian supply chains: A systematic literature review. Annals of Operations Research . https://doi.org/10.1007/s10479-021-04079-z Midya, S., Roy, S. K., & Yu, V . F. (2021). Intuitionistic fuzzy multi-stage multi-objective ﬁxed-charge solid transportation problem in a green supply chain. International Journal of Machine Learning and Cyber- netics, 12 (3), 699–717. Mishra, D., Gunasekaran, A., Papadopoulos, T., & Childe, S. J. (2018). Big Data and supply chain management: A review and bibliometric analysis. Annals of Operations Research, 270 (1), 313–336. Mondal, A., & Roy, S. K. (2021). Multi-objective sustainable opened-and closed-loop supply chain under mixed uncertainty during COVID-19 pandemic situation. Computers & Industrial Engineering, 159 , 107453. Mondal, A., &",
        "start_idx": 11368,
        "end_idx": 11496
      },
      {
        "text": "pandemic situation. Computers & Industrial Engineering, 159 , 107453. Mondal, A., & Roy, S. K. (2022). Application of Choquet integral in interval type-2 Pythagorean fuzzy sustainable supply chain management under risk. International Journal of Intelligent Systems, 37 (1), 217–263. Papadopoulos, T., Baltas, K. N., & Balta, M. E. (2020). The use of digital technologies by small and medium enterprises during COVID-19: Implications for theory and practice. International Journal of Information Management, 55 , 102192. Sarkis, J. (2021). Supply chain sustainability: Learning from the COVID-19 pandemic. International Journal of Operations & Production Management, 41 (1), 63–73. Schreyer, K. E., Daniel, A., King, L. L., Blome, A., DeAngelis, M., Stauffer, K., Desrochers, K., Donahue, W., Politarhos, N., Raab, C., & McNamara, R. (2020). Emergency department management of the Covid-19 pandemic.",
        "start_idx": 11484,
        "end_idx": 11612
      },
      {
        "text": "C., & McNamara, R. (2020). Emergency department management of the Covid-19 pandemic. The Journal of emergency medicine, 59 (6), 946–951. Thakur, V ., Mangla, S. K., & Tiwari, B. (2021). Managing healthcare waste for sustainable environmental development: A hybrid decision approach. Business Strategy and the Environment, 30 (1), 357–373. Tirkolaee, E. B., Goli, A., Ghasemi, P., & Goodarzian, F. (2022). Designing a sustainable closed-loop supply chain network of face masks during the COVID-19 pandemic: Pareto-based algorithms. Journal of Cleaner Production, 333 , 130056. Qayyum, A., Razzak, I., Tanveer, M., & Kumar, A. (2021). Depth-wise dense neural network for automatic COVID19 infection detection and diagnosis. Annals of Operations Research .https://doi.org/10.1007/ s10479-021-04154-5 Queiroz, M. M., Ivanov, D., Dolgui, A., & Wamba, S. F. (2020). Impacts of epidemic outbreaks on supply",
        "start_idx": 11600,
        "end_idx": 11728
      },
      {
        "text": "A., & Wamba, S. F. (2020). Impacts of epidemic outbreaks on supply chains: Mapping a research agenda amid the COVID-19 pandemic through a structured literature review.Annals of Operations Research .https://doi.org/10.1007/s10479-020-03685-7 Sanders, N. R., & Ganeshan, R. (2018). Big data in supply chain management. Production and Operations Management, 27 (10), 1745–1748. Sarıyer, G., & Ataman, M. G. (2020). The likelihood of requiring a diagnostic test: Classifying emergency department patients with logistic regression. Health Information Management Journal, 51 (1), 13–22. Sarıyer, G., Ataman, M. G., & Kızılo˘ glu, ˙I. (2020). Analyzing main and interaction effects of length of stay determinants in emergency departments. International Journal of Health Policy and Management, 9 (5), 198–205. Sözen, M. E., Sarıyer, G., & Ataman, M. G. (2022). Big data analytics and COVID-19: Investigating the",
        "start_idx": 11716,
        "end_idx": 11844
      },
      {
        "text": "& Ataman, M. G. (2022). Big data analytics and COVID-19: Investigating the relationship between government policies and cases in Poland, Turkey, and South Korea. Health Policy and Planning, 37 (1), 100–111. Sharma, M., Luthra, S., Joshi, S., & Kumar, A. (2020). Developing a framework for enhancing survivability of sustainable supply chains during and post-COVID-19 pandemic. International Journal of Logistics Research and Applications, 25 (4–5), 433–453. Rubbio, I., Bruccoleri, M., Pietrosi, A., & Ragonese, B. (2020). Digital health technology enhances resilient behaviour: Evidence from the ward. International Journal of Operations and Production Management, 40(1), 34–67. Teece, D. J., Pisano, G., & Shuen, A. (1997). Dynamic capabilities and strategic management. Strategic Management Journal, 18 (7), 509–533. Teece, D., Peteraf, M., & Leih, S. (2016). Dynamic capabilities and organizational agility: Risk,",
        "start_idx": 11832,
        "end_idx": 11960
      },
      {
        "text": "Peteraf, M., & Leih, S. (2016). Dynamic capabilities and organizational agility: Risk, uncertainty, and strategy in the innovation economy. California Management Review, 58 (4), 13–35. Verma, S., & Gustafsson, A. (2020). Investigating the emerging COVID-19 research trends in the ﬁeld of busi- ness and management: A bibliometric analysis approach. Journal of Business Research, 118 , 253–261. 123 [Página 31] Annals of Operations Research (2023) 328:1073–1103 1103 Wamba, S. F., Gunasekaran, A., Akter, S., Ren, S. J. F., Dubey, R., & Childe, S. J. (2017). Big data analytics and ﬁrm performance: Effects of dynamic capabilities. Journal of Business Research, 70 , 356–365. Whitt, W., & Zhang, X. (2019). Forecasting arrivals and occupancy levels in an emergency department. Operations Research for Health Care, 21 , 1–18. Wieczorek, M., Siłka, J.,",
        "start_idx": 11948,
        "end_idx": 12076
      },
      {
        "text": "Operations Research for Health Care, 21 , 1–18. Wieczorek, M., Siłka, J., & Wo´ zniak, M. (2020). Neural network powered COVID-19 spread forecasting model. Chaos, Solitons & Fractals, 140 , 110203. Yu, W., Zhao, G., Liu, Q., & Song, Y . (2021). Role of big data analytics capability in developing integrated hospital supply chains and operational ﬂexibility: An organizational information processing theory per- spective. T echnological F orecasting and Social Change, 163 , 120417. Zollo, M., & Winter, S. G. (2002). Deliberate learning and the evolution of dynamic capabilities. Organization Science, 13 (3), 339–351. Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afﬁliations. Springer Nature or its licensor holds exclusive rights to this article under a publishing agreement with the",
        "start_idx": 12064,
        "end_idx": 12192
      },
      {
        "text": "holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article issolely governed by the terms of such publishing agreement and applicable law. 123",
        "start_idx": 12180,
        "end_idx": 12219
      }
    ],
    "932927b3-d9fe-4477-9ec0-bbf37f794ab3": [
      {
        "text": "[Página 1] Expert Systems With Applications 115 (2019) 543–556 Contents lists available at ScienceDirect Expert Systems With Applications journal homepage: www.elsevier.com/locate/eswa BIGOWL: Knowledge centered Big Data analytics /p82 Cristóbal Barba-González, José García-Nieto ∗, María del Mar Roldán-García, Ismael Navas-Delgado, Antonio J. Nebro, José F. Aldana-Montes Departmento de Lenguajes y Ciencias de la Computación, University of Málaga, ETSI Informática, Campus de Teatinos, Málaga 29071, Spain a r t i c l e i n f o Article history: Received 5 April 2018 Revised 26 July 2018 Accepted 14 August 2018 Available online 23 August 2018 Keywords: Ontology Big Data analytics Semantics Knowledge extraction a b s t r a c t Knowledge extraction and incorporation is currently considered to be beneﬁcial for eﬃcient Big Data an- alytics. Knowledge can",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "considered to be beneﬁcial for eﬃcient Big Data an- alytics. Knowledge can take part in workﬂow design, constraint deﬁnition, parameter selection and con- ﬁguration, human interactive and decision-making strategies. This paper proposes BIGOWL, an ontology to support knowledge management in Big Data analytics. BIGOWL is designed to cover a wide vocab- ulary of terms concerning Big Data analytics workﬂows, including their components and how they are connected, from data sources to the analytics visualization. It also takes into consideration aspects such as parameters, restrictions and formats. This ontology deﬁnes not only the taxonomic relationships be- tween the different concepts, but also instances representing speciﬁc individuals to guide the users in the design of Big Data analytics workﬂows. For testing purposes, two case studies are developed, which consists in:",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "workﬂows. For testing purposes, two case studies are developed, which consists in: ﬁrst, real-world streaming processing with Spark of traﬃc Open Data, for route optimization in urban environment of New York city; and second, data mining classiﬁcation of an academic dataset on local/cloud platforms. The analytics workﬂows resulting from the BIGOWL semantic model are validated and successfully evaluated. ©2 0 1 8 Elsevier Ltd. All rights reserved. 1. Introduction In accordance with the recent Gartner’s report, 1 an emerging challenge in Big Data is to construct data-driven intelligent appli- cations that capture and inject domain knowledge in the analyt- ical processes, including context and using a standardized format. Context refers to all the relevant (meta)-information to support the analysis and to help interpreting its results. This will facilitate",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "support the analysis and to help interpreting its results. This will facilitate the integration (in a standardized way) with third parties’ data, algo- rithms, business intelligence (BI) and visualization services. The use of semantics as contextual information will enhance the analytical power of the algorithms, as well as the reuse of single components in data analytics workﬂows ( Ristoski & Paul- /p82 This work has been partially funded by Grants TIN2014-58304, TIN2017-86049- R (Spanish Ministry of Education and Science) and P12-TIC-1519 (Plan Andaluz de Investigación, Desarrollo e Innovación). Cristóbal Barba-González is supported by Grant BES-2015-072209 (Spanish Ministry of Economy and Competitiveness). José García-Nieto is the recipient of a Post-Doctoral fellowship of “Captación de Talento para la Investigación” Plan Propio at Universidad de Málaga. ∗Corresponding author. E-mail addresses: cbarba@lcc.uma.es",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "Investigación” Plan Propio at Universidad de Málaga. ∗Corresponding author. E-mail addresses: cbarba@lcc.uma.es (C. Barba-González), jnieto@lcc.uma.es (J. García-Nieto), mmar@lcc.uma.es (M.d.M. Roldán-García), ismael@lcc.uma.es (I. Navas-Delgado), antonio@lcc.uma.es (A.J. Nebro), jfam@lcc.uma.es (J.F. Aldana- Montes). 1 https://www.gartner.com/doc/3656517/adopt-datadriven-approach- consolidating-infrastructure . heim, 2016 ). Therefore, the development of ways to make the do- main knowledge explicit and usable is needed to improve the data processing and analysis tasks. The Semantic Web technolo- gies can be used to annotate not only the knowledge domain of the data, but also the analytics’ meta-data ( Keet, Ławrynow- icz, d’Amato, Kalousis, Nguyen, Palma, Stevens, & Hilario, 2015 ), including: algorithms’ parameters, input variables, tuning experi- ences, expected behaviors and taxonomies. This will facilitate the reuse and composition of Big Data analytics in a proper manner, as well as to",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "of Big Data analytics in a proper manner, as well as to enhance the quality of consumed and produced data. In this regard, ontologies describe concepts, relationships, classes, individuals, formal logic axioms and objects of a particu- lar domain ( Gruber, 1995 ). The objects refer to entities and events (concepts) in the real world, and their relations represent the se- mantic links between these entities. A series of studies have been appearing in the last few years, in which ontological approaches are suggested to enhance Big Data analytics ( Konys, 2016; Kuiler, 2014 ). However, they are presented as conceptual frameworks, still in an early stage of development, and mostly oriented to the spe- ciﬁc domain of health system applications. This motivates us to propose an ontology-driven",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "domain of health system applications. This motivates us to propose an ontology-driven approach to support knowledge management in Big Data analytics workﬂows. The proposed ontology is called BIGOWL (BIG data analytics OWL 2 2 OWL refers to the Web Ontology Language described in Section 2.1 . https://doi.org/10.1016/j.eswa.2018.08.026 0957-4174/© 2018 Elsevier Ltd. All rights reserved. [Página 2] 544 C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 ontology), which acts as a formal schema for the representation and consolidation of knowledge in Big Data analytics. Knowledge incorporation is in turn beneﬁcial for an eﬃcient algorithmic per- formance, by taking part in operator’s design, parameter selection, human interactive and decision-making strategies. Our scientiﬁc hypothesis is as follows: “The semantic annotation of Big Data sources, components and algorithms",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "as follows: “The semantic annotation of Big Data sources, components and algorithms can acts as a link to capture and incorporate the domain knowledge to guide and enhance the analytical processes ”. In addition, the semantic annotation can provide the background for reasoning methods based on axiomatic and rule logic recommendations. To test this hypothesis, a semantic model has been gener- ated, which comprises an RDF 3 (Resource Description Framework) repository that follows the BIGOWL scheme. This repository can be queried by high level algorithms using SPARQL. The goal is to prop- erly feed artiﬁcial intelligence procedures capable of guiding the design of Big Data analytics workﬂows. As a proof-of-concept, we show how BIGOWL can be used to guide the design of real-world and academic analytic workﬂows. A",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "used to guide the design of real-world and academic analytic workﬂows. A ﬁrst case study consists in optimizing vehicular routes based on New York real-time Open Data about urban traﬃc (average speeds of vehicles, traﬃc densities, etc.). 4 The data source is managed by streaming processing tasks (Kafka and Spark), after which they are optimized (jMetalSP 5 ) and visualized. The second case study is a classiﬁcation workﬂow modeled by using the popular Weka 6 li- brary for data mining, as well as the BigML in-cloud service. 7 The main contributions of this study are: •The proposed ontology, BIGOWL, has been designed and imple- mented for the representation and consolidation of knowledge in Big Data analytics. It considers a large and complemented set of concepts, attributes and relationships",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "It considers a large and complemented set of concepts, attributes and relationships that have been taken from Big Data ecosystem. •A semantic approach has been implemented to annotate (i.e. to “semantize”) all the involved meta-data from multiple data sources, processing components and analytic algorithms. The meta-data are integrated following the BIGOWL structure and stored in a common RDF repository. •The semantic model is evaluated in the context of two realis- tic use cases: real-time routing calculation in urban traﬃc and classical classiﬁcation with decision trees. The proof-of-concept lead us to test our initial hypothesis. The remaining of this paper is structured as follows. In Section 2 , background concepts and literature overview are pre- sented. Section 3 presents current practices in Big Data analyt- ics. Section 4 describes",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "3 presents current practices in Big Data analyt- ics. Section 4 describes the semantic model, comprising the on- tology, RDF repository, mappings and workﬂow composition assis- tant. Section 5 presents the use case for testing and validation. In Section 6 , a series of discussions are included. Conclusions and fu- ture work are drawn in Section 7 . 2. Background and related work To make this paper self-contained, this section describes back- ground concepts in the Semantic Web ﬁeld. A review of the state of the art is also provided to point out the main differences of the related works with the proposed approach. 3 RDF in W3C https://www.w3.org/RDF/ . 4 https://www.data.cityofnewyork.us/Transportation/Real- Time- Traﬃc- Speed- Data/ xsat-x5sa . 5 http://www.jmetal.sourceforge.net/ . 6 https://www.cs.waikato.ac.nz/ml/weka/ . 7 https://www.bigml.com/ . Table",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "xsat-x5sa . 5 http://www.jmetal.sourceforge.net/ . 6 https://www.cs.waikato.ac.nz/ml/weka/ . 7 https://www.bigml.com/ . Table 1 Basic OWL-DL semantic syntax used to formally deﬁne the proposed ontology. Descriptions Abstract syntax DL syntax Operators intersection ( C 1 , C 2 , /22c5/22c5/22c5, C n ) C 1 /2293C 2 /2293/22c5/22c5/22c5/2293C n union ( C 1 , C 2 , /22c5/22c5/22c5, C n ) C 1 /2294C 2 /2294/22c5/22c5/22c5/2293C n Restrictions for at least 1 value V from C ∃ V.C for all values V from C ∀ V.C R is Symmetric R ≡R − Class Axioms A partial ( C 1 , C 2 , /22c5/22c5/22c5, C n ) A /subsetsqequal C 1 /2293C 2 /2293/22c5/22c5/22c5/2293C n A complete ( C 1 , C 2 , /22c5/22c5/22c5, C n ) A",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "( C 1 , C 2 , /22c5/22c5/22c5, C n ) A ≡C 1 /2293C 2 /2293/22c5/22c5/22c5/2293C n 2.1. Background concepts •Ontology. In accordance with Noy, McGuinness et al. (2001) , an ontology provides a formal representation of the real world. It deﬁnes an explicit description of concepts in a domain of discourse (classes or concepts), properties of each concept de- scribing various features and attributes of the concept (proper- ties) and restrictions on properties. Ontologies are part of the W3C standard stack of the Semantic Web. 8 An ontology to- gether with a set of individual instances of classes constitutes a knowledge base and offer services to facilitate interoperability across multiple heterogeneous systems and databases. •RDF. Resource Description Framework ( McBride, 2004 ) is a W3C recommendation",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "•RDF. Resource Description Framework ( McBride, 2004 ) is a W3C recommendation that deﬁnes a language for describ- ing resources on the web. RDF describes resources in terms of triples, consisting of a subject, predicate and object. RDF Schema (RDFS) ( Staab & Studer, 2013 ) describes vocabularies used in RDF descriptions. •OWL. The Ontology Web Language is used to deﬁne ontolo- gies on the Web, which extends RDF and RDFS, but adding a vocabulary. From a formal description, OWL is equivalent to a very expressive description logic DL, where an ontology cor- responds to a Tbox ( Gruber et al., 1993 ). In this sense, OWL- DL is syntactic description that gives maximum expressive- ness while retaining computational completeness and decid- ability ( McGuinness, Van Harmelen et",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "while retaining computational completeness and decid- ability ( McGuinness, Van Harmelen et al., 2004 ). In this work, we use OWL-DL syntax summarized in Table 1 to formalize the proposed ontology. •SPARQL is a query language for easy access to RDF stores. It is the query language recommended by W3C ( Harris, Seaborne, & Prud’hommeaux, 2013 ) to work with RDF graphs ( Prud, Seaborne et al., 2006 ), then supporting queries and web data sources identiﬁed by URIs. •SWRL. The Semantic Web Rule Language provides the OWL-based ontologies with procedural knowledge, which compensates for some of the limitations of ontology in- ference, particularly in identifying semantic relationships between individuals ( Horrocks, Patel-Schneider, Bechhofer, & Tsarkov, 2005 ). SWRL uses the typical logic expres- sion “Antecedent ⇒ Consequent",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "2005 ). SWRL uses the typical logic expres- sion “Antecedent ⇒ Consequent ”t o represent semantic rules. Both antecedent (rule body) and consequent (rule head) can be conjunctions of one or more atoms written as “atom 1 ∧ atom 2 ∧ /22c5/22c5/22c5∧ atom n ”. Each atom is attached to one or more parameters represented by a question mark and a vari- able (e.g., ? x ). The most common uses of SWRL include trans- ferring characteristics and inferring the existence of new indi- viduals ( Grosof & Poon, 2004 ). 9 8 https://www.w3.org/standards/semanticweb/ . 9 https://www.w3.org/Submission/SWRL/ . [Página 3] C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 545 2.2. Related work In the last decade, there have been appearing a series of stud-",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "In the last decade, there have been appearing a series of stud- ies in which ontological approaches are deﬁned to express the knowledge domain in data mining and optimization algorithms. A representative set of these works are compiled in a recent sur- vey ( Dou, Wang, & Liu, 2015 ), in which they are organized by categories of algorithms and applications: association rule discov- ery ( Marinica & Guillet, 2010 ), classiﬁcation ( Allahyari, Kochut, & Janik, 2014 ) and clustering ( Jing, Ng, & Huang, 2010 ). In these ap- plications, semantics is used with different objectives, such as: to reduce the search space by specifying restrictions, to ﬁlter results in the post-processing stage, and to annotate the results of data mining processes. Following with this research",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "to annotate the results of data mining processes. Following with this research line, some recent works include ontologies to guide the processes in machine learning tasks. For example, in Pinto, Scioscia, Loseto, and Ruta (2015) and Roldán- García, García-Nieto, and Aldana-Montes (2017) , two different on- tologies are used in the classiﬁcation process to infer incon- sistencies between concepts by means of semantic reasoning. In Phan, Dou, Wang, Kil, and Piniewski (2015) , an ontology-driven deep learning model is proposed to predict human behavior. In the ﬁeld of optimization, an interesting approach has been recently proposed in Yaman, Hallawa, Coler, and Iacca (2017) , where the ECO ontology is deﬁned to formally represent knowl- edge in evolutionary computation algorithms. This ontology can be used for suggesting strategies for",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "evolutionary computation algorithms. This ontology can be used for suggesting strategies for solving optimization prob- lems. At the same time, an OWL ontology has been pro- posed in Li, Yevseyeva, Basto-Fernandes, Trautmann, Jing, and Emmerich (2017) to model and systematize the knowledge of preference-based multi-objective evolutionary algorithms. These ontologies are validated in use cases focused on algorithmic and parameter selection in academic problems. From a different point of view, a parallel line of research focuses on deﬁning ontologies for the semantic annotation of data analytic workﬂows. The main objective is to model the input and output of algorithms involved in data mining and knowledge base discov- ery (KDD) workﬂows to generate valid compositions. To this end, several OWL ontologies such as: KDDONTO ( Diamantini, Potena, & Storti ),",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "several OWL ontologies such as: KDDONTO ( Diamantini, Potena, & Storti ), DMWF ( Kietz, Serban, Bernstein, & Fischer, 2010 ) and KD ( Záková, Kremen, Zelezny, & Lavrac, 2011 ), were proposed. How- ever, they did not describe the problem domain, or those basic concepts (algorithm, type of analysis, task, dataset, attribute, etc.) that can be combined to deﬁne entities or constraints. In fact, these ontologies were not designed with the objective of opti- mizing the performance of the data mining algorithms, since they do not offer detail enough to provide support to what is known as meta-learning. In Nguyen, Hilario, and Kalousis (2014) , meta- learning is deﬁned as the KDD procedure to improve performance in data mining processes, using information collected during the experimentation phase",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "performance in data mining processes, using information collected during the experimentation phase of these algorithms. In this regard, the use of semantics is considered not only for the algorithmic composi- tion, but also for the improvement of data mining processes, taking advantage of acquired knowledge from past experience. In this context, the EU-FP7 European initiative e-LICO 10 pro- posed the DMOP ontology ( Keet et al., 2015 ), which is de- ﬁned to support the analytic workﬂow composition by follow- ing the standard CRISP-DM ( Shearer, 20 0 0 ). DMOP is used to de- ﬁne analytical workﬂows, as well as to describe algorithms, pa- rameters, inputs/outputs and a large amount of meta-data in- cluded in typical data mining processes. A step further was taken by Kumara, Paik,",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "typical data mining processes. A step further was taken by Kumara, Paik, Zhang, Siriweera, and Koswatte (2015) that use 10 http://www.e-lico.eu/ . Automatic Service Composition to automate the analytic workﬂow generation. As a summary, Table 2 outlines the main features of the related work with regards to the semantic approach proposed here. These features consist of specifying whether the existing approaches: fo- cus on data mining or optimization, are oriented to Big Data, pro- vide proof-of-concepts, align with other ontologies, use OWL/RDF in the semantic model and/or describe workﬂow composition tasks. Then, it is possible to identify the actual contributions of the proposed semantic model beyond the state of the art, as follows: •BIGOWL is conceived to semantically model data analytics in Big Data environments. Similarly to other",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "to semantically model data analytics in Big Data environments. Similarly to other ontologies in the literature, it is oriented to general KDD procedures, although considering those Big Data ecosystem elements with class in- stances, e.g., ontology individuals. •It is aligned with the DMOP ontology, which is in turn aligned with CRISP-DM. They have been validated to construct data mining workﬂows. •Besides data mining, BIGOWL is also focused on optimization algorithms, although with special interest on covering multi- objective metaheuristics in Big Data environments. •The proposed approach is validated on two real-world use- cases consisting of classical data mining and streaming data processing for multi-objective optimization. 3. Current practices in Big Data analytics In current Big Data technology ecosystems, when facing a spe- ciﬁc data analytic task, it is",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "technology ecosystems, when facing a spe- ciﬁc data analytic task, it is usual to support on already existing tools. Some of those consist in commercial services often provided through cloud computing Software-as-a-Service (SaaS), which can be used by no skilled people by means of workﬂow compositions (e.g., Azure ML, Amazon ML, BigML, Data Mining Cloud Frame- work, and Kognitio); other tools are open-source frameworks re- quiring skilled users who prefer to program their application using more technical approaches. Additional factors (such as: data for- mat, data source, volume and velocity required to analyse data) are also determinant when choosing the proper technology ( Zomaya & Sakr, 2017 ). Hadoop ecosystem represents the most used frame- work for developing distributed Big Data analytic applications. However, it is conceived for",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "for developing distributed Big Data analytic applications. However, it is conceived for high skilled users, so even the stan- dard workﬂow composition service of Hadoop (Oozie) requires cer- tain programming ability to be properly used. Besides technological or commercial aspects, current Big Data platforms still follow the common procedure when facing data an- alytics tasks ( ACM-SIGKDD, 2014 ), which comprises typical steps of classical KDD: data collection, data transformation, data mining, pattern evaluation, and knowledge presentation. Keeping this in mind, the proposed semantic approach is ori- ented to general KDD procedures, then leading the underlying Big Data technological platform to be semantically annotated with class instances, e.g., individuals in the ontology. 4. Semantic model One of the main goals in this study is to capture all the",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "of the main goals in this study is to capture all the needed semantics to guide the smart design of Big Data analytics work- ﬂows and to enhance their performance. For this reason, we opted to design an OWL 2 ontology to describe analytic algorithms, datasets, problems, and workﬂows in the Big Data context. To this end, the standard Ontology 101 development pro- cess ( Noy & McGuinness, 2001 ) has been followed, which com- prises seven steps: 1. Determine the domain and scope of the ontology . The main scope of BIGOWL is data processing and data analytics in Big Data en- [Página 4] 546 C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 Table 2 Summary ontologies’ features. Feature/Ontology CRISP-DM KDDONTO PMOEA ECO",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "(2019) 543–556 Table 2 Summary ontologies’ features. Feature/Ontology CRISP-DM KDDONTO PMOEA ECO (Pinto’2015) (Phan’2015) DMWF KD DMOP BIGOWL Data Mining /check /check /check /check /check /check /check /check Optimization /check /check /check Big Data environments /check Proof of concepts /check /check /check /check /check /check /check /check Aligned to other ontology /check /check OWL/RDF /check /check /check /check /check /check /check /check Workﬂow composition /check /check /check /check Fig. 1. Overview of the BIGOWL ontology. Continuous arrows refer to subclasses, whereas dotted ones refer to properties. vironments. This considers not only classical data analytic pro- cedures, but also speciﬁc data processing and underlying soft- ware platform features oriented to Big Data. 2. Consider reusing existing ontologies . As commented before, the proposed ontology is aligned with DMOP, which",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": ". As commented before, the proposed ontology is aligned with DMOP, which has been successfully validated to construct data mining workﬂows. DMOP is in turn aligned with the foundational ontology DOLCE ( Masolo, Borgo, Gangemi, Guarino, & Oltramari, 2003 ) and follows the standard CRISP-DM in the deﬁnition of data mining processes. 3. Enumerate important terms in the ontology . Important terms were selected from the literature related to Big Data and op- timization. In addition, terms from the ontologies aligned ( Keet et al., 2015; Yaman et al., 2017 ) were also incorporated. Exam- ples of such terms are: Component, Workﬂow, Task, Data, Dat- aProcessing and Software . 4. Deﬁne the classes and the class hierarchy . We have followed a top-down approach in developing the class",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "hierarchy . We have followed a top-down approach in developing the class hierarchy. This fact facilitates among others, the alignment with DMOP and DOLCE, the design of annotation mappings and the use of a seman- tic reasoner. Fig. 1 shows the ontology core classes and hier- archy. For instance, the class Component has several subclasses, including DataAnalysing and DataCollection . Classes modeling al- gorithms, components and workﬂows are aligned with the class dmop:DataType . BIGOWL has been developed using Protégé11 and OWL 2. 11 https://protege.stanford.edu/ . 5. Deﬁne the properties of classes and slots . With the purpose of relating classes and deﬁning attributes, we have included ob- ject and data properties. A representative set of properties are shown in Table 3 , where the class Component is",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "properties are shown in Table 3 , where the class Component is related to class Algorithm by means of the object property hasAlgorithm . Data properties of class Component are path, author, numberOfInputs and numberOfOutputs . 6. Deﬁne the facets of the slots . This step includes the deﬁnition of cardinality constraints and value restrictions for the ontology’s properties. For example, the range of the property order is re- stricted to integer (to specify in which step this task is carried out), when the class Task is its domain. 7. Create instances . Instances or individuals in BIGOWL are speciﬁc of the Big Data analytics domain. For exam- ple, GeneratorDataTraﬃc is an instance of the class Kafka , which is a subclass of DataIngestion . The class Kafka",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "Kafka , which is a subclass of DataIngestion . The class Kafka has a property topicKafka (with range “string”) to indicate streams of records of Apache Kafka 12 services. 4.1. The BIGOWL ontology BIGOWL has been developed following the steps described above, producing 184 classes, 16 object properties (binary re- lationships between individuals), 20 data properties (individ- ual attributes), 488 axioms, 66 individuals and growing. It is worth mentioning that classes DM-DataClass ≡DMDataClass and IO- 12 Data Streaming Processing https://www.kafka.apache.org/ . [Página 5] C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 547 Table 3 Component: object and data properties. Object properties Description logic hasAlgorithm ∃ hasAlgorithm.Thing /subsetsqequal Component hasParameter ∃ hasParameter.Thing /subsetsqequal Workﬂow /2294Algorithm /2294Component isConnected ∃ isConnected.Thing /subsetsqequal Algorithm /2294Component /2294Task isCorrect ∃",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "Workﬂow /2294Algorithm /2294Component isConnected ∃ isConnected.Thing /subsetsqequal Algorithm /2294Component /2294Task isCorrect ∃ isCorrect.Thing /subsetsqequal Algorithm /2294Component speciﬁesInputClass ∃ speciﬁesInputClass.Thing /subsetsqequal Algorithm /2294Component /2294Task speciﬁesOutputClass ∃ speciﬁesOutputClass.Thing /subsetsqequal Algorithm /2294Component /2294Task Data Properties Description Logic author ∃ author.Datatype Literal /subsetsqequal Workﬂow /2294Algorithm /2294Component /2294Problem /2294Software hasDataValue ∃ hasDataValue.Datatype Literal /subsetsqequal DataType /2294IO-Class /2294Parameter /2294Workﬂow /2294Algorithm /2294Component /2294Problem numberOfInputs ∃ numberOfInputs.Datatype Literal /subsetsqequal Algorithm /2294Component numberOfOutputs ∃ numberOfOutputs.Datatype Literal /subsetsqequal Algorithm /2294Component path ∃ path.Datatype Literal /subsetsqequal IO-Class /2294Algorithm /2294Component Table 4 Task: object and data properties. Object properties Description logic compatibleWith ∃ compatibleWith.Thing /subsetsqequal Task /latticetop /subsetsqequal ∀ compatibleWith.Task hasComponent /latticetop /subsetsqequal ∀ hasComponent.Component isConnected ∃ isConnected.Thing /subsetsqequal Algorithm /2294Component /2294Task speciﬁesInputClass ∃ speciﬁesInputClass.Thing /subsetsqequal Algorithm /2294Component /2294Task speciﬁesOutputClass ∃ speciﬁesOutputClass.Thing /subsetsqequal Algorithm /2294Component /2294Task Data Properties Description Logic",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "/2294Task speciﬁesOutputClass ∃ speciﬁesOutputClass.Thing /subsetsqequal Algorithm /2294Component /2294Task Data Properties Description Logic order ∃ order.Datatype Literal /subsetsqequal Task /latticetop /subsetsqequal ∀ order.Datatype Class ≡Data are declared as equivalent (with relation ≡) to align with those classes from other ontologies (DMOP) that describe similar concepts. We use OWL-DL syntax (see Table 1 ) to formal- ize the proposed ontology. The complete ontology is developed in “bigowl.owl ”ﬁ l e and available in the GitHub repository. 13 A representative set of the main classes are described here, to- gether with their object and data properties. These classes are: Component, Task, Algorithm, Data , and Workﬂow . Each class has de- ﬁned a set of properties or conditions in order to be conceptual- ized. That is, an individual that satisﬁes those",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "order to be conceptual- ized. That is, an individual that satisﬁes those properties is consid- ered to be a member of that class. - Component . This class represents each processing step in the analytic workﬂow. It is used to encapsulate one concrete function- ality, its parameters and the corresponding inputs and outputs it considers. The class Component has four subclasses that are ori- ented to deﬁne speciﬁc functionalities in typical data analytics pro- cessing chains: DataCollection , to connect to data sources; DataPro- cessing , to clean, curate, fuse and consolidate data; DataAnalysis , to perform the algorithmic function; and DataSink , to represent ﬁnal steps in the data ﬂow, e.g., store and visualization. Table 3 con- tains the object and data properties deﬁned for Component .",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "3 con- tains the object and data properties deﬁned for Component . In ac- cordance with these, a component can specify Input classes and Output classes, to deﬁne the type of data it is accepting and gener- ating, respectively. Therefore, a component can connect with other one if their linking inputs and outputs are compatible among them. - Task . A task represents an instance of a component that is used in a workﬂow and can be run. As shown in Table 4 , the class Task has similar properties to those of Component , but including the object property compatibleWith , to specify compatibility among connected tasks, and the data property order , which indicates the speciﬁc step of execution in which this task is scheduled, in",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "the speciﬁc step of execution in which this task is scheduled, in the scope of the workﬂow. A Component is then a template for one or more tasks, which will be used to carry out its speciﬁc functional- ity in a workﬂow. 13 URL link https://www.github.com/KhaosResearch/BIGOWL . - Algorithm . This class is devoted to cover all possible kinds It has two main subclasses: DataMiningAlgorithm and OptimizationAl- gorithm ; which are used to distinguish between these two fami- lies of algorithms. The former one is included in form of equiv- alence with the class DM-Algorithm , which is linked from DMOP. This way, all subclasses deriving from this class in DMOP are also used in BIGOWL. For the later, i.e., OptimizationAlgorithm , a new hi- erarchical classiﬁcation of",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "For the later, i.e., OptimizationAlgorithm , a new hi- erarchical classiﬁcation of classes has been elaborated in this study for the annotation of this family, which comprises: Exact, Heuristic , and Metaheuristic algorithms as main subclasses. Table 5 includes the object and data properties of Algorithm . Among its main object properties it is worth mentioning: imple- ments , which is referred to a learning model or search strategy; manages , to annotate the type of data it works; and resolves , which is related to the Problem it is oriented to solve. This is a use- ful mechanism to relate classes Algorithm and Problem , which also share the data property dealWith that indicates the speciﬁc fea- tures an algorithm should fulﬁll to deal with a problem.",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "speciﬁc fea- tures an algorithm should fulﬁll to deal with a problem. In this regard, the class Problem deﬁnes a series of data proper- ties like: numberOfConstraints, numberOfObjectives, encodedBy , and numberOfVariables , that will lead a future reasoner to recommend the correct algorithm to solve it. These two classes have to be declared as DisjointWith , in order to avoid future inconsistencies when querying the annotated data in a workﬂow. - Data . The class Data is devoted to annotate all the data ﬂow- ing throughout the analytic workﬂow. It is declared as EquivalentTo IO-Class of DMOP. This aligning enables datatypes deﬁned by third parties’ ontologies to be contextualized in the analysis. Table 6 contains the main data properties deﬁned for this class, namely: path , to",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "the main data properties deﬁned for this class, namely: path , to annotate the origin of data; and hasDataType , which de- ﬁnes the relation with class DataType . This last is used to deﬁne the type of data, i.e. PrimitiveType (Double, Integer, Boolean, etc.) or StructuredType (Graph, Tree, Matrix, Vector, Tuple, etc.). - Workﬂow . It is used to guide the correct orchestration of those tasks involved in a data analysis job. Its main object prop- erties are hasTask and hasParameter , which are formally described in Table 7 . These properties are used by the workﬂow to obtain the execution order, as well as the input/output speciﬁcations of each [Página 6] 548 C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 Table 5",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "et al. / Expert Systems With Applications 115 (2019) 543–556 Table 5 Algorithm: object and data properties. Object properties Description logic hasComponent /latticetop /subsetsqequal ∀ hasComponent.Component hasParameter ∃ hasParameter.Thing /subsetsqequal Workﬂow /2294Algorithm /2294Component speciﬁesInputClass ∃ speciﬁesInputClass.Thing /subsetsqequal Algorithm /2294Component /2294Task speciﬁesOutputClass ∃ speciﬁesOutputClass.Thing /subsetsqequal Algorithm /2294Component /2294Task implements Transitive Property implements ∃ implements.Thing /subsetsqequal Algorithm /latticetop /subsetsqequal ∀ implements.Strategy manages ∃ manages.Thing /subsetsqequal Algorithm /latticetop /subsetsqequal ∀ manages.DataType resolves ∃ resolves.Thing /subsetsqequal Algorithm /latticetop /subsetsqequal ∀ resolves.Problem Data Properties Description Logic author ∃ author.Datatype Literal /subsetsqequal Workﬂow /2294Algorithm /2294Component /2294Problem /2294Software hasDataValue ∃ hasDataValue.Datatype Literal /subsetsqequal DataType /2294IO-Class /2294Parameter /2294Workﬂow /2294Algorithm /2294Component /2294Problem numberOfInputs ∃ numberOfInputs.Datatype Literal /subsetsqequal Algorithm /2294Component numberOfOutputs ∃ numberOfOutputs.Datatype Literal /subsetsqequal Algorithm /2294Component dealWith ∃ dealWith.Datatype Literal /subsetsqequal Algorithm /latticetop /subsetsqequal ∀ dealWith.Datatype Table",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "/2294Component dealWith ∃ dealWith.Datatype Literal /subsetsqequal Algorithm /latticetop /subsetsqequal ∀ dealWith.Datatype Table 6 Data: object and data properties. Object properties Description logic hasDataType ∃ hasDataType.Thing /subsetsqequal Parameter /2294Data /latticetop /subsetsqequal ∀ hasDataType.DataType path ∃ path.Datatype Literal /subsetsqequal IO-Class /2294Algorithm /2294Component Table 7 Workﬂow: object and data properties. Object properties Description logic hasTask ∃ hasTask.Thing /subsetsqequal Workﬂow /latticetop /subsetsqequal ∀ hasTask.Task hasParameter ∃ hasParameter Thing /subsetsqequal Workﬂow /2294Algorithm /2294Component Data Properties Description Logic author ∃ author.Datatype Literal /subsetsqequal Workﬂow /2294Algorithm /2294Component /2294Problem /2294Software hasDataValue ∃ hasDataValue.Datatype Literal /subsetsqequal DataType /2294IO-Class /2294Parameter /2294Workﬂow /2294Algorithm /2294Component /2294Problem isCorrectWorkﬂow ∃ isCorrectWorkﬂow.Datatype Literal /subsetsqequal Workﬂow /latticetop /subsetsqequal ∀ isCorrectWorkﬂow.Datatype numTasks ∃ numTask.Datatype /subsetsqequal Workﬂow /latticetop /subsetsqequal ∀ numTask.Datatype task. This information, together with the data properties numTasks and isCorrectWorkﬂow , is then used",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "together with the data properties numTasks and isCorrectWorkﬂow , is then used in reasoning time to check whether the workﬂow is correctly composed or not, i.e., to address semantic validation of the analytic workﬂow. 4.2. Overall approach An overview of the proposed semantic model is illustrated in Fig. 2 , which is arranged together with the underlying operational model, hence enabling actual composition of analytic workﬂows. In this approach, BIGOWL is the ontological scheme driving the whole process. It is the terminological box (TBox) that deﬁnes the vocabulary with concepts and properties in the domain of Big Data analysis. As explained before, BIGOWL is developed in OWL 2 ac- cording to which, concepts are represented by classes and relations are represented by data properties or object properties. As",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "classes and relations are represented by data properties or object properties. As repre- sented in Fig. 2 , BIGOWL is conceived as an abstract top-level on- tology that enables not only subontology replication e.g., to focus on speciﬁc use cases or algorithmic families, but also linkage with external domain knowledge ontologies, which are oriented to the speciﬁc problem domain (Smart Cities, Biology, etc.). At bottom-level, the Assertional Box (ABox) deﬁnes all the in- stances in the knowledge domain (in OWL 2 an instance is rep- resented by an individual) involving the analytic workﬂows’ meta- data. These instances are stored in RDF triple format in a Stardog 14 repository, which is a commercial version of the Pellet OWL 2 rea- soner ( Sirin, Parsia, Grau, Kalyanpur, & Katz, 2007",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "OWL 2 rea- soner ( Sirin, Parsia, Grau, Kalyanpur, & Katz, 2007 ), but enhanced with persistence capabilities. Once the ontology (Tbox) has been 14 http://www.stardog.com/ . loaded together with SWRL rules, a series of reasoning tasks are launched by using the Stardog OWL 2 reasoner to derive new infor- mation that is not explicitly expressed in the knowledge base. The new information will indicate, when applicable and among others, whether an analytic workﬂow is correctly composed, or not. In this model, the Annotation Module is used to populate the RDF repository with new instances that involve the required meta- data (annotated) to be used in workﬂows, for example: algorithms, operators, parameters, input/output (paths), data sources, database connections, data sinks, software, execution order, etc. The Operational Model will",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "database connections, data sinks, software, execution order, etc. The Operational Model will make use of these annotated meta- data for driving the workﬂow composition. In this process, each step a new component is to be selected and used, a SPARQL query is launched to obtain the required meta-data and to suggest the next possible component/s to be included. A very simple (hypothetical) case of use would comprise the following steps: (i) A user desires to extract patterns from a dataset and visual- ize the results; (ii) Then, the user selects one algorithm from a list of data mining algorithms (in form of analysis component) queried throughout the semantic model; (iii) The selected algorithm requires speciﬁc input parameters and data to train, so the semantic model will supply them;",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "parameters and data to train, so the semantic model will supply them; (iv) The initial dataset should be then formatted in form of data collection task; (v) In case collected data need transformation, an intermediate data processing component is included between collection and analysis; [Página 7] C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 549 Fig. 2. General overview of the semantic model that follows the ontology’s scheme of BIGOWL. The analytic operational model address the workﬂow composition driven by the semantic model (vi) The semantic model will suggest suitable output component (visualization) to be linked after the analytic algorithm. It is worth mentioning that each step in the workﬂow is instan- tiated by a task, which entails an execution order. Then, the entire",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "tiated by a task, which entails an execution order. Then, the entire workﬂow is arranged according to all the ordering values in tasks. In summary, the semantic model acts as a mediator between data provider components and data consumers. It also acts as a data source and meta-data registry with functions to make “agree- ments” on the provision and traceability of the whole data value chain. 5. Validation For validation purposes, two different cases of study have been developed to show how the proposed semantic approach is used for driving the composition of data analytic workﬂows. The ﬁrst one is focused on Big Data streaming processing and optimiza- tion of real-world traﬃc routes in the domain of Smart Cities. The second case study is centered on classic data",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "of Smart Cities. The second case study is centered on classic data mining analysis on academic problem instances, although considering local and cloud computing environments. In this way, we aim at covering, as much as possible, different aspects in Big Data applications: algorithmic analyses (optimization and data mining), velocity and volume is- sues (streaming processing), real-world and academic data prob- lems, and Big Data ecosystems (Apache Spark local and on-premise cluster, BigML cloud SaaS API). In these two cases, a similar semantic annotation and query- ing procedure has been followed, which consists in the man- ual annotation (guided by domain experts) of: algorithms, tech- nological/platform features, and attributes of problem domain of knowledge; and automatic querying by means of SPARQL sen- tences. To distinguish individuals belonging to each",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "by means of SPARQL sen- tences. To distinguish individuals belonging to each case study, two different namespaces has been deﬁned, i.e. traﬃc: http:// www.khaos.uma.es/perception/traﬃc/khaosteam# and weka: http: //www.khaos.uma.es/perception/weka/khaosteam# , respectively. 5.1. Case study 1: streaming processing of New York City traﬃc open-data The ﬁrst case study consists in a dynamic version of the bi-objective Traveling Salesman Problem (TSP), to minimize the “travel time” and the “distance” to cover certain routing points in a urban area. The algorithm for solving it is a dynamic variant of the well-known multi-objective metaheuristic NSGA-II provided in jMetalSP ( Barba-González, García-Nieto, Nebro, Cordero, Durillo, Navas-Delgado, & Aldana-Montes, 2017 ), 15 which allows parallel processing of evaluation functions in Apache Spark environment. In the case of the dynamic bi-objective TSP, which is formu- lated",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "In the case of the dynamic bi-objective TSP, which is formu- lated in terms of a distance matrix and a time travel matrix, the periodic changes can affect any of them. Our particular dynamic TSP problem instance is based on real-world data. Speciﬁcally, it is feed from the Open Data API provided by the New York City Department of Transportation, 16 which updates traﬃc information several times per minute. The information is provided as a text ﬁle where each line includes the average speed to traverse the two end points deﬁning a link in the most recent interval. The goal is then, given a list of nodes in New York city and the distances between each pair of nodes, calculate the shortest possible route that visits each node.",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "pair of nodes, calculate the shortest possible route that visits each node. New York’s traﬃc data is read periodically by an external appli- cation that writes a ﬁle in HDFS whenever new data are acquired, so we have implemented a streaming data component for that pur- pose. This component reads periodically the new data appeared in the speciﬁc directory (this is done automatically by Spark) and makes a simple processing: if a change in a link is detected (time or distance), then the corresponding problem matrices are up- dated. The analysis of the streaming data sources can be carried out in parallel by using Spark. In fact, we used a Hadoop cluster com- 15 https://www.github.com/jMetal/jMetalSP . 16 https://www.data.cityofnewyork.us/Transportation/Real- Time- Traﬃc- Speed- Data/ xsat-x5sa . [Página 8] 550 C.",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "16 https://www.data.cityofnewyork.us/Transportation/Real- Time- Traﬃc- Speed- Data/ xsat-x5sa . [Página 8] 550 C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 Fig. 3. Workﬂow for dynamic bi-objective optimization of TSP problem instance with Open Data New York posed of 100 cores in the previous study where the Big Data op- timization model was presented ( Barba-González et al., 2017 ). In addition, two other streaming data sources where used as sepa- rate components, which based on Twitter and Kafka. In the ﬁrst one, tweets are read from Twitter API with the topic “New York traﬃc” and a processing of each tweet is simulated, so the prob- lem is updated in accordance with it (for testing purposes we set random changes in traﬃc scenario). This way, we",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "testing purposes we set random changes in traﬃc scenario). This way, we combine a differ- ent streaming source with the possibility of adjusting the process- ing time, which will serve for performance evaluation purposes. In the second source, the idea is to enrich the case study with an- other data source that will produce artiﬁcial data. Then we created a Kafka message producer that generates, following uniform and normal distributions, a series of random messages with data to up- date the problem. Every 5 s at least 10 0 0 messages are produced, but on average about 10,0 0 0 messages are created. Both the Twit- ter and Kafka streaming source classes have the same behavior as the HDFS based one: they iteratively collect and analyze the data",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "as the HDFS based one: they iteratively collect and analyze the data to somehow update the problem. After data processing, the analytic task is then carried out, which entails dynamic optimization computed by NSGAII algo- rithm of the jMetalSP library. The results of the analysis are used to feed data sinks. In this case study, we consider two of them: one that stores the produced Pareto fronts in HDFS, and other one that visualizes information about the Pareto front approximation (as the number of solutions and the number of generated fronts) using R-plot library. The workﬂow implementing this case study is represented in Fig. 3 , 17 where all the components are arranged according to data ﬂow. In this workﬂow, the numeric indexes (1)–(7) correspond to those steps",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "ﬂow. In this workﬂow, the numeric indexes (1)–(7) correspond to those steps as indicated in Table 8 , which contain the required SPARQL queries the semantic model apply to recommend forth- coming component/s to use, in design time. For this case study, the main set of individuals annotated in the semantic model and their relationships, are shown in Fig. 4 . Then it is possible to follow the complete process step-by-step: •Step (1) . The workﬂow designer fetch all the optimization prob- lems from BIGOWL to select the implementation that better ﬁts the required model for TSP instances. Interestingly, they are all subclasses of OptimizationProblem , which is integrated from DMOP. As a result, (s)he selects TSP. •Step (2) . Given a problem to solve, TSP in this",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "TSP. •Step (2) . Given a problem to solve, TSP in this case, the seman- tic model recommends a series of optimization algorithms that could deal with it, i.e., those annotated algorithms that better 17 Ontology instances available at https://www.github.com/KhaosResearch/ BIGOWL/blob/master/traﬃc.owl . adapt to the problem in terms of properties, such as: solution encoding, manages, dealWith , etc. After this, the designer selects NSGAII. •Step (3) . This is an intermediate step followed by the semantic model to recommend speciﬁc annotated component and task instancing the underlying software that implements TSP and NSGAII. •Step (4) . Now, the objective of this query is to obtain the spe- ciﬁc data model to properly host data in problem and algorithm tasks. This step is thought to use speciﬁc domain knowledge",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "and algorithm tasks. This step is thought to use speciﬁc domain knowledge information (traﬃc routes in this case) coming from external ontologies. The resulting annotated instance here is MatrixNY , which refers to a data model comprising a matrix of points and distances in the scenario of New York city. •Step (5) . Once the workﬂow designer has a clear idea about the data model, (s)he can set data sources and connect them to feed the analysis. The semantic model is then queried to show all possible data collectors, i.e., those previously anno- tated. Among all the resulting possibilities, ReadWebNYDataTraf- ﬁc, DataCollectionDataTraﬃcKafka and DataCollectionTwitter are selected for this case study. •Step (6) . Before connecting data sources to analytic component, a previous task is required for data processing",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "sources to analytic component, a previous task is required for data processing and consolida- tion. In this case study, the corresponding component is im- plemented as a Spark processing task to join Kafka messages, Tweets and traﬃc data streams. •Step (7) . Last steps usually correspond to data sink tasks to al- locate results from analyses. For this case study, Visualization- Task and HDFSStoreTask are selected, which implement R-plot visualization and storage in HDFS, respectively. •Step (8) . Finally, the semantic model is queried to obtain the corresponding task instances that are mutually compati- ble among them. The analytic workﬂow is now ready to be launched on the underlying running platform. Moreover, once the whole process is completed, a further rea- soning procedure can now be started to",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "is completed, a further rea- soning procedure can now be started to check whether the gen- erated workﬂow is semantically consistent, or not. This reasoning task will be explained in Section 5.3 . 5.2. Case study 2: classiﬁcation with Iris ﬂower dataset As commented before, the second case study consists in the academic problem of Irish ﬂower classiﬁcation by means of deci- sion tree J48, a classical algorithm for data mining analytics. For materialization, two different approaches have been used in this case: the well-known library for data mining Weka and the BigML SaaS API for analysis on-cloud. The aim is to illustrate how similar annotation and querying procedures with BIGOWL can be used to [Página 9] C. Barba-González et al. / Expert Systems With Applications 115 (2019)",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "9] C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 551 Table 8 SPARQL queries for case study of streaming processing of New York city traﬃc open-data. Step SPARQL Result (1)SELECT DISTINCT ?problem WHERE { ?problem rdf:type ?type . ?type rdfs:subClassOf* dmop:OptimizationProblem .}TSP, ZDT1, ZDT2, ZDT3, ZDT4, ZDT5, ZDT6, Kursawe.. (2)SELECT DISTINCT ?algorithm (count(DISTINCT ?propertiesAlgorithm) AS numProperties) WHERE { traffic:TSP bigowl:encodedBy ?solution.?algorithm rdf:type ?type.?type rdfs:subClassOf* bigowl:OptimizationAlgorithm. ?entity bigowl:manages ?solution . ?algorithm bigowl:dealWith ?propertiesAlgorithm . traffic:TSP bigowl:hasFeature ?propertiesTSP . FILTER ( ?propertiesTSP in (?propertiesAlgorithm)). } GROUP BY ?algorithm ORDER BY DESC(?numProperties)NSGAII, MOCell, SMSEMOA,SPEA2, IBEA, PAES, PESA2, WASFGA (3)SELECT distinct ?comp ?task WHERE { ?comp bigowl:hasProblem traffic:TSP . ?comp bigowl:hasAlgorithm traffic:NSGAII . ?comp rdf:type bigowl:Optimization .?task rdf:type bigowl:Task . ?task bigowl:hasComponent ?comp. }OptmimizationComponent, OptimizationTask (4)SELECT distinct ?data",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": ".?task rdf:type bigowl:Task . ?task bigowl:hasComponent ?comp. }OptmimizationComponent, OptimizationTask (4)SELECT distinct ?data WHERE { ?comp bigowl:hasProblem traffic:TSP . ?comp bigowl:hasAlgorithm traffic:NSGAII . ?comp rdf:type bigowl:Optimization . ?task rdf:type bigowl:Task . ?task bigowl:hasComponent ?comp. ?task bigowl:specifiesInputClass ?data . }MatrixNY (5)SELECT distinct ?dataCollection WHERE { ?dataCollection rdf:type ?type. ?type rdfs:subClassOf* bigowl:DataCollection.}ReadWebNYDataTraﬃc, DataCollectionHDFS, DataCollectionDataTraﬃcKafka, DataCollectionTwitter, DataCollectionDB, ... (6)SELECT distinct ?taskProcessing ?compProcessing WHERE { ?taskCollection bigowl:hasComponent bigowl:ReadNYDataTraffic. ?taskCollection bigowl:specifiesOutputClass ?out. ?dataProcessing rdf:type ?typeProcessing . ?typeProcessing rdfs:subClassOf* bigowl:DataProcessing. ?taskProcessing bigowl:hasComponent ?dataProcessing .?taskProcessing bigowl:specifiesInputClass ?out. ?taskProcessing bigowl:specifiesOutputClass traffic:MatrixNY. }SparkTask, ComponentSpark (7)SELECT distinct ?dataSink WHERE { ?dataSink rdf:type ?type. ?type rdfs:subClassOf* bigowl:DataSink.}VisualizationPlot, DataSinkHDFSStore, DataSinkOracleStore, ... (8)SELECT distinct ?task1 ?task2 WHERE { ?task1 rdf:type bigowl:Task . ?task2 rdf:type bigowl:Task . ?task1 bigowl:specifiesOutputClass ?output . ?task2 bigowl:specifiesInputClass ?output . }GeneratorDataTraﬃcTask, SparkTask, TwitterCollectorTask, KafkaMGTask, ReadNYDataTraﬃcTask, OptimizationTask, VisualizationTask compose",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "?task2 bigowl:specifiesInputClass ?output . }GeneratorDataTraﬃcTask, SparkTask, TwitterCollectorTask, KafkaMGTask, ReadNYDataTraﬃcTask, OptimizationTask, VisualizationTask compose workﬂows on different platforms when solving the same problem. Fig. 5 shows the individuals (and their relationships) anno- tated in the ontology, and Fig. 6 18 represents graphically the an- alytic workﬂow for this case study. The numeric labels (1)–(5) are 18 Ontology instances available at https://www.github.com/KhaosResearch/ BIGOWL/blob/master/weka.owl . aligned with their corresponding steps in Table 9 that contain the SPARQL queries used and their results. In a nutshell, steps (1)–(3) are used to guide the workﬂow de- signer on the selection of data model, algorithm, and analysis com- ponents and tasks, respectively. Step (4) is used to query suit- able data collector components, in this case the designer selects DataCollectionBigML for BigML API instance and",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "in this case the designer selects DataCollectionBigML for BigML API instance and DataCollectorFS for Weka instance dataset. Step (5) queries are devoted to select possi- ble data sink components, and speciﬁcally DataSinkFSStore and Vi- [Página 10] 552 C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 Fig. 4. BIGOWL’s individuals annotated in the workﬂow for dynamic bi-objective optimization of TSP problem Fig. 5. BIGOWL’s individuals in workﬂow for Irish ﬂower classiﬁcation with J48 decision tree instanced from Weka Fig. 6. Workﬂow for Irish ﬂower classiﬁcation with J48 decision tree instanced from Weka and BigML. sualizationPlot , which implement orders to save results in ﬁle sys- tem and API method for plotting in BigML, respectively. Finally, step (6) obtains the corresponding task instances that are mutu-",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "respectively. Finally, step (6) obtains the corresponding task instances that are mutu- ally compatible among them throughout the complete workﬂow. 5.3. Reasoning with BIGOWL Reasoning procedure is built in BIGOWL with formulation of se- mantic rules on top of the OWL ontology, to deduce new informa- tion from the existing knowledge. These rules are formulated in SWRL and used to perform semantic reasoning jobs mainly de- voted to check correctness of workﬂows, e.i., to discover those components and tasks with (non-)compatible connectivity of in- puts/outputs, execution orders, data domains, data formats, data [Página 11] C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 553 Table 9 SPARQL queries for case study Irish ﬂower classiﬁcation on Weka, as well as on BigML. Step SPARQL Result (1)SELECT",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "classiﬁcation on Weka, as well as on BigML. Step SPARQL Result (1)SELECT DISTINCT ?individual WHERE { ?individual rdf:type ?type . ?type rdfs:subClassOf* bigowl:DMDataClass . }Iris, Contact-lens, CPU, Diabetes, Glass, Ionosphre, Labor, ReutersCorn, Segment,.. (2)SELECT ?algorithm WHERE { weka:Iris rdf:type ?typeD .?typeD rdfs:subClassOf* ?classSomePropertyAlgorithm.?algorithm rdf:type ?type.?type rdfs:subClassOf* bigowl:DataMiningAlgorithm. bigowl:DataMiningAlgorithm rdfs:subClassOf* [ a owl:Restriction ; owl:onProperty bigowl:manages ; owl:someValuesFrom ?classSomePropertyAlgorithm ] . }J48, LogisticRegression, NaiveBayes, RepTree, IBk, LinearNNSearch, SMO, ... (3)SELECT distinct ?comp ?taskWHERE { ?comp bigowl:hasAlgorithm weka:J48 .?task rdf:type bigowl:Task . ?task bigowl:hasComponent ?comp. } ClassiﬁcationJ48Component, ClassiﬁcationJ48Task (4)SELECT distinct ?dataCollection WHERE { ?dataCollection rdf:type ?type. ?type rdfs:subClassOf* bigowl:DataCollection.}DataCollectionOpenData, DataCollectionBigML, DataCollectionHDFS, DataCollectorFS, ... (5)SELECT distinct ?dataSink WHERE { ?dataSink rdf:type ?type. ?type rdfs:subClassOf* bigowl:DataSink.}VisualizationPlot, DataSinkHDFSStore, DataSinkOracleStore, DataSinkFSStore, ... (6)SELECT distinct ?task1 ?task2 WHERE { ?task1 rdf:type bigowl:Task . ?task2 rdf:type",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "(6)SELECT distinct ?task1 ?task2 WHERE { ?task1 rdf:type bigowl:Task . ?task2 rdf:type bigowl:Task . ?task1 bigowl:specifiesOutputClass ?output .?task2 bigowl:specifiesInputClass ?output . }ClassAsignerIrisTask, ClassiﬁcationJ48Task, ClassiﬁerPerformanceEvaluatorTask, CrossValidaionFolderMarkerTask, TextViewerTask types, etc. SWRL rules are then evaluated by the reasoner after classifying Big Data components in accordance with axioms, as de- ﬁned in Table 1 . In concrete, there are two types of axioms associ- ated with OWL-DL classes for reasoning, namely: subClassOf , which is used to deﬁne the necessary conditions for a class to be consid- ered a member of a given OWL class; and equivalentClass , for an- notating when two classes can be considered as equivalent, if they comply the conditions. BIGOWL imports subClassOf axioms from DMOP to specify tax- onomy classiﬁcation of Data Mining contexts and their",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "DMOP to specify tax- onomy classiﬁcation of Data Mining contexts and their data. In this sense, subclasses are also the natural way of describing hierarchy of algorithmic families and versions in optimization analyses. For instance, Genetic Algorithms are subclasses of Evolutionary Algo- rithms and these in turn, are subclasses of Population Based Algo- rithms. This structural information is then considered in reasoning time for algorithm recommendation. The main axioms for subclass classiﬁcation are deﬁned in Table 10 , which correspond to Data Mining and Optimization algorithmic families. Furthermore, a series of speciﬁc SWRL rules are described for assessing the compatibility of components. As commented before, the main goal is to address the generation of well-formed Big Data workﬂows. A description of these rules is as follows: - Compatibility",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "Data workﬂows. A description of these rules is as follows: - Compatibility between task, component and Data Mining algorithm . This rule is used to check that input data model is com- patible with the task that is indeed an instance (or implementa- tion) of a component. In this speciﬁc case, the used component refers to a Data Mining Algorithm to perform a speciﬁc analysis. In short, this rule is used by the reasoner to validate compatibility between data mining component and data source. The result is a predicate indicating that data “feeding” the component are com- patible with the analytic algorithm, so a task can be launched to run it on the underlying platform. bigowl:specifiesInputClass(?task, ?data) ˆ bigowl:hasComponent(?task, ?comp) ˆbigowl:hasAlgorithm(?comp, ?alg) ˆbigowl:DataMiningAlgorithm(?alg) ˆ bigowl:DMDataClass(?data) -> bigowl:isCorrect(?alg, ?data)",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "?data) ˆ bigowl:hasComponent(?task, ?comp) ˆbigowl:hasAlgorithm(?comp, ?alg) ˆbigowl:DataMiningAlgorithm(?alg) ˆ bigowl:DMDataClass(?data) -> bigowl:isCorrect(?alg, ?data) Note that a similar rule is deﬁned in the semantic model to consider optimization algorithms. - Compatibility between tasks of a workﬂow . This rule is ap- plied to a complete workﬂow. It is used to check that input/output data connections of each pair of consecutive tasks are “semanti- [Página 12] 554 C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 Table 10 OWL axioms for algorithmic subclass classiﬁcation. Class Classiﬁcation rule Optimization AlgorithmOptimizationAlgorithm subClassOf ((implements some OptimizationStrategy) and (resolves some OptimizationProblem)) or Algorithm DataMining AlgorithmOptimizationAlgorithm subClassOf (manages some DMDataClass) or Algorithm Optimization ComponentOptimization subClassOf (hasAlgorithm only (OptimizationAlgorithm or MachineLearning)) DataMining ComponentDataMining subClassOf (hasAlgorithm only (DataMiningAlgorithm or MachineLearning)) cally” similar. The outcome is",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "ComponentDataMining subClassOf (hasAlgorithm only (DataMiningAlgorithm or MachineLearning)) cally” similar. The outcome is a new predicate indicating whether each two consecutive tasks are mutually compatible, or not. Workflow(?w) ˆ bigowl:hasTask(?w, ?task1) ˆbigowl:order(?task1, ?ord1) ˆbigowl:hasTask(?w, ?task2) ˆ bigowl:order(?task2, ?ord2) ˆ swrlb:add(?ord2, ?ord1, 1) ˆbigowl:specifiesInputClass(?task2, ?data)ˆ bigowl:specifiesOutputClass(?task1, ?data) -> bigowl:compatibleWith(?task1, ?task2) - Connectivity between tasks and data . Similarly to the pre- vious one, this rule is used to indicate that two instances of tasks are properly linked, that is to say, it checks that the input data of task2 are covered with the output data of task1 , according to the execution order established in the workﬂow. Workflow(?w) ˆbigowl:hasTask(?w, ?task1) ˆbigowl:order(?task1, ?ord1) ˆbigowl:hasTask(?w, ?task2) ˆ bigowl:order(?task2, ?ord2) ˆ swrlb:add(?ord2, ?ord1, 1) ˆbigowl:specifiesInputClass(?task2, ?data) ˆ bigowl:specifiesOutputClass(?task1, ?data) -> bigowl:isConnected(?task2, ?data) -",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "swrlb:add(?ord2, ?ord1, 1) ˆbigowl:specifiesInputClass(?task2, ?data) ˆ bigowl:specifiesOutputClass(?task1, ?data) -> bigowl:isConnected(?task2, ?data) - Workﬂow correctness . Finally, this rule validates that all the components, instanced by corresponding tasks and data sources, are correctly arranged and connected. The result is then a new predicate indicating whether the complete workﬂow is correct, or not. Workflow(?w) ˆbigowl:hasTask(?w, ?task) ˆbigowl:numberOfInput(?task, ?nIn) ˆbigowl:isConnected(?task, ?data). sqwrl:makeSet(?set, ?data) ˆ sqwrl:groupBy(?set, ?task).sqwrl:size(?cont, ?set) ˆ swrlb:equal(?cont, ?nIn) -> sqwrl:select(?cont, ?nIn, ?task) ˆbigowl:isCorrectWorkflow(?w, true)In summary, these case studies are used as a “proof of concept ” to somehow highlight that the proposed semantic model is able to support in the design of Big Data analytics. In this regard, BIGOWL enables automatic SPARQL querying for component recommenda- tion, as well as reasoning procedures for workﬂow validation. 6. Discussions One of",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "as well as reasoning procedures for workﬂow validation. 6. Discussions One of the main research ﬁndings we claim with the design and implementation of BIGOWL is the ability to represent and con- solidate knowledge involving Big Data analytics. This semantic ap- proach allows us to annotate (i.e. to “semantize”) all the meta- data ﬂowing from multiple data sources, processing components and analytic algorithms. The meta-data are integrated following the BIGOWL structure and stored in an RDF repository. On the one hand, the results obtained in the two case stud- ies indicate that, driven by the ontological model, it is possible to progressively deliver component recommendations for the con- struction of Big Data analytics workﬂows. The resulting workﬂows are indeed enhanced with semantic knowledge that explicitly de- scribes and",
        "start_idx": 8236,
        "end_idx": 8364
      },
      {
        "text": "workﬂows are indeed enhanced with semantic knowledge that explicitly de- scribes and registers the data lineage (data provenance in database systems), from sources to results. It also would enable to replay speciﬁc portions or inputs of the data ﬂow for step-wise debug- ging or regenerating lost outputs. In the BIGOWL semantic model, data linage is mapped with RDF triples referring to records of the inputs, entities, systems, algorithms and processes that inﬂuence data of interest, hence providing a historical record of the data ob- tained (as results) and its origins (as sources). Based on the analysis provided in the two cases studies, the user is able to identify the correct path the data follow and how they are modiﬁed to obtain added value, for a given domain of",
        "start_idx": 8352,
        "end_idx": 8480
      },
      {
        "text": "they are modiﬁed to obtain added value, for a given domain of knowledge. For example, in the ﬁrst case study, a series of data sources involving information about urban traﬃc in the city of New York (with geo-locations, travel times, densities, tweets, etc.) are semantically related (or linked) to the results obtained, in form of optimized routes in a problem characterization of the classical TSP. In this case study, the outputs are encoded in form of routes, where the travel time and the routing distance are optimized. This way, the resulting routes are linked to the traﬃc densities and the Twitter messages, so the data lineage is registered with semantic annotations. Similarly, in the second case study, it is possible to connect prediction accuracies with classiﬁcation algorithms, for",
        "start_idx": 8468,
        "end_idx": 8596
      },
      {
        "text": "study, it is possible to connect prediction accuracies with classiﬁcation algorithms, for the Irish ﬂower database. In addition, the running experiences acquired when using different execution frameworks, e.g., in-house/in-cloud, are also annotated as results. [Página 13] C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 555 Another important ﬁnding lies in the possibility of using the semantic knowledge-base, now consolidated in the RDF repository, to perform reasoning tasks, hence to infer new knowledge. In this study, a series of SWRL rules are used to train the reasoner. In this study, a reasoner is used to evaluate a set of SWRL rules deﬁned for the speciﬁc task of workﬂow validation. In this regard, the val- idation analysis performed by the reasoner required 644 ms for case",
        "start_idx": 8584,
        "end_idx": 8712
      },
      {
        "text": "val- idation analysis performed by the reasoner required 644 ms for case study 1 and 673 ms for case study 2. Taking into account that we used the Stardog OWL 2 reasoner, the time spent in reasoning tasks is acceptable for workﬂow validation. On the other hand, the main constraint of the proposed seman- tic model is that it needs a domain ontology to cover the prob- lem knowledge domain. This domain ontology contains the spe- ciﬁc concepts for a given case, so it can be reused in domains where previous efforts provided such model. However, if such on- tology is not available, then its design is required. As explained in Section 4.1 , the class Data in BIGOWL is used, not only to an- notate all the",
        "start_idx": 8700,
        "end_idx": 8828
      },
      {
        "text": "Data in BIGOWL is used, not only to an- notate all the data ﬂowing in the analytic workﬂow, but also to allow alignment with third parties’ ontologies covering the spe- ciﬁc problem domain of knowledge. Additionally, the general on- tology could miss concepts that would be needed in some cases and are not described in the current model. This constraint can be solved by proposing an extension, in form of new version release of BIGOWL, though a collaborative portal. In this sense, BIGOWL is publicly available at WebProtégé, 19 where any registered user can introduce changes. These changes will be reviewed in a regular ba- sis to approve or reject them. The last stable version of the ontol- ogy will be provided in the project GitHub repository. 20",
        "start_idx": 8816,
        "end_idx": 8944
      },
      {
        "text": "the ontol- ogy will be provided in the project GitHub repository. 20 In addition, a secondary constraint arises when a new workﬂow is generated or executed by a user, since a series of new annota- tions are required to store all the meta-data involved in the data analytic process, in form of RDF triples. This makes the RDF repos- itory to increase signiﬁcantly, which would promote, not only fu- ture reasoning procedures to infer new knowledge from these data, but also their connection with other Linked Data. In this sense, the eﬃcient management of large RDF repositories has become a challenging task attracting many scholars to research ( Zomaya & Sakr, 2017 ), which means a clear implication for academia. In terms of practical implications, the proposed semantic",
        "start_idx": 8932,
        "end_idx": 9060
      },
      {
        "text": "clear implication for academia. In terms of practical implications, the proposed semantic model represents an initial demonstrator for the experimental piloting of Big Data frameworks enhanced with semantics. The objective is to obtain “Smart Data” and promote the data value chain in industry processes, which is a key challenge nowadays as reﬂected in the Strategic Research and Innovation Agenda of the Big Data Value As- sociation (EU SRIA 4.0 BDVA). 21 Several industrial projects in this association, like BigDataEurope 22 and BigOceanData, 23 are focused on exploiting semantics in Big Data analytics, so they could par- tially take advantage of BIGOWL as reference ontological model. 7. Conclusions In this work, an ontological approach called BIGOWL is pro- posed to provide a conceptual framework for the annotation of Big",
        "start_idx": 9048,
        "end_idx": 9176
      },
      {
        "text": "pro- posed to provide a conceptual framework for the annotation of Big Data analytics. The proposed semantic model is materialized by means of an RDF repository, and programmatic querying and reasoning functions. To test the initial hypothesis, two case studies have been devel- oped, which consist in: (1) real-world streaming traﬃc data pro- cessing for route optimization in urban environment, and (2) aca- demic data mining classiﬁcation on local/on-cloud platforms. The 19 WebProtégé https://www.goo.gl/F6fYUc . 20 GitHub https://www.github.com/KhaosResearch/BIGOWL . 21 http://www.bdva.eu/sites/default/ﬁles/BDVA _ SRIA _ v4 _ Ed1.1.pdf . 22 https://www.big- data- europe.eu/ . 23 http://www.bigoceandata.com/ . experience on these cases revealed that BIGOWL approach is useful when integrating knowledge domain concerning a speciﬁc analytic problem. Consequently, the integrated knowledge is used for guid- ing the design of Big",
        "start_idx": 9164,
        "end_idx": 9292
      },
      {
        "text": "the integrated knowledge is used for guid- ing the design of Big Data analytics workﬂows, by recommending next components to be linked, and supporting ﬁnal validation. It is worthy to declare that the proposed semantic model is cur- rently populated with those annotated elements required to set the case studies reported in this work, although it can be feed with new instances regarding other Big Data workﬂows. This motivates our future research agenda, which entails a ﬁrst phase to provide automatic facilities for ontology population, hence to enrich the semantic approach; second, to provide new mechanisms to promote the use of contextual domain of knowl- edge in the generation of Big Data analytic solutions; and third, to generate new and heterogeneous use cases of analytics workﬂows that would",
        "start_idx": 9280,
        "end_idx": 9408
      },
      {
        "text": "to generate new and heterogeneous use cases of analytics workﬂows that would led us to ﬁnd and solve new possible deﬁciencies, as well as to enrich the knowledge base. References ACM-SIGKDD (2014). Data mining curriculum. ACM SIGKDD 2006-04-30. Retrieved 2014-01-27. Allahyari, M. , Kochut, K. , & Janik, M. (2014). Ontology-based text classiﬁcation into dynamically deﬁned topics. In 2014 IEEE international conference on semantic computing (pp. 273–278) . Barba-González, C. , García-Nieto, J. , Nebro, A. J. , Cordero, J. A. , Durillo, J. J. , Navas-Delgado, I. , et al. (2017). Jmetalsp: A framework for dynamic multi-ob- jective big data optimization. Applied Soft Computing . In–Press–Online Diamantini, C., Potena, D., & Storti, E.. Ontology-driven kdd process composition. Dou, D. , Wang, H. , & Liu, H. (2015).",
        "start_idx": 9396,
        "end_idx": 9524
      },
      {
        "text": "process composition. Dou, D. , Wang, H. , & Liu, H. (2015). Semantic data mining: A survey of ontolo- gy-based approaches. In Semantic computing (icsc), 2015 ieee international con- ference on (pp. 244–251). IEEE . Grosof, B. N. , & Poon, T. C. (2004). SweetDeal: Representing agent contracts with exceptions using semantic web rules, ontologies, and process descriptions. In- ternational Journal of Electronic Commerce, 8 (4), 61–97 . Gruber, T. R. (1995). Toward principles for the design of ontologies used for knowledge sharing? International Journal of Human-Computer Studies, 43 (5–6), 907–928 . Gruber, T. R. , et al. (1993). A translation approach to portable ontology speciﬁca- tions. Knowledge Acquisition, 5 (2), 199–220 . Harris, S. , Seaborne, A. , & Prud’hommeaux, E. (2013). Sparql 1.1 query language. W3C",
        "start_idx": 9512,
        "end_idx": 9640
      },
      {
        "text": "Seaborne, A. , & Prud’hommeaux, E. (2013). Sparql 1.1 query language. W3C Recommendation, 21 (10) . Horrocks, I. , Patel-Schneider, P. F. , Bechhofer, S. , & Tsarkov, D. (2005). OWL rules: A proposal and prototype implementation. Web Semantics: Science, Services and Agents on the World Wide Web, 3 (1), 23–40 . Jing, L. , Ng, M. , & Huang, J. (2010). Knowledge-based vector space model for text clustering. Knowledge and Information Systems, 25 (1), 35–55 . Keet, C. , Ławrynowicz, A. , d’Amato, C. , Kalousis, A. , Nguyen, P. , & Palma, R. (2015). The data mining optimization ontology. Web Semantics, 32 , 43–53 . Kietz, J. , Serban, F. , Bernstein, A. , & Fischer, S. (2010). Data mining workﬂow tem- plates for intelligent discovery",
        "start_idx": 9628,
        "end_idx": 9756
      },
      {
        "text": "& Fischer, S. (2010). Data mining workﬂow tem- plates for intelligent discovery assistance and auto-experimentation. In Proceed- ings- of the ecml/pkdd: 10 (pp. 1–12) . Konys, A. (2016). Ontology-based approaches to big data analytics. In International multi-conference on advanced computer systems (pp. 355–365) . Kuiler, E. W. (2014). From big data to knowledge: An ontological approach to big data analytics. Review of Policy Research, 31 (4), 311–318 . Kumara, B. T. G. S. , Paik, I. , Zhang, J. , Siriweera, T. H. A. S. , & Koswatte, K. R. C. (2015). Ontology-based workﬂow generation for intelligent big data analytics. In 2015 ieee international conference on web services (pp. 495–502) . Li, L. , Yevseyeva, I. , Basto-Fernandes, V. , Trautmann, H. , Jing, N. , & Em-",
        "start_idx": 9744,
        "end_idx": 9872
      },
      {
        "text": ", Basto-Fernandes, V. , Trautmann, H. , Jing, N. , & Em- merich, M. (2017). Building and using an ontology of preference-based multi- objective evolutionary algorithms. In H. Trautmann, G. Rudolph, K. Klamroth, O. Schütze, M. Wiecek, Y. Jin, & C. Grimme (Eds.), Evolutionary multi-criterion optimization: 9th international conference, EMO 2017, Münster, Germany, March 19–22, 2017, proceedings (pp. 406–421). Cham: Springer International Publish- ing . Marinica, C. , & Guillet, F. (2010). Knowledge-based interactive postmining of associa- tion rules using ontologies. IEEE Transactions on Knowledge and Data Engineering, 22 (6), 784–797 . Masolo, C. , Borgo, S. , Gangemi, A. , Guarino, N. , & Oltramari, A. (2003). Wonderweb deliverable d18, ontology library (ﬁnal). ICT Project, 33052 . McBride, B. (2004). The resource description framework (rdf) and its",
        "start_idx": 9860,
        "end_idx": 9988
      },
      {
        "text": "33052 . McBride, B. (2004). The resource description framework (rdf) and its vocabulary de- scription language rdfs. In Handbook on ontologies (pp. 51–65). Springer . McGuinness, D. L. , Van Harmelen, F. , et al. (2004). Owl web ontology language overview. W3C Recommendation, 10 (10), 2004 . Nguyen, P. , Hilario, M. , & Kalousis, A. (2014). Using meta-mining to support data mining workﬂow planning and optimization. Journal of Artiﬁcial Intelligence Re- search, 51 , 605–644 . Noy, N. , & McGuinness, D. L. (2001). Ontology development 101: A guide to creating your ﬁrst ontology. Technical report . [Página 14] 556 C. Barba-González et al. / Expert Systems With Applications 115 (2019) 543–556 Noy, N. F., McGuinness, D. L. et al. (2001). Ontology development 101: A guide to",
        "start_idx": 9976,
        "end_idx": 10104
      },
      {
        "text": "McGuinness, D. L. et al. (2001). Ontology development 101: A guide to creating your ﬁrst ontology. Phan, N. , Dou, D. , Wang, H. , Kil, D. , & Piniewski, B. (2015). Ontology-based deep learning for human behavior prediction in health social networks. In Proceed- ings of the 6th ACM conference on bioinformatics, computational biology and health informatics (pp. 433–442). ACM . Pinto, A. , Scioscia, F. , Loseto, G. , Ruta, M. , Bove, E. , & Sciascio, E. D. (2015). A seman- tic-based approach for machine learning data analysis. In 2015 IEEE international conference on semantic computing (ICSC) (pp. 324–327) . Prud, E. , & Seaborne, A. (2006). Sparql query language for rdf. W3C Recommendation . Ristoski, P. , & Paulheim, H. (2016). Semantic web in",
        "start_idx": 10092,
        "end_idx": 10220
      },
      {
        "text": "Recommendation . Ristoski, P. , & Paulheim, H. (2016). Semantic web in data mining and knowledge discovery: A comprehensive survey. Web Semantics: Science, Services and Agents on the World Wide Web, 36 , 1–22 . Roldán-García, M. , García-Nieto, J. , & Aldana-Montes, J. F. (2017). Enhancing seman- tic consistency in anti-fraud rule-based expert systems. Expert Systems with Ap- plications, 90 (Supplement C), 332–343 . Shearer, C. (20 0 0). The crisp-dm model: The new blueprint for data mining. Journal of Data Warehousing, 5 (4), 13–22 . Sirin, E. , Parsia, B. , Grau, B. C. , Kalyanpur, A. , & Katz, Y. (2007). Pellet: A practical owl-dl reasoner. Web Semantics: Science, Services and Agents on the WWW, 5 (2), 51–53 . Staab, S. , & Studer, R.",
        "start_idx": 10208,
        "end_idx": 10336
      },
      {
        "text": "the WWW, 5 (2), 51–53 . Staab, S. , & Studer, R. (2013). Handbook on ontologies . Springer Science & Business Media . Yaman, A. , Hallawa, A. , Coler, M. , & Iacca, G. (2017). Presenting the ECO: Evolution- ary computation ontology. In European conference on the applications of evolu- tionary computation (pp. 603–619) . Záková, M. , Kremen, P. , Zelezny, F. , & Lavrac, N. (2011). Automating knowledge dis- covery workﬂow composition through ontology-based planning. IEEE Transac- tions on Automation Science and Engineering, 8 (2), 253–264 . Zomaya, A. Y. , & Sakr, S. (2017). Handbook of big data technologies (1st). Springer International Publishing .",
        "start_idx": 10324,
        "end_idx": 10432
      }
    ],
    "036993c2-b385-494a-a35a-12b1a21af260": [
      {
        "text": "[Página 1] Testing of big data analytics systems by benchmark Mingang Chen Shanghai Key Laboratory of Computer Software Testing and Evaluating Shanghai Development Center of Computer Software Technology Shanghai, China cmg@ssc.stn.sh.cn Wenjie Chen, Lizhi Cai Shanghai Key Laboratory of Computer Software Testing and Evaluating Shanghai Development Center of Computer Software Technology Shanghai, China cwj@ssc.stn.sh.cn, clz@ssc.stn.sh.cn Abstract —With the rapid development of big data technologies and applications, various big data analytics systems have been released by open source communities and industry. So testing and evaluating the overall performance of these big data analytics systems has become an important research topic. The paper analyzes in detail the challenges of testing big data analytics systems and proposes the method and strategies for the testing. Furthermore, the paper presents two cases of",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "and strategies for the testing. Furthermore, the paper presents two cases of testing big data analytics systems by benchmark. Keywords—testing; big data; benchmark; TPC-DS; TPCx- BigBench. I. INTRODUCTION In recent years, big data has become a hot topic for governments and enterprises, and it is considered as a new driving force for innovation in the information era. This is based on the following two facts: firstly, in the past ten years, the speed of data generating is becoming faster and faster, and we have already entered the big data era; secondly, big data contains huge values, and has brought about revolutionary developments in many fields, such as e-commerce, finance, transportation, medical and health service, etc. However, the “3V” characteristics (volume, variety, and velocity) of big data make challenges",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "the “3V” characteristics (volume, variety, and velocity) of big data make challenges for data processing and analytics. Recently, industry and academia have launched a variety of big data analytics system to cope with the challenges, such as open source Apache Hive [1], Apache Spark [2], and commercial Transwarp Inceptor, Cloudera Impala, IBM Big SQL and so on. More and more enterprises or organizations use big data analytics system to build the business application and obtain decision support from data. Therefore, testing and evaluating big data analytics systems has become one of the important research subjects of the big data fields. Testing of big data analytics system mainly has the following three roles. (1) We can verify the correctness of functionalities and the reliability of the big data analytics",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "the correctness of functionalities and the reliability of the big data analytics system before it is deployed and put to use. (2) We can carry out a fair comparison of the performance of different big data analytics systems. (3) We can optimize the performance of big data analytics systems by testing. Presently, testing of big data analytics system mainly uses benchmarks, and by benchmark testing, we can analyze and evaluate the functionalities, performance, reliability, and compatibility of the system. There are three categories of benchmark in the testing of big data analytics systems. The first category is the micro benchmark. This category of benchmark principally aims at testing a certain component of the big data analytics system thus is also called component-level benchmark. Such as TeraSort can only",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "system thus is also called component-level benchmark. Such as TeraSort can only be used to test the system’s performance for sorting text data, and GridMax can only be used to test the performance of various MapReduce job in the Hadoop clusters. Therefore, the micro benchmark cannot evaluate the performances of big data analytics system entirely. The second category is the comprehensive benchmark. This category of the benchmark can test more than one components of big data analytics system. For example, Hibench is a comprehensive benchmark, and its workload including micro benchmarks, web search, SQL query and machine learning [3]. The third category is the application oriented benchmark, which is characterized by simulating the scenario of big data applications in the enterprise. TPC-DS is a benchmark for testing big",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "data applications in the enterprise. TPC-DS is a benchmark for testing big data decision support systems [4, 5]. TPCx- BigBench [6, 7] is the first end-to-end, application-level big data benchmark based on TPC-DS. Due to the standardization and usability of TPC-DS and TPCx-BigBench, more and more organizations begin to use these two benchmarks to test, evaluate and compare the overall performance of big data analytics systems. This paper will discuss in detail the challenges of testing big data analytics systems in Part II, and propose method and strategies of how to test big data analytics systems in Part III. In Part IV, two cases of testing will be presented, that is testing of Transwarp Inceptor by TPC-DS and performance comparison of Hive and Spark SQL by TPCx-BigBench. In",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "TPC-DS and performance comparison of Hive and Spark SQL by TPCx-BigBench. In addition, some preliminary analysis will be made on how to optimize the performance of Spark SQL by benchmark testing. Finally, we conclude the paper in section V. II. THE CHALLENGES OF TESTING BIG DATA ANALYTICS SYSTEM Due to the “3V” characteristics of big data and the complexity of big data analytics system, this brings about challenges for testing big data analytics system. First is the complexity of the technologies on big data analytics system. It generally adopts distributed architectures, such as master-slave or peer-to-peer. And factors that will This work was funded by Science and Technology Commission of Shanghai Municipality Program (16511101202, 17411952800). 2312018 IEEE International Conference on Software Testing, Verification and Validation Workshops 0-7695-6432-1/18/$31.00 ©2018",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "IEEE International Conference on Software Testing, Verification and Validation Workshops 0-7695-6432-1/18/$31.00 ©2018 IEEE DOI 10.1109/ICSTW.2018.00054 [Página 2] affect the performance of the system under test are complex, such as network environment, hardware configurations, system configuration parameters, and virtualization etc. For instance, Hadoop system has over 200 configuration parameters. Second is the complexity of test datasets. The test datasets of big data analytics system need not only to meet the “3V” characteristics of big data but also to represent typical business scenes. Third are the challenges of testing methods and tools, such as the traditional testing tools can no longer be appropriate, lacking automatic testing methods and the customization of testing and diagnosing schemes. Different modules in the big data analytics require different testing techniques. For example, we test",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "the big data analytics require different testing techniques. For example, we test the performance of Spark SQL by SQL’s queries, while we test throughput and latency of Spark Streaming by loading streaming data. Fourth, the testing of big data analytics system requires more professional and more comprehensive testing abilities. Testers not only need to have the testing expertise but also need to master the big data analysis and processing technology. For example, testers need to know how to load data from Hadoop HDFS into a Hive table and verify if the loading is correct. III. BENCHMARK TESTING METHOD OF BIG DATA ANALYTICS SYSTEM The testing of big data analytics system with benchmark can generally be divided into 6 phases, that is requirement analysis for testing big data analytics",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "into 6 phases, that is requirement analysis for testing big data analytics systems, preparing the testing environment, preparing the test datasets and workload, loading the test datasets, testing for the big data analytics system and analysis of the testing result, as shown in Fig.1. Fig.1. Benchmark testing method of big data analytics system A. Requirement analysis of testing for big data analytics systems The phase of requirement analysis for testing big data analytics systems is by and large same as traditional software testing, including specifying the objects of testing, the purposes of testing, the environment of testing, the datasets of testing, technology and tools of testing and the risk of testing, etc. But the key point of testing big data analytics system is the performance and reliability of",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "of testing big data analytics system is the performance and reliability of the system. For example, how efficient is the system's processing and analysis of data with large-scale datasets? Whether tasks of data processing can be migrated automatically or not when a node in the cluster goes down? Will the data be lost in a distributed environment when a node crashes? B. Preparing the testing environment In order to test a big data analytics system, we need to prepare a cluster of distributed data storage and computing, at the same time a sufficient storage space is required to store and analyze the large-scale datasets. It is worth noticing that the storage space here not only refers to the hard disk space but also memory space, especially in testing",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "to the hard disk space but also memory space, especially in testing Apache Spark, due to the 60% occupation of memory is used for buffering RDD (the data structure of Spark), so enough memory space should be set apart for the testing program. We should be careful that, the testing environment should be ensured “clean”. In other words, we should ensure that there is no other applications running in the cluster, the CPU and memory of the node in the cluster are both at their minimum utilization. C. Preparing the test datasets and workload The datasets for testing big data analytics system comes from two sources: one is the real data from business, such as data from weblogs or database of business; the other is simulated data generated",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "from weblogs or database of business; the other is simulated data generated by big data benchmarking tools. TPC- DS and TPCx-BigBench are two benchmarks that have been nominated in the industry. It should be noted that we should set appropriate data scale, data type, and data model according to the requirement of the testing. The workload is the core of performance testing of big data analytics system. It needs to reflect business scenarios and data analytical techniques. The workload in TPC-DS or TPCx-BigBench is the set of queries to be executed against the test datasets. D. Loading the test datasets During the phase of loading test datasets, we should verify if the data has been loaded correctly into the distributed storage system. For example, whether the data is",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "correctly into the distributed storage system. For example, whether the data is loaded into the right HDFS storage directory? Is the size of data file correct? If the data need to be loaded into the distributed database system, we should verify if the data can be load into the table in the database correctly. E. Testing of the big data analytics system Testing of the big data analytics system needs to focus on system’s functionality, performance, reliability, and compatibility. 1) Functionality testing The Functionality testing of big data analytics system mainly verifies whether functions of the system in data storage, data processing, data I/O etc. are correct? For example, whether data processing based on MapReduce is correct? Whether the results of SQL queries on the SQL-On-Hadoop system are",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "correct? Whether the results of SQL queries on the SQL-On-Hadoop system are correct? And whether the data I/O is complete? 232 [Página 3] 2) Performance testing The performance testing of big data analytics system needs to test the performance of data I/O, data processing and analytic and the performance of SQL query on the system and so on. For example, we can test the reading and writing performance of Hadoop HDFS using single large data file or multiple large data files. For SQL-On-Hadoop systems, the performance of SQL query is the most important performance metric. 3) Reliability testing The reliability testing of big data analytics system needs to focus on the following two aspects: • If the task can be automatically migrated when a task of data analytics",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "the task can be automatically migrated when a task of data analytics failed at a certain node (may be due to lack of memory), so as to ensure the task is executed correctly? • If one or some nodes in the cluster go down, will the task of the data analytic be executed correctly due to the fault- tolerant mechanism of the system? 4) Compatibility Testing The compatibility testing of big data analytics system needs to verify the compatibility of the file system, the compatibility of data storage format, the compatibility of SQL syntax and so on. F. Analysis of the testing result During the phase of analysis of the testing result, we need to analyze system’s testing metrics (functionality, performance, reliability, and compatibility) comprehensively according to the",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "system’s testing metrics (functionality, performance, reliability, and compatibility) comprehensively according to the testing requirement and finish the testing report. IV. CASES OF TESTING BIG DATA ANALYTICS According to the test method described in Part ċ, in this section, we present two cases of testing big data analytics system. A. Testing for Transwarp Inceptor by TPC-DS 1) Requirement analysis of testing Transwarp Inceptor The purpose of testing Transwarp Inceptor is to verify the functionality of ETL ˈand evaluate the performance of SQL query and compatibility of SQL syntax through automated testing scripts. The method of testing follows the TPC-DS specification. 2) The system under test and environment a) Transwarp Inceptor big data analytics system Inceptor is a commercial big data analytics system developed by Transwarp Technology Co., Ltd. It",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "commercial big data analytics system developed by Transwarp Technology Co., Ltd. It provides high-speed SQL analytics based Apache Spark. It can help businesses to build high-speed, scalable data warehouses, and perform interactive analysis, real-time reporting, and visualization of data. Transwarp Inceptor has a three-tier structure from bottom to top: the storage layer, the distributed computing engine layer and the interface layer, as is shown in Fig.2. Fig.2. Architecture of Transwarp Inceptor b) Test environment The test environment consists of four physical servers, and the configurations of servers are same, as is shown in Table I. Four servers make up a Transwarp cluster through Gigabit network. TABLE I. THE HARDWARE CONFIGURATION OF THE TESTING SERVERS Node1 Node2 Node3 Node4 Model Dell PowerEdge R720 CPU Intel(R) Xeon(R) CPU E5-2620 v2",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "Node3 Node4 Model Dell PowerEdge R720 CPU Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz ( 2 CPU x 6 cores) Memory (GB) 256 256 256 256 Storage 24 TB HDD hard drive Operating System Red Hat Enterprise Linux 6.5 Hadoop Transwarp DataHub v3.4 Hadoop 2.2 Inceptor Transwarp Inceptor v4.0 Roles Primary NameNode, Inceptor Server, DataNode Secondary NameNode, Inceptor MetaStore, DataNode DataNode DataNode 3) Generating test datasets and workload by TPC-DS a) TPC-DS TPC-DS is testing benchmark for decision support system proposed by TPC (Transaction Processing Performance Council). TPC-DS models the decision support functions of a retail product supplier. The business model of benchmark simulates sales and returns of the three main channels (stores, online retailers, and catalogs). The business model contains 7 fact tables and 17 dimension tables,",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "catalogs). The business model contains 7 fact tables and 17 dimension tables, and tables are organized by star and snowflake mixed model. A reduced business model of TPC-DS is shown in Fig.3. 233 [Página 4] Fig.3. TPC-DS database schema TPC-DS allows users to generate the different scale of datasets from 100G to 100T according to the user’s test requirements and test environment. In general, the TPC-DS benchmark has following characteristics: • A large amount of business data and test cases (SQL queries) can answer real business problems. • A total of 99 SQL queries follow the SQL 99 and SQL 2003 core syntax standard, and SQL queries are complex. • The test cases include a variety of business models, such as interactive query, statistical analysis, iterative OLAP and",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "of business models, such as interactive query, statistical analysis, iterative OLAP and data mining. • Almost all of the test cases need high I/O loading and CPU computing. b) The generation of test datasets and workload In this phase, we use the data generation and query generation tools (DSTools v1.3.0) provided by the TPC-DS benchmark to generate 500GB test datasets and 99 SQL queries through automated shell scripts, and the script fragment is as follows. # Generate 500GB test datasets in the specified HDFS directory 1: dbgen2 -scale 500 -dir HDFS_LOCATION # Generate 99 queries compatible with Oracle syntax for 500GB datasets through the query template 2: qgen2 –query99.tpl –directory QUERY_TEMPLATE –dialect oracle -scale 500 The 500GB test datasets consist of 24 tables of the database (7 fact",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "500GB test datasets consist of 24 tables of the database (7 fact tables and 17 dimension tables) mentioned above. The 99 SQL queries implement business intelligence by answering real business questions. 4) Data loading In the data loading phase, we first create 24 tables in Transwarp Inceptor to build the data warehouse for testing. The schemas of tables are provided by the TPC-DS benchmark. Then we load the datasets that have been generated in the HDFS into tables. The following script fragment shows how to load datasets in HDFS into the inventory table. # load inventory.dat into the inventory table 1: LOAD DATA inpath '/tpc_ds/data/inventory.dat' INTO TABLE inventory; 5) Testing for Transwarp Inceptor The core of the TPC-DS based benchmark testing is the execution of 99 SQLs one",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "the TPC-DS based benchmark testing is the execution of 99 SQLs one by one. In testing, we verify the correctness of the test results and record the execution time of SQL. We execute 99 SQLs with automated scripts by three rounds and take the average time of three rounds as SQL’s execution time. The following script fragment shows how to execute 99 SQL queries sequentially in Transwarp Inceptor. # Execute all 99 SQL queries one by one 1: for(i = 1; i<=99; i++ ){ 2: sql = \"query\"+ i + \".sql\"; 3: system( \"transwarp -t -h localhost -f ./sql/\" + sql); 4:} 6) Testing Analysis In the case of the 500GB test datasets, the four categories of SQL execution time are shown in Table II. Test results show",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "of SQL execution time are shown in Table II. Test results show that 96 out of 99 SQL queries can be run directly in Transwarp Inceptor. There only 3 SQL queries need minor modification to be compatible with SQL compiler of Transwarp Inceptor. Considering that the TPC-DS specification allows SQL’s minor modification, so Transwarp Inceptor has good compatibility with SQL 2003 standard. TABLE II. SQL QUERIES ’ EXECUTION TIME OF TRANSWARP INCEPTOR SQL Categories The number of SQL The total execution time (seconds) The average execution time (seconds) Interactive query 9 197 21.9 Statistical analysis 69 7705 111.7 Iterative OLAP 10 4232 423.2 Data mining 11 3502 318.4 B. Testing Hive vs. Spark SQL by TPCx-BigBench 1) Requirement analysis of te sting Hive vs. Spark SQL Testing Hive",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "1) Requirement analysis of te sting Hive vs. Spark SQL Testing Hive vs. Spark SQL has two purposes. One is to utilize TPCx-BigBench as a benchmark for evaluating and comparing the performance of two SQL-On-Hadoop analytics systems. The other is to tune system parameters for optimizing analytics system’s performance. 2) Systems under test and test environment a) Hive Hive is one of the first data analytics engines to be built on top of MapReduce. It was originally developed by Facebook to support data analysts to analyze large datasets in Hadoop by queries in a SQL-like declarative query language. This SQL- like language is called HiveQL and is based on the SQL language, but does not strictly follow the SQL 99 standard. Hive has now become the foundation of",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "follow the SQL 99 standard. Hive has now become the foundation of new SQL on Hadoop projects, such as Impala, Presto, and Spark SQL. Hive metadata has become the de facto standard for users to store and manage metadata (table names, column names, and types, etc.) in Hadoop ecosystem. Although Hive is a widely used project, historically its biggest drawback has been performance. Most of the performance problems can be attributed to Hive's use of MapReduce as its execution engine. MapReduce is not a good choice for running ad hoc, interactive queries. The main reason 234 [Página 5] is that MapReduce reads and writes to disk extensively, and there is a high startup cost for MapReduce jobs. b) Spark SQL Apache Spark is a cluster computing platform designed",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "jobs. b) Spark SQL Apache Spark is a cluster computing platform designed to be fast and general-purpose. Spark extends the popular MapReduce model to efficiently support more types of computations, including interactive queries and stream processing. One of the main features of Spark is to be able to run computing in memory, so Spark has faster computing speed than MapReduce. Spark SQL [8] is the component that Spark uses to manipulate structured data. It allows querying data via SQL as well as the HiveSQL and it supports many sources of data, including Hive tables, Parquet, and JSON. Spark SQL is fully compatible with Hive. Spark SQL supports HiveSQL and Hive metastore, so we can compare the performance of Hive and Spark SQL under the same test datasets. Spark",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "performance of Hive and Spark SQL under the same test datasets. Spark SQL also seamlessly integrates with Spark machine learning libraries MLlib and Spark ML. For example, in a machine learning application, the DataFrame API provided by Spark SQL can easily be used for data cleaning and feature engineering. c) Test environment The test environment is a Cloudera Data Hub (CDH) cluster with 4 nodes connected directly through Gigabit network, and detail hardware and software are shown in Table III. Cloudera CDH 5.10 with default configurations was used for all tests. TABLE III. TEST ENVIRONMENT FOR TESTING HIVE VS . SPARK Node1 Node2 Node3 Node4 CPU Intel(R) Xeon(R) CPU E5-2695 v3 @ 2.30GHz (8 cores) Memory (GB) 64 80 80 80 Storage 4TB HDD hard drive Operating System",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "(GB) 64 80 80 80 Storage 4TB HDD hard drive Operating System CentOS 6.7 x86_64 Hadoop Cloudera Data Hub 5.10.0 (Hadoop 2.6.0) Hive Hive 1.1.0 Spark Spark 2.1.0 (--driver-memory 10g –execuotr-memory 20g ) Roles HDFS NameNode, ResourceManager HDFS DataNode NodeManager 3) Generating test datasets and workload by TPCx- BigBench BigBench covers the “3Vs” characteristics of the big data system. The initial implementation of BigBench was at the Teradata Aster platform in 2014. Later on, BigBench was standardized by TPC in Nov. 2016, and TPC released TPCx- BigBench v1.2.0 as the benchmark for big data analytics system. BigBench benchmark consists of the data model, the data generator and the specification of the workload. a) Data model of BigBench The data model of BigBench includes structured data, semi- structured data,",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "BigBench The data model of BigBench includes structured data, semi- structured data, and unstructured data, as shown in Fig.4. The structured data of BigBench is adapted from TPC-DS. The semi-structured data is composed of clicks made by customers and guest users visiting the retailer’s website. The unstructured data is covered by product reviews submitted by actual customers or guest users. Therefore, BigBench satisfies the “variety” property of big data. Fig.4. Data model of TPCx-BigBench b) Data generator of BigBench The data generator of BigBench is based on an extension of PDGF [9] and allows generating data in accordance with the data model. It can not only generate the structured data but also generate the semi-structured and unstructured data. PDGF is a parallel data generator that is capable of",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "unstructured data. PDGF is a parallel data generator that is capable of generating large amounts data based on a scale factor. So, the “volume” property of big data is reflected in BigBench. In addition, the “velocity” property of big data is implemented through a periodic refreshing scheme that continually adds new data to different tables in the data model. The following script fragment shows how to set data storage directory and generate 50GB datasets parallel by BigBench. # Set dataset’s HDFS storage path in userSettings.conf 1: export BIG_BENCH_HDFS_ABSOLUTE_PATH =\"/user/$BIG_BENCH_USER\" 2: export BIG_BENCH_HDFS_RELATIVE_HOME =\"benchmarks/bigbench\" # Generate 50GB test datasets with TPCx-BigBench 1: $INSTALL_DIR/bin/bigBench runBenchmark –f 50 –m 8 –i DATA_GENERATION -f <scale factor of dataset> -m [number of map tasks for data generation] -i <benchmark phases to perform >",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "of map tasks for data generation] -i <benchmark phases to perform > c) Query workload of BigBench The BigBench query workload includes 30 queries, which are defined as questions about the business model. Ten of them have been taken from the TPC-DS workload. The other 20 queries were adapted from a McKinsey big data use cases and opportunities report. The 30 queries of BigBench can be classified from two aspects: data types and analysis methods, as shown in Table IV and Table V. Analysis methods can be grouped into four categories: Pure Hive Queries(Pure HQL), Hive Queries with MapReduce programs, Hive Queries using natural language processing(NLP/UDF/UDTF), and Queries using Apache Spark MLLIB(Machine Learning). 235 [Página 6] TABLE IV. DATA TYPES OF BIGBENCH ’S WORKLOAD Data type Queries Number",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "TABLE IV. DATA TYPES OF BIGBENCH ’S WORKLOAD Data type Queries Number Structured data query1,query6, query7, query9, query11, query13, query14, query15, query16, query17, query20, query21, query22, query23, query24, query25, query26, query29 18 Semi-structured data query2, query3, query4, query5, query8, query12, query30 7 Unstructured data query10, query18, query19, query27, query28 5 TABLE V. ANALYTIC METHOD OF BIGBENCH ’S WORKLOAD Analytic method Queries Number Pure HQL query6, query7, query9, query11, query12, query13, query14, query15, query16, query17, query21, query22, query 23, query 24 14 MapReduce query2, query3, query4, query8, query30 5 Machine Learning query5, query20, query25, query26, query28 5 NLP/UDF/UDTF query1, query10, query18, query19, query27, query29 6 4) Data Loading in BigBench Data loading in BigBench refers to load test datasets into Hive tables. The following script fragment shows how to",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "test datasets into Hive tables. The following script fragment shows how to load test datasets created in the phase of “DATA_ GENERATION” into Hive tables. We can verify whether data loading was successful or not by Hive’s shell command. # Load test datasets into Hive tables 1: $INSTALL_DIR/bin/bigBench runBenchmark –i LOAD_TEST # Verify the test datasets was loaded successfully 2: hive> use bigbench; 3: hive> show tables; 5) Testing for Hive vs. Spark SQL In order to compare the performance of Hive and Spark SQL, we use Hive engine and Spark engine respectively. We execute 30 queries in sequence to compare the execution time, as shown in Table Ď. T h e s c r i p t f r a g m e n t i s",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "p t f r a g m e n t i s a s f o l l o w s . I t i s worth noting that before using Spark engine we need to ensure that Spark had access to the tables in Hive. # Test Hive performance with BigBench 1: $INSTALL_DIR/bin/bigBench runBenchmark –i POWER_TEST # Test Spark SQL performance with BigBench 2: $INSTALL_DIR/bin/bigBench runBenchmark –i POWER_TEST –e spark_sql TABLE VI. EXECUTION TIME FOR ALL QUERIES WITH SF 50(50G DATA ) Query No. Analytic method Execution time (seconds) Hive Spark SQL query1 UDF/UDTF 296 124 query2 MapReduce 3904 1634 query3 MapReduce 1046 568 query4 MapReduce 3932 989 query5 Machine Learning 535 344 query6 Pure HQL 603 238 query7 Pure HQL 897 260 query8 MapReduce 680 251",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "HQL 603 238 query7 Pure HQL 897 260 query8 MapReduce 680 251 query9 Pure HQL 1123 138 query10 NLP/UDF/UDTF 1133 1868 query11 Pure HQL 242 110 query12 Pure HQL 271 146 query13 Pure HQL 361 152 query14 Pure HQL 93 92 query15 Pure HQL 151 124 query16 Pure HQL 823 236 query17 Pure HQL 230 118 query18 NLP/UDF/UDTF 1066 903 query19 NLP/UDF/UDTF 401 317 query20 Machine Learning 341 322 query21 Pure HQL 613 175 query22 Pure HQL 160 128 query23 Pure HQL 254 145 query24 Pure HQL 307 118 query25 Machine Learning 483 350 query26 Machine Learning 249 291 query27 NLP/UDF/UDTF 121 201 query28 Machine Learning 456 510 query29 UDF/UDTF 237 154 query30 UDF/UDTF/MapReduce 3769 922 6) Performance analysis of Hive vs. Spark SQL According to Table Ď,",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "6) Performance analysis of Hive vs. Spark SQL According to Table Ď, Fig.5 and Fig.6, Spark SQL performance is 1-8 times that of Hive under 14 Pure HQL queries and 5 Hive queries with MapReduce. The main reason is that Spark SQL uses memory computing and optimized SQL engine. So Spark SQL is more efficient than Hive that uses MapReduce as a computing engine. Fig.5. Hive and Spark SQL performance comparison by Pure HQL query Fig.6. Hive and Spark SQL performance comparison by MapReduce query 236 [Página 7] For machine learning workload 㸪Hive and Spark SQL are similar in performance, since both Hive and Spark SQL use Spark MLLIB as a machine learning engine, as shown in Fig.7. Fig.7. Hive and Spark SQL performance comparison by machine learning",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "in Fig.7. Fig.7. Hive and Spark SQL performance comparison by machine learning Since NLP programs were written in the Python language, neither Hive nor Spark SQL can take advantage of the system’s parallel computing features. As a result, for NLP/UDF/UDTF workload, Hive and Spark SQL performance’s gap is not large, and Hive outperformed Spark SQL even on query 10 and query 27, as shown in Fig.8. Fig.8. Hive and Spark SQL performance comparison by NLP/UDF/UDTF For query10, we modify the parameter of spark.sql.shuffle.partition from the default of 200 to 50 to optimize the performance of Spark SQL. In Spark SQL, a large number of shuffle partitions means more tasks when shuffle operation occurs. More tasks in Spark SQL will increase the overhead of tasks startup and decrease the",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "Spark SQL will increase the overhead of tasks startup and decrease the performance of the system. As shown in Fig.9, by optimizing Spark SQL’s parameter, query 10 reduce its run time from 1868 seconds to 1376 seconds. Fig.9. Spark SQL performance improvement through optimization V. CONCLUSION With the continuous development of big data applications and technologies, industry and academia pay more and more attention to the benchmark testing of big data analytics systems. It not only equitably compares the performance of multiple big data analytics systems, but also allows you to tune system parameters and optimize system performance. The paper analyzes the challenges of testing big data analytics system and summarizes the methods and strategies of the test. And the paper presents two cases of benchmark testing for",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "the test. And the paper presents two cases of benchmark testing for big data analytics systems. In case 1, we present an automated system testing solution for Transwarp Inceptor by TPC-DS in detail, and the test includes system’s functionality, performance, reliability and compatibility of SQL. In case 2, we test and compare the performance of Hive and Spark SQL by TPCx-BigBench, an application oriented end-to-end benchmark. Test results show that the performance of Spark SQL significantly better than Hive on the workload of pure HQL and query with MapReduce. In the future, we will further research new technologies of big data benchmarks [10], such as testing and evaluation of streaming analytics and graph analytics systems. R EFERENCES [1] A.Thusoo, J.S. Sarma, N. Jain, et al, “Hive-a petabyte scale",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "EFERENCES [1] A.Thusoo, J.S. Sarma, N. Jain, et al, “Hive-a petabyte scale data warehouse using hadoop”, IEEE 26th International Conference on Data Engineering. IEEE, 2010, pp.996-1005. [2] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, et al, “Spark: Cluster Computing with Working Sets”, Usenix Conference on Hot Topics in Cloud Computing, Boston, USA, 2010. [3] S. Huang, J. Huang, J. Dai, et al, “The HiBench Benchmark Suite: Characterization of the MapReduce-Based Data Analysis”. ICDE Workshops, 2010, pp. 41 - 51. [4] R. O. Nambiar, M. Poess, “The making of TPC-DS”, Proceedings of the 32nd international conference on Very large data bases. VLDB Endowment, 2006, pp.1049-1058. [5] M. Poess, R. O. Nambiar, D. Walrath,“Why you should run TPC-DS: a workload analysis”, Proceedings of the 33rd international conference on",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "run TPC-DS: a workload analysis”, Proceedings of the 33rd international conference on Very large data bases. VLDB Endowment, 2007, pp.1138-1149. [6] A. Ghazal, T. Rabl, M. Hu, et al, “BigBench: towards an industry standard benchmark for big data analytics”, Proceedings of the 2013 ACM SIGMOD international conference on Management of data, 2013, pp.1197-1208. 237 [Página 8] [7] TPCx-BigBench Standard Specification Version 1.2.0, November 2016, http://www.tpc.org/ [8] M. Armbrust, R. S. Xin, C. Lian, et al, “Spark sql: Relational data processing in spark”, Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data, 2015, pp.1383-1394. [9] T. Rabl, M. Frank, H. M. Sergieh, et al, “A Data Generator for Cloud- Scale Benchmarking”, TPCTC, 2010, pp.41-56. [10] T. Rabl, M. Frank, M. Danisch, et al, “The vision of",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "[10] T. Rabl, M. Frank, M. Danisch, et al, “The vision of BigBench 2.0”, Proceedings of the Fourth Workshop on Data analytics in the Cloud., ACM, 2015. 238",
        "start_idx": 4872,
        "end_idx": 4900
      }
    ],
    "94553f7c-0219-4683-8566-938f0d311229": [
      {
        "text": "[Página 1] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 1 AutoDiagn: An Automated Real-time Diagnosis Framework for Big Data Systems Umit Demirbaga, Zhenyu Wen\u0003Member, IEEE , Ayman Noor, Karan Mitra, Member, IEEE , Khaled Alwasel, Saurabh Garg, Albert Zomaya, Fellow, IEEE , Rajiv Ranjan, Senior Member, IEEE Abstract—Big data processing systems, such as Hadoop and Spark, usually work in large-scale, highly-concurrent, and multi-tenant environments that can easily cause hardware and software malfunctions or failures, thereby leading to performance degradation. Several systems and methods",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "malfunctions or failures, thereby leading to performance degradation. Several systems and methods exist to detect big data processing systems’ performance degradation, perform root-cause analysis, and even overcome the issues causing such degradation. However, these solutions focus on speciﬁc problems such as stragglers and inefﬁcient resource utilization. There is a lack of a generic and extensible framework to support the real-time diagnosis of big data systems. In this paper, we propose, develop and validate AutoDiagn. This generic and ﬂexible framework provides holistic monitoring of a big data system while detecting performance degradation and enabling root-cause analysis. We present an implementation and evaluation of AutoDiagn that interacts with a Hadoop cluster deployed on a public cloud and tested with real-world benchmark applications. Experimental results show that AutoDiagn can offer a",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "with real-world benchmark applications. Experimental results show that AutoDiagn can offer a high accuracy root-cause analysis framework, at the same time as offering a small resource footprint, high throughput and low latency. Index Terms—Root-cause analysis, Big data systems, QoS, Hadoop, Performance F 1 I NTRODUCTION The rapid surge of data generated through sectors like social media, ﬁnancial services and industries has led to the emergence of big data systems. Big data systems enable the processing of massive amounts of data in relatively short time frames. For instance, the Netﬂix big data pipeline processes approximately 500 billion events and 1.3 petabytes (PB) of data per day, further, during peak hours, it processes approximately 11 million events and 24 gigabytes (GB) of data on a per-second basis. Facebook has one",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "24 gigabytes (GB) of data on a per-second basis. Facebook has one of the largest data warehouses in the world, capable of executing more than 30,000 queries over 300 PB data every day. However, the enormousness and complexity of the big data system runs in heterogeneous computing resources, multiple tenant environments, as well as has many concurrent execution of big data processing tasks, which makes it a challenge to utilize the big data systems efﬁciently and reliably[1]. For example, Fig. 1 shows that the performance degrades at least 10% when the resources are not utilized efﬁciently with Setting 2. \u000fU. Demirbaga is with Newcastle University, United Kingdom and Bartin University, Turkey. E-mail: u.demirbaga2@newcastle.ac.uk \u000fZ. Wen is with Newcastle University, United Kingdom. E-mail: zhenyu.wen@newcastle.ac.uk, corresponding author. \u000fA. Noor is",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "with Newcastle University, United Kingdom. E-mail: zhenyu.wen@newcastle.ac.uk, corresponding author. \u000fA. Noor is with Newcastle University, United Kingdom and Taibah University, Saudi Arabia. E-mail: anoor@taibahu.edu.sa \u000fK. Mitra is with Lule˚ a University of Technology, Sweden. E-mail: karan.mitra@ltu.se \u000fK. Alwasel is with Newcastle University, United Kingdom and Saudi Electronic University, Saudi Arabia. E-mail: kalwasel@gmail.com \u000fS. Garg is with University of Tasmania, Australia. E- mail:Saurabh.Garg@utas.edu.au \u000fA. Zomaya is with Sydney University, Australia, E-mail: al- bert.zomaya@sydney.edu.au \u000fR. Ranjan is with Newcastle University, United Kingdom. E-mail: raj.ranjan@newcastle.ac.uk 0 50 100 150 200 250 300 350 WordCountGrepTPC-HTPC-DS K-means PageRankMakespan (sec) Big data applicationsSetting 1 Setting 2Fig. 1. Six big data applications are executed in a cloud-based Hadoop cluster with two settings: 1) the input data and jobs are allocated in the same node; 2)",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "the input data and jobs are allocated in the same node; 2) the input data and jobs are allocated in different nodes. In Setting 2, the execution time of each application is delayed by transmitting data across nodes. To overcome this, it is imperative to continuously mon- itor and analyze all available system resources at all times in a systematic, holistic and automated manner. These re- sources include CPU, memory, network, I/O and the big data processing software components. Most of the commercial [2][3][4] and academic big data monitoring systems mainly focus on visualizing task progress, and the system’s resource utilization [5]. How- ever, they do not focus on the interaction between multiple factors and performing root-cause analysis for performance degradation [6][7]. Moreover, works such as [8], [9]",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "root-cause analysis for performance degradation [6][7]. Moreover, works such as [8], [9] aim to ﬁnd the best parameters to optimize the performance of Manuscript received ???; revised ??? Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 2] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 2 big data processing systems, they do not focus on the root- cause analysis that may indicate the viable reasons behind performance degradation and may",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "analysis that may indicate the viable reasons behind performance degradation and may provide intuitions for parameter tweaking. Mantri [10] presents a systematic method that catego- rizes the main reasons causing outliers in a big data system. The authors’ work was focused on the MapReduce pro- gramming framework in the Hadoop system; they do not discuss how Mantri can be applied to other big processing frameworks (e.g., Apache Spark1, and Apache Flink2). Gar- raghan et al. [11] proposed an online solution to detect long- tail issues in a distributed system. However, these solutions were built for speciﬁc scenarios with much scope left for analyzing a variety of problems that can exist in a large scale big data processing system. To the best of our knowledge, there is a lack",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "processing system. To the best of our knowledge, there is a lack of a generic and comprehensive solution for the detection of a wide range of anomalies and performance of root-cause analysis in big data systems. Developing a general and extensible framework for diagnosing a big data system is not trivial. It requires well-deﬁned requirements which could enable the broader adoption of root-cause analysis for the big data systems, ﬂexible APIs to interact with an underlying monitoring system and integration of multiple solutions for detecting performance reduction problems while enabling the automatic root-cause analysis. In this paper, we tackle this research gap, and design and develop AutoDiagn to au- tomatically detect performance degradation and inefﬁcient resource utilization problems, while providing an online detection and semi-online root-cause analysis for",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "utilization problems, while providing an online detection and semi-online root-cause analysis for a big data system. Further, it is designed as a microservice architecture that offers the ﬂexibility to plug a new detection and root-cause analysis module for various types of big data systems. The contributions of this paper are as follows: \u000fAn online and generic framework: We develop a general framework called AutoDiagn which can be adapted for the detection of a wide range of performance degrada- tion problems while pinpointing their root-causes in big data systems. \u000fA case study: We develop a novel real-time stream pro- cessing method to detect symptoms regarding outliers in a big data system. After that, we develop a set of query APIs to analyze the reasons that cause the outlier regarding",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "of query APIs to analyze the reasons that cause the outlier regarding a task. \u000fA comprehensive evaluation: We evaluate the feasibility, scalability and accuracy of AutoDiagn through a set of real-world benchmarks over a real-world cloud cluster. The paper is organized as follows. The design require- ments and idea are outlined in §2. In §3, we illustrate the high-level system architecture. §4 presents a case study that we implemented and the case study is evaluated in §5. §6 discusses the limitations of this paper and highlights our further work . Before drawing a conclusion in §8, we discuss the related work in §7. 1. https://spark.apache.org/ 2. https://ﬂink.apache.org/2 R EQUIREMENTS AND DESIGN IDEA In this section, we analyze the key requirements of the real-time big data diagnosis system, extracting",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "analyze the key requirements of the real-time big data diagnosis system, extracting the essential features from the literature. Next, we present the key idea of the framework design. 2.1 Fundamental prerequisite for diagnosing big data processing systems In order to design a generic framework for diagnosing big data processing systems, we classiﬁed the fundamental re- quirements of building a diagnosis system on such systems as follows: \u000fInfrastructure monitoring: Collecting the information about the underlying system, such as network condi- tions, CPU utilization, memory utilization, and disk I/O status. \u000fTask execution monitoring: Collecting the task infor- mation, including execution time, progress, location, location of its input data, input data size, output data size, CPU/memory usage, and process state (running, waiting, succeeded, failed, killed). \u000fAbnormal behavior or fault detection: Detecting",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "state (running, waiting, succeeded, failed, killed). \u000fAbnormal behavior or fault detection: Detecting ab- normal behaviors in big data processing systems, such as slowing tasks, failed tasks, very high/low resource usage, and experiencing very high response time for the requests. \u000fRoot-cause analysis: Finding the root cause of perfor- mance reduction in big data processing systems, such as the reasons why: tasks are slowing down, resource utilization is low, the response time is high, or when the network latency is high. \u000fVisualization: Visualizing the collected metrics and the results of root-cause analysis of any failures caus- ing performance reduction in the cluster with a user- friendly interface in real-time. 2.2 Key design idea Motivated by the above-mentioned requirements and in- spired by medical diagnosis, we highlight the design idea of",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "and in- spired by medical diagnosis, we highlight the design idea of root-cause analysis for big data processing systems as shown Fig. 2, which aims to provide holistic monitoring and root cause analysis for big data processing systems. First, a set of Symptom Detectors is deﬁned and developed in Symptom Detection to detect the abnormalities of the big system by processing collected system information stream in real-time. Once a symptom (abnormality) is detected, theDiagnosis Management may launch the corresponding Diagnosers to troubleshoot the cause of the symptom. One symptom may correspond to root causes. Finally, the deci- sions are made based on the root-cause analysis results. 2.3 The generalizability of AutoDiagn Modern big data processing systems consists of two main types: Big data analytics (e.g., Hadoop, Spark) and",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "consists of two main types: Big data analytics (e.g., Hadoop, Spark) and Stream processing (e.g., Flink, Spark Stream). Based on our de- sign idea, our AutoDiagn is an independent framework that can be deployed alongside existing big data cluster management systems (e.g., Apache YARN), and ideally it is suitable for root-cause analysis of any big data processing Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 3] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639,",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 3 system. However, for the scope of this paper and practi- cal certainty, the implementation of AutoDiagn focuses on debugging root causes of performance degradation (e.g., slow task execution time) in Hadoop due to faults such as data locality, cluster hardware heterogeneity, and network problems (e.g., disconnection). Although we have validated the functionality of AutoDiagn in the context of Hadoop and considering different classes of workload (e.g., WordCount, Grep, TPC-H, TPC-DC, K-means clustering, PageRank), it is generalizable to other big data processing systems executing similar classes of workload. 3 A UTODIAGN ARCHITECTURE Following the design idea laid out in §2, we introduce Auto- Diagn, a novel big data diagnosing system. We ﬁrst",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "we introduce Auto- Diagn, a novel big data diagnosing system. We ﬁrst illustrate the high-level system architecture and then describe the details of each component. AutoDiagn is implemented in Java and all source code is open-source on GitHub3. 3.1 Architecture overview AutoDiagn provides a systematic solution that automati- cally monitors the performance of big data systems while troubleshooting the issues that cause performance reduc- tion. Fig. 3 shows its two main components: AutoDiagn Monitoring and AutoDiagn Diagnosing. AutoDiagn Monitoring collects the deﬁned metrics (logs) and feeds AutoDiagn Diag- nosing with them in real-time. Once the abnormal symptoms are detected by analyzing the collected metrics, a deeper analysis is conducted to troubleshoot the cause of abnormal symptoms. AutoDiagn Monitoring. AutoDiagn Monitoring is a de- centralized real-time stream processing system",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "AutoDiagn Monitoring. AutoDiagn Monitoring is a de- centralized real-time stream processing system that collects comprehensive system information from the big data system (e.g., Hadoop Cluster). The Collected Metrics is a set of pre-deﬁned monitoring entities (e.g., CPU usage, memory usage, task location, task status) used to detect the abnormal symptoms. Moreover, the system information, required for understanding the cause of detected abnormal symptoms, is collected in this modular. AutoDiagn Diagnosing. AutoDiagn Diagnosing is an event based diagnosing system. First, the carefully crafted metrics are injected into the Symptom Detection Engine which is a real-time stream processing module to detect the abnormal symptoms in a big data system. In this paper, we use the outlier which is a common symptom for performance reduction in a Hadoop cluster as a",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "a common symptom for performance reduction in a Hadoop cluster as a case study to demon- strate the proposed framework. §4.1 illustrates the details of technology that we developed for symptom detection. Moreover, our system follows the principle of modular programming; the new symptom detection method can be easily plugged in. Diagnoser Plugins is a component for trouble-shooting the reasons behind the detected symptom. A set of Diagnosers is instantiated by the Diagnoser Manager when their corresponding symptoms are detected. Then the instantiated Diagnosers query a time series database to obtain the required input and their outputs illustrate the cause of the detected symptoms. 3. https://github.com/umitdemirbaga/AutoDiagn3.2 AutoDiagn monitoring framework AutoDiagn monitoring framework is a holistic solution for continuous information collection in a big data cluster. The framework needs",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "for continuous information collection in a big data cluster. The framework needs to have a fast, ﬂexible and dynamic pipeline to transfer the collected data as well as a high per- formance, large scale storage system. We now describe an implementation of the framework for a big data computer cluster, and the high-level system architecture is shown in Fig. 4. Information Collection. In each compute node, we develop and deploy an Agent to collect real-time system information. For the worker node, the Agent collects the usage of com- puting resource via SIGAR APIs4, including CPU, memory, network bandwidth, and disk read/write speeds. Moreover, theAgent in the master node collects the usage of computing resource as well as the job and tasks information. The Filter is developed by using",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "as the job and tasks information. The Filter is developed by using GSon Library5to remove the less im- portant information obtained from ResourceManager REST API’s6, thereby reducing the size of data transmission. The collected information is sent to RabbitMQ7cluster which is a lightweight and easy-to-deploy messaging system in each time interval via Publisher. Storage. The acquired information is time series data, we therefore choose InﬂuxDB8for data storage. InﬂuxDB is a high performance, scalable and open source time series data base which provides a set of ﬂexible open APIs for real-time analytics. The Consumer subscribes the related stream topics from RabbitMQ and interacts with InﬂuxDB APIs to inject the information to the data base. Interacting with AutoDiagn Diagnosing. The information required for symptom detection is directly forwarded and processed",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "Diagnosing. The information required for symptom detection is directly forwarded and processed in AutoDiagn diagnosing via a consumer. If a symptom is detected, InﬂuxDB will be queried by AutoDi- agn diagnosing for root-cause analysis. Finally, the analysis results are sent back to the database to be stored. User visualization. The user visualization allows the users to have a visible way to monitor their big data system. We utilize InﬂuxDB’s client libraries and develop a set of REST- ful APIs to allow the users to query various information, including resource utilization, job and task status, as well as root cause of performance reduction. 3.3 AutoDiagn diagnosing framework In this section, we discuss the core components of the AutoDiagn Diagnosing framework (see Fig. 3), as well as the interactions with",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "AutoDiagn Diagnosing framework (see Fig. 3), as well as the interactions with each other and the AutoDiagn Monitoring framework. Symptom Detection Engine. The symptom detection en- gine subscribes a set of metrics from the real-time streaming system. §4.1 illustrates the technique that we developed for outlier detection. This component follows microservices architecture to which new symptom detection techniques can be directly attached to our AutoDiagn, interacting with other existing techniques to detect new symptoms. 4. https://github.com/hyperic/sigar 5. https://github.com/google/gson 6. https://hadoop.apache.org/docs/r3.2.1/hadoop-yarn 7. https://www.rabbitmq.com/ 8. https://www.inﬂuxdata.com/ Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 4] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 4 Symptom Detection Diagnosis Management Decision MakingSymptoms (N) Root -cause 1 • • •• • •Root -cause 2 Root -cause M• • •Root -cause 3Diagnosis (M) Root -cause 4MetricsSymptom Detector 2 Symptom Detector NSymptom Detector 1Diagnoser 1 Diagnoser 2 Diagnoser 3 Diagnoser 4 Diagnoser MDecision 1 Decision 2 • • • Decision N Root -cause M -1 Diagnoser M -1 Fig. 2. The key design idea of root-cause analysis for big data processing systems AutoDiagn Diagnosing Diagnoser Plugins Diagnoser 1 Task Input Output … Diagnoser",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "systems AutoDiagn Diagnosing Diagnoser Plugins Diagnoser 1 Task Input Output … Diagnoser N Task Input OutputAutoDiagn Monitoring Symptom Detection Engine Diagnosis decisionsCollected metricsDetected Symptoms Root -causes of the symptoms Diagnoser Manager Fig. 3. The high-level architecture of the AutoDiagn system Diagnoser Manager. The diagnoser manager is the core entity responsible for selecting the right diagnosers to ﬁnd the reasons that cause the detected symptoms. Additionally, the diagnoser manager is developed as a front-end com- ponent, triggered by various detected symptoms (events) via a RESTful API, exposing all diagnosing actions within our framework. The API includes general actions such as starting, stopping or loading a diagnoser dynamically, and speciﬁc actions such as retrieving some metrics. Importantly, the diagnoser manager is able to compose a set of diagnosers to complete",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "diagnoser manager is able to compose a set of diagnosers to complete the diagnosing jobs that may require the coop- eration of different diagnosers. Diagnoser Plugins. The diagnoser plugin contains a set of diagnosers; and a diagnoser is the implementation of the speciﬁc logic to perform root-cause analysis of a symptom. Each diagnoser refers to a set of metrics stored in a time series database as the input of its analysis logic. Whenever it is activated by the diagnoser manager, it will perform an analysis, querying the respective metrics, executing the analytic algorithm, and storing the results. §4.2 discusses the algorithms to detect the outlier problems, for example, in aHadoop cluster. The diagnoser plugin is also designed as a microservice architecture which has two advantages: i) a new",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "designed as a microservice architecture which has two advantages: i) a new diagnoser can be conveniently plugged or unplugged on-the-ﬂy without affecting other components; ii) new root- cause analysis tasks can be composed by a set of diagnosers via RESTful APIs. 3.4 AutoDiagn diagnosing interfaces for Hadoop AutoDiagn exposes a set of simple interfaces for system monitoring, symptom detection and root-cause analysis. Table 1 shows that two types of APIs are deﬁned: high- level APIs and low-level APIs. The high-level APIs consist ofSymptom Detection, Diagnoser and Decision Making. The Symptom Detection APIs are a set of real-time stream processing functions used to detect the deﬁned symptoms causing the performance reduction in the Hadoop system. Each Diagnoser is a query or a set of queries, which aim to ﬁnd",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "is a query or a set of queries, which aim to ﬁnd one of the causes of a symptom. For example, QueryNonLocal() tries to ﬁnd all non-local tasks within a time interval, which is one of the reasons that causes an out- lier. Finally, the Decision Making APIs are used to analyze the results from each Diagnoser and make the conclusion. These high-level APIs have to interact with the low-level APIs (Information Collection) to obtain system information including resource usage, and the execution information of the big data system (e.g., ask and job status in a Hadoop system). Based on this ﬂexible design, users can deﬁne and develop their own Symptom Detection, Diagnoser and Decision Making APIs and plug them into AutoDiagn. 3.5 Example applications We now discuss",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "APIs and plug them into AutoDiagn. 3.5 Example applications We now discuss several examples for big data system root cause applications using AutoDiagn API. Outliers. Outliers are the tasks that take longer to ﬁnish than other similar tasks, which may prevent the subse- quent tasks from making progress. To detect these tasks, the real-time stream query QueryOutlier() is enabled in the Symptom Detection Engine. This function consumes each task’s completion rate (i.e., progress) and the executed time to identify the outlier tasks (detailed in §4.1). Next, three APIs QueryNonlocal(), QueryLessResource() Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 5] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 5 Computer Cluster Master Node Publisher FilterCollector AgentResource InformationTask Information …Message BrokerAutoDiagn DiagnosingManagement Node User Visualization StorageConsumer Consumer Worker Node 1Publisher CollectorAgent Resource Information …Task 1 Task N Worker Node 2Publisher CollectorAgent Resource Information …Task 1 Task N Worker Node NPublisher CollectorAgent Resource Information …Task 1 Task N Fig. 4. The high-level architecture of the monitoring framework andQueryNodeHealth(), corresponding to three Diag- nosers that are used to analyze the reasons causing the de- tected symptom, are executed. QueryNonlocal() queries",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "analyze the reasons causing the de- tected symptom, are executed. QueryNonlocal() queries whether the input data is allocated on the node on which an outlier task is processed. In addition, QueryLessRe- source() investigates whether outlier tasks are running on the nodes that have less available resource. Moreover, QueryNodeHealth() examines if an outlier task is the task that is a restarted task due to the disconnected nodes from the network. Finally, RootcauseOutlier() is used to process the results from the three Diagnosers and make the conclusion. All the APIs are shown in Table 1 and the technical details are illustrated in §4. Inefﬁcient resource utilization. In our case this means that some tasks are pending (or waiting) to be on worker nodes; at the same time, some worker nodes",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "to be on worker nodes; at the same time, some worker nodes are idle, e.g., low CPU and memory usage. There are many reasons that cause this issue, but here we consider two key causes: task heterogeneity and resource heterogeneity. The type of tasks in a big data sys- tem are various, including CPU intensive tasks, IO intensive tasks and memory intensive tasks. However, the underlying computing resources are typically equally distributed to these tasks, thereby causing inefﬁcient resource utilization. The latter is caused by the heterogeneous underlying com- puting resources due to the multiple concurrent processing task environments and the queues are built on the saturated nodes. To detect the inefﬁcient resource utilization in a big data system, the real-time stream query QueryResourceU- til() is used within",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "big data system, the real-time stream query QueryResourceU- til() is used within a deﬁned time interval. We com- pute the mean and standard deviation of the usage re- sources of the whole cluster. If the standard deviation is far from the mean, we will further query whether the tasks are queued on the nodes which have high resource usage rates. If inefﬁcient resource utilization is detected, two Diagnosers, QueryOversubscribed() and QueryDiskIOboundTasks(), which are the root- cause analysis APIs shown in Table 1, are executed toperform root-cause analysis. QueryOversubscribed() checks the type of tasks queuing on the saturated nodes. TheQueryDiskIOboundTasks() checks whether the sat- urated nodes have less available computing resource, while processing the allocated tasks. The conclusion of the cause of inefﬁcient resource utilization is made in Root-",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "conclusion of the cause of inefﬁcient resource utilization is made in Root- causeResInef(). 3.6 Parallel execution Following the key design idea, the diagnosers are triggered by the corresponding detected symptom. However, we are able to parallelize the execution of each symptom detector and its diagnosers by partitioning the input data. For ex- ample, if one symptom detector needs to process too many data streams, we can use two of the same instances of the symptom detector to process the data streams and aggregate the results from two symptom detectors. The diagnoser can follow the same strategy for parallel execution. 3.7 Reliability analysis AutoDiagn follows the centralized design for data collec- tion, which simpliﬁes the implementation of the Symptom Detection, Diagnosis Management and Decision Making. They can easily obtain",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "the Symptom Detection, Diagnosis Management and Decision Making. They can easily obtain the required information from one place, instead of interacting with the entire big data system. More- over, the centralized design does not mean unreliability, due to the high-availability of RabbitMQ. The RabbitMQ cluster can overcome the node fail in the message queuing system while ensuring scalability. 4 C ASESTUDY In the previous section, we have discussed that our frame- work supports detection of multiple types of symptoms (e.g., outliers, inefﬁcient resource utilization). However, de- tecting these symptoms is non-trivial; and each symptom can be detected by using different algorithms with different input metrics. In this section, we present a case study that Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 6] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 6 TABLE 1 AutoDiagn diagnosing interface. See §3.4 for deﬁnitions and examples Symptom Detection (High-level APIs) Description QueryOutlier() Execute a Query that returns the list of outliers if any. QueryResourceUtil() Execute a Query that returns the list of the worker nodes in which the computing resources are not uti- lized effectively if any. Diagnoser (High-level",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "the computing resources are not uti- lized effectively if any. Diagnoser (High-level APIs) Description QueryNonLocal() Execute a Query that return the list of non-local tasks if any. QueryLessResource() Execute a Query that returns false if the cluster is not homogeneous in terms of having resource capacity (CPU/memory). QueryNodeHealth() Execute a Query that returns the list of disconnected worker nodes in the cluster if any. QueryOversubscribed() Execute a Query that returns the list of the oversubscribed tasks if any. QueryDiskIOboundTasks() Execute a Query that returns the list of the disk- or IO-bound tasks if any. Decision Making (High-level APIs) Description RootcauseOutlier() Execute a Query that illustrate the main reason of the cause of the outlier. RootcauseResInef() Execute a Query that illustrate the main reason of the cause of inefﬁcient",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "a Query that illustrate the main reason of the cause of inefﬁcient resource utilization. Information Collection (Low-level APIs) Description taskExecTime() Return the execution time since the task started in sec. taskProgress() Return the progress of the running task as a percentage. taskInput() Return the input data size of the running task in mb. taskBlock() Return the block id this task process. taskHost() Return the name of the node thistask ran on. taskCPUusage() Return the CPU usage of the task. taskMemoryUsage() Return the memory usage of the task. taskContainerCPU() Return the allocated CPU to the container this task ran on. taskContainerMemory() Return the allocated memory to the container this task ran on. blockHost() Return the names of the nodes that host the block. pendingTasks() Return the number of the",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "the nodes that host the block. pendingTasks() Return the number of the tasks waiting to be run. nodeTotalCoreNum() Return the number of the CPU core number of the node. nodeCPUUsage() Return the CPU utilization of the node. nodeTotalMem() Return the total memory capacity of the node. restartedTasks() Return the name of the restarted tasks due to nodes that got disconnected from the network. nodeMemUsage() Return the memory utilization of the node. nodeDiskReadSpeed() Return the disk read speed of the node. nodeDiskWriteSpeed() Return the disk write speed of the node. nodeUploadSpeed() Return the network upload speed of the node. nodeDownloadSpeed() Return the network download speed of the node. details the technology of detecting outliers and the root- causes analysis for the detected outliers. The notations used in this paper",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "causes analysis for the detected outliers. The notations used in this paper are summarized in Table 2. TABLE 2 A summary of symbols used in the paper Symbols Description Jp Job progress N Name of the task Nl List ofN P Performance of the N Pl List ofP O Progress of theN Ol List ofO T Execution time of the N Tl List ofT med The performance of median task D Non-local tasks Dl List of Non-local task R Task running on the node with less resources Rl List ofR W Restarted tasks due to the nodes’ network failure Wl List ofW Sl List of outlier task Sd Non-local outlier Sdl List of Sd Sr Outlier stemming from the resource variation Srl List of Sr Sw Outlier stemming",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "stemming from the resource variation Srl List of Sr Sw Outlier stemming from disconnected nodes Swl List of Sw F Factor value of 1.5 used to ﬁnd the S4.1 Symptom detection for outliers Ananthanarayanan et al. [10] deﬁned the outlier tasks’ run- time to be 1.5 times higher than that of the median task execution time; their method is based on the assumption that all tasks are started at the same time and are the same type (i.e., the same input data and the same processing code), which is not suitable for real-time symptom detection, because in a time interval the tasks may be submitted at different times; the input data size of the tasks and the code for tasks are not always the same. In this paper,",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "the code for tasks are not always the same. In this paper, we use Performance (P) to measure the outlier as shown in Eq 1. O represents the normalized value of the task progress in terms of percent work complete, and Tis the normalized value of the task execution time. P=O T(1) Eq 2 is used to normalize the OandT, where xmin and xmax are the minimal and maximal values of the given metrics (eg., task progress and execution time) in a time interval. We set b= 1 anda= 0:1 to restrict the normalized values within the range from 0.1 to 1 [12]. xnorm =a+(x\u0000xmin)(b\u0000a) xmax\u0000xmin(2) Moreover, we deﬁne the outlier tasks which have 1.5 times less performance value than the median performance value in each time interval.",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "less performance value than the median performance value in each time interval. Fig. 5 shows a snapshot of a time Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 7] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 7 Algorithm 1: Automated symptom detection for outliers Input: Jp- job progress in percentage, F- factor, N- name of the running task, Nl- list ofN, O- progress of the task, Ol- list ofO,",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "task, Nl- list ofN, O- progress of the task, Ol- list ofO, T- execution time of the task, Tl- list ofT. Output: Sl- list of outliersS. 1// Create a list Slto store theS 2Sl Sl[0] 3// Initialize the med 4med med[0] 5while Jp<100.0 do 6 //Clear the SlandPl 7 Sl Clear (Snew l ,Sl) 8 Pl Clear (Pnew l ,Pl) 9 foreachNinNldo 10 //ComputeP 11P=O T 12 //Insert thePinto the Pl 13 Pl.add(P ) 14 end 15 //Get themedfrom thePl 16 med Median value of Pl 17 foreach value of Pldo 18 if(P*F)< m edthen 19 //Insert theNinto theSl 20 Sl.add(N ) 21 end 22 end 23 //Update the SlinDiagnosis Generation component 24 Sl Update (Snew l ,Sl) 25 //Update the Nl,Ol,Tl,Jp 26 Nl Replace (Nnew l ,Nl)",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "l ,Sl) 25 //Update the Nl,Ol,Tl,Jp 26 Nl Replace (Nnew l ,Nl) 27 Ol Replace (Onew l ,Ol) 28 Tl Replace (Tnew l ,Tl) 29 Jp Replace (Jnew p ,Jp) 30end interval (e.g., three seconds), and two mappers are identiﬁed as outliers. More evaluations will be discussed in §5. Algorithm 1 demonstrates the proposed ASD (auto- mated symptom detection) algorithm in the AutoDiagn system. It is fed by the streaming data provided by the AutoDiagn Monitoring system during job execution. First, the performance of each running task is calculated (see Algorithm 1, Line 11) using Eq 1. Next, the median value of the performance of all tasks is taken to be used to detect outliers (see Algorithm 1, Line 16). Then, the tasks whose performance is 1.5 times",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "Algorithm 1, Line 16). Then, the tasks whose performance is 1.5 times less than the performance of the median task are selected as outliers (see Algorithm 1, Line 20). As a ﬁnal step, these tasks detected as outliers are sent to theDiagnosis Generation component for root-cause analysis (see Algorithm 1, Line 24). 4.2 Root cause analysis for outliers When the detected symptoms are passed to the Diagnoser Manager, the corresponding Diagnosers are executed for trouble-shooting. The following subsection illustrates the technologies that we have developed for analyzing the causes of outliers in a Hadoop cluster. 4.2.1 Root cause of outliers In this paper, we follow the three main reasons that cause outliers, discussed in [10], i.e., Data locality, Resource het- erogeneity, and Network failures. Progress (%)Execution time (sec)",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "Data locality, Resource het- erogeneity, and Network failures. Progress (%)Execution time (sec) 0 1 2OutliersMedian=1.11Performance levels 30 35 40 45 50 55 60 65 14 16 18 20 22 24 26 28 30 32 Performance 0.2 0.4 0.6 0.8 1 1.2 1.4 Fig. 5. Performance evaluation of the tasks Data locality. Hadoop Distributed File System (HDFS) stores the data in a set of machines. If a task is scheduled to a machine which does not store its input data, moving data over the network may introduce some overheads to cause the outliers issue. Resource heterogeneity. The machines in a Hadoop cluster may be homogeneous with the same hardware conﬁgura- tion, but the run-time computing resources are very hetero- geneous due to the multiple talents environment, multiple concurrent processing",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "very hetero- geneous due to the multiple talents environment, multiple concurrent processing task environment, machine failures, machine overloaded etc. If a task is scheduled to a bad machine (e.g., has less computing resource) it may cause an outlier issue. Moreover, resource management systems for a large-scale cluster like YARN split the tasks over the nodes equally without considering the resource capacities of the nodes in the cluster, but only takes into account sharing the node’s resources among the tasks running on the node equally by default [13]. That is more likely to raise an outlier problem in the cluster. Network failure. In Hadoop clusters, the network discon- nection can cause the running tasks allocated on a discon- nected node to be restarted on other nodes, which may lead",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "discon- nected node to be restarted on other nodes, which may lead to the task becoming an outlier and, increase the completion time. The following illustrates the three algorithms that we developed to identify the outliers caused by the three reasons. 4.2.2 Detecting data locality issues We assume that a non-local task (D ) (e.g., mapper) is ex- ecuted on a node where its input data is not stored (In the following, we use Sdto represent non-local outliers). To detect these tasks, we develop Algorithm 2 to check whether a set of outliers is caused by a data locality issue. The input of our algorithm is a list of detected outliers during the time interval from ttot+ 1 and one of its outputs is a list of outliers",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "ttot+ 1 and one of its outputs is a list of outliers which also belongs to the non-local tasks. First, we query our time series database to obtain all non-local tasks within the given time interval (see Algorithm 2, Line 2). Here, QueryNonLocal(), a root-cause analysis API, is used to ﬁnd the non-local ones among the running tasks in that period of time. It compares the location where the task is running (host node of the task) with the nodes where the data block is replicated for fault tolerance via Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 8] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 8 information collection APIs shown in Table 1, taskHost() andblockHost(). If the task is not running on any of these nodes (nodes hosting a copy of the block), this task is marked as a non-local task. In the second step (Algorithm 2, Line 4), we obtain the common elements of list DlandSl. These elements symbolize the non-local outliers stemming from a data locality issue. 4.2.3 Detecting resource heterogeneity issues Algorithm 2 is designed to identify the outliers caused by",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "heterogeneity issues Algorithm 2 is designed to identify the outliers caused by the resource heterogeneity. The tasks running on the nodes which have less computing resource (R ) tend to be outliers [14] (in the following, we use Srto represent outliers running on the nodes which have less computing resource). In Algorithm 2, the list of detected outliers during the time interval from ttot+ 1 is used as input and one of the outputs of the algorithm is a list of outliers which also belongs to the tasks running on the node with less computing resource. The time series database is queried to obtain all the tasks running on the node with less computing resource within the given time interval (see Algorithm 2, Line 6). Here, QueryLessResource(), a",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "the given time interval (see Algorithm 2, Line 6). Here, QueryLessResource(), a root-cause analysis API, is used to check the heterogeneity of the nodes that host only the running tasks based on the resource speciﬁcations of them in that period of time. It detects the nodes with less resource capacity in terms of CPU core numbers and the to- tal amount of memory among the nodes hosting the running tasks. The resource speciﬁcations of the nodes (i.e., CPU core numbers, total amount of memory) are obtained from each node via information collection APIs shown in Table 1, nodeTotalCoreNum() andnodeTotalMem() APIs. As a second step (Algorithm 2, Line 8), we obtain the common elements of list RlandSl. These elements symbolize the outliers stemming from a cluster heterogeneity issue. 4.2.4",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "These elements symbolize the outliers stemming from a cluster heterogeneity issue. 4.2.4 Detecting network failure issues Since Slis obtained from Algorithm 1, a Diagnoser is exe- cuted via QueryNodeHealth() to ﬁnd all restarted tasks due to the nodes disconnected by network failure within the given time interval (see Algorithm 2, Line 10). The low-level APIrestartedTasks() is called which distinguishes the restarted tasks due to network failure from the speculation of straggler tasks by analyzing the information of the tasks that is provided by the monitoring agent. Thereafter, we compute the list Swlthat contains the outlier tasks caused by the network failure (see Algorithm 2, Line 12). 4.2.5 Decision making In this case study, we use a simple decision make method that compares the lists Sdl,SrlandSwland the probability of",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "simple decision make method that compares the lists Sdl,SrlandSwland the probability of the reasons causing the outliers by using the number of the elements of a list divided the total number of out- lier tasks. For instance, the probability of the performance reduction caused by data locality isjSdlj jSlj. More advanced methods such as deep learning models can be used for pro- cessing more complicated decision making tasks in future work.Algorithm 2: Root-cause analysis of outliers Input: Sl- list of outliers in time interval from ttot+ 1 Output: Sdl- list of non-local outliers Sd, Srl- list of outliers stemming from resource variation Sr, Swl- list of outliers stemming from disconnected nodes Sw. 1// Find allDwithin the given time interval 2Dl QueryNonLocal(t, t+1) 3//Find the common elements in the",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "given time interval 2Dl QueryNonLocal(t, t+1) 3//Find the common elements in the DlandSl, and add them into theSdl 4Sdl RetainAll (Dl,Sl) 5// Find allRwithin the given time interval 6Rl QueryLessResource(t, t+1) 7//Find the common elements in the RlandSl, and add them into theSll 8Srl RetainAll (Rl,Sl) 9// Find allWwithin the given time interval 10Wl QueryNodeHealth(t, t+1) 11//Find the common elements in the WlandSl, and add them into theSwl 12Swl RetainAll (Wl,Sl) 5 E VALUATION In this section, we present a comprehensive evaluation showing the capacity and the accuracy rate of AutoDiagn, as well as a analysis of its resource consumption and over- heads. 5.1 Experimental setup Environments. We set up the Hadoop YARN clusters over 31 AWS nodes with 1 master and 30 slaves with the Oper- ating",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "AWS nodes with 1 master and 30 slaves with the Oper- ating system of each node being Ubuntu Server 18.04 LTS (HVM). The Hadoop version is 3.2.1 and the Hive version is 3.1.1. To meet our experimental requirements, we built two types of cluster. In Type I each node has the same conﬁguration (i.e., 4 cores and 16 GB memory). In Type II, 25 nodes have 4 cores and 16 GB memory and 6 nodes have 2 cores and 4 GB memory. Benchmarks and workload. We used six well-known Hadoop benchmarks in our evaluations namely: Word- Count9, Grep10, TPC-H11, TPC-DS12, K-means clustering13, and PageRank14. The input of each benchmark application is 30GB. Methodology. Our experiments aim to evaluate the effec- tiveness of AutoDiagn. To this end, we manually",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "to evaluate the effec- tiveness of AutoDiagn. To this end, we manually inject the above-mentioned three main reasons to cause the outliers, which can be summarized as three types of execution en- vironment. EnvA: we perform all benchmark experiments in the cluster Type I. EnvB: we perform all benchmark experiments in the cluster Type I, but skew the input size stored on different nodes. EnvC: we perform all benchmark experiments in the cluster Type II (a heterogeneous cluster). EnvH: we perform all benchmark experiments in the cluster Type I, and disconnect some nodes’ network during execu- tion. Each benchmarking is repeated 5 times and results are reported as the average and standard deviation. In total, there are 90 experiments conducted in our evaluation. 9. http://wiki.apache.org/hadoop/WordCount 10. http://wiki.apache.org/hadoop/Grep 11.",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "are 90 experiments conducted in our evaluation. 9. http://wiki.apache.org/hadoop/WordCount 10. http://wiki.apache.org/hadoop/Grep 11. http://www.tpc.org/tpch/ 12. http://www.tpc.org/tpcds/ 13. https://en.wikipedia.org/wiki/K-means clustering 14. https://en.wikipedia.org/wiki/PageRank Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 9] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 9 TABLE 3 The accuracy of symptom detection for non-local outliers in a homogeneous cluster Benchmark Total tasksD Outliers (detected as Sd)Accuracy (%)Error (\u001b) WordCount 234 32 29 90.63 3.9 Grep 236 37",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "Sd)Accuracy (%)Error (\u001b) WordCount 234 32 29 90.63 3.9 Grep 236 37 33 89.19 4.8 TPC-H 102 13 12 92.31 6.72 TPC-DS 126 13 12 92.31 6.1 K-means 234 34 29 85.29 1.25 PageRank 235 28 25 89.29 6.2 TABLE 4 The accuracy of symptom detection for the outliers stemming from resource variation in a heterogeneous cluster Benchmark Total tasksR Outliers (detected as Sr)Accuracy (%)Error (\u001b) WordCount 234 37 33 89.19 2.77 Grep 236 26 24 92.31 4.77 TPC-H 102 9 8 88.89 5.47 TPC-DS 126 13 12 92.31 6.9 K-means 234 36 33 91.67 2.88 PageRank 235 30 28 93.33 5.35 5.2 Diagnosis detection evaluation In this section, we evaluate the accuracy of our symptom detection method. To this end, we execute our benchmarks inEnvBto increase number",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "detection method. To this end, we execute our benchmarks inEnvBto increase number of Sdtasks (see §4.2.2). Next, to increase the issue of resource heterogeneity (Sr referring to §4.2.3), we run the benchmarks in EnvC. Thereafter, we run the benchmarks in EnvHto emulate the network failure (Sw referring to §4.2.4). Finally, we compare the detected Outlier tasks with the ground truths that are the data locality, resource heterogeneity, and network failure issues observed by the AutoDiagn diagnosing system. Table 3, Table 4, and Table 5 summarize all the results. All benchmarks achieve high accuracy by using our proposal symptom detection method. The highest accuracy for both Sdand Srare 92.3%, and for Swis 94.7% and the overall accuracy for outlier detection is 91.3%, where the Error represents the variation of",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "for outlier detection is 91.3%, where the Error represents the variation of the accuracy depending on the repeated experiments. We compute the accuracy of our symptom detection method by using the number of detected outlier tasks di- vided by the actual number of the tasks that can cause the outlier issue. Table 3, for example, Dis the total number of non-local tasks and Outliers (Sd) is the number of detected outlier tasks that belong to non-local task. Therefore, the accuracy isSd D. Table 4 and Table 5 follow the same approach to compute the accuracy. Outlier veriﬁcation. To further verify the Sd,Sr, and Sw are the main reasons causing the outliers, we conduct the following comparison experiments: 1) comparing the exe- cution time of local tasks and non-local",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "experiments: 1) comparing the exe- cution time of local tasks and non-local tasks; 2) comparing the execution time of the tasks running in EnvAand Env C; and 3) comparing the execution time of normal tasks and restarted tasks due to network failure. Fig. 6(a) proves that non-local tasks consume more time than local tasks due to the overload introduced by data shufﬂing. Additionally, weTABLE 5 The accuracy of symptom detection for the outliers stemming from network failures Benchmark Total tasksW Outliers (detected as Sw)Accuracy (%)Error (\u001b) WordCount 234 11 10 90.91 1.83 Grep 236 13 12 92.31 6.73 TPC-H 102 13 12 92.31 6.54 TPC-DS 126 15 14 93.33 5.43 K-means 234 17 16 94.12 4.33 PageRank 235 19 18 94.74 4.23 compare the throughput of the local",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "PageRank 235 19 18 94.74 4.23 compare the throughput of the local tasks and non-local tasks in terms of how much data can be processed in each second. Fig. 7 reveals that the throughput of non-local tasks is only 70% that of local tasks. Moreover, Fig. 6(b) shows that the execution time of the tasks running on EnvAis less than that on EnvC. This is because the tasks are equally distributed to all computing nodes and the less powerful nodes are saturated. Furthermore, Fig. 9(a) shows that the CPU usage of less powerful hosts reaches 100%, thereby building a task queue in these hosts, increasing the overall execution time. How- ever, Fig. 9(b) reveals that the powerful hosts have sufﬁcient computing resources for processing the allocated tasks. Furthermore,",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "powerful hosts have sufﬁcient computing resources for processing the allocated tasks. Furthermore, Fig. 6(c) shows that the execution time of the restarted tasks are longer than the normal tasks. As Fig. 8 illustrates, we compute the execution time of the restarted task by adding the execution time of the task in the disconnected node and that in the rescheduled node. 5.3 Performance and overheads Performance evaluation. We evaluate the performance of AutoDiagn by measuring the end-to-end response time of symptom detection and root-cause analysis. Since they are not affected by the types of benchmark, we report the average of the response time. Fig. 10(a) shows that the real-time symptom detection can achieve a low response time, which only has 96 milliseconds and 1059 milliseconds with 100 tasks and",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "which only has 96 milliseconds and 1059 milliseconds with 100 tasks and 1000 tasks, respectively. Although the re- sponse time increases linearly, the parallel execution method discussed in §3.6 can be applied to reduce the latency. The response time for root cause analysis is higher than that of symptom detection. For 100 tasks and 1000 tasks, their response times are 0.354 seconds and 5.974 seconds, respec- tively. Unlike the symptom detection which is very sensitive to latency because of the follow-up processes, triggering the further root-cause analysis or alerting the system managers, Root-cause analysis aims to provide a holistic diagnosing of a big system and the analysis results may help to improve the system performance in future. As a result, the real-time root-cause analysis is not compulsory. System",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "future. As a result, the real-time root-cause analysis is not compulsory. System overheads. To evaluate the system overhead intro- duced by AutoDiagn, we measure the CPU and memory usage of AutoDiagn Monitoring (agent) and AutoDiagn Diagnosing. Table 6 shows that -AutoDiagn Monitoring only consumes approximately 2.52% memory and 4.69% CPU; while -AutoDiagn Diagnosis uses 2.08% memory and 3.49% CPU. Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 10] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information:",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 10 0 5 10 15 20 25 30 35 40 WordCountGrepTPC-HTPC-DS K-means PageRankExecution time (sec) Types of BenchmarkingLocal tasks running on Env A Non-local tasks (D) running on Env B (a) Local tasks vs Non-local tasks 0 5 10 15 20 25 30 35 40 WordCountGrepTPC-HTPC-DS K-means PageRankExecution time (sec) Types of BenchmarkingTasks running on Env A Tasks (R) running on Env C (b) Homogeneous cluster vs Heterogeneous cluster 0 10 20 30 40 50 60 WordCountGrepTPC-HTPC-DS K-means PageRankExecution time (sec) Types of BenchmarkingTasks running on Env A Tasks (W) running on Env H(c) Normal tasks vs Restarted tasks caused by network failure Fig. 6. Comparison of execution time of",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "tasks caused by network failure Fig. 6. Comparison of execution time of the tasks 0 1 2 3 4 5 6 WordCountGrepTPC-HTPC-DS K-means PageRankThroughput (MB/s) Types of BenchmarkingLocal tasks Non-local tasks Fig. 7. The throughput of AutoDiagn 0 20 40 60 80 100 0510152025303540455055Progress (%) Elapsed time (sec) Fig. 8. The life cycle of the restarted task Fig 10(b) shows the network overhead of AutoDiagn. The extra communication cost introduced by our tool is small but it increases when the number of parallel tasks increases. For example, when the number of parallel task is 100, there are about 45 messages per second sent from agents to RabbitMQ cluster and the total size of these messages is 13.5 KB/s. The message rate and network overhead increase to 615 per",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "13.5 KB/s. The message rate and network overhead increase to 615 per second and 223 KB/s, respectively, when the number of parallel tasks is 1000. Storage overheads. AutoDiagn needs to dump the system information to a database which may consume extra storage resource. In our evaluation experiments, it only cost 3.75 MB disk space in total. Obviously, increasing the types of symptom detection and root cause analysis will also consume more storage resources. We discuss the potentialTABLE 6 Resource overhead caused by AutoDiagn components Components Mem (%) CPU (%) AutoDiagn Monitoring 2.52 4.69 AutoDiagn Diagnosing 2.08 3.49 future work in §6. 6 D ISCUSSION AND FUTURE WORK Populating applications. In this paper, we propose a gen- eral and ﬂexible framework to uncover the performance reduction issues in a",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "eral and ﬂexible framework to uncover the performance reduction issues in a big data system. In particular, we develop and evaluate big data applications for outliers. New applications (including symptom detection and root-cause analysis) are required to populate our system for future work. Overhead cost reduction. Our system is designed in a loosely-coupled manner, the processing components can be easily scaled. However, the storage overhead increases with the number of applications increasing. [15] proposed a caching method to aggregate the information before sending to destination nodes. We will explore this direction in future work to reduce the storage overhead and network overhead. Performance improvement. Mantri [10] utilized the outputs of the root cause analysis to improve the resource allocation in Hadoop clusters. Thus, one open research direction is",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "the resource allocation in Hadoop clusters. Thus, one open research direction is to build a system which can react to analysis results, thereby improving the performance of the big data system. 7 R ELATED WORK Much recent work in big data systems focuses on improving workﬂows [16], [17], [18], programming framework [19], [20], [21], task scheduling [22], [23], [24]. Root-cause analysis. There is a large volume of published studies describing the role of root-cause analysis. The au- thors of [10], [25], [26] take the next step of understanding the reasons for performance reduction. Mantri [10] charac- terizes the prevalence of stragglers in Hadoop systems as well as troubleshooting the cause of stragglers. Dean and Barroso [25] analyze the issues causing tail latency in big Authorized licensed use limited",
        "start_idx": 8236,
        "end_idx": 8364
      },
      {
        "text": "analyze the issues causing tail latency in big Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 11] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 11 0 20 40 60 80 100 CPU usage (%) TimelineCPU utilization Outliers (a) CPU utilization of less powerful hosts and outliers 0 20 40 60 80 100 CPU usage (%) TimelineCPU utilization (b) CPU utilization of high power hosts Fig. 9. CPU utilization of",
        "start_idx": 8352,
        "end_idx": 8480
      },
      {
        "text": "(b) CPU utilization of high power hosts Fig. 9. CPU utilization of two nodes running simultaneously. Outliers are most likely to occur in the nodes which have less computing resource. 0 1 2 3 4 5 6 50100 200 300 400 500 600 700 800 9001000Response time (sec) Number of tasks running in parallelSymptom detectionRoot-cause analysis (a) The end-to-end response time of AutoDiagn diag- nosis system 0 100 200 300 400 500 600 501002003004005006007008009001000 0 50 100 150 200 250Messages per second Data rate (KB/s) Number of tasks running in parallelMessage rates Size (KB/s)(b) The message rates and network overhead Fig. 10. Performance evaluation and network overhead of AutoDiagn data systems. Garraghan et al. [11], [27] proposed a new method to identify long tail behavior in big data",
        "start_idx": 8468,
        "end_idx": 8596
      },
      {
        "text": "proposed a new method to identify long tail behavior in big data systems and evaluated in google data trace. The authors in [28] use ofﬂine log analysis methods to identify the root cause of outliers in a large-scale cluster consisting of thousands of nodes by tracking the resource utilization. Similarly, Zhou et al. [29] use a simple but efﬁcient rule based method to identify the root cause of stragglers. Along with these similar works, there are some re- searchers using statistical and machine learning methods for root-cause analysis. The authors of [30] introduce a Regres- sion Neural Network (RNN) based algorithm to trouble- shoot the causes of stragglers by processing Spark logs. More algorithms such as the associated tree and fuzzy data envelopment analysis [31] and Reinforcement Learning",
        "start_idx": 8584,
        "end_idx": 8712
      },
      {
        "text": "the associated tree and fuzzy data envelopment analysis [31] and Reinforcement Learning [32] are applied for ﬁnding the reasons of stragglers in Hadoop and Spark. In [33], a Pearson coefﬁcient of correlation is used for root cause analysis to measure linear correlation between system metrics, workload and latency. However, these works lack a systematic solution for root cause analysis for big data processing systems and the proposed methods are not applicable for real-time systems. Different to other work, the authors of [34] propose a new algorithm that aims to reduce the proportion of strag- gler tasks in machine learning systems that use gradient- descent-like algorithms. This work offers an idea to develop new Diagnosers for machine learning systems using our framework. Anomaly detection and debugging. The authors in",
        "start_idx": 8700,
        "end_idx": 8828
      },
      {
        "text": "learning systems using our framework. Anomaly detection and debugging. The authors in [35] pro- pose a rule-based approach to identify anomalous behaviorsin Hadoop ecosystems by analyzing the task logs. This work only analyzes the task logs, which fails to capture the performance reduction issues caused by inefﬁcient utilizing the underlying resources. Next, Khoussainova et al. [36] build a historical log analysis system to study and track the MapReduce jobs which cause performance reduction based on their relevance, precision and generality principles. However, this cannot be performed for real-time anomaly detection. Du et al. [37] train a machine learning model from the normal condition data by using Long Short-Term Mem- ory (LSTM) and this trained model is used for detecting in Hadoop and OpenStack environments. Our AutoDiagn provides infrastructure",
        "start_idx": 8816,
        "end_idx": 8944
      },
      {
        "text": "used for detecting in Hadoop and OpenStack environments. Our AutoDiagn provides infrastructure into which the trained models can be plugged to enrich the applications. Real-time operational data analytic system. Agelastos et al. [38] propose a monitoring system for HPC systems, which can capture the cases of applications competing for shared resources. However, this system does not consider root- cause analysis of the performance reduction. The authors of [5], [39] do not only provide the feature of real-time monitoring, but are also able to identify the performance issues and trouble-shoot the cause of the issues. In addition to them, [40] uses a type of artiﬁcial neural network called autoencoder for anomaly detection. They ﬁrst monitor the system in real-time and collect the normal data for training the model used",
        "start_idx": 8932,
        "end_idx": 9060
      },
      {
        "text": "in real-time and collect the normal data for training the model used to discern between normal and abnormal conditions in an online fashion. However, these systems are developed for HPC clusters and are not suitable for big data systems. Table 7 presents a brief overview of various monitoring tools for big data frameworks. Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 12] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers",
        "start_idx": 9048,
        "end_idx": 9176
      },
      {
        "text": "prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 12 TABLE 7 The features supported by existing tools and AutoDiagn Feature DataDog [2]Sequence IQ [3]Sematext [4]TACC [5]Mantri [10]DCDB [39]Nagios [41]Ganglia [42]Chukwa [43]DMon [44]AutoDiagn Real-time monitor- ingYes Yes Yes Yes Yes Yes Yes Near real-timeYes Near real- timeYes Root-cause analysis No No No No Yes Yes No No No Yes Yes BigData frameworks supportGood Poor Good No Poor No Poor Poor Poor Good and ExtensibleGood and Extensible Underlying resource monitoringYes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Real-time monitor- ing for big data tasksYes Yes Yes No Yes No No No Yes Yes Yes Auto-scaling Yes Yes Yes Yes Yes Yes No No Yes Yes Yes Alerts Yes No Yes No No No",
        "start_idx": 9164,
        "end_idx": 9292
      },
      {
        "text": "No No Yes Yes Yes Alerts Yes No Yes No No No Yes No No No Yes Visualization of big data tasksYes No Yes No No No No Yes No No Yes User customized root-cause analysisNo No No No No No No No No No Yes 8 C ONCLUSION In this paper, we have presented AutoDiagn, a framework for enabling diagnosing of large-scale distributed systems to ascertain the root cause of outliers, with the core purpose of unravelling the concretization of complicated models for system management. After making a comprehensive literature review and identifying the requirements for real- world problems, we conceived its design. The combination of user-deﬁned functions powered by APIs and the agent- based monitoring system along with the ﬁndings obtained from an empirical analysis of",
        "start_idx": 9280,
        "end_idx": 9408
      },
      {
        "text": "monitoring system along with the ﬁndings obtained from an empirical analysis of the experiments we conducted play a fundamental role in the development of the system. AutoDiagn can be applied to most big data systems along with the monitoring systems. We have also presented the implementation and integration of the AutoDiagn system to the SmartMonit [45], real-time big data monitoring system, combined in our production environment. In our implemen- tation on a large cluster, we ﬁnd AutoDiagn very effective and efﬁcient. Outliers are one of the main problems in big data sys- tems that overwhelm the whole system and reduce perfor- mance considerably. AutoDiagn embraces this problem to reveal the bottlenecks alongside their root causes. ACKNOWLEDGEMENT This research is funded by the Turkish Ministry of Na- tional Education.",
        "start_idx": 9396,
        "end_idx": 9524
      },
      {
        "text": "This research is funded by the Turkish Ministry of Na- tional Education. This research is partially funded by the following UKRI projects: SUPER (EP/T021985/1), PACE (EP/R033293/1), and Centre for Digital Citizens (EP/T022582/1). This work is also supported by the grant of National Natural Science Foundation of China (62072408) and Zhejiang Provincial Natural Science Foundation of China (LY20F020030). REFERENCES [1] A. Noor, K. Mitra, E. Solaiman, A. Souza, D. N. Jha, U. Demirbaga, P . P . Jayaraman, N. Cacho, and R. Ranjan, “Cyber-physical appli- cation monitoring across multiple clouds,” Computers & Electrical Engineering, vol. 77, pp. 314–324, 2019.[2] Datadog. Accessed: 2020-07-13. [Online]. Available: https: //www.datadoghq.com/ [3] Sequenceiq. Accessed: 2020-07-14. [Online]. Available: https: //github.com/sequenceiq [4] Sematext. Accessed: 2020-07-13. [Online]. Available: https: //sematext.com/ [5] R. T. Evans, J. C. Browne,",
        "start_idx": 9512,
        "end_idx": 9640
      },
      {
        "text": "2020-07-13. [Online]. Available: https: //sematext.com/ [5] R. T. Evans, J. C. Browne, and W. L. Barth, “Understanding application and system performance through system-wide moni- toring,” in 2016 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW). IEEE, 2016, pp. 1702–1710. [6] G. Iuhasz, D. Pop, and I. Dragan, “Architecture of a scalable platform for monitoring multiple big data frameworks,” Scalable Computing: Practice and Experience, vol. 17, no. 4, pp. 313–321, 2016. [7] I. Dr ˘agan, G. Iuhasz, and D. Petcu, “A scalable platform for monitoring data intensive applications,” Journal of Grid Computing, vol. 17, no. 3, pp. 503–528, 2019. [8] S. Babu, “Towards automatic optimization of mapreduce pro- grams,” in Proceedings of the 1st ACM symposium on Cloud com- puting, 2010, pp. 137–142. [9] R. S. Xin, J.",
        "start_idx": 9628,
        "end_idx": 9756
      },
      {
        "text": "on Cloud com- puting, 2010, pp. 137–142. [9] R. S. Xin, J. Rosen, M. Zaharia, M. J. Franklin, S. Shenker, and I. Stoica, “Shark: Sql and rich analytics at scale,” in Proceedings of the 2013 ACM SIGMOD International Conference on Management of data, 2013, pp. 13–24. [10] G. Ananthanarayanan, S. Kandula, A. G. Greenberg, I. Stoica, Y. Lu, B. Saha, and E. Harris, “Reining in the outliers in map- reduce clusters using mantri.” in Osdi, vol. 10, no. 1, 2010, p. 24. [11] P . Garraghan, X. Ouyang, P . Townend, and J. Xu, “Timely long tail identiﬁcation through agent based monitoring and analytics,” in2015 IEEE 18th International Symposium on Real-Time Distributed Computing. IEEE, 2015, pp. 19–26. [12] J. Han, J. Pei, and M. Kamber, Data mining: concepts",
        "start_idx": 9744,
        "end_idx": 9872
      },
      {
        "text": "19–26. [12] J. Han, J. Pei, and M. Kamber, Data mining: concepts and techniques. Elsevier, 2011. [13] T. Renner, L. Thamsen, and O. Kao, “Coloc: Distributed data and container colocation for data-intensive applications,” in 2016 IEEE International Conference on Big Data (Big Data). IEEE, 2016, pp. 3008–3015. [14] A. Rasooli and D. G. Down, “Guidelines for selecting hadoop schedulers based on system heterogeneity,” Journal of grid com- puting, vol. 12, no. 3, pp. 499–519, 2014. [15] A. Rabkin, M. Arye, S. Sen, V . S. Pai, and M. J. Freedman, “Aggregation and degradation in jetstream: Streaming analytics in the wide area,” in 11thfUSENIXg Symposium on Networked Systems Design and Implementation (fNSDIg 14), 2014, pp. 275–288. [16] Z. Wen, T. Lin, R. Yang, S. Ji, R. Ranjan, A. Romanovsky,",
        "start_idx": 9860,
        "end_idx": 9988
      },
      {
        "text": "Z. Wen, T. Lin, R. Yang, S. Ji, R. Ranjan, A. Romanovsky, C. Lin, and J. Xu, “Ga-par: Dependable microservice orchestration frame- work for geo-distributed clouds,” IEEE Transactions on Parallel and Distributed Systems, vol. 31, no. 1, pp. 129–143, 2019. [17] Z. Wen, J. Cała, P . Watson, and A. Romanovsky, “Cost effective, reliable and secure workﬂow deployment over federated clouds,” IEEE Transactions on Services Computing, vol. 10, no. 6, pp. 929–941, 2016. Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 13] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but",
        "start_idx": 9976,
        "end_idx": 10104
      },
      {
        "text": "been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 13 [18] Z. Wen, R. Qasha, Z. Li, R. Ranjan, P . Watson, and A. Romanovsky, “Dynamically partitioning workﬂow over federated clouds for optimising the monetary cost and handling run-time failures,” IEEE Transactions on Cloud Computing, 2016. [19] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn, N. Leiser, and G. Czajkowski, “Pregel: a system for large-scale graph processing,” in Proceedings of the 2010 ACM SIGMOD Inter- national Conference on Management of data, 2010, pp. 135–146. [20] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, I. Stoica et al., “Spark: Cluster",
        "start_idx": 10092,
        "end_idx": 10220
      },
      {
        "text": "Chowdhury, M. J. Franklin, S. Shenker, I. Stoica et al., “Spark: Cluster computing with working sets.” HotCloud, vol. 10, no. 10-10, p. 95, 2010. [21] M. Abadi, P . Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat, G. Irving, M. Isard et al., “Tensorﬂow: A system for large-scale machine learning,” in 12thfUSENIXg symposium on operating systems design and implementation (fOSDIg 16), 2016, pp. 265–283. [22] M. Isard, V . Prabhakaran, J. Currey, U. Wieder, K. Talwar, and A. Goldberg, “Quincy: fair scheduling for distributed computing clusters,” in Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles, 2009, pp. 261–276. [23] N. J. Yadwadkar and W. Choi, “Proactive straggler avoidance using machine learning,” White paper, University of Berkeley, 2012. [24] A. Badita,",
        "start_idx": 10208,
        "end_idx": 10336
      },
      {
        "text": "using machine learning,” White paper, University of Berkeley, 2012. [24] A. Badita, P . Parag, and V . Aggarwal, “Optimal server selection for straggler mitigation,” IEEE/ACM Transactions on Networking , vol. 28, no. 2, pp. 709–721, 2020. [25] J. Dean and L. A. Barroso, “The tail at scale,” Communications of the ACM, vol. 56, no. 2, pp. 74–80, 2013. [26] K. Ousterhout, R. Rasti, S. Ratnasamy, S. Shenker, and B.-G. Chun, “Making sense of performance in data analytics frameworks,” in12thfUSENIXg Symposium on Networked Systems Design and Implementation (fNSDIg 15), 2015, pp. 293–307. [27] P . Garraghan, X. Ouyang, R. Yang, D. McKee, and J. Xu, “Straggler root-cause and impact analysis for massive-scale virtualized cloud datacenters,” IEEE Transactions on Services Computing, vol. 12, no. 1, pp. 91–104, 2016. [28]",
        "start_idx": 10324,
        "end_idx": 10452
      },
      {
        "text": "Transactions on Services Computing, vol. 12, no. 1, pp. 91–104, 2016. [28] X. Ouyang, P . Garraghan, R. Yang, P . Townend, and J. Xu, “Re- ducing late-timing failure at scale: Straggler root-cause analysis in cloud datacenters,” in Fast Abstracts in the 46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks. DSN, 2016. [29] H. Zhou, Y. Li, H. Yang, J. Jia, and W. Li, “Bigroots: An effective approach for root-cause analysis of stragglers in big data system,” IEEE Access, vol. 6, pp. 41 966–41 977, 2018. [30] S. Lu, X. Wei, B. Rao, B. Tak, L. Wang, and L. Wang, “Ladra: Log-based abnormal task detection and root-cause analysis in big data processing with spark,” Future Generation Computer Systems, vol. 95, pp. 392–403, 2019. [31] Z. He,",
        "start_idx": 10440,
        "end_idx": 10568
      },
      {
        "text": "Future Generation Computer Systems, vol. 95, pp. 392–403, 2019. [31] Z. He, Y. He, F. Liu, and Y. Zhao, “Big data-oriented product infant failure intelligent root cause identiﬁcation using associated tree and fuzzy dea,” IEEE Access, vol. 7, pp. 34 687–34 698, 2019. [32] H. Du and S. Zhang, “Hawkeye: Adaptive straggler identiﬁcation on heterogeneous spark cluster with reinforcement learning,” IEEE Access, vol. 8, pp. 57 822–57 832, 2020. [33] J. P . Magalh ˜aes and L. M. Silva, “Root-cause analysis of perfor- mance anomalies in web-based applications,” in Proceedings of the 2011 ACM Symposium on Applied Computing, 2011, pp. 209–216. [34] R. Bitar, M. Wootters, and S. El Rouayheb, “Stochastic gradient coding for straggler mitigation in distributed learning,” IEEE Journal on Selected Areas in Information Theory, vol.",
        "start_idx": 10556,
        "end_idx": 10684
      },
      {
        "text": "in distributed learning,” IEEE Journal on Selected Areas in Information Theory, vol. 1, no. 1, pp. 277–291, 2020. [35] A. M. Chacko, J. S. Medicherla, and S. M. Kumar, “Anomaly detection in mapreduce using transformation provenance,” in Advances in Big Data and Cloud Computing. Springer, 2018, pp. 91–99. [36] N. Khoussainova, M. Balazinska, and D. Suciu, “Perfx- plain: debugging mapreduce job performance,” arXiv preprint arXiv:1203.6400, 2012. [37] M. Du, F. Li, G. Zheng, and V . Srikumar, “Deeplog: Anomaly detection and diagnosis from system logs through deep learning,” inProceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, 2017, pp. 1285–1298. [38] A. Agelastos, B. Allan, J. Brandt, P . Cassella, J. Enos, J. Fullop, A. Gentile, S. Monk, N. Naksinehaboon, J. Ogden et al., “The",
        "start_idx": 10672,
        "end_idx": 10800
      },
      {
        "text": "Fullop, A. Gentile, S. Monk, N. Naksinehaboon, J. Ogden et al., “The lightweight distributed metric service: a scalable infrastructure for continuous monitoring of large scale computing systems and applications,” in SC’14: Proceedings of the International Conferencefor High Performance Computing, Networking, Storage and Analysis. IEEE, 2014, pp. 154–165. [39] A. Netti, M. M ¨uller, C. Guillen, M. Ott, D. Tafani, G. Ozer, and M. Schulz, “Dcdb wintermute: Enabling online and holistic op- erational data analytics on hpc systems,” in Proceedings of the 29th International Symposium on High-Performance Parallel and Distributed Computing, 2020, pp. 101–112. [40] A. Borghesi, A. Bartolini, M. Lombardi, M. Milano, and L. Benini, “Anomaly detection using autoencoders in high performance computing systems,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligence, vol. 33, 2019, pp. 9428–9433.",
        "start_idx": 10788,
        "end_idx": 10916
      },
      {
        "text": "of the AAAI Conference on Artiﬁcial Intelligence, vol. 33, 2019, pp. 9428–9433. [41] Nagios. Accessed: 2020-07-15. [Online]. Available: https://www. nagios.org/ [42] Ganglia. Accessed: 2020-07-15. [Online]. Available: http://ganglia. info/ [43] Apache chukwa. Accessed: 2020-07-14. [Online]. Available: https://chukwa.apache.org/ [44] Dmon. Accessed: 2020-07-12. [Online]. Available: https://github. com/Open-Monitor/dmon [45] U. Demirbaga, A. Noor, Z. Wen, P . James, K. Mitra, and R. Ranjan, “Smartmonit: Real-time big data monitoring system,” in 2019 38th Symposium on Reliable Distributed Systems (SRDS). IEEE, 2019, pp. 357–3572. Umit Demirbaga (Member, IEEE) is a PhD stu- dent in the School of Computing, Newcastle University, UK. He received an MSc degree in Computer Science from Newcastle University, UK in 2017 and the BSc degree in Electronics and Computer Education from Marmara Univer- sity, Turkey in 2011. His research interests",
        "start_idx": 10904,
        "end_idx": 11032
      },
      {
        "text": "Computer Education from Marmara Univer- sity, Turkey in 2011. His research interests in- clude big data analytics, cloud computing and distributed systems. He was awarded Outstand- ing Performance Award with Best Team Project Award in his MSc in 2017. Zhenyu Wen (Member, IEEE) received MSc and PhD degrees in Computer Science from New- castle University, Newcastle upon Tyne, UK, in 2011 and 2016, respectively. He is currently a Postdoc Researcher with the School of Com- puting, Newcastle University, UK. His current re- search interests include IoT, crowd sources, AI system, and cloud computing. For his contribu- tions to the area of scalable data management for the Internet of Things. He was awarded the IEEE TCSC Award for Excellence in Scalable Computing (Early Career Researchers) in 2020. Ayman Noor",
        "start_idx": 11020,
        "end_idx": 11148
      },
      {
        "text": "for Excellence in Scalable Computing (Early Career Researchers) in 2020. Ayman Noor is a PhD student in Computer Science at Newcastle University, UK. His cur- rent research interests include cloud computing, monitoring, and machine learning. He earned a Master of Science in Computer and Information Science from Gannon University, PA, USA in 2013 and a Bachelor in Computer Science from the College of Computer Science and Engineer- ing from Taibah University, Madinah, SA in 2006. Karan Mitra is an Assistant Professor at Lule ˚a University of Technology, Sweden. He received his Dual-badge PhD from Monash University, Australia and Lule ˚a University of Technology in 2013. His research interests include cloud and mobile cloud computing, performance bench- marking of distributed systems, context-aware computing and QoE. He is a member",
        "start_idx": 11136,
        "end_idx": 11264
      },
      {
        "text": "marking of distributed systems, context-aware computing and QoE. He is a member of the IEEE and ACM. Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply. [Página 14] 0018-9340 (c) 2021 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TC.2021.3070639, IEEE Transactions on Computers 14 Khaled Alwasel has a BS and MS in informa- tion technology from Indiana University-Purdue University Indianapolis (2014) and Florida Inter- national University (2015), USA. He is currently working toward a PhD in the School of",
        "start_idx": 11252,
        "end_idx": 11380
      },
      {
        "text": "USA. He is currently working toward a PhD in the School of Com- puting Science at Newcastle University (UK). Khaled’s interests lie in the areas of software- deﬁned networking (SDN), big data, IoT, edge computing, and cloud computing Saurabh Garg is a lecturer at the University of Tasmania, Hobart, Tasmania. He has published more than 30 papers in highly cited journals and conferences with H-index 24. He has gained about three years of experience in industrial re- search while working at IBM Research Australia and India. His areas of interest are distributed computing, cloud computing, HPC, IoT, big data analytics, and education analytics. Albert Y. Zomaya is currently the Chair Pro- fessor of High Performance Computing & Net- working in the School of Computer Science, University of Sydney.",
        "start_idx": 11368,
        "end_idx": 11496
      },
      {
        "text": "& Net- working in the School of Computer Science, University of Sydney. He is also the Director of the Centre for Distributed and High Performance Computing which was established in late 2009. Professor Zomaya was an Australian Research Council Professorial Fellow during 2010-2014 and held the CISCO Systems Chair Professor of Internetworking during the period 2002–2007 and also was Head of School for 2006–2007. Rajiv Ranjan is a Full professor in Comput- ing Science at Newcastle University, UK. Before moving to Newcastle University, he was Julius Fellow (2013-2015), Senior Research Scientist and Project Leader in the Digital Productivity and Services Flagship of Commonwealth Scientiﬁc and Industrial Research Organization (CSIRO C Australian Government’s Premier Research Agency). Prior to that he was a Senior Research Associate (Lecturer level B) in",
        "start_idx": 11484,
        "end_idx": 11612
      },
      {
        "text": "to that he was a Senior Research Associate (Lecturer level B) in the School of Computer Science and Engineering, University of New South Wales (UNSW). Dr Ranjan has a PhD (2009) from the department of Computer Science and Software Engineering, the University of Melbourne. Authorized licensed use limited to: San Francisco State Univ. Downloaded on June 23,2021 at 08:53:22 UTC from IEEE Xplore. Restrictions apply.",
        "start_idx": 11600,
        "end_idx": 11665
      }
    ],
    "9ae75c26-80ce-494f-af6d-7db9503ae926": [
      {
        "text": "[Página 1] BigBench: TowardsanIndustryStandardBenchmarkfor Bi gDataAnalytics AhmadGhazal1,5,TilmannRabl2,6,MinqingHu1,5, Francois Raab4,8,MeikelPoess3,7,AlainCrolotte1,5,Hans-Arno Jacobsen2,9 1TeradataCorp.,2UniversityofToronto,3OracleCorp.,4InfoSizing,Inc. 5{ahmad.ghazal,minqing.hu,alain.crolotte}@teradata.com,6tilmann@msrg.utoronto.ca 7meikel.poess@oracle.com,8francois@sizing.com,9jacobsen@eecg.toronto.edu ABSTRACT There is a tremendous interest in big data by academia, industryanda large user base. Several commercial andopen source providers unleashed a variety of products to support big data storage and processing. As these products mature, there is a need to evaluate and compare the performance of these systems. In this paper, we present BigBench, an end-to-end big data benchmark proposal. The underlying business model of BigBench is a product retailer. The proposal covers a data model and synthetic data generator that addresses the variety, velocity and volume aspects of big data systems con- taining structured, semi-structured and unstructured data. The structured part of the BigBench data model is adopted from the TPC-DS benchmark, which",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "of the BigBench data model is adopted from the TPC-DS benchmark, which is enriched with semi- structured and unstructured data components. The semi- structured part captures registered and guest user clicks on the retailer’s website. The unstructured data captures product reviews submitted online. The data generator de- signed for BigBench provides scalable volumes of raw data based on a scale factor. The BigBench workload is designed around a set of queries against the data model. From a busi- ness prospective, the queries cover the diﬀerent categories of big data analytics proposed by McKinsey. From a technical prospective, the queries are designed to span three diﬀerent dimensions based on data sources, query processing types and analytic techniques. We illustrate the feasibility of BigBench by implement- ing it on the",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "We illustrate the feasibility of BigBench by implement- ing it on the Teradata Aster Database. The test includes generating and loading a 200 Gigabyte BigBench data set and testing the workload by executing the BigBench queries (written using Teradata Aster SQL-MR)and reporting their response times. CategoriesandSubjectDescriptors D.2.8[Software Engineering ]: Metrics— performance mea- sures Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not madeor distributed for proftor commercial advantage and that copies bearthisnoticeandthefullcitation onthefrstpage. Tocopyotherwise,to republish,topostonserversortoredistributetolists,requirespriorspecifc permissionand/or afee. SIGMOD’13, June22–27,2013,NewYork,NewYork,USA. Copyright 2013ACM978-1-4503-2037-5/13/06 ...$15.00.Keywords Benchmarking; big data; map reduce 1. INTRODUCTION Today’s data explosion, fueled by emerging applications, such as social networking, micro blogs, and the“crowd intel- ligence”capabilities of many sites,",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "as social networking, micro blogs, and the“crowd intel- ligence”capabilities of many sites, has led to the“big data” phenomenon. It is characterized by increasing volumes of data of disparate types (i.e., structured, semi-structuredand unstructured)from sources that generate new data at a high rate (e.g., click streams captured in web server logs). This wealth of data provides numerous new analytic and business intelligence opportunitieslike fraud detection, customer pro- ﬁling, and churn and customer loyalty analysis. Consequently, there is tremendous interest in academia and industry to address the challenges in storing, access- ing and analyzing this data. Several commercial and open source providers already unleashed a variety of products to support big data storage and processing. These tools are mostly parallel database management systems (e.g., Green- plum[4], Netezza’s TwinFin[9], Teradata[8], Oracle[6])",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "mostly parallel database management systems (e.g., Green- plum[4], Netezza’s TwinFin[9], Teradata[8], Oracle[6]) or MapReduce (MR) based systems (e.g., Hadoop [1], Cloud- era’s CDH [3], Hive[2] and many other systems like those in [15, 17, 24, 27]). As big data systems mature, the pressure to evaluate and compare performance and price performance of these sys- tems rises. However, to date there are no standard bench- marks available. This takes us back to the middle of the 1980’s, when the lack of standard database benchmarks led manydatabasemanagementsystemvendorstopracticewhat is now referred to as“benchmarketing”– a practice in which organizations makeperformanceclaims basedonself-deﬁned, highly biased benchmarks. The goal of publishing results fromsuchtailoredbenchmarkswastostatemarketingclaims, regardless of the absence of relevant and veriﬁable technical merit. In essence, these benchmarks were designed as for- gone conclusions to ﬁt",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "In essence, these benchmarks were designed as for- gone conclusions to ﬁt a pre-established marketing message. Similarly, vendors would create conﬁgurations, referred to as “benchmark specials”, that were speciﬁcally designed to maximize performance against a speciﬁc benchmark with limited beneﬁt to real-world applications. Towards the end of the 1980’s, as a response to this grow- ing practice, benchmark consortia such as the Transaction Processing Performance Council (TPC) and the Standard Performance Corporation (SPEC) were founded. Inﬂuenced byacademic databaseexpertsandwell-known industrylead- 1197 [Página 2] Unstructured Da ta Semi-Structured Data Structured Data Sales Customer Item Marketprice Web Page Web Log Reviews Adapted TP C-DS BigBench Specific Figure 1: Big Data Benchmark Data Model er s, industry standard benchmarks such as TPC-A, TPC-C and TPC-D were engineered and rules around publishing results",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "as TPC-A, TPC-C and TPC-D were engineered and rules around publishing results were agreed upon. Recently a few eﬀorts in the area of big data benchmarks emerged, such as YCSB[16], PigMix[7], GridMix [5] and GraySort [20]. These eﬀorts are island solutions and not policed by any industry consortia. While some are focused on one or a subset of components and tasks typical for big data systems, others are based on speciﬁc map-reduce-style systems. Webelieveanindustrystandardbigdatabenchmarkmust be an end-to-end benchmark covering all major characteris- tics in the lifecycle of a big data system including the three Vs described by Douglas Laney[21]: (i) volume(larger data set sizes), (ii) velocity (higher data arrival rates, such as click streams) and (iii) variety(increased data type dispar- ity, such as structured data from relational tables,",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "variety(increased data type dispar- ity, such as structured data from relational tables, semi- structured data from key-value web clicks and un-structured data from social media content). In this paper, we present our proposal for an end-to-end big data benchmark. After a presentation of initial ideas for the benchmark at the ﬁrst Workshop on Big Data Bench- marking1a group formed that collaborated on building the speciﬁcation. We call it “BigBench”. It is based on a ﬁcti- tious retailer who sells products to customers via physical and online stores. The proposal covers a data model, syn- thetic data generator and workload description. The work- load queries are speciﬁed in English, since no clear standard for big data systems has yet emerged. We also suggest di- rections for big data metrics",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "has yet emerged. We also suggest di- rections for big data metrics speciﬁc to data loading and workload execution. Furthermore, the feasibility of the pro- posal is validated by implementing it on the Teradata Aster DBMS (TAD). This experiment involves generating 200 Gi- gabyte of raw data and loading it into TAD. The English like workload queries are implemented using TAD’s SQL- MR syntax and executed as a single stream of queries. Theﬁrstmajor componentofBigBenchisthespeciﬁcation of a data model that focuses on volume, variety and velocity. The variety property of our model is illustrated in Figure 1. The structured part of BigBench is adapted from the TPC-DS data model, which also depicts a product retailer [23]. We borrowed the store and online sales portion from that model and added",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "borrowed the store and online sales portion from that model and added a table for prices from the retailer’s competitors. The structured part is enriched with semi-structured and 1WBDB, May 2012, San Jose – http://clds.ucsd.edu/ wbdb2012un-structured data shown in the lower and right hand side of Figure 1. The semi-structured part is composed by clicks made by customers and guest users visiting the retailer’s web site. Our design assumes the semi-structured data to be in a key-value format similar to Apache’s web server log format. The un-structured data in our model is covered by product reviews that can be submitted by guest users or actual customers. We also provide the design and implementation of a data generator for the proposed BigBench data model. Our data generator is based",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "generator for the proposed BigBench data model. Our data generator is based on an extension of PDGF [29]. PDGF is a parallel data generator that is capable of producing large amounts data for an arbitrary schema. The existing PDGF can be used to generate the structured part of the BigBench model. However, it is not capable of producing neither the semi-structured web clicks nor the unstructured product re- viewstext. Partofourcontributioninthispaperistoextend PDGF to coverthesemi-structuredandun-structuredparts. We enhanced PDGF to produce a key-value data set for a ﬁxed set of required and optional keys. This is suﬃcient to generate the web logs part of BigBench. The main challenge in generating product reviews is to produceun-structuredtext. Wedevelopedandimplemented an algorithm that produces synthetic text based on some sample inputtext. The algorithm usesa MarkovChain",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "produces synthetic text based on some sample inputtext. The algorithm usesa MarkovChain tech- nique that extracts key words and builds a dictionary based on these key words. The new algorithm, called TextGen, is applied or our retailer model by using some real product re- views from amazon.com for the initial sample data. PDGF interacts with TextGen through an API sending product category as input and getting a product review text for that category. The volumedimension of ourmodel is far simpler thanthe variety discussion and previous data generators had a good handle on that. PDGF handles the volume well since it can scale the size of the data based on a scale factor. It also runs eﬃciently for large scale factors since it runs in parallel and can leverage",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "for large scale factors since it runs in parallel and can leverage large systems dedicated for the benchmark. We also address big data velocity by establishing a periodic refresh scheme that constantly adds data to the diﬀerent areas of the data model. The second major component of BigBench is the speci- ﬁcation of workload queries applied on the BigBench data model. In terms of business questions, we found that the big data retail analytics by McKinsey [22] serves our pur- pose given that BigBench is about retail. In [22] ﬁve major areas, or business levers, of big data analytics are identiﬁed: marketing, merchandising, operations, supplychainandnew business models. In addition to the big data retail business levers above, we looked at three diﬀerent technical dimensions the Big- Bench queries",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "above, we looked at three diﬀerent technical dimensions the Big- Bench queries should span. The ﬁrst technical dimension is about the type of data used in queries. This implies mak- ing sure that structured types, semi-structured types, un- structured types and their combinations are each covered in the queries. The second technical dimension covers the two common paradigms of declarative processing (SQL and similar constructs like HQL) and procedural MR processing. To that end, some queries are best suited to be declarative, others to be procedural and others to be a mix of both. The thirdtechnical dimensionis aboutthediﬀerentalgorithms of analytic processing as described by the Apache MAHOUT system. Examples of these algorithms are classiﬁcations, 1198 [Página 3] pattern matching, clustering, regression, dimensional redu c- tion, etc. In summary,",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "3] pattern matching, clustering, regression, dimensional redu c- tion, etc. In summary, our key contributions are as follows: 1. Wepresentthe ﬁrstend-to-endbenchmarkfor big data analytics while previous work focused on few selected types of data or processing. BigBench implements the complete use-case of a realistic retail business. 2. We specify 30 queries that cover all important aspects of big data analytics. The queries are speciﬁed in En- glish as well as TAD’s SQL-MR syntax. 3. We develop and implement a novel technique for pro- ducing un-structured text data and integrate it with a traditional structured data generator. 4. We conduct a proof of concept implementation and evaluation of BigBench by executing the benchmark on the Teradata Aster DBMS. The remainder of this paper is organized as follows. Sec- tion",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "DBMS. The remainder of this paper is organized as follows. Sec- tion 2 covers previous work related to big data benchmark- ing. Section 3 gives a detailed description of the BigBench benchmark. The data model and data generation are de- scribed in detail in Sections 3.1 and 3.2. We describe the workload queries in Section 3.3 and the benchmark metrics in Section 3.4. We present our proof of concept implemen- tation of BigBench using TAD in Section 4 including results involving 200 Gigabyte database. Finally, Section 5 summa- rizes the paper and suggests future directions. 2. RELATEDWORK The requirement for well deﬁned benchmarks that mea- suretheperformanceofDBMSdealingwithverylarge amounts of data emerged when the ﬁrst generation of commercial systems appeared in the 1980’s by Teradata Corporation and other more traditional",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "systems appeared in the 1980’s by Teradata Corporation and other more traditional DBMS vendors, who followed. Driven by vendor’s needs to compare commercial systems, the Transaction Processing Performance Council developed a series of data warehouse end-to-end benchmarks starting with TPC-D in the beginning of the 90’s and TPC-H and TPC-R in the dawn of 2000 (all speciﬁcations available from the TPC website2). These benchmarks, restricted to ter- abyte data sizes, emphasized single and multi-user perfor- mance of complex SQL query processing capabilities with some updates on an enterprise data warehouse. Even ear- lier, academia started developing micro benchmarks such as the Wisconsin benchmark, the OO7 [12] and BUCKY [13] benchmarks for object-oriented DBMSs, XMark [31] and EXRT [14] benchmarks for XML-related DBMS technolo- gies. As data volumes grew",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "EXRT [14] benchmarks for XML-related DBMS technolo- gies. As data volumes grew from megabytes of data and simple data models (small number of tables with few relationships) over time to petabytesandcomplex data models (large num- ber of tables with many complex relationships) the TPC responded with the development of its next generation deci- sion support benchmark, TPC-DS [23], in the early 2000’s. Still based on the SQL programming language it contains many big data elements, such as very large data and system sizes. Although the current limit is 100 terabyte, the data generator and schema can be extended to petabytes. It also 2TPC -http://www.tpc.orgcontains very complex analytical queries using sophisticated SQL structures and a concurrent update model. In parallel, academia as well as emerging big data com- panies",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "model. In parallel, academia as well as emerging big data com- panies have started deﬁning the next generation big data benchmarks, which are mostly component and micro bench- marks. Yahoo! developed its cloud serving benchmark, YCSB, to evaluate NoSQL data stores [16]. It is a ﬂexi- ble multiuser benchmark with two tiers, a performance tier (testing latency) and a scalability tier. In the original paper, three workloads were runagainst four diﬀerent data stores: HBase, Cassandra, PNUTs, and MySQL. Other evaluations followed that extended the scope of YCSB [30, 25]. The CALDA eﬀort [26] deﬁned a micro-benchmark for big data analytics based on Google’s MapReduce paper and com- pared Hadoop with two RDBMS systems, one that is row and one that is column organized. Another widely used benchmark is",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "row and one that is column organized. Another widely used benchmark is the TeraSort or GraySort benchmark [20], which can be considered a micro benchmark that sorts a large number of 100-byte records doing considerable amount of computation, networking, and storage I/O. Other bench- marks are the GridMix [5] and PigMix [7]. TPC-DS [23, 28] is TPC’s latest decision support bench- mark. It covers the major three disciplines in the life-cycle of a relational decision support benchmark, namely (i) load- ing the initial database (ii) executing queries in both single- and multi-user modes (iii) refreshing the database. TPC-DS handles some aspects of big data like volume and some as- pects of velocity. Still, it lacks key components of big data like semi-structured and unstructured data and their asso-",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "components of big data like semi-structured and unstructured data and their asso- ciated analytics. In summary, previous benchmarks described in this sec- tion are mostly micro and component benchmarks. Others like TPC-DS lack key big data characteristics. This brings a need for an end-to-end benchmark for big data processing. 3. BIGDATABENCHMARK This section covers the major parts of the BigBench speci- ﬁcation. Due to space restrictions, not all details can be pre- sented here. Additional details can be found in an extended version of this paper, to be made available at publication time. 3.1 DataModel The three cornerstone aspects of big data systems are vol- ume,variety,velocity. Big data systems need to be able to deal with large volumes of data, sometimes in the mul- tiple petabyte range. We",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "large volumes of data, sometimes in the mul- tiple petabyte range. We deal with the volume aspect in the following section about data scaling. Variety refers to the ability to deal with diﬀerently organized data, from un- structured to semi-structured and structured data. The fol- lowing section about variety lays out a structure that cov- ers all the types of data integrated in one model. Velocity refers to the ability of a big data system to stay current through periodic refreshes, commonly referred to as extrac- tion,transformation andload(ETL). A big data system is not a one-time snapshot of a business operations database nor is it a database where OLTP applications are running concurrently. Hence, staying current with the operational side is a very important aspect of analytical systems,",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "with the operational side is a very important aspect of analytical systems, and even more so in the context of a big data system. In the following subsection, we develop the data model showing how the 3 Vs in big data are addressed in Big- Bench. We show how volumeis addressed by using scale 1199 [Página 4] 127.0.0.1 - - [Jun/23/2003:05:59:23 +0200] \"G ET/page33.html?wcs_click_date=2452814 &wcs_click_ time=21563&wcs_user_id=95789 &wcs_web_page_sk=32&wcs_item_sk=28 HTTP/1.1\" 200 2256 \"http://www.someurl.org\" \"Mozilla/5.0\" Figure 2: Example of a web log entry factors in the data generators to scale data up to petabytes of data, how varietyis addressed through the usage of data from many sources and how velocityis achieved by periodic refreshes of the data repository. 3.1.1 Variety The general benchmark data model is summarized in Fig- ure 1,",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "Variety The general benchmark data model is summarized in Fig- ure 1, which shows the three data components of the bench- mark namely structured data, semi-structured data and un- structureddatatogetherwiththerelationshipsbetweenthem. The structured component of BigBench is adapted from the TPC-DS benchmarkrecently publishedbytheTPC [10]. A description of this benchmark can be found in [23, 28]. BigBench is however not a simple extension of TPC-DS. Instead, BigBench focuses chieﬂyon the analytics associated with semi-structured and unstructured data. With a few exceptions most of the tables contained in TPC-DS are used by BigBench; the main focus being store and web sales, which only contain structured data. These tables cover data relating to the purchases made in stores and over the web, but also related tables such as itemde- scribing the items",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "the web, but also related tables such as itemde- scribing the items oﬀered by the retailer, customer and its ancillary tables containing all relevant client data, webpage an dwebsitedescribing pages and web sites used by on- l ine clients and all associated dimension tables. To better support our functional design, we also added a new table calleditemmarketprices to the structured data. It contains competitor names and prices for each item so that price comparisons performed by online users who are interested in particular items could also be captured. The semi-structured data focuses on click-streams, con- tained in web log ﬁles. While some of the clicks result in sales thereby necessitating a link to structured area tables containing online sales, item, web pages, customer and asso- ciated dimensions, the",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "containing online sales, item, web pages, customer and asso- ciated dimensions, the large majority of these clicks are as- sociated with browsing activity not resulting in sales. These clicks focus on items and are associated with registered users or guests.The format retained for the clicks is that of Apache logs. A typical entry of such a log associated with a regis- tered user could look like the example in Figure 2. Web logs can be processed either directly at run time (late binding) or parsed and stored into a structured table/ﬁle. Since all values are surrogate keys referring to the struc- tured schema, the above record once processed could look like Table 1. The unstructured data resembles written text associated with product reviews of items oﬀered by the",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "resembles written text associated with product reviews of items oﬀered by the retailer. Such reviews could be from several sources, namely guest users, registered users withapurchaseandregistered userswithout a purchase. This implies a relationship between reviews and structred data like customer, sales and item tables. The reviews and its relationship with the structred data can be captured bya table/ﬁle. The table/ﬁle capturesthe primary keys of the referenced tables. The review itself is containedin a large variable character ﬁeld containing free form text, the rating score and the date and time of the review are also contained in the table/ﬁle. 3.1.2 Volume The size of the structured area is based on the size of the tables involved, using a well-understood and known quan- tity similar to the scale factor in",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "a well-understood and known quan- tity similar to the scale factor in TPC-DS. The size of the semi-structured and unstructured areas are also based on this scale factor. Consequently, the size of the complete BigBench data set is based on a single scale factor and is predictable and deterministic at any volume. For the item marketprice table, it is assumed that an av- er age of 5 competitor prices are stored for each item. Thus, the sizing of item marketprice is |it em|×5. The size of web logs dependson the number of clicks made by buyers (making entries in web sales) and visitors who do no t endupbuying. Each rowin web sales represents a single li ne item, thus the number of clicks per sale is comprised of",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "ne item, thus the number of clicks per sale is comprised of the number of clicks per item and the number of clicks to make a sale (i.e. login, go to cart, checkout). The number of clicks for buyers cbcan be speciﬁed with the following equation: cb=|websales|×( pages per item+pages per buy items per sale) As sumingbothpages per itemandpages per buytobe equal to 4 on average and setting the avergae value of items per sale to be 12 (from TPC-DS), the value of cbis simpliﬁed to cb=|websales|×4.3 3 We assume that 80% of surfers are visitors (20% buyers) which makes the ratio of visitors to buyers to be 4:1. We also assume that on average visitors browse items the same way as buyers. Based on these assumptions, the",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "browse items the same way as buyers. Based on these assumptions, the formula for the number of clicks for visitors cvis: cv= (|websales|×p ages per item) ×visitor ratio cv=|websales|×1 6 Overall, the size of the web log is cb+cvand can be ex- pressedasamultipleofthesizeofweb sales. Itisweb sales× 20.3 3. The web sales table scales linearly with the scale fac- to r, the size for scale factor 1 is 720K, thus the number of entries for the web log at scale factor 1 is 14,600K. Given to the log format, the raw ﬁle size is 3 gigabyte. For the review sizing, a similar approach is chosen. Three sources for reviews are considered: anonymous reviews, ran- dom item reviews by registered users (customers), and re- views based on sales. The number",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "by registered users (customers), and re- views based on sales. The number of anonymous reviews is related to the number of items, an average of 5 anonymous Table 1: Representation of a web log entry Field Name Value wcsclicksk 996146 wcsclickdatesk2452814 wcsclicktimesk21563 wcsitemsk 28 wcswebpagesk 32 wcsusersk 95789 1200 [Página 5] reviews per item is assumed. The number of reviews by re gistered users is dependent on the number of users in the system. Because not all users are actually writing reviews, an average of one review per 5 users is assumed. Finally, a certain amount of the sales will directly lead to a review. This amount is set to 15%. The number of reviews can be computed by the following formula: |reviews|=|items|×5+|customers |×0.2+|websales|×0.1 5 3.1.3 Velocity Velocity,",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "be computed by the following formula: |reviews|=|items|×5+|customers |×0.2+|websales|×0.1 5 3.1.3 Velocity Velocity, i.e. a periodic data refresh process, is an inte- gral part of the life cycle of a big data system. A production data refresh process consists of three steps: (i) data extract (ii)datatransformation, and(iii)dataload. Inaproduction system environment, the data extraction step may consist of numerousseparateextractoperations, executedagainst mul- tiple operational systems and ancillary data sources. As it is unlikely that the full list of these operational data sources resides on the system running the big data application, it is doubtful the measurement of the data extraction perfor- mance would result in a metric appropriate or meaningful to the scope of this benchmark. In light of this, the data extract step is assumed and represented in the benchmark",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "this, the data extract step is assumed and represented in the benchmark in the form of generated ﬁles. There are two aspects to discuss in a periodic refresh model for the tables in BigBench: (i) amount of data to include in the refresh process and (ii) the time interval at which the refresh occurs. Both aspects apply to the struc- tured(websales channeland itemmarketprice ta bles), semi- structured ( clickstream) and un-structured data ( p roduct review). W e implement BigBench’s periodic refresh process based on the well studied methodology for data maintenance of TPC-DS. It deﬁnes the insertion of new data and the dele- tion of old data from all fact tables as well as insert and updated data of dimensions. Dimensions are divided into three sets, history",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "and updated data of dimensions. Dimensions are divided into three sets, history keeping, non-history keeping and static dimensions. Static dimensions, such as date and time are not updated. History keeping dimensions never overwrite any data, but they keep a history of all former changes. Non-History keeping dimensions resemble almost a one-to- one copy of the table in the operational system of the busi- ness, i.e. they update existing data. Both, history keeping and non-history keeping dimensions, accept new data and never delete any old data. According to the above deﬁni- tions,clickstreamandp roductrevieware fact tables and i temmarketprice is a history keeping table. Pseudo code for the insertion, deletion of fact table data as well as insert and update operations for the dimension tables can be found in [23]",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "and update operations for the dimension tables can be found in [23] and the oﬃcial TPC-DS speciﬁcation3. One of the fundamental aspects of the above methodol- ogy is the concurrent execution of the refresh process with the query workload. Queries must be interspersed with in- sert, delete and update operations. In BigBench we run N concurrent query streams containing queries against the structured, semi-structuredandunstructuredportionsofthe schema. The numberof refresh processes executedis a linear function of the number of query streams, S. In real systems, data against the diﬀerent data portions is updated with dif- ferent frequencies. Hence we deﬁne a vector V with the following three separate data refresh velocities for each of 3TPC –http://www.tpc.org/g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g1005/g1007/g910 /g94/g3/g395/g437/g286/g396/g349/g286/g400 /g89/g282/g258/g410/g258/g410/g455/g393/g286/g400/g882/g1010/g3/g395/g437/g286/g396/g349/g286/g400 /g89/g282/g258/g410/g258/g410/g455/g393/g286/g400/g882/g1010/g3/g395/g437/g286/g396/g349/g286/g400 /g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g1006 /g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g94/g882/g1005 /g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g94/g857 /g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g94 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g1005 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g1006 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g1007 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g94/g876/g1006 Figure 3: Scheduling",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "/g89/g282/g258/g410/g258/g410/g455/g393/g286/g400/g882/g1010/g3/g395/g437/g286/g396/g349/g286/g400 /g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g1006 /g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g94/g882/g1005 /g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g94/g857 /g89/g437/g286/g396/g349/g286/g400/g3/g94/g410/g396/g286/g258/g373/g3/g94 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g1005 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g1006 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g1007 /g90/g286/g296/g396/g286/g400/g346/g3/g87/g396/g381/g272/g286/g400/g400/g3/g94/g876/g1006 Figure 3: Scheduling of refresh processes based on ex ecuted queries per data type the diﬀerent data portions, V= (Vstructured ,Vsemistructured andVunstructured ). We suggest the following values for V, which are subject to change as we run more experiments. The structured data being the least frequently updated por- tion of the schema has a velocity of Vstructured = 1, i.e. S refresh process. The unstructured data gets a velocity of Vunstructured = 2∗Vstructured , i.e. 2∗Srefresh process, and the semi-structured data being the most frequently updated portion gets a velocity of Vsemistructured = 2∗Vunstructured , i.e. 4∗Srefresh process. The total number of refresh pro- cesses is 7 ∗S. During a BigBench run the following two requirements guarantee that",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "7 ∗S. During a BigBench run the following two requirements guarantee that the queries are interspersed with the queries (Sis the total number of query streams and Qdatatypeis the total number of queries against the three portions of the schema ): 1. The Nthrefresh set canonly startafter [((3 ∗S)+((N− 1)∗2∗Qdatatype)] queries have completed (aggregated over all streams), and 2. The [(3 ∗S)+(N∗(Qdatatype−6))+1]th query (ag- gregated over all streams) can only start after the Nth refresh set has completed. This means that at least (3 ∗S) queries must complete be- fore the ﬁrst refresh set can start and at least Qdatatype−6 additional queries must complete before the second refresh setcan start. Ingeneral atleast (3 ∗S)+((N−1)∗Qdatatype− 6)) queries must complete before the Nth refresh set can start. Figure 3 shows",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "must complete before the Nth refresh set can start. Figure 3 shows how the refresh processes are sched- uled depending on the number of executed queries. All three type of data tables follow the well-understood scale factors of TPC-DS as outlined in the previous section. That is the amount of data to be inserted in each ETL op- eration is a percentage of the initial load, e.g. 0.1%. 3.2 DataGeneration Our data generation design is based on an existing tech- nology called Parallel Data Generation Framework (PDGF). PDGF was designed to address structured data. Part of the work presented in this paper is to extend the framework to produce the semi-structured and unstructured data. The semi-structured data is generated in form of weblogs and the unstructured data in",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "data is generated in form of weblogs and the unstructured data in form of item reviews. In the following section, we give an overview of PDGF and then elaborate on its extensions for semi-structured and unstructured data. 1201 [Página 6] 3.2.1 PDGF PD GF is a generic, parallel data generator which was de- veloped at the Universityof Passau [18, 29]. PDGF is imple- mented in Java and fully platform independent. Currently, PDGF is used to implement the default data generator for the TPC’s new ETL benchmark TPC-DI [33]. PDGF’s gen- eration approachexploits theinherentparallelism of xorshift random number generators by using a novel seeding strat- egy. The seeding strategy hierarchically assigns seeds to the tables, columns and rows of a database schema and thus makes it possible to generate",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "rows of a database schema and thus makes it possible to generate data completely in parallel as well as re-calculate any value in the database without ac- cessing the original data. Originally, PDGF is designed to generate relational data. The data is speciﬁed in two XML documents, the schema conﬁgurationandthegenerationconﬁguration. Asthename suggests, the schema conﬁguration speciﬁes the data simi- lar to the deﬁnition of a relational schema. The generation conﬁguration makes it possible to specify additional post- processing of the generation. The post-processing includes formatting data, merging and splitting tables, as well as ad- vanced procedures by providing a script like programming interface using the Javassist4library. PDGF can be used as isto generate the structured parts of the data model. As discussed above, the current Big- Bench schema",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "of the data model. As discussed above, the current Big- Bench schema comprises three additional entities on top of the TPC-DS schema: the Item marketprice table, an ap ache-style web server log, and the online reviews. The Itemmarketprice table is a regular table and can easily be ge nerated using PDGF. In Listing 1, an excerpt of the spec- iﬁcation of Item marketprice can be seen. The table is de- ﬁn ed in a way similar to the SQL deﬁnition language, with an additional speciﬁcation of the generation rules. The sur- rogate key (imp sk) is, for example, generated with a ID ge nerator. PDGF supports more complex generation spec- iﬁcations as can be seen in the case of the imp competitor ﬁe ld, this ﬁeld is generated",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "the case of the imp competitor ﬁe ld, this ﬁeld is generated as a random string that is null with a probability of 0.025%. <property name= \"Item_marketprice\" type=\"double\" > ${ item}*${avg_competitors_per_item} </property> <table name= \"Item_marketprice\" > <s ize>${Item_marketprice}</size> <field name= \"imp_sk\" size=\"\"type=\"NUMERIC\" > <g en_IdGenerator/> </field> [..] <field name= \"imp_competitor\" size=\"20\" type=\"VARCHAR\" > <g en_NullGenerator> <probability>0.00025</probability> <gen_RandomAString> <size>20</size> </gen_RandomAString> </gen_NullGenerator> </field> [..] </table> Listing 1: Excerpt of the Schema Deﬁnition for It emmarketprice Th e web server log has a special formatting, an example 4Javassist project homepage - http://www.csg.is.titech. ac.jp/~chiba/javassist/is shown in Figure 2. To generate a realistic web log, we speciﬁed a table in PDGF that has all required columns for a web log entry and formated it using PDGF’s scripting ca- pabilities. Below in Listing 2",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "and formated it using PDGF’s scripting ca- pabilities. Below in Listing 2 an excerpt of the deﬁnition of the web server log table can be seen. The excerpt shows the deﬁnition of the size of the web log, and the table deﬁni- tion with two attributes. The sizing is computed according to the formula in Section 3.1.2, the speciﬁcation of the pa- rameters of the formula is omitted. For the table itself only two attributes are shown: a surrogate key wcsclickskand t he reference to the web page wcswebpagesk. This ref- e rence is null with a probability of 0 .00025. In Listing 3, the formatting code for the web log can be seen. As shown in the listing, some of the values in the log are static.",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "in the listing, some of the values in the log are static. For example the request IP address is always “127.0.0.1” while other values such as the time and date are extracted from the table. <property name= \"Web_clickstreams\" type=\"double\" > ($ {sales} * (${pages_per_item} + (${pages_to_buy} / ${items_per_cart}))) + (${sales} * ${buy_ratio} * ${pages_per_item}) </property> <table name= \"Web_clickstreams\" > <s ize>${Web_clickstreams}</size> <field name= \"wcs_click_sk\" size=\"\"type=\"NUMERIC\" > <g en_IdGenerator/> </field> [..] <field name= \"wcs_web_page_sk\" size=\"\" type=\"NUMERIC\" > <g en_NullGenerator> <probability >0.00025</probability> <gen_LongGenerator> <min>1</min> <max>${web_page}</max> </gen_LongGenerator> </gen_NullGenerator> </field> [..] </table> Listing 2: Excerpt of the web log speciﬁcation <output name= \"CompiledTemplateOutput\" > <t emplate><!-- String nl = pdgf.util.Constants.DEFAULT_LINESEPARATOR; buffer.append( \"127.0.0.1 - - [\" + fields[4] + \":\"+ fi elds[5] + \" +0200] \" ); bu ffer.append( \"\\\"GET /page\" +fields[7]+ \".html?\"",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "elds[5] + \" +0200] \" ); bu ffer.append( \"\\\"GET /page\" +fields[7]+ \".html?\" ); [. .] buffer.append( \" HTTP/1.1\\\" 200 0 - \\\"\" +fields[1]); bu ffer.append( \"\\\" \\\"Mozilla/5.0 \\\"\" + nl); -- ></template> </output> Listing 3: Excerpt of the formatting instructions for th e web log The review generator was built as a standalone program, it is conﬁguredusingan XMLdocumentthatspeciﬁes all pa- rameters for each review. In order to generate reviews that correlate with the structured data, e.g. the items that are reviewed exist in the database and the registered reviewers are actual customers, PDGF is used to generate the XML conﬁguration for the review generator. This is also done using the scripting interface. Again, a table is speciﬁed in PDGF that contains all required information and the rows 1202",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "speciﬁed in PDGF that contains all required information and the rows 1202 [Página 7] /g75/g296/g296/g367/g349/g374/g286/g3/g87/g396/g286/g393/g396/g381/g272/g286/g400/g400/g349/g374/g336/g18/g258/g410/g286/g336/g381/g396/g349/g460/g258/g410/g349/g381/g374/g90/g286/g258/g367/g3 /g90/g286 /g448/g349/g286/g449/g400 /g100/g381/g364/g286/g374/g349/g460/g258/g410/g349/g381/g374 /g39/g286/g374/g286/g396/g258/g367/g349/g460/g258/g410/g349/g381/g374/g68/g258/g396/g364/g381/g448/g3 /g18/g346 /g258/g349/g374/g3/g47/g374/g393/g437/g410/g87/g396/g381/g282/g437/g272/g410 /g18/g437/g400/g410/g381/g373/g349/g460/g258/g410/g349/g381/g374 /g100/g286/g454/g410/g3/g39/g286/g374/g286/g396/g258/g410/g349/g381/g374 /g87/g258/g396/g258/g373/g286/g410/g286/g396/g3 /g39/g286/g374/g286/g396/g258/g410/g349/g381/g374 /g894/g87/g24/g39/g38/g895 /g75/g374/g367/g349/g374/g286/g3/g24/g258/g410/g258/g3/g39/g286/g374/g286/g396/g258/g410/g349/g381/g374/g39/g286/g374/g286/g396/g258/g410/g286/g282/g3 /g90/g286 /g448/g349/g286/g449/g400 /g87/g396/g381/g282/g437/g272/g410 /g18/g437/g400/g410/g381/g373/g349/g460/g258/g410/g349/g381/g374 /g100/g286/g454/g410/g3/g39/g286/g374/g286/g396/g258/g410/g349/g381/g374 /g87/g258/g396/g258/g373/g286/g410/g286/g396/g3 /g39/g286/g374/g286/g396/g258/g410/g349/g381/g374 /g894/g87/g24/g39/g38/g895 Figure 4: Review Generation Process ar e output as XML document fragments. Details on the review generation are given in the section below. 3.2.2 ReviewGeneration Reviews buildthe unstructuredpart of our data set. They are an integral part of the data model and have to be pro- cessed. Thus they need to contain realistic and useful in- formation. As discussed below in the workload section, the benchmark contains queries that require sentiment analysis and similar text analysis on the reviews. We have developed a novel approach for generating the reviews that is based on",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "developed a novel approach for generating the reviews that is based on text generation using Markov chains [11]. In Figure 4 an overview of the review generation process can be seen. The process can be separated in two phases. An oﬄine phase, that processes real reviews and generates a knowledge base for the review generation and an online phase that generates reviews based on the knowledge base. The oﬄine process starts with collecting real reviews from online resources. For our proof of concept, we collected a set of 150 reviews per category from an online retailer. In the ﬁrst processing step the reviews are categorized by prod- uct type. For the categorization, we use an intersection of product categories from the online retailer and the class and category",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "of product categories from the online retailer and the class and category hierarchy in the item dimension in the TPC-DS schema. The online reviews have a rating which is used to create an orthogonal categorization for the review senti- ment. The crawler also collects statistical information about the number of reviews per item, the length of reviews and the distribution of ratings. Since reviews are tailored to a speciﬁc product, they are tokenized and the review subject is generalized. For now this process only includes ﬁltering out product names and replacing them with generic iden- tiﬁers. Although this approach removes the product name from reviews, they are still highly domain speciﬁc. Since the generalization is an oﬄine process that has to be done only once, the computation can",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "oﬄine process that has to be done only once, the computation can be more involved. In future versions of the generator more sophisticated approaches will be implemented. Using the tokenized and generalized reviews, the transi- tion probabilities between words in the text are analyzed and stored. These probabilities are know as Markov chains. Anorder-1 chain will only store the frequency of a word appearing after another one. So for each word all possible successors and the frequency in which they appear is stored. To get more realistic text, more than one predecessor can be taken into account for generating the text. In practice, weuse order-2 to order-4 text to achieve high quality reviews. An excerpt of an order-2 generated text can be seen below. My review title says",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "an order-2 generated text can be seen below. My review title says it all. I wanted to like it, because it’s a good subject. Didn’t ﬂow well, some times confusing. This book is not a self help book, this may be worth reading for that alone. The review generator was implemented as a standalone program that is conﬁgured by an XML document. The con- ﬁguration contains one <review> element for each review that should be generated. For each review the item ID, category, user name, transaction ID, date, time, rating and word count are speciﬁed. This information is generated by PDGF and later fed to the review generator. This way, it is assured that the review data is consistent with the data generated by the other generators. In",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "data is consistent with the data generated by the other generators. In future revisions of the benchmark, all parts of the data generation will be imple- mented within PDGF. 3.3 Workload In this section, we present the proposed workload for Big- Bench. In addition to the queries described below, we con- sidertheinitial databasepopulationaspartoftheworkload. We refer to this initial phase as transformation ingest (TI). TI covers the ETL process, including any steps needed to prepare the data before querying (e.g., indexing or statistics collection). The main part of the workload is the set of queries to be executedagainst thedata model. These queries are designed along one business dimension and three technical dimen- sions, aiming to cover diﬀerent business cases and technical perspectives. Our business cases are based on Mckinsey’s",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "business cases and technical perspectives. Our business cases are based on Mckinsey’s re- port on big data [22]. From a technical perspective, we focus on data sources, processing types and analytical techniques. Following the approach used for most TPC benchmarks, The BigBench queries are deﬁned in terms of business ques- tions and expressed in plain English. We created a total of 30 business questions for the BigBench workload. Note that, dueto the limited space, we do not presentall of the 30 queries in this paper. The complete set of BigBench queries can be found in an extended version of this paper. In addi- tion to the English deﬁnition of the queries, we also present them using Teradata Aster’s SQL-MR syntax [19, 32]. The remainder of this section is",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "Teradata Aster’s SQL-MR syntax [19, 32]. The remainder of this section is organized as follows: ﬁrst, we discuss the business cases with query examples. We then present the three technical dimensions and show the distri- bution of queries along each of the dimensions. 3.3.1 BusinessCases The McKinsey report gives a comprehensive view of big data’s transformative potentials for retail business. From the report, we identiﬁed nine big data retail levers that ﬁt in theBigBenchworkload. Furthermore, weaddedreturnanal- ysis under the category Operations which makes a total of ten levers. (Returns are often connected with frauds, which makes it important from a business perspective.) These ten levers fall into the following ﬁve main categories: Marketing, Merchandising, Operations, SupplyChainandNewBusiness Models. The organization of the ten levers into these ﬁve categories is",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "Models. The organization of the ten levers into these ﬁve categories is shown in Table 2. In the following, we present the ten retail levers and we illustrate each lever with a sample query. 1203 [Página 8] Table 2: Levers Within Business Categories Business category Big data lever Marketing -Cross-selling -Customer micro-segmentation -Sentiment analysis -Enhancing multichannel consumer experience Merchandising -Assortment optimization -Pricing optimization Operations -Performance transparency -Return analysis Supply chain -Inventory management New business models -Price comparison 1.Cross-selling: I n this lever, we include queries in- volving market basket analysis and collaborative ﬁl- tering based recommendations. For example, Query 1 computes the probability of browsing products from a category after customers viewed items from another category. Query 1: Perform category aﬃnity analysis for prod- ucts purchased online together.",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "Query 1: Perform category aﬃnity analysis for prod- ucts purchased online together. 2.Customer micro-segmentation: Queriesinthislever ranges from grouping users using one dimension to clusteringusersusingmoresophisticatedfeatures. Query 2 tries to cluster users into eight groups based on their purchase history. Query 2: Customers are separated along the follow- ing key shopping dimensions: recency of last visit, fre- quency of visits and monetary amount. Use the in- store and online purchase data over a calendar year to compute. 3.Sentiment analysis: These queries involve an enor- mous amount of text and natural language processing, including detecting sentiment words or phrases from reviews, determining sentiment polarity, etc., as shown in Query 3. Query 3: For a given product, extract sentences from its product reviews that contain sentiments and dis- play their sentiment polarity.",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "its product reviews that contain sentiments and dis- play their sentiment polarity. 4.Enhancing multi-channel consumer experience: Queries in this lever are targeted at understanding users shopping behaviors through both online and in- store channels. Query 4 checks if online browsing af- fects customers’ in-store purchase behaviors by mea- suring the number of days between the two activities. Query 4: Find all customers who viewed items of a given category on the web site in a given month and year and subsequently made an in-store purchase in the same category within the following three months. 5.Assortment optimization: In this lever we focus on queries that identifyproducts, categories or stores that can be targeted for improvements. Query 5 ﬁnds the products with decreasing sales. Query 5: Find the categories with",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "ﬁnds the products with decreasing sales. Query 5: Find the categories with ﬂat or declining sales for in-store purchases during a given year for a given store.6.Pricing optimization: Queries in this lever are fo- cused on measuring the impact of price changes on sales, as shown in Query 6. Query 6: Compute the impact on sales of an item price change by computing the total sales for items in a 30-day period before and after the price change. Group the total sales by items and location of ware- house where they were delivered from. 7.Performance transparency: Our queries for this lever are about ﬁnding stores with downward or up- wardperformance. Query7identiﬁesstoreswithdown- ward sales and ﬁnds possible reasons through available reviews. Query 7: Identify stores with ﬂat or declining",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "reasons through available reviews. Query 7: Identify stores with ﬂat or declining sales in 3 consecutive months, check if there are any negative online reviews regarding these stores. 8.Return analysis: These queries target two areas; identifying problematic products and detecting refund fraud. Query 8 ﬁrst ﬁnds products with high return rate and then identiﬁes if there are any issues from product reviews. Query 8: Retrieve the items with the highest number of returns where the number of returns was approxi- mately equivalent across all stores and web channels (within a tolerance of +/- 10%), within a week end- ing a given date. Analyze the online reviews for these items to see if there are any major negative reviews. 9.Inventory management: Queriesfor thisleverfocus on statistical analysis on product inventory.",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "negative reviews. 9.Inventory management: Queriesfor thisleverfocus on statistical analysis on product inventory. Query 9 computes the mean and variation of item inventories and identiﬁes those with large variations. Query 9: This query contains multiple, related itera- tions. Iteration 1 calculates the coeﬃcient of variation and mean of inventory by item and warehouse for two consecutive months. Iteration 2 ﬁnds items that had a coeﬃcient of variation in the ﬁrst months of 1.5 or larger. 10.Price comparison: In this lever, we have one query that measures the correlations between competitor’s prices and item sales, as shown in Query 10. Query 10: For a given product, measure the eﬀect of competitor’s prices on products’ in-store and online sales. The business cases were the main driver for the deﬁnition of the",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "The business cases were the main driver for the deﬁnition of the BigBench queries. The bulk of the queries are within the Marketing and Merchandising categories since these two are the most commonly used and can be further divided in sub-categories, as discussed in [22]. The overall breakdown of queries over the ﬁve business categories is shown in Table 3. 3.3.2 TechnicalDimensions In the following, we elaborate on the three technical di- mensions with examples based on the ten queries above. Data source dimension: It measures the type of in- put data the query is targeting. We have three types of input data in BigBench: structured, semi-structured and un-structured. For example, Query 1 uses semi-structured 1204 [Página 9] Table 3: Business Categories Query Breakdown Business category TotalPercentage(%) Marketing",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "[Página 9] Table 3: Business Categories Query Breakdown Business category TotalPercentage(%) Marketing 18 60.0 Merchandising 5 16.7 Operations 4 13.3 Supply chain 2 6.7 New business models 1 3.3 web click streams as data source, while Query 3 does sen- ti ment words extraction on un-structured product reviews data. In addition to using single data source, data source combinations are covered in the queries as well. For exam- ple, user click analysis (semi-structured) before store pur- chasing (structured) will join the two largest data sources, as is the case in Query 4. Processing type dimension: It measures the type of processing appropriate for the query. This dimension covers the two common paradigms of declarative and procedural languages. In other words, some of our queries can be an- swered",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "languages. In other words, some of our queries can be an- swered by declarative languages, others by procedural lan- guages and others by a mix of both. In the scope of our benchmark, examples of declarative languages are SQL and similar constructs like Hive-QL. Map-Reduce is an example of a procedural language and Pig Latin has a mix of declara- tive and procedural constructs. Note that while some of the queries can be expressed in either declarative or procedu- ral languages, there are queries that can only be expressed through procedural programming. In the former case, if the query is written through complex SQL constructs (e.g., window functions or user deﬁned functions) we consider it a procedural query. However, queries that involve text analy- sis or sentiment analysis,",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "procedural query. However, queries that involve text analy- sis or sentiment analysis, like Query 3 and 7 ﬁt in the later case as they have to be written using procedural program- ming. In the 10 queries above, Query 5, 6 and 9 can be written using SQL and thus are in the declarative category, while the other seven queries need procedural programming or a mix of procedural and declarative constructs. Analytic technique dimension: It measures diﬀerent techniques for answering business analytics questions. In general, we identiﬁedthreemajor categories ofanalytic tech- niques: statistical analysis, data mining and simple report- ing. Statistical analysis involves correlation analysis, time series, regression, etc. Statistical analysis is exempliﬁed in Query 5, 9 and 10. For the data mining categories we use classiﬁcation, clustering, association mining,",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "10. For the data mining categories we use classiﬁcation, clustering, association mining, pattern analy- sis and text analysis in our BigBench workload. Examples of data mining queries include Query 1, 2, 3, 4, 7 and 8. The reporting category is included in the BigBench as we believe that these queries represents a small but signiﬁcant part of business analytics.This category covers the ad hoc queries and those that do not belong to statistical analysis or data mining. Most reporting queries are simple tasks that can be expressed in simple SQL. Note that most of our queries in the reporting category come from TPC-DS. Query 6 is an example of a reporting query. While the query deﬁnition was driven by the business case represented by BigBench, their distribution over",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "was driven by the business case represented by BigBench, their distribution over the three technical dimensions is believed to be reasonable and repre- sentative of the workload portrayed by the benchmark. We conclude this section by summarizing in Table 4 the query distribution along the three technical dimensions.Table 4: Technical Dimensions Breakdown Query processing type TotalPercentage(%) Declarative 10 33.3 Procedural 7 23.3 Mix of Declarative and Pro- ce dural13 43.3 Data sources TotalPercentage(%) Structured 18 60.0 Semi-structured 7 23.3 Un-structured 5 16.7 Analytic techniques TotalPercentage(%) Statistics analysis 6 20.0 Data mining 17 56.7 Reporting 8 26.7 3.4 Metrics Pr evious TPCbenchmarkslike TPC-HandrecentlyTPC- DS have metrics based mostly on individual query execution times. The metric for BigBench could simply be the same or similar to either TPC-H or TPC-DS",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "could simply be the same or similar to either TPC-H or TPC-DS since from a high level it has similar phases, such as initial load, data refresh and query execution. Wedefer theﬁnaldesign for theBigBench metricto future work. However, we believe that data loading and the type of processing dimension described in Section 3.3 is a nec- essary factor in BigBench’s metric. Our rationale is that, on the one hand, DBMS and MR engines have diﬀerent strengths in terms of loading, declarative and procedural processing. For example, Hadoop related systems are very eﬃcient at loading and are generally optimized for MR pro- cessing. On the other hand, DBMS engines are optimized to process SQL, but MR/UDF processing and data load- ing may be less optimized. In addition, there is",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "and data load- ing may be less optimized. In addition, there is a recent eﬀort for DBMS engines to process MR more eﬃciently, ei- ther natively or through an eﬃcient co-existence with an MR engine (e.g., Hadoop, HIVE or Pig). One option to reﬂect the importance of the processing type dimension is to use the diﬀerent processing types in the metric compu- tation instead of using individual queries. Let TLbe the l oading time, TDthe total time for queries in declarative p rocessing, TPthe time for procedural processing queries a ndTBthe time for the remaining queries that have both d eclarative and procedural. A meaningful way of combin- ing these four values in a composite metric is by computing their geometric mean as4√ TL∗TD∗TP∗TB. If the w orkload",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "is by computing their geometric mean as4√ TL∗TD∗TP∗TB. If the w orkload queries are used, the geometric mean could be cal- culated as30/radicalbig/producttext30 i= 1Pi(wherePidenotes the execution time forQuery i). 4. EVALUATION BigBench is targeted at DBMS and MR systems that claim to provide big data solutions. Therefore, any of those systems can be used to establish the feasibility of this bench- mark. Standard DBMSes most likely will capture all data as relational tables by parsing the semi-structured data and establishing a schema. The un-structured data can also be captured as a table where the review text can be stored as VARCHAR or a blob column. Such DBMSes can imple- ment our queries using SQL and some procedural constructs like UDF or even built in MR processing within",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "some procedural constructs like UDF or even built in MR processing within the DBMS. 1205 [Página 10] Aster nCluster Database Loader/Exporter Server Group Worker Server Group Queen Server Group Reports, Analytics, Applications (SQL / ODBC / JDBC) Queries / Answers Queries Data Figure 5: nCluster Architecture Ha doop and its ecosystem with HIVE and Pig can also run BigBench. The data can be captured in HDFS or similar structures. The main strength of these systems is MR but they also have some relational operators like those in H-QL or Pig [2, 24]. Such relational operators can do joins, group- ing and aggregations. BigBench can also be run on systems that have both DBMS and MR engines like Hadoop or any of its ecosystem products. Such systems consists most",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "like Hadoop or any of its ecosystem products. Such systems consists most likely of a DBMS that connects or co-exists with an MR engine. We chose to initially run BigBench on the Teradata Aster DBMS. TAD has all features needed to store and process big data. Data can be stored as tables and queries can be exe- cuted using the SQL-MR interface that extends declarative SQL with MR processing. 4.1 TeradataAsterDBMS TAD is based on the nCluster technology. nCluster is a shared-nothing parallel database, optimized for data ware- housing and analytic workloads [19]. nCluster manages a cluster of commodity server nodes, and is designed to scale out to hundreds of nodes and scale up to petabytes of active data. Figure 4.1 depicts the nCluster architecture. Query pro- cessing",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "of active data. Figure 4.1 depicts the nCluster architecture. Query pro- cessing is managed by one or more Queennodes. These nodes analyze client requests and distribute partial process- ing among the Worker nodes. Each relation in nCluster is hash-partitioned (fact tables) or duplicated (dimension tables) across the Worker nodes to enable intra-query par- allelism. Loading is done by special Worker nodes shown at the bottom of Figure 4.1. Inadditiontodatabasequeryprocessing, automatedman- ageability functionality in nCluster allows adding new ma- chines and redistributing data. The system performs auto- matic fail-over, retry of queries, and restoration of replica- tion levels after a node failure. These features are essential in a large cluster of machines, where failures of various kinds occur regularly. The SQL-MR supports a mix of SQL and polymorphicUDFs that",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "occur regularly. The SQL-MR supports a mix of SQL and polymorphicUDFs that process MR logic. The MR functions are paral- lelizable, self-describinganddynamicallypolymorphicwhere thefunctioninputschemasaredeterminedimplicitlyatquery execution time. Output schemas are determined program- matically by the function itself at query execution time as well. They are also equivalent to subqueries, making them subject to query optimization along with the other relations in a query. nCluster allows MR UDFs to be written using Java, C/C++, and scripting languages like Python. 4.2 End-to-EndExecution The test was executed on a 8 node Teradata Aster appli- ance. Each node is a Dell server with two quad-core Xeon 5500 at 3.07Ghz and hardware RAID 1 with 8 2.5”drives. Due to time limitation, DSDGEN is used to produce the original TPC-DS tables in the structured part of",
        "start_idx": 8236,
        "end_idx": 8364
      },
      {
        "text": "used to produce the original TPC-DS tables in the structured part of our model. We used PDGF to generate the new parts of the data and the XML conﬁguration for the review generator. The new parts produced by PDGF include the new Item marketprice ta ble, an apache-styleweb server log, and theonline reviews. PDGF is also conﬁgured to match the references (PK-FK relationships) in the new data with the TPC-DS data. In the future, we plan on extending PDGF to handle the whole data generation aspects without the need for DSDGEN. The data was loaded into TAD as tables. The web logs were parsed and converted to a table similar to the structure shown in Section 3.1. Product reviews are also interpreted as a table assuming the review",
        "start_idx": 8352,
        "end_idx": 8480
      },
      {
        "text": "3.1. Product reviews are also interpreted as a table assuming the review text as a VARCHAR(5000). As a proof of concept, we executed the workload as a single stream without velocity. Since we adapt the velocity methodology from TPC-DS adding it will not be diﬃcult and can be implemented with a simple driver that adds data to the system periodically and re-submits a new stream of queries. Concurrent streams can also be handled similar to previous benchmarks like TPC-H. The queries are written using TAD SQL-MR interface basedonthedescriptioninSection3.3. Thereportingqueries were written using SQL only and the rest were done through either an MR call or a mix of both SQL and MR. Below, we show the SQL-MR version of a sample of the 30 queries. The full list",
        "start_idx": 8468,
        "end_idx": 8596
      },
      {
        "text": "SQL-MR version of a sample of the 30 queries. The full list of the 30 queries written in SQL-MR can be found on our technical report that will be published with this paper. Note that all TAD MR functions used in the evaluation are part of a library TAD provides and packaged with the nCluster DBMS. The queryin Listing 4 is the SQL-MRequivalentof Query 3 in Section 3.3 which extracts sentiments and their polarity. The query retrieves from a reducer function called Extract- Sentimentthat takes inputthe source table product reviews. The call to the functions also speciﬁes the column that has the text, the model for the sentiment analysis and the level of the search (sentence or word). The WHERE clause at the end picks positive or negative",
        "start_idx": 8584,
        "end_idx": 8712
      },
      {
        "text": "or word). The WHERE clause at the end picks positive or negative polarity. The second example is for Query 1 as described in Sec- tion 3.3. The query is shown in Listing 5. It is the SQL- MR version equivalent for Query 1. It consists of 3 blocks. The most inner block is a SQL fragment that joins the websales and item tables and projects out category id and cu stomerid. The inner block is fed as input to an MR func- ti on called basket generator which ﬁnds the categories of pa irwise items purchased together by customers. The in- put is partitioned by customer id as speciﬁed by the PAR- TI TION clause. The call to market basket also speciﬁes 1206 [Página 11] SELECT pr_item_sk, out_content,",
        "start_idx": 8700,
        "end_idx": 8828
      },
      {
        "text": "call to market basket also speciﬁes 1206 [Página 11] SELECT pr_item_sk, out_content, out_polarity, ou t_sentiment_words FROMExtractSentiment ( ONproduct_reviews TE XT_COLUMN ( ’pr_review_content’ ) MO DEL (’dictionary’ ) LEVEL(’sentence’ ) AC CUMLATE ( ’pr_item_sk’ ) ) WHEREout_polarity = ’NEG’orout_polarity = ’POS’; Listing 4: Query 3 wh ich ﬁeld should the basket analysis be done on using the BASKET ITEM clause. The last clause for the call to ba sketgenerator is ITEM SETMAX(500) which limits the an alysis to 500 pairs of items for each customer. The output of basket generator is the input to the main query which ba- si cally ﬁnds the degree of aﬃnity for each pair of categories. SELECT category_cd1 AScategory1_cd, ca tegory_cd2 AScategory2_cd, COUNT(*)AScnt FROM basket_generator( ON (SELECT i.i_category_id AScategory_cd, s. ws_bill_customer_sk AScustomer_id FROMweb_sales s",
        "start_idx": 8816,
        "end_idx": 8944
      },
      {
        "text": "COUNT(*)AScnt FROM basket_generator( ON (SELECT i.i_category_id AScategory_cd, s. ws_bill_customer_sk AScustomer_id FROMweb_sales s INNER JOIN item i ONs.ws_item_sk = i_item_sk WHEREi.i_category_id is not NULL ) PA RTITION BYcustomer_id BA SKET_ITEM( ’category_cd’ ) IT EM_SET_MAX(500) ) GROUP BY 1,2 order by 1,3,2; Listing 5: Query 1 Th e last example of our evaluation queries is Query 6 de- scribed in Section 3.3. The query is SQL only and adapted from the TPC-DS benchmark, it can be seen in Listing 6. As described before, Query 6 ﬁnds the impact of pricing change done on March 16, 1998. The query joins the fol- lowing tables: web sales used to capture sales done online, we breturns for returns of web sales, warehouse which cap- tu res information about warehouses, item table that cap-",
        "start_idx": 8932,
        "end_idx": 9060
      },
      {
        "text": "warehouse which cap- tu res information about warehouses, item table that cap- tures the products sold and date dim which is a date lookup ta ble. The join with web returns is done as an outer join si ncenotallordershavereturns. Thequerycomputestheto- talsalesbeforeandafterMarch16, 1998 aliasedassales before an d salesafter in Listing 6. The query group on state loca- ti on of the warehouse and ID of the items. The run time of each of the 30 queries can be found at our technical report that will be published with this paper. Figure 7 lists the run time of the 10 queries used in the workload section. We also show the values of TL,TD, TPandTBas discussed in Figure 6. Note that we did n ot try any hardware or software optimizations",
        "start_idx": 9048,
        "end_idx": 9176
      },
      {
        "text": "Note that we did n ot try any hardware or software optimizations to run the above 30 queries since our goal is to just make sure these queries run and produce meaningful results. The run time of the 30 queries varies from seconds to a little bit over an hour. This illustrates that we do not have a runway query situation and we also have a range of query complexities.SELECT w_state,i_item_id ,sum(case when (cast(d_date as date ) < cast(’1998-03-16’ as date )) thenws_sales_price - coalesce (wr_refunded_cash ,0) else0end) assales_before ,sum(case when (cast(d_date as date ) >= cast(’1998-03-16’ as date )) thenws_sales_price - coalesce (wr_refunded_cash ,0) else0end)assales_after FROM web_sales left outer join web_returns on (ws_order_number = wr_order_number andws_item_sk = wr_item_sk) ,w arehouse, item, date_dim WHERE i_item_sk = ws_item_sk andws_warehouse_sk",
        "start_idx": 9164,
        "end_idx": 9292
      },
      {
        "text": "andws_item_sk = wr_item_sk) ,w arehouse, item, date_dim WHERE i_item_sk = ws_item_sk andws_warehouse_sk = w_warehouse_sk andws_sold_date_sk = d_date_sk andd_date between (cast(’1998-03-16’ as date ) -interval ’30 da y’) and(cast(’1998-03-16’ as date ) +interval ’30 day’ ) GROUP by w_state,i_item_id ORDER by w_state,i_item_id; Listing 6: Query 6 0 20 00 4000 6000 8000 10000 12000 Loading Declarative Procedural Both Run Time [s] Components Figure 6: Runtime of Metric Components 5. CONCLUSION In this paper we presented BigBench, a proposal for an end-to-end big data benchmark. The proposal covers a data model addressing the velocity, variety and volume common in big data. Velocity is accomplished bycontinuous feed into the data store while variety is addressed by including struc- tured, semi-structured and unstructured in the data model. The data model also can",
        "start_idx": 9280,
        "end_idx": 9408
      },
      {
        "text": "semi-structured and unstructured in the data model. The data model also can scale to large volumes based on as scale factor. We used PDGF as a starting point for our data generator that covers the structured part. PDGF is enhanced to produce the semi-structured and unstructured data. The unstructured component is based on a novel technique we developed leveraging the Markov chain model. The proposal also provides a comprehensive list of workload queries and sets directions for a novel metric that focuses on the diﬀerent types of processing in big data. Finally, we 1207 [Página 12] 0 10 0 200 300 400 500 600 700 Q1 Q2 Q3 Q4 Q5 Q6 Q7 Q8 Q9 Q10 Run Time [s] Queries Figure 7: Runtime of Sample Queries ve riﬁed the",
        "start_idx": 9396,
        "end_idx": 9524
      },
      {
        "text": "Time [s] Queries Figure 7: Runtime of Sample Queries ve riﬁed the feasibility and applicability of our proposal by implementing and running it on Teradata Aster DBMS. For future work, we are planning to extend this work in three main areas. First, we would like to enhance the pro- posal to be a concrete speciﬁcation that can lead to an in- dustry standard benchmark. This work include ﬁnalizing and detailing the data, workload and metric speciﬁcations. We also think system availability during failure should be addressed in the ﬁnal speciﬁcation. Second, we think it will be useful to provide a downloadable kit that can be used to setup and run the benchmark. This work include ﬁnal- izing the implementation of our data and query generators. Finally, we are",
        "start_idx": 9512,
        "end_idx": 9640
      },
      {
        "text": "izing the implementation of our data and query generators. Finally, we are planning to extend the benchmark proof of concept to include velocity and multi-user test. We also would like to run the benchmark on one the Hadoop eco- system like HIVE. 6. REFERENCES [1] Apache Hadoop Project. http://hadoop.apache.org . [2] Apache Hive Project. http://hadoop.apache.org/hive . [3] Cloudera Distribution Including Apache Hadoop (CDH). http://www.cloudera.com . [4] Greenplum Database. http://www.greenplum.com . [5] GridMix Benchmark. http://hadoop.apache.org/docs/ mapreduce/current/gridmix.html . [6] Oracle Database - Oracle. http://www.oracle.com . [7] PigMix Benchmark. https: //cwiki.apache.org/confluence/display/PIG/PigMix . [8] Teradata Database - Teradata Inc. http://www.teradata.com . [9] TwinFin - Netezza, Inc. http://www.netezza.com/ . [10] TPC Benchmark DS, 2012. [11] J. Bentley. Programming Pearls . Addison-Wesley, 2000. [12] M. J. Carey, D. J. DeWitt, and J. F. Naughton.",
        "start_idx": 9628,
        "end_idx": 9756
      },
      {
        "text": "2000. [12] M. J. Carey, D. J. DeWitt, and J. F. Naughton. The oo7 Benchmark. In P. Buneman and S. Jajodia, editors, SIGMOD’93 , pages 12–21. ACM Press, 1993. [13] M. J. Carey, D. J. DeWitt, J. F. Naughton, M. Asgarian, P. Brown, J. Gehrke, and D. Shah. The BUCKY Object-Relational Benchmark (Experience Paper). In SIGMOD , pages 135–146, 1997.[14] M. J. Carey, L. Ling, M. Nicola, and L. Shao. EXRT: Towards a Simple Benchmark for XML Readiness Testing. InTPCTC, pages 93–109, 2010. [15] C. Chambers, A. Raniwala, F. Perry, S. Adams, R. R. Henry, R. Bradshaw, and N. Weizenbaum. FlumeJava: Easy, Eﬃcient Data-Parallel Pipelines. In PLDI, pages 363–375, 2010. [16] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears. Benchmarking Cloud Serving Systems with",
        "start_idx": 9744,
        "end_idx": 9872
      },
      {
        "text": "E. Tam, R. Ramakrishnan, and R. Sears. Benchmarking Cloud Serving Systems with YCSB. In SoCC, pages 143–154, 2010. [17] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed Data Processing on Large Clusters. Communications of the ACM, 51(1):107–113, 2008. [18] M. Frank, M. Poess, and T. Rabl. Eﬃcient Update Data Generation for DBMS Benchmark. In ICPE, 2012. [19] E. Friedman, P. Pawlowski, and J. Cieslewicz. SQL/MapReduce: A Practical Approach to Self-Describing, Polymorphic, and Parallelizable User-Deﬁned Functions. PVLDB , 2(2):1402–1413, 2009. [20] J. Gray. GraySort Benchmark. Sort Benchmark Home Page –http://sortbenchmark.org . [21] D. Laney. 3D Data Management: Controlling Data Volume, Velocity and Variety. Technical report, Meta Group, 2001. [22] J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs, C. Roxburgh, and A. H. Byers. Big data: The Next Frontier",
        "start_idx": 9860,
        "end_idx": 9988
      },
      {
        "text": "Dobbs, C. Roxburgh, and A. H. Byers. Big data: The Next Frontier for Innovation, Competition, and Productivity. Technical report, McKinsey Global Institute, 2011. http://www.mckinsey.com/insights/mgi/research/ technology_and_innovation/big_data_the_next_ frontier_for_innovation . [23] R. O. Nambiar and M. Poess. The Making of TPC-DS. In VLDB, pages 1049–1058, 2006. [24] C. Olston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig Latin: A Not-So-Foreign Language for Data Processing. In SIGMOD , 2008. [25] S. Patil, M. Polte, K. Ren, W. Tantisiriroj, L. Xiao, J. Lopez, G. Gibson, A. Fuchs, and B. Rinaldi. YCSB++: benchmarking and performance debugging advanced features in scalable table stores. In SoCC, pages 9:1–9:14, 2011. [26] A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J. DeWitt, S. Madden, and M. Stonebraker. A Comparison of Approaches to Large-Scale Data",
        "start_idx": 9976,
        "end_idx": 10104
      },
      {
        "text": "S. Madden, and M. Stonebraker. A Comparison of Approaches to Large-Scale Data Analysis. In SIGMOD , pages 165–178, 2009. [27] R. Pike, S. Dorward, R. Griesemer, and S. Quinlan. Interpreting the Data: Parallel Analysis with Sawzall. Scientiﬁc Programming , 13(4):277–298, 2005. [28] M. P ¨oss, R. O. Nambiar, and D. Walrath. Why You Should Run TPC-DS: A Workload Analysis. In VLDB, pages 1138–1149, 2007. [29] T. Rabl, M. Frank, H. M. Sergieh, and H. Kosch. A Data Generator for Cloud-Scale Benchmarking. In TPCTC, pages 41–56, 2010. [30] T. Rabl, M. Sadoghi, H.-A. Jacobsen, S. G´ omez-Villamor, V. Munt´ es-Mulero, and S. Mankowskii. Solving Big Data Challenges for Enterprise Application Performance Management. PVLDB , 5(12):1724–1735, 2012. [31] A. Schmidt, F. Waas, M. L. Kersten, M. J. Carey, I. Manolescu,",
        "start_idx": 10092,
        "end_idx": 10220
      },
      {
        "text": "A. Schmidt, F. Waas, M. L. Kersten, M. J. Carey, I. Manolescu, and R. Busse. XMark: A Benchmark for XML Data Management. In VLDB, pages 974–985, 2002. [32] Teradata Aster. Teradata Aster Big Analytics Appliance 3H - Analytics Foundation User Guide , release 5.0.1 edition, 2012. http://www.info.teradata.com/edownload. cfm?itemid=123060004 . [33] L. Wyatt, B. Cauﬁeld, and D. Pol. Principles for an ETL Benchmark. In TPCTC, pages 183–198, 2009. 1208",
        "start_idx": 10208,
        "end_idx": 10276
      }
    ],
    "01fbb0dc-75e5-4de9-be8a-026bed817d8f": [
      {
        "text": "[Página 1] BigSift: Automated Debugging of Big Data Analytics in Data-Intensive Scalable Computing Muhammad Ali Gulzar University of California, Los Angeles USA gulzar@cs.ucla.eduSiman Wang Hunan University China simanw@ucla.eduMiryung Kim University of California, Los Angeles USA miryung@cs.ucla.edu ABSTRACT Developing Big Data Analytics often involves trial and error debug- ging, due to the unclean nature of datasets or wrong assumptions made about data. When errors ( e.g. program crash, outlier results, etc.) arise, developers are often interested in pinpointing the root cause of errors. To address this problem, BigSift takes an Apache Spark program, a user-defined test oracle function, and a dataset as input and outputs a minimum set of input records that reproduces the same test failure by combining the insights from delta debugging with data provenance . The",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "by combining the insights from delta debugging with data provenance . The technical contribution of BigSift is the design of systems optimizations that bring automated debugging closer to a reality for data intensive scalable computing. BigSift exposes an interactive web interface where a user can monitor a big data analytics job running remotely on the cloud, write a user-defined test oracle function, and then trigger the auto- mated debugging process. BigSift also provides a set of predefined test oracle functions, which can be used for explaining common types of anomalies in big data analytics—for example, finding the origin of the output value that is more than kstandard deviations away from the median. The demonstration video is available at https://youtu.be/jdBsCd61a1Q. CCS CONCEPTS •Software and its engineering →Software testing and",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "available at https://youtu.be/jdBsCd61a1Q. CCS CONCEPTS •Software and its engineering →Software testing and de- bugging ;•Information systems →Data provenance ; KEYWORDS Automated debugging, fault localization, data provenance, data- intensive scalable computing (DISC), big data, and data cleaning ACM Reference Format: Muhammad Ali Gulzar, Siman Wang, and Miryung Kim. 2018. BigSift: Au- tomated Debugging of Big Data Analytics in Data-Intensive Scalable Com- puting. In Proceedings of the 26th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ES- EC/FSE ’18), November 4–9, 2018, Lake Buena Vista, FL, USA. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/3236024.3264586 0Work done by Siman Wang as an intern at University of California, Los Angeles. Permission to make digital or hard copies of all or part of this work for",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA ©2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5573-5/18/11. . . $15.00 https://doi.org/10.1145/3236024.3264586 Test Predicate PushdownPrioritizing Backward TracesBitmap based Test MemoizationInput: A Spark Program, ATest Function, and Input",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "Backward TracesBitmap based Test MemoizationInput: A Spark Program, ATest Function, and Input data Output: Minimum Fault-Inducing Input Records Data Provenance + Delta DebuggingFigure 1: BigSift Overall Architecture 1 INTRODUCTION Data-Intensive Scalable Computing (DISC) systems such as Google’s MapReduce, Apache Spark, and Apache Hadoop enable processing massive data sets. Similar to other software development platforms, developers often deal with unclean data or make wrong (or in- complete) assumptions about the data. It is therefore crucial to equip these developers with toolkits that can better pinpoint the root cause of an error. Unfortunately, debugging big data analytics is currently an ad-hoc, time-consuming process. Data scientists typically write code that implements a data processing pipeline and test it on their local development workstation with a small sample data, downloaded from a",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "their local development workstation with a small sample data, downloaded from a TB-scale data warehouse. They cross fingers and hope that the program works in the expensive production cloud. When a job fails or they get results that end up being suspicious, data scientists must identify the source of the error, often by digging through post-mortem logs. In such cases, the programmer ( e.g. data scientist) may want to pinpoint the root cause of errors by investigating a subset of corresponding input records. One possible approach is to track data provenance (input output record mappings created in individual dis- tributed worker nodes). However, according to our prior study [ 1], backward tracing based on data provenance finds an input subset in the order of millions, which is still",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "finds an input subset in the order of millions, which is still too large for a developer to man- ually sift through. Delta Debugging (DD) is a well-known algorithm that re-executes the same program with different subsets of input records [ 10]. Applying the DD algorithm naively on big data analyt- ics is not scalable because DD is a generic, black box procedure that does not consider the key-value mapping generated from individual dataflow operators. Therefore, DD cannot prune irrelevant input records easily by considering the semantics of dataflow operators. The technical contribution of BigSift is two folds. First, it com- bines delta debugging with data provenance . Second, it implements three systems-level optimizations—(1) test predicate pushdown, (2) backward trace prioritization, and (3) bitmap-based memoiza- tion to be",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "pushdown, (2) backward trace prioritization, and (3) bitmap-based memoiza- tion to be discussed in Section 2 in details—to improve debugging performance. Figure 1 shows the overall architecture of BigSift . 863 [Página 2] ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA Muhammad Ali Gulzar, Siman Wang, and Miryung Kim (a) Job completion time and output (b) User and pre defined test function selection (c) Area plot reports the real time information of the debugging process➊ ➋ ➌➍ ➎ ➏➐ ➑ Figure 2: BigSift ’s Web-based User Interface Our evaluations show that BigSift improves the accuracy of fault localizability by several orders-of-magnitude ( ∼103to107×) compared to Titian’s [ 4] data provenance only. BigSift improves performance by up to 66×compared to using Delta Debugging alone [ 1]. For",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "by up to 66×compared to using Delta Debugging alone [ 1]. For each faulty output, BigSift is able to localize fault- inducing data in less than 62% of the original job running time. This tool demonstration paper builds on our prior work [ 1] and focuses on the tool features and corresponding implementation details of BigSift .BigSift is fully integrated with the current Apache Spark’s web-based UI. A user can directly inspect raw out- put records, and write a test-oracle function on the fly or select from pre-defined test oracle functions. BigSift streams real time debugging progress information from the remote cluster to the user through an interactive area plot and presents the current set of fault-inducing input records in a table format. Our current imple- mentation targets",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "fault-inducing input records in a table format. Our current imple- mentation targets Apache Spark 2.1.1 with programs written in Scala and Java [9]. 2 TECHNICAL APPROACH The contribution of BigSift is to adapt delta debugging for big data analytics by designing new systems optimizations and by lever- aging data provenance in tandem, which provides backward and forward tracing capabilities for Apache Spark [ 4]. The overview of our approach is described in Figure 1. Without such systems optimizations, delta debugging could take hours if not days. This is because the input dataset size is huge and thus an exhaustive, binary-search like algorithm such as delta debugging could take sig- nificant amount of time. In our evaluation, BigSift is up to 66 times faster than DD. Below we summarize",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "BigSift is up to 66 times faster than DD. Below we summarize three systems optimizations at a high level, and further details are described elsewhere [1]. 2.1 Test Function Push Down In the map-reduce programming paradigm, a combiner performs partial aggregation for operators such as reduceByKey on the map side before sending data to reducers in order to minimize network communication. Since delta debugging uses a user-defined test function to check if each final record is faulty, our insight is that, during backward tracing, we should isolate the exact partitions withfault-inducing intermediate inputs to further reduce the backward tracing search scope. In Apache Spark, certain aggregation operators ( e.g.reduceByKey ) require a user to provide an associative andcommutative function as an argument. BigSift implements a new optimization by",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "associative andcommutative function as an argument. BigSift implements a new optimization by push- ing down a user-defined test function to partitions in the previ- ous stage to test intermediate results. This optimization is enabled when (1) the program ends with an aggregation operator (such as reduceByKey ) that requires an associative function f1; (2) f1◦f2 is associative, when f2is a test function; and (3) f1◦f2is failure- monotone. If this monotonicity property is not satisfied (which can be verified by testing final output), or none of the partitions fail the test function, BigSift rolls back to the default case of backward tracing the final faulty record. 2.2 Overlapping Backward Traces Multiple faulty output records may be caused by the same input records due to operators such as flatMap orjoin",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "by the same input records due to operators such as flatMap orjoin , where a single data record can produce multiple intermediate records, leading to multiple faulty outputs. Therefore, BigSift prioritizes the common input records leading to multiple outputs before applying DD. To check the eligibility for this optimization, BigSift explores a pro- gram DAG to find at least one 1-to-many or many-to-many operator such as flatMap andjoin . In order to explore all the possible overlapping traces, BigSift overlaps the two smallest backward traces (let’s say t1and t2), to find the intersection, t1∩t2. If the test function evaluated on t1∩t2 finds any fault, then DD is applied to t1∩t2and the remaining (potential) failure-inducing inputs t1−t2and t2−t1. Otherwise, DD is executed over both initial traces t1and t2. If",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "t2−t1. Otherwise, DD is executed over both initial traces t1and t2. If any fault-inducing inputs are found in the overlap, there could be potential time saving from not processing the overlapped trace twice. 2.3 Bitmap Based Memoization of Test Results DD is not capable of detecting redundant trials of the same input configuration and therefore may test the same input configuration multiple times. To avoid waste of computational resources, BigSift uses a test results memoization optimization. A naive memoization 864 [Página 3] Automated Debugging of Big Data Analytics in Data-Intensive ... ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA 1class BigSift(sc:SparkContext, logFile:String){ 2 def runWithBigSift[T]( 3 sparkProgram : (RDD[String],Lineage[String]) => RDD[T] , test : T => Boolean ) : Unit 4 ... } Figure 3: BigSift",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "T => Boolean ) : Unit 4 ... } Figure 3: BigSift ’s API strategy would require scanning of the content of an input configu- ration to check whether it was tested already; such content-based memoization would be time consuming and not scalable. BigSift instead leverages bitmaps to compactly encode the offsets of the input dataset to refer to a sub-configuration. The universal splitting function for DD is thus instrumented to generate sub-configurations along with their related bitmap descrip- tions. BigSift maintains the list of already executed bitmaps, each of which points to the test result of running a program on the input sub-configuration. Before processing an input sub-configuration, BigSift uses its bitmap description to perform a look-up in the list of bitmaps. If the result is positive,",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "a look-up in the list of bitmaps. If the result is positive, the test result for the target sub- configuration is directly reused by the look-up. Otherwise, BigSift tests the sub-configuration and enrolls its bitmap and the corre- sponding test result in the list. This technique avoids redundant testing of the same input sub-configuration and reduces the total debugging time. BigSift uses the compressed Roaring Bitmaps representation to describe large scale datasets [5]. 2.4 Implementation To enable automated debugging of big data analytics applications, a user can instantiate BigSift class with SparkContext and in- put file path as input arguments, as shown in Figure 3. Internally, this class instantiates LineageContext that enables Titian’s instru- mentation for data provenance support. More details on the usage of Titian is described",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "data provenance support. More details on the usage of Titian is described in our prior VLDB 2016 paper [ 4]. A user can then call runWithBigSift method with a test oracle function, and a sparkProgram —a directly acyclic graph (DAG) workflow that takes in an input Resilient Distributed Dataset (RDD–i.e., an abstraction of distributed collection) and returns the final RDD. BigSift is designed as an external Java library (jar) and can be deployed by importing the jar file in a Spark application running on a data-provenance enabled Spark distribution such as Titian [ 4]. BigSift ’s interactive UI is available on port 8989 on the Spark driver node. Figure 2 shows the web-based user interface. Once the job is completed, a user can examine the job execution time,",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "the job is completed, a user can examine the job execution time, raw output, etc. She can write her own custom test-oracle function or select from pre-defined test functions. BigSift also displays a set of input records that reproduce the same test failure. The area chart reports the real time debugging progress information. A user can click on the graph to see the size and samples of failure-inducing inputs. 3 DEMONSTRATION SCENARIO Suppose Alice is a data scientist and she writes a big data application in Apache Spark to analyze a large scale dataset that contains passenger transit information in the US. Since the data is in the scale of terabytes, she takes a small sample of the dataset (say 10 MB) and builds a data processing pipeline",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "of the dataset (say 10 MB) and builds a data processing pipeline using Spark in a local machine. Alice wants to find the total transit time for all passengers spending less than 45 minutes while in transit for each airport in (a) Key-value output visualization (b) Data prove- nance visualization Figure 4: BigSift ’s histogram visualization of key-value based output records 1val countoftranist = sc.textFile(dataset).map{ s => 2 val tokens = s.split(\",\") 3 val arrival_hr = tokens(2).split(\":\")(0) 4 val diff = getDiff(tokens(2) , tokens(3)) 5 val airport = tokens(4) 6 ((airport, arrival_hr), diff)} 7 .filter{ v => v._2 < 45} 8 .reduceByKey(_+_) 9 .collect() Figure 5: A Spark program written in Scala that finds the total layover time of all passengers spending less than 45 minutes per airport",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "layover time of all passengers spending less than 45 minutes per airport at each hour. the US for each hour. A row in the dataset represents a passenger’s transit information in the following format. [date, passenger, arrival, departure, airport code] 9/4/17 , 161413 , 6:52 , 8:22 , MNN The program in Figure 5 first loads the dataset (line 1) and scans each row to retrieve a key-value pair. A key consists of the airport code and arrival hour of a passenger and the value is the transit time spent in minutes (departure time -arrival time) at the airport (line 2-6). Line 7 filters passengers with the transit time less than 45 minutes. Finally, the program sums up the transit times of all passengers per airport at each",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "sums up the transit times of all passengers per airport at each arrival hour (line 8). After writing this application, Alice submits the job to the pro- duction cloud which results in the following output: ((SEA,7) , 175080) ((LAX,11) , 173460) ((MNN,23) , -27804120) ..... She then realizes that some output records look suspicious. For example, the total transit time of MNN is-27804120 , when she expects the total transit time to be a positive value. Alice wants to investigate what are the exact input records responsible for producing a negative value. This task is challenging because the large scale dataset is infeasible to inspect manually and there is no one-to-one mapping between input records and output records due to an aggregation step that applies user-defined functions. Alice",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "output records due to an aggregation step that applies user-defined functions. Alice decides to use BigSift that takes her program, input data set, and a test oracle function as input and, eventually, returns the following culprit input record responsible for the suspicious negative output value. 11/9/12 , 141011 , 22:53 , 0:23 , MNN The following describes BigSift demonstration step by step. 865 [Página 4] ESEC/FSE ’18, November 4–9, 2018, Lake Buena Vista, FL, USA Muhammad Ali Gulzar, Siman Wang, and Miryung Kim Step 1: Program Output Inspection. Figure 2 shows the landing page of BigSift . It shows the size of input dataset as the number of records, the job processing time, final output records in a text box. See➊,➋, and➌in Figure 2 respectively. To better visualize",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "in a text box. See➊,➋, and➌in Figure 2 respectively. To better visualize output records, BigSift provides interactive and dynamic visualization of key-value pairs using a histogram to make it easier for a user to identify anomalous records visually (Figure 4(a)). For example, Alice can mark any negative value as incorrect using a histogram and note down this threshold to construct a test function. Step 2: Classifying Suspicious or Wrong Output Records by Defin- ing a Test-Oracle Function. BigSift enables a user to write a test function—a predicate to be applied to each final output record to distinguish correct outputs from incorrect or anomalous outputs. BigSift also enables user to choose from a list of pre-defined test predicate functions (Figure 2(b)- ➍) to help explain the common types of",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "predicate functions (Figure 2(b)- ➍) to help explain the common types of anomalies in big data analytics: for example, (1) explain how a minimum output value is created, (2) explain how a maximum output value is created, (3) explain how the output value greater than kstandard deviations from the median is created, etc. Once the selection is made from the radio buttons, a user can press the RunBigSift button (Figure 2(b)- ➏). Internally, BigSift selects the corresponding pre-defined test function to initiate debugging. Step 3: Visualization of Data Provenance. To help understand the propagation of fault-inducing intermediate input records across transformation steps, BigSift provides a pie chart based DAG vi- sualization of the workflow (Figure 4(b)). Each node in this graph is represented as a pie chart where",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "Each node in this graph is represented as a pie chart where a red segment shows the ratio of fault-inducing intermediate records against the total number of records processed by that transformation. By viewing data ratio at each transformation, a user may get deeper insight. Step 4: Automated DISC Debugging. When BigSift is invoked by the user, a realtime area chart appears on the UI. In Figure 2(c), Y- axis represents the number of fault-inducing input records isolated byBigSift in log scale and X-axis represents debugging time. As the time passes, BigSift streams debugging progress information from the cloud. A user can click on any part of the chart to view sample fault-inducing input records at the selected time. A mouse hover-over will show the number of fault-inducing",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "the selected time. A mouse hover-over will show the number of fault-inducing input records. As soon as BigSift finds the minimum set of fault-inducing input records, BigSift reports the total debugging time through a push notification (green container in Figure 2(c)- ➐). 4 RELATED WORK Delta debugging (DD) is a well known technique for finding the min- imal failure-inducing input [ 10] that requires multiple tests of the program, which alone, is not tractable for DISC system workloads. HDD tries to minimize DD tests by assuming that the input is in a well defined hierarchical structure which rarely holds [ 7]. RAMP [3] and Newt [ 6] add data provenance support to DISC systems. BigSift differs from these by leveraging DD and data provenance in tandem and by",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "from these by leveraging DD and data provenance in tandem and by implementing unique systems optimizations to im- prove performance for DISC workloads. BigDebug is an interactive debugger for Spark [ 2] and it leaves to the developer to manually identify the root cause of errors. Data X-ray [ 8] extracts a set of features representing input data properties and summarizes the errors in a SQL table, but does not support automated debugging.5 EVALUATION AND SUMMARY We are in the early days of debugging big data analytics. This tool demonstration paper showcases BigSift , an automated debugging toolkit in the context of data-intensive scalable computing (DISC). Finding failure-inducing inputs is just the beginning. We see further opportunities for automated debugging of DISC applications, such as automated data cleaning",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "opportunities for automated debugging of DISC applications, such as automated data cleaning and faulty code localization. In our prior work [ 1], we evaluated BigSift on a 16-node cluster with 8 subject program where faults were injected in both input datasets or code. The datasets used in the evaluation ranges from few GB to 80GB. In comparison to using DD alone, BigSift re- duced the fault localization time (as much as 66×) by pruning out input records that are not relevant to faulty outputs. Further, our trace overlapping heuristic decreases the total debugging time by 14%, and our test memoization optimization provides up to 26% decrease in debugging time. Indeed, the total debugging time taken byBigSift is on average 62% less than the original job running time per",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "is on average 62% less than the original job running time per single faulty output. ACKNOWLEDGMENT We would like to thank Tyson Condie and Matteo Interlandi with their insights in the design of BigSift optimizations. Participants in this project are in part supported through AFRL grant FA8750- 15-2-0075, NSF grants CCF-1764077, CCF-1527923, CCF-1460325, CCF-1723773, ONR grant N00014-18-1-2037, and gifts from Google and Huawei. REFERENCES [1]Muhammad Ali Gulzar, Matteo Interlandi, Xueyuan Han, Mingda Li, Tyson Condie, and Miryung Kim. 2017. Automated Debugging in Data-intensive Scal- able Computing. In Proceedings of the 2017 Symposium on Cloud Computing (SoCC ’17). ACM, New York, NY, USA, 520–534. https://doi.org/10.1145/3127479.3131624 [2]Muhammad Ali Gulzar, Matteo Interlandi, Seunghyun Yoo, Sai Deep Tetali, Tyson Condie, Todd Millstein, and Miryung Kim. 2016. BigDebug: Debugging Prim- itives for Interactive",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "Todd Millstein, and Miryung Kim. 2016. BigDebug: Debugging Prim- itives for Interactive Big Data Processing in Spark. In Proceedings of the 38th International Conference on Software Engineering (ICSE ’16) . ACM, New York, NY, USA, 784–795. https://doi.org/10.1145/2884781.2884813 [3]Robert Ikeda, Hyunjung Park, and Jennifer Widom. 2011. Provenance for gen- eralized map and reduce workflows. In In Proc. Conference on Innovative Data Systems Research (CIDR) . [4]Matteo Interlandi, Kshitij Shah, Sai Deep Tetali, Muhammad Ali Gulzar, Se- unghyun Yoo, Miryung Kim, Todd Millstein, and Tyson Condie. 2015. Titian: Data Provenance Support in Spark. Proc. VLDB Endow. 9, 3 (Nov. 2015), 216–227. https://doi.org/10.14778/2850583.2850595 [5]Daniel Lemire, Gregory Ssi-Yan-Kai, and Owen Kaser. 2016. Consistently Faster and Smaller Compressed Bitmaps with Roaring. Softw. Pract. Exper. 46, 11 (Nov. 2016), 1547–1569. https://doi.org/10.1002/spe.2402 [6]Dionysios Logothetis, Soumyarupa",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "Softw. Pract. Exper. 46, 11 (Nov. 2016), 1547–1569. https://doi.org/10.1002/spe.2402 [6]Dionysios Logothetis, Soumyarupa De, and Kenneth Yocum. 2013. Scalable lineage capture for debugging DISC analytics. In Proceedings of the 4th annual Symposium on Cloud Computing . ACM, 17. [7]Ghassan Misherghi and Zhendong Su. 2006. HDD: Hierarchical Delta Debugging. InProceedings of the 28th International Conference on Software Engineering (ICSE ’06). ACM, New York, NY, USA, 142–151. https://doi.org/10.1145/1134285.1134307 [8]Xiaolan Wang, Xin Luna Dong, and Alexandra Meliou. 2015. Data X-Ray: A Diag- nostic Tool for Data Errors. In Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (SIGMOD ’15) . ACM, New York, NY, USA, 1231–1245. https://doi.org/10.1145/2723372.2750549 [9]Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2012. Re-",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "Murphy McCauley, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2012. Re- silient Distributed Datasets: A Fault-tolerant Abstraction for In-memory Cluster Computing. In Proceedings of the 9th USENIX Conference on Networked Systems Design and Implementation (NSDI’12) . USENIX Association, Berkeley, CA, USA, 2–2. http://dl.acm.org/citation.cfm?id=2228298.2228301 [10] Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and isolating failure- inducing input. Software Engineering, IEEE Transactions on 28, 2 (2002), 183–200. 866",
        "start_idx": 3596,
        "end_idx": 3663
      }
    ],
    "ac59e517-f93a-4192-9fe3-efa1ff711315": [
      {
        "text": "[Página 1] Implementation of Big Data Analytics for Machine Le arning Model Using Hadoop and Spark Environment on Resizing Iris Dataset 1st Tresna Maulana Fahrudin Department of Data Science Universitas Pembangunan Nasional “Veteran” Jawa Timur Surabaya, Indonesia tresna.maulana.ds@upnjatim.ac.id2nd Prismahardi Aji Riyantoko Department of Data Science Universitas Pembangunan Nasional “Veteran” Jawa Timur Surabaya, Indonesia prismahardi.aji.ds@upnjatim.ac.id3rd Kartika Maulida Hindrayani Department of Data Science Universitas Pembangunan Nasional “Veteran” Jawa Timur Surabaya, Indonesia kartika.maulida.ds@upnjatim.ac.id Abstract — The concept of Big Data to refer to huge volumes of data and attributes, but data samples through the use of a diverse set of features gathered from various sources. A significant amount of time is spent constructing a pre-processing workflow and an analysis process that make possible impactful for machine learning. Big Data analytics is",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "process that make possible impactful for machine learning. Big Data analytics is being driven by the need to process Machine Learning data, actual real-time processing, and graphics processing. Hadoop and Spark, both accessible data warehousing frameworks that allow for the distribution and computation of massive datasets across several clusters of computer nodes, are the most efficient prospects for Big Data analysis in a distributed setting. To test the ability of these Big Data Tools, this research use Iris dataset as experimental data which is resized to a larger file. Multinomial Naive Bayes algorithm was employed to create a classification model for Iris flowers using Spark Machine Learning Library. The experimental result reported that there is a difference in accuracy and execution time during testing machine learning performance in",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "difference in accuracy and execution time during testing machine learning performance in Hadoop. The experiment given the best performance used Iris dataset is resized to 148 MB consisting of 5,184,000 samples, the model accuracy reached 95.32% with an execution time of 1 minute 4 seconds. The increase in the number of samples in the dataset is also positively correlated with increasing execution time. However, execution time is relatively cheap in the Hadoop Environment. Keywords—big data analytics, machine learning, hadoop, spark, iris dataset I. INTRODUCTION Big Data is directly related to an increase in the peak in various data streams as new technologies are gradually deployed. Knowledge is increasingly widely obtainable than it ever has been, thanks to the rise of the internet, and the consumption of social platforms,",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "to the rise of the internet, and the consumption of social platforms, phone app, connected, and demodulated things is increasing at an alarming rate [1]. Big Data Analytics are methods for analyzing and developing Big Data in the context of strategic planning. Data mining is a subset of big data analysis that seeks to discover the relationship between previously unknown aspects of a dataset by employing a variety of field approaches such as machine learning algorithm, database, statistical method, and mathematics formulations [2]. Data analytics approach provides both granted and interpreted technology in a variety of domains for future predictions. [3][4]. One of the most critical data volumes is the ability to handle a large amount of complex content from an increasing number of different and autonomous sources.",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "of complex content from an increasing number of different and autonomous sources. Quite a companies were using the concept \"Big Data\" to refer to huge volumes of data and attributes, but data samples through the use of a diverse set of features gathered from various sources have also been referred to as Big data [5][6]. A significant amount of time is spent constructing a pre- processing workflow and an analysis process that make possible impactful for machine learning. Data pre-processing identifies an issues such as data redundancy, instability, noise, variability, difficulty, unsupervised machine learning, and transformation. Human knowledge is widely used especially for data pre-processing and planning, in addition to the availability of a variety of alternative methods. Sophisticated data interpretations do not apply to large-scale datasets, rendering",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "alternative methods. Sophisticated data interpretations do not apply to large-scale datasets, rendering processing ineffective. As a result, massive amounts of data identify the opportunity to reduce reliance on human insight by drawing from larger, more complicated and difficult, and occasionally improved data sets [7]. The potential for heterogeneous data to be used to amount of coverage machine learning techniques to unique varieties of market opportunities and behaviors also seems to be greater than ever before, but their legitimacy is mostly questioned [8][9]. According to the source, huge data provides an exceptional amount of informative depth, but traditional machine learning is hindered by the enormous number of variables. All are getting bigger and more intricate, necessitating extensive research and advances in machine learning [10]. Because learning algorithms are incredibly",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "research and advances in machine learning [10]. Because learning algorithms are incredibly strong and can conduct continuous learning, which reduces the need for human contact, machine learning will swiftly displace multiple people's employment in the future [11]. Big Data analytics is being driven by the need to process Machine Learning data, actual real-time processing, and graphics processing. Hadoop and Spark, both accessible data warehousing frameworks that allow for the distribution and computation of massive datasets across several clusters of computer nodes, are the most efficient prospects for Big Data analysis in a distributed setting [12]. Hadoop, the main software that forms the basis of an ecosystem consisting of software that works together. Primarily, as a system for processing very large volumes of data. Besides Hadoop, there are Hadoop",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "for processing very large volumes of data. Besides Hadoop, there are Hadoop Distributed File System (HDFS) that provide high throughput access to application data. Apache Spark is useful as an open-source unified analytics engine for large-scale data processing. Therefore, the research presents the performance of Hadoop and Spark as Big Data analytic environment. We use Hadoop and Spark to solve big data analytics and employee the machine learning model using Multinomial Naïve Bayes to solve classification task on resizing Iris dataset. The standard size of benchmark Iris dataset is in kilobytes. However, the dataset is resized to up to megabytes in the experiment. The purpose is to test the performance of the big data environment and to evaluate the accuracy, execution time of different file sizes and validation",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "to evaluate the accuracy, execution time of different file sizes and validation sampling. 2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS) 978-1-6654-7327-9/22/$31.00 ©2022 IEEE 4292022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS) | 978-1-6654-7327-9/22/$31.00 ©2022 IEEE | DOI: 10.1109/ICIMCIS56303.2022.10017465 Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore. Restrictions apply. [Página 2] II. RELATED WO RKS Despite the fact that several applications are attempting to run Big data including a variety of existing approaches that managed datasets, in this section two, the applications that were chosen to be reviewed and determined the conclusions by authors in recent years, more or less every research in below for one implementation from big data, machine",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "less every research in below for one implementation from big data, machine learning, Hadoop, and Spark, which also implementing several algorithms that performed inside applications to represent the obvious impact of these conceptual methods. Ilham Kusuma, et al. focused on the effectiveness of smart K-means throughout the environment of Hadoop using Spark [13]. The idea behind using Spark to develop smart K-means is that many types of data, such as genomes, are relatively large and continue to grow in size, making it relatively simple to expand the Hadoop environment. The open-source design that aids various computations in both Map-reduce and Hadoop. In the Big Data mining process, the spark design is scalable. In place of the standard Resilient Distributed Dataset, the design included data batching (RDD). Using the",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "standard Resilient Distributed Dataset, the design included data batching (RDD). Using the first data RDD, compare its specification to the implementation. According to experience, data batch implementation is faster than first RDD implementation. Anjuman Prabhat and Vikas Khullar employed the method of machine learning classifiers using Naïve Bayes and logistics-type were utilized in this work to cope with mission challenges that included Twitter comments [14]. They also included Hadoop and Mahout in the classifier. To improve the efficiency of the experiment, an extra module for instance the observation controller is inserted. A further examination of logistics regression analysis yields 10.1% and delivers 4.34% more accuracy for the same dataset scale (sample size). This article contains some supplementary language that describes tweets as a potential future job. This may",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "supplementary language that describes tweets as a potential future job. This may boost categorization performance by combining text and graphics. Bi-gram, trigram, and other formalized forms may be more accurate. TABLE I. THE COMPARISON BETWEEN RELATED WORKS AND PROPOSED WORK IN BIG DATA RESEARCH No . Author(s) Dataset Method and Tools Evaluation 1. I. Kusuma, et al. [13]  First dataset has ch aracteristi c of 5 features and each feature has 5 peaks  The second dataset has. characteristi c of 10 features and it is created from 10 different centroids  K-Means Based on Sp ark for Big Data Clustering  Cluster has 4 slave node and one master node  Every node utilizes with Intel Core i7 and RAM 32 GB • Speed up c",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "with Intel Core i7 and RAM 32 GB • Speed up c omputational time in big data problem reach 58.4- 3075.2 seconds • Higher silhouette value than original k- means using synthetic data reach 0.628 - 0.7476 2. A. Prabhat, V. Khullar. [1 4]  Real t ime twitter with tw o categories: positive and negative reviews (6 MB)  Naïve Bayes and Lo gistic Regression  Hadoop 2.7.1 and Mahout 0.9  Single node with Intel • Accuracy of N aïve Bayes reach 66.67% and Logistic Regression reach 76.76% • Computationa l time of Naïve Bayes reach No . Author(s) Dataset Method and Tools Evaluation Core i3 and RAM 4 GB 15732 mile - seconds and Lo gistic Regression reach 3689 mile-seconds 3. I. R. Prabaswar a,",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "and Lo gistic Regression reach 3689 mile-seconds 3. I. R. Prabaswar a, R. Saputra. [15]  Mapping of dengue fever in cidence based on twitter data in Southeast Asia (4.056.690 tweets)  Visualizatio n of dengue fe ver  Hadoop 3.1.2 and Spark 2.4.0  Cluster has one slave node and one master node  Every node with Intel Core i7, RAM 16 GB in master- node and 8 GB in slave- node • The minimum e xecution time reach 5,3 minutes • The optimal allocation of memory is 3 GB and maximum memory scheduler is 4 GB 4. V. Suriya Narayanan , et al. [16]  Protein interaction pr oblem using graphs and its semantic representatio n  Large scale distributed gr aphs using Apache Spark ",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "representatio n  Large scale distributed gr aphs using Apache Spark  Word2Vec language for model vocabulary • Achieved a pproximatel y 97% accuracy using a 128- dimensional embedding as compared to around 95% using a 2- dimensional embedding 5. T. M. Fahrudin, et al. (Proposed Research) • Resizing I ris Dataset in 5 KB - 148 MB  Apache Hadoop 3. 2.1  Apache Spark 3.0.0  Machine Learning Model using Multinomia l Naïve Bayes  Single node with Intel Core i3 and RAM 4 GB • Accuracy a nd execution time of building classification model based on resizing Iris dataset (5 KB – 148 MB) Ir wan Rizqi Prabaswara and Ragil Saputra carried out research about trend mapping on the fever data Dengue Hemorrhagic Fever (DHF)",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "research about trend mapping on the fever data Dengue Hemorrhagic Fever (DHF) from social media twitter. This research desires to construct a visualization of data obtained from twitter with using Hadoop and spark in tracking the growth of dengue in the Southeast Asia area [15]. The findings of trend mapping reveal that there is a substantial association between twitter data and the original data on dengue incidence collected from WHO. This research also investigated the performance of Hadoop and Spark. The larger the memory 2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS) 430Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore. Restrictions apply. [Página 3] allocation of the executor that is applied and the larger",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "[Página 3] allocation of the executor that is applied and the larger and similar the maximum allocation of the memory scheduler applied to each node, the shorter the time required to complete the task. However, at some point Hadoop and Spark configurations hit a breaking point, so if the allocation is increased it produces the same result. In the other works, V. Suriya Narayanan, et al. demonstrated how and when to use Apache Spark to implement a distributed learning algorithm that operates through large graphs [16]. It is technologically applicable making it easy and applicable for large-scale deployments, and it has a fairly clear set of powerful predictive capabilities. It is advantageous for graph-based developers to use comprehensible word embedding as physical world focuses. The use of the",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "use comprehensible word embedding as physical world focuses. The use of the Spark design and optimization for the efficient loading of distributed edge records and in-graph frameworks. The application Word2Vec then used Spark (MLlib) fetch node interconnection with a random walk according to each graph node. Excellent results have been achieved. Typically, 70% of the data could be used to visualize and reinforce learning. However, this approach uses only 1% of the dataset to training the models. By implementing this model, they help in achieving a prediction precision of close to 95%. Table I show the comparison between related works and proposed works in Big Data research. Big Data tools and methods used in the research commonly are Hadoop, Spark, and Mahout. However, what makes the difference is",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "commonly are Hadoop, Spark, and Mahout. However, what makes the difference is the dataset tested using the Iris dataset which has been resized to a larger size. We test the performance of big data when faced with exponentially growing data sizes. Then, we evaluate the accuracy and execution time. III. SYSTEM DESIGN I n this chapter, the system design implemented in the proposed research will be explained. It will be discussed regarding Iris dataset, Hadoop, Hadoop Distributed File System (HDFS), Spark and MLlib, Multinomial Naïve Bayes, and the evaluation model. Fig.1 show the system design of proposed research. Fig. 1. S ytem design of proposed research A. I ris Dataset The Iris dataset is a popular dataset used in the learning and experiment about data science. The dataset",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "dataset used in the learning and experiment about data science. The dataset stores tabular data about flower such as sepal length and width, petal length and width. Types of flowers are classified into three categories namely Setosa, Versicolour, and Virginica. The dataset was chosen because it is benchmark dataset that has been widely used and popular. The type of the features is continuous, while the categories are discrete. The number of instances on Iris dataset is 150 which each category is proportional to 30 instances. Almost all studies using iris datasets by researcher achieve good accuracy above 90%. B. Hadoop Hadoop is a big data technology product of the open- source Apache software [17]. The function of Hadoop is to solve the problem of large amounts of data",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "of Hadoop is to solve the problem of large amounts of data and computing with a set of computer networks. Hadoop has an architecture with three components, namely HDFS (Hadoop Distributed File System), MapReduce, and YARN. The characteristics of Hadoop include: • Hadoop is optimally used to handle large amounts of structured, semi-structured, and unstructured dataset. • Hadoop replicates data across multiple computers (clustering). If one computer has a problem, the data can be processed from one of the other computers that are still alive. • The Hadoop process is a batch operation handling a very large amount of data, so the response time is not real time. C. Hadoop Distributed File System Hadoop Distributed File System (HDFS) is part of Hadoop that functions as a consistent data",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "System (HDFS) is part of Hadoop that functions as a consistent data storage place [18]. An important process in HDFS is data replication to different partitions by massive and parallel. The replication is balanced in different blocks. Distributed file systems designed for fault- tolerant file systems can run on multiple servers with low-cost specifications. HDFS is designed to support applications with large data sets, even terabytes of files. When a file is processed via HDFS, it is split into smaller parts and then the smaller part of the file is distributed across multiple nodes in the cluster system thus enabling parallel processing. D. Spark and MLlib Spark is an open-source framework that is suitable for use in iterative algorithmic processes [19]. Spark allows connecting analytics engines with high-scale",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "in iterative algorithmic processes [19]. Spark allows connecting analytics engines with high-scale data processing. MLlib is part of Spark with Machine Learning libraries. The goal is to make machine learning easier and more scalable. MLlib has many uses including regression, classification, clustering, can perform linear and statistical algebraic calculations and handle pipelines. E. Multinomial Naïve Bayes Multinomial Naïve Bayes event in the model is a multinomial vector \u0001\u0002\u0003, … . , \u0002 \u0007\b where \u0002\u0003 is the probability t hat event i will occur. Vector \u0001 , … . , \u0007\b in the form of a histogram. \u0003 is the total number of events occurring within a c ertain range. The Multinomial Naïve Bayes formula follows equation 1. p \u0001x | \u000f\u0010 \b \u0011\u0012∑ \u0014\u0015\u0016 \u0017 \u0018\u0019\u001a! ∏",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "equation 1. p \u0001x | \u000f\u0010 \b \u0011\u0012∑ \u0014\u0015\u0016 \u0017 \u0018\u0019\u001a! ∏ \u0014\u0015!\u0016 \u0017 \u0019 ∏ \u0002 \u0014\u0017 \u0007 \u0003 (1) F . Evaluation Model The evaluation of the model that will be used is accuracy and execution time. Accuracy is how accurate the predictions made by the model are with the actual predictions. The accuracy formula follows equation 2. CR \u0011# $ (2) Where: CR : The correct rate C : The number of samples recognized correctly 2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS) 431Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore. Restrictions apply. [Página 4] A : The number of all samples. While the execution time is recorded from start time",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "of all samples. While the execution time is recorded from start time to end time during the running code script program. The execution time formula follows equation 3. Execution time \u0011 ./0 1 23 4 51671 1 23 (3) IV. EXPER IMENTAL RESULT AND DISCUSSION I n this section will discussed about dataset preparation, configuration Hadoop and Spark environment, building model of Multinomial Naïve Bayes, evaluation model and execution time. A. Dataset Preparation Iris dataset consists of 4 features including Sepal Length (cm), Sepal Width (cm), Petal Length (cm), and Petal Width (cm), while 3 species as class label including Iris-setosa, Iris- versicolor, and Iris-virginica which the total number of samples for each class is 50 proportionally. If the experiment uses an iris dataset of 150 samples (5",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "proportionally. If the experiment uses an iris dataset of 150 samples (5 kb) to test machine learning models into a big data environment, this will certainly not have much impact. Therefore, the iris dataset in this experiment was resized to 72 MB of 2,592,000 samples and 145 MB of 5,184,000 samples. Fig. 2 show the resizing samples of Iris dataset. Fig. 2. R esizing samples of Iris dataset B. C onfiguration Hadoop and Spark Environment Before implementing big data analytics, there are several tools and software that need to be prepared. The following are the specifications of the device used in the experiment: • Processor : Intel core i3 • RAM: 4.00 GB • System type: 64-bit Operating System, x64-based processor • OS: Windows 10 Home Single Language",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "64-bit Operating System, x64-based processor • OS: Windows 10 Home Single Language While the specifications of the software used in the experiment as follow: • Oracle VM VirtualBox Manager • Ubuntu 64-bit: Virtual hard disk 15 GB • Open Java Development Kit (OpenJDK) • Apache Hadoop 3.2.1 (stable version) • Apache Spark 3.0.0 (stable version) To install and configure Hadoop on Ubuntu, first step is install Java via Software Development Kit Manager (SDKMAN), check the path $JAVA_HOME, and then use JDK version 8.0.242.hs-adpt. Unzipped Apache Hadoop 3.2.1 (file extension *.tar.gz) and will extract several files such as start-all.sh, stop-yarn.sh, workers.sh, start-dfs.sh, httpfs.sh, stop-balancer.sh, and etc. Hadoop directory needs to have permissions set to change the ownership of its username by using the command “chown username:username -R name_directory”. Bash",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "of its username by using the command “chown username:username -R name_directory”. Bash shell script (~/.bashrc) also set Hadoop home directory with a variable named HADOOP_HOME and set the binary files in Hadoop home directory which is located in HADOOP_HOME/bin. To check Hadoop configuration is running well or not through the command \"hadoop version\". Master files such as hadoop-env.sh also need to be set the Java home directory in path so that Yarn, HDFS, MapReduce, and others can run correctly. To install and configure Spark on Ubuntu, unzipped Apache Spark 3.0.0 (file extension *.tar.gz) and will extract several libraries such as Machine Learning Library (MLlib), R, Kubernetes, and etc. Spark directory needs to have permissions set to change the ownership of its username. Bash shell script (~/.bashrc) also set",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "change the ownership of its username. Bash shell script (~/.bashrc) also set Spark home directory with a variable named SPARK_HOME and set the binary files in Spark home directory which is located in SPARK_HOME/bin. To check Spark configuration is running well or not through the command \"spark-shell --version\". After installing Hadoop and Spark, the next step is Hadoop Distributed File System (HDFS) configuration. There are several files that must be configured in the /opt/hadoop/etc/hadoop/ directory, including: • core-site.xml : set the default file system, localhost address and port • hdfs-site.xml : set directory locations of name node and data node • mapred-site.xml : set the MapReduce framework name • yarn-site.xml : set manage node manager and handling the shuffle on MapReduce The final step is formatting HDFS, make",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "handling the shuffle on MapReduce The final step is formatting HDFS, make sure there is no important data in HDFS because the data will be deleted. Formatting HDFS with command \"hdfs namenode -format - force\". Then boot HDFS with the commands “start-dfs.sh && start-yarn.sh”. To check HDFS is running correctly or not use the command “jps”, the terminal shows the information the currently active services such as the Java Virtual Machine Process Status Tool (JPS), ResourceManager, NodeManager, DataNode, NameNode, and SecondaryNameNode. Hadoop provides monitoring dashboards for cluster node metrics, scheduler metrics, nodes, datanodes, startup progress and more as shown in Fig. 3. 2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS) 432Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore. Restrictions apply. [Página 5] Fig. 3. C luster metrics dashboard monitoring on Hadoop While to start Spark by using the command \"spark-shell\" a s shown in Fig. 4. Fig. 4. S park-shell on Spark 3.0.0 After Hadoop and Spark are properly configured, machine l earning can be implemented. C. Building Model of Multinomial Naïve Bayes Spark provides alternative programming languages to implement Machine Learning such as Scala, Java, and Python The experiment used Scala programming language and importing the classification model (supervised learning) from MLlib such as Multinomial Naive Bayes algorithm. The Iris dataset is loaded into Scala, then splitting the dataset is 60% for training data and 40% for testing data.",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "the dataset is 60% for training data and 40% for testing data. The performance evaluation of classification model based on accuracy matrix and execution time. Fig. 5 show the running Multinomial Naïve Bayes model using Scala on Spark, the program running the code script execution until the stage completely. Fig. 5. R unning Multinomial Naïve Bayes Model using Scala on Spark D. E valuation Model and Execution Time TABLE II. THE EXPERIMENTAL RESULT OF BIG DATA ANALYTICS FO R MACHINE LEARNING MODEL USING MULTINOMIAL NAÏVE BAYES IN HADOOP AND SPARK ENVIRONMENT ON RESIZING IRIS DATASET N o. Size Number of Samples Validation Sampling Accuracy Execution Time 1. 5 KB 150 Pecentage Split (60:40) 65% 1.27 seconds 2. 8 MB 199,729 Pecentage Split (60:40) 96.04% 2.52 seconds 3. 14",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "2. 8 MB 199,729 Pecentage Split (60:40) 96.04% 2.52 seconds 3. 14 MB 399,458 Pecentage Split (60:40) 95.31% 5.29 seconds 4. 27 MB 798,916 Pecentage Split (60:40) 95.27% 10.95 seconds 5. 87 MB 2,591,848 Pecentage Split (60:40) 95.31% 38.16 seconds 6. 148 MB 5,184,000 Pecentage Split (60:40) 95.32% 1 minute 4 seconds Table II show when the number of samples in Iris dataset was resized from KB to MB, there is a difference in accuracy and execution time during testing machine learning performance in Hadoop. If the test used the original iris dataset of 5 KB consisting of 150 samples, the accuracy of the Multinomial Naïve Bayes model only reached 65% with an execution time of 3.66 seconds. On other hand, the test used Iris dataset that has",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "3.66 seconds. On other hand, the test used Iris dataset that has been resized to 87 MB consisting of 2,592,848 samples, the accuracy of the model reached 95.31% with an execution time of 38,16 seconds. If the Iris dataset is resized to 148 MB consisting of 5,184,000 samples, the model accuracy reached 95.32% with an execution time of 1 minute 4 seconds. Table III show the performance of Multinomial Logistic Regression with the same size file, the number of samples, and validation sampling. TABLE III. THE EXPERIMENTAL RESULT OF BIG DATA ANALYTICS FO R MACHINE LEARNING MODEL USING MULTINOMIAL LOGISTIC REGRESSION IN HADOOP AND SPARK ENVIRONMENT ON RESIZING IRIS DATASET N o. Size Number of Samples Validation Sampling Accuracy Execution Time 1. 5 KB 150 Pecentage Split (60:40)",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "Validation Sampling Accuracy Execution Time 1. 5 KB 150 Pecentage Split (60:40) 90% 3.66 seconds 2. 8 MB 199,729 Pecentage Split (60:40) 98.68% 12.29 seconds 3. 14 MB 399,458 Pecentage Split (60:40) 98.66% 25,73 seconds 4. 27 MB 798,916 Pecentage Split (60:40) 98.65% 55.78 seconds 5. 87 MB 2,591,848 Pecentage Split (60:40) 98.66% 2 minutes 9 seconds 6. 148 MB 5,184,000 Pecentage Split (60:40) 98,65% 4 minutes 11 seconds Figure 6 and Figure 7 show that the Multinomial Logistic Regression has the highest accuracy, but is also followed by high execution time consumption, while the Naïve Bayes 2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS) 433Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore. Restrictions",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore. Restrictions apply. [Página 6] Multinomial has the lowest execution time consumption, but reached a fairly good accuracy. Fig. 6. T he Comparison of Execution Time Between Multinomial Naïve Bayes and Multinomial Logistic Regression on Big Data Environment Fig. 7. T he Comparison of Accuracy Between Multinomial Naïve Bayes and Multinomial Logistic Regression on Big Data Environment The experimental results indicates that with the increase in t he number of samples in the dataset is also positively correlated with increasing execution time. However, execution time is relatively cheap in the Hadoop Environment. The evaluation regarding the increase in accuracy needs to be investigated further because the Iris dataset is a benchmark dataset consisting of only 150 samples, while",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "Iris dataset is a benchmark dataset consisting of only 150 samples, while the iris dataset is resized in this experiment which allows the sample to be duplicated. V. CONCLUSION T he implementations of big data analytics using Hadoop and Spark are configured and running well on resizing Iris dataset. Iris dataset load in Hadoop environment through Hadoop Distributed File System, while Spark provide several algorithms for classification, clustering, and regression. The experiment employed Multinomial Naïve Bayes to classify the Iris dataset. The experimental result reported that there is a difference in accuracy and execution time during testing machine learning performance in Hadoop. The experiment given the best performance used Iris dataset is resized to 148 MB consisting of 5,184,000 samples, the model accuracy reached 95.32% with an execution",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "consisting of 5,184,000 samples, the model accuracy reached 95.32% with an execution time of 1 minute 4 seconds. The increase in the number of samples in the dataset is also positively correlated with increasing execution time. However, execution time is relatively cheap in the Hadoop Environment. Further research needs to upgrade the device to experiment with a larger processor and RAM and the number of datasets that reach GB. REFERENCES [1] B . Zerhari, A. A. Lahcen, and S. Mouline, \"Big Data Clustering: Algorithms and Challenges,\" in Proceedings of the International Conference on Big Data, Cloud, and Applications (BDCA’15), pp. 1- 6, 2015. [2] S. W. Kareem, \"Secure Cloud Approach Based on Okamoto-Uchiyama Cryptosystem,\" Journal of Applied Computer Science and Mathematics, vol. 14, no. 29, pp. 9-13, 2020.",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "Applied Computer Science and Mathematics, vol. 14, no. 29, pp. 9-13, 2020. [3] H. B. Patel and S. Gandhi, \"A Review on Big Data Analytics in Healthcare using Machine Learning Approaches,\" in 2018 2nd International Conference on Trends in Electronics and Informatics (ICOEI), pp. 84-90, 2018. [4] S. Suthaharan, \"Machine Learning Models and Algorithms for Big Data Classification,\" Integrated Series in Information Systems, vol. 36, pp. 1-12, 2016. [5] V. Ajin and L. D. Kumar, \"Big Data and Clustering Algorithms,\" in 2016 International Conference on Research Advances in Integrated Navigation Systems (RAINS), pp. 1-5, 2016. [6] S. W. Kareem, R. Z. Yousif, S. M. J. Abdalwahid, and C. Science, \"An Approach for Enhancing Data Confidentiality in Hadoop,\" Indonesian Journal of Electrical Engineering and Computer Science, vol. 20, no.",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "Hadoop,\" Indonesian Journal of Electrical Engineering and Computer Science, vol. 20, no. 3, pp. 1547-1555, 2020. [7] A. L’heureux, K. Grolinger, H. F. Elyamany, and M. A. Capretz, \"Machine Learning with Big Data: Challenges and Approaches,\" Ieee Access, vol. 5, pp. 7776-7797, 2017. [8] E. Hossain, I. Khan, F. Un-Noor, S. S. Sikander, and M. S. H. Sunny, \"Application of Big Data and Machine Learning in Smart Grid, and Associated Security Concerns: A Review,\" IEEE Access, vol. 7, pp. 13960- 13988, 2019. [9] M. Mohammadi, A. Al-Fuqaha, S. Sorour, M. Guizani, \"Deep Learning for IoT Big Data and Streaming Analytics: A Survey,\" IEEE Communications Surveys and Tutorials, vol. 20, no. 4, pp. 2923-2960, 2018. [10] L. Zhou, S. Pan, J. Wang, and A. V. Vasilakos, \"Machine learning on",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "Zhou, S. Pan, J. Wang, and A. V. Vasilakos, \"Machine learning on Big Data: Opportunities and Challenges,\" Neurocomputing, vol. 237, pp. 350- 361, 2017. [11] H. K. Tripathy, B. R. Acharya, R. Kumar, and J. M. Chatterjee, \"Machine Learning on Big Data: A Developmental Approach on Societal Applications,\" in Big Data Processing Using Spark in Cloud: Springer, pp. 143- 165, 2019. [12] Benlachmi, Y., Yazidi, A.E., Hasnaoui, M.L.“A Comparative Analysis of Hadoop and Spark Frameworks Using Word Count Algorithm”. International Journal of Advanced Computer Science and Applications. Vol. 12, No. 4, pp. 778-788, 2021. [13] I. Kusuma, M. A. Ma'Sum, N. Habibie, W. Jatmiko, and H. Suhartanto, \"Design of Intelligent K-Means based on Spark for Big Data Clustering,\" in 2016 International Workshop on Big Data and Information Security",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "Data Clustering,\" in 2016 International Workshop on Big Data and Information Security (IWBIS), pp. 89-96, 2016. [14] A. Prabhat and V. Khullar, \"Sentiment Classification on Big Data using Naïve Bayes and Logistic Regression,\" in 2017 International Conference on Computer Communication and Informatics (ICCCI), pp. 1-5, 2017. [15] Prabaswara, I.R., Saputra, R. “Implementation of Hadoop and Spark for Analysis of The Spread of Dengue Hemorrhagic Fever based on Twitter Data,” in IT Journal Research and Development (ITJRD), vol. 4, no. 2, pp. 164-171, 2020. [16] V. S. Narayanan, V. B. Vijayakumar, S. R. Venkatraman, and P. K. Baruah, \"Semantic Node Embeddings of Distributed Graphs using Apache Spark,\" in 2016 Fourth International Conference on Parallel, Distributed and Grid Computing (PDGC), pp. 709-713, 2016. [17] O. Azeroual and R. Fabre, “Processing",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "Computing (PDGC), pp. 709-713, 2016. [17] O. Azeroual and R. Fabre, “Processing Big Data with Apache Hadoop in The Current Challenging Era of COVID-19,” Big Data Cognitive and Computing: MDPI. , vol. 5, no. 1, pp.1-18, 2021. [18] D. Veeraiah and J. N. Rao, “An Efficient Data Duplication System based on Hadoop Distributed File System,” in 2020 International Conference on Inventive Computation Technologies (ICICT) , pp. 197– 200, 2020. [19] A. Mostafaeipour, A. Jahangard Rafsanjani, M. Ahmadi, and J. Arockia Dhanraj, “Investigating The Performance of Hadoop and Spark Platforms on Machine Learning Algorithms,” The Journal of Supercomputing. , vol. 77, no. 2, pp. 1273–1300, 2021. 2022 International Conference on Informatics, Multimedia, Cyber and Information System (ICIMCIS) 434Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "434Authorized licensed use limited to: UNIVERSIDADE ESTADUAL DO CEARA. Downloaded on October 08,2025 at 13:05:11 UTC from IEEE Xplore. Restrictions apply.",
        "start_idx": 4988,
        "end_idx": 5009
      }
    ],
    "4d12ebf2-2b41-4f6f-b49e-9558d6004dc6": [
      {
        "text": "[Página 1] Investigating the adoption of big data analytics in healthcare: the moderating role of resistance to change Muhammad Shahbaz* , Changyuan Gao*, LiLi Zhai, Fakhar Shahzad and Yanling Hu Introduction Big data analytics (BDA) is a course of action to examine large and complex data sets (i.e., big data) and select veiled information that can help organizations with efficient decision making [ 1]. The volume of data related to healthcare organizations has grown dramatically in past years and is expected to increase in coming years due to the use of innovative technologies [ 2]. Meanwhile, healthcare reimbursement methods are chang - ing, and pay for performance is an emerging factor in the current healthcare environ - ment. Recently, healthcare organizations have only focused on profit and have",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "- ment. Recently, healthcare organizations have only focused on profit and have neglected to acquire the essential tools, infrastructure, and technologies for effective control of big data to ensure citizens’ health care [ 3, 4]. Big data incorporates features such as variety, velocity, and veracity. BDA techniques can be applied to the massive amount of prevail - ing patient-related medical information to analyze outcomes for improvement of the healthcare sector [ 5, 6]. Using BDA in the healthcare sector will help inform each physi - cian of the medical histories of individuals and the population and enable appropriate Abstract Big data analytics is gaining substantial attention due to its innovative contribution to decision making and strategic development across the healthcare field. Therefore, this study explored the adoption mechanism",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "development across the healthcare field. Therefore, this study explored the adoption mechanism of big data analytics in healthcare organiza- tions to inspect elements correlated to behavioral intention using the technology acceptance model and task-technology fit paradigm. Using a survey questionnaire, we analyzed 224 valid responses in AMOS v21 to test the hypotheses. Our results posit that the credentials of the technology acceptance model together with task-technology fit contribute substantially to the enhancement of behavioral intentions to use the big data analytics system in healthcare, ultimately leading towards actual use. Meanwhile, trust in and security of the information system also positively influenced the behavioral intention for use. Employee resistance to change is a key factor underlying failure of the innovative system in organizations and has been proven in this",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "of the innovative system in organizations and has been proven in this study to nega- tively moderate the relationship between intention to use and actual use of big data analytics in healthcare. Our results can be implemented by healthcare organizations to develop an understanding of the implementation of big data analytics and to promote psychological empowerment of employees to accept this innovative system. Keywords: Big data analytics, Healthcare, TrustOpen Access © The Author(s) 2019. This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (http://creat iveco mmons .org/licen ses/by/4.0/ ), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "source, provide a link to the Creative Commons license, and indicate if changes were made.RESEARCHShahbaz et al. J Big Data (2019) 6:6 https://doi.org/10.1186/s40537-019-0170-y *Correspondence: shahbaz755@yahoo.com; gaocy2002@126.com School of Economics and Management, Harbin University of Science and Technology, Harbin, China [Página 2] Page 2 of 20 Shahbaz et al. J Big Data (2019) 6:6 decision-making regarding treatment of a particular patient [4]. However, compared with the banking and retailing industries, healthcare organizations have lagged behind in the sophisticated use of BDA [7]. The healthcare industry also strives to make inter - nal developments in BDA implementation based on their quality and data performance, which provides timely feedback to interested parties [8]. Therefore, describing the cru - cial factors that are required for understanding is important prior to creating a",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "factors that are required for understanding is important prior to creating a strategy for the acceptance of BDA in the healthcare industry, particularly in developing coun - tries such as Pakistan, where the industry requires filling the gap of BDA adoption. Fur - thermore, data (i.e., big data) related to healthcare are generated at a very high pace [9], and existing systems are unable to store and analyze the huge volume, velocity and vari - ety of data [10]. Therefore, a need exists for a system with the ability to store and analyze data with high volumes, velocities, and variety, all of which are provided by BDA sys - tems [9]. BDA is in the initial adoption phase, and many healthcare organizations want to implement BDA to obtain",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "adoption phase, and many healthcare organizations want to implement BDA to obtain its benefits [11]. Thus, a comprehensive adoption model related to BDA is needed to fulfill the existing gap in the literature and help healthcare organizations replace traditional systems incapable of competing with BDA systems. Few studies have described the importance of BDA in healthcare [4, 12, 13], although studies have investigated the technological aspects and required qualifications for big data in healthcare [14–18]. Previous studies focused on technological and policy issues and not on adoption factors, such as security, trust, and fitness of technology for the tasks required to manage BDA in healthcare [13]. According to Dishaw [19], the tech - nology acceptance model (TAM) and task-technology fit (TTF) provide better outputs than either TAM or",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "(TAM) and task-technology fit (TTF) provide better outputs than either TAM or TTF alone in the adoption of information technology systems. The prior literature tries to explain BDA adoption through perceptions of technology, such as perceived ease of use and perceived usefulness [20–24]. However, emphasizing only the end user’s perception of technology may not be sufficient. According to Goodhue and Thompson [25], the TTF model claims that the user will adopt the system when the characteristics of the technology fit the task requirements. Adoption will also occur when the user perceives the technology as useful, easy and advanced, but the technol - ogy may not be adopted if a mismatch exists with his required tasks and the technology cannot enhance his job performance [26–29]. Therefore, not only should",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "the technology cannot enhance his job performance [26–29]. Therefore, not only should the user have the perception that the technology is useful and easy but also the technology charac - teristics should match with the required job tasks. Furthermore, the previous literature showed that perceived security of information [30–32] and perceived trust [33–35] were the biggest hurdles for users adopting innovative information systems. Security of infor - mation is the main reason for the slow pace of BDA adoption [36, 37]. Perceived trust is a major concern in the BDA acceptance procedure, and thus organizations should generate more trust in BDA adoption [38]. Prior studies by Malaka, Shin, and Sivarajah [23, 39, 40] also highlighted that perceived security and perceived trust were the biggest challenges and hurdles for",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "perceived security and perceived trust were the biggest challenges and hurdles for BDA acceptance. Resistance to change (RTC) from employees is also a key factor that affects the adoption of different innovative systems, especially in developing countries [41–43]. In previous literature concerning electronic health record system adoption, RTC from physicians was repeatedly reported as a key barrier for sys - tem adoption [44, 45], and RTC of employees mitigated the willingness of those who wanted to adopt the system [46]. RTC also resists or slow down the pace of information [Página 3] Page 3 of 20 Shahbaz et al. J Big Data (2019) 6:6 system acceptance in the health sector [47, 48]. The study considers RTC a key factor in the adoption of BDA in the healthcare sector,",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "a key factor in the adoption of BDA in the healthcare sector, which has never been discussed in this scenario. Despite the fame of BDA, insufficient empirical research has investigated factors that can influence BDA adoption in healthcare [21, 49]. Empirical evidence from Paki - stan’s healthcare organizations represents a big gap in the literature from both dimen - sions (i.e., knowledge about BDA and adoption of BDA) [50]. This study summarizes real facts from Pakistan for the healthcare BDA literature. The gap between the poten - tial pros of BDA and the slow and low geared adoption represents a superior opportu - nity for scholars to realize how BDA can be adopted in the healthcare industry. BDA is in the initial adoption phase in Pakistan, and the",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "industry. BDA is in the initial adoption phase in Pakistan, and the government should develop a clear policy and mechanism for the acceptance of BDA in government and the private sec - tor [50]. Therefore, to bridge this gap in the literature, the major focus of this paper is to provide comprehensive research insights into the adoption of BDA in healthcare. To fulfill said gap, the study has two main objectives. The first objective is to help govern - ment and private healthcare organizations determine the important factors that play key roles in the adoption of BDA in healthcare in developing countries, such as Pakistan. The second objective is to cover the on-hand gap in the literature concerning the influ - ence of RTC from employees for BDA",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "literature concerning the influ - ence of RTC from employees for BDA adoption. To achieve the above-mentioned objec - tives, this study incorporated both TAM and TTF models to explain BDA adoption in the healthcare sector from both viewpoints (the user’s perception of technology and the task-technology fitness) with the most important and substantial factors involved in the adoption of information systems (i.e., perceived security and perceived trust). The study also considers RTC as a moderator in the proposed model to address the most impor - tant hurdle for developing countries, such as Pakistan [33, 41]. The results justified the use of a composite of both TAM and TTF with security and trust as significant predic - tors of behavioral intentions (BIs) to adopt BDA, whereas RTC negatively",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "- tors of behavioral intentions (BIs) to adopt BDA, whereas RTC negatively moderated the relationship between BIs and actual use of BDA. In the next section, we describe the theoretical background and develop a research model for this study to analyze the predictors linked to BDA adoption. The research methods are discussed in section three, and section four provides results from our data analysis using structural equation modeling and discussions. “ Conclusion ” section con - cludes the overall findings. In addition to the research limitations, our study also has theoretical and practical implications, as discussed in “ Conclusion ” section. References are given in “Reference” section. Relevant work and hypotheses During this phase, we underpin the relevant theories and work based on the prior lit - erature",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "the relevant theories and work based on the prior lit - erature regarding the acceptance of a BDA system across various sectors (see Table 1) and produce the research hypotheses for analysis based on the research framework (see Fig. 1). Perceived trust Trust is described as a belief that a person or a particular thing will respond in a helpful way without manipulating the results [60]. Perceived trust is a state of mind in which [Página 4] Page 4 of 20 Shahbaz et al. J Big Data (2019) 6:6 an individual has acquired assurance and confidence in the information provided by the system [61]. Individual expectations towards technology build trust in its use. Typi - cally, trust in use of technology is used to mitigate the uncertainty of",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "trust in use of technology is used to mitigate the uncertainty of mind when a per - son lacks experience and knowledge with using innovated technology and information Table 1 Relevant work Authors Year Important aspects Limitations Esteves and Curto [21] 2013 Predicted behavioral intention to use big data technology by using the theory of planed behavior based on risk and benefits point of viewSmall sample size was used to test the proposed model and insufficient theoretical base provided Mahmood and Afzal [51] 2013 Provided survey on description, tech- nology, trend, and tools of cyber - crime security in Pakistan by using big data analyticsBig data analytics adoption model not provided Tsai et al. [52] 2015 Provided a brief introduction of big data analytics to help in developing",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "Provided a brief introduction of big data analytics to help in developing high performance platform and min- ing algorithm for big data analyticsDid Not predict the behavior of a user regarding use of big data analytics Malaka and Brown [40] 2015 Investigated the adoption of big data analytics in organization prospective by using technology organization environment modelUser centric approach was ignored by the study Archenaa and Anita [53] 2015 Conducted a survey to explore the importance, benefits, and need of big data analytics in healthcare and governmentEmpirical evidence regarding adoption from citizen prospective and security of information was ignored in the study Soon et al. [54] 2016 Demonstrated the big data analytics adoption by using the technology acceptance model and diffusion of innovation model and explored the moderating",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "technology acceptance model and diffusion of innovation model and explored the moderating effects of training in MalaysiaThe scope of the study was restricted to only private organizations which inferred the generalization of the study LaBrie et al. [55] 2017 Provided a comparative study of china and USA to understand the technol-ogy change and big data analytics adoption from a societal perspectiveStudy missed the fit between technol- ogy and cultural dimensions of people Sivarajah et al. [39] 2017 The systematic literature view was performed to identify the challenges in big data analyticsTo develop the link between theories and practice the empirical analysis was not performed Memon et al. [56] 2017 Apache Hadoop open source technol- ogy was used to check the big data analytics application in the healthcare sector",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "used to check the big data analytics application in the healthcare sector of PakistanBig data analytics application from a user’s perspective in the healthcare sector of Pakistan was not provided Brock and Khan [24] 2017 Combined technology acceptance model and organization learning capabilities to explore the factors linked with big data analytics usagePre-implementation assessment for practitioners was not performed considering the user’s perspective in the adoption of big data analytics Weerakkody et al. [57] 2017 To investigate the user’s behavioral intentions of big open data. The study applied extended technology acceptance modelThe study only focused on intention to use only instead of focusing also on actual use of big open data Arunachalam et al. [58] 2018 Provided comprehensive literature view on capabilities of big data analytics to demonstrate",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "Provided comprehensive literature view on capabilities of big data analytics to demonstrate the challenges which help to develop a big data analytics maturity modelA phenomenon of restriction to change in the user perspective was not discussed Gupta et al. [59] 2018 Reviewed big data analytics and pro - vide future research directions of big data analyticsTrust, privacy, and information security can be further explained by utilizing the characteristics of big data and cognitive computing [Página 5] Page 5 of 20 Shahbaz et al. J Big Data (2019) 6:6 systems [35]. Perceived trust is particularly important in the context of BDA, because adoption of a BDA system is a risk. Many studies have proved that perceived trust is a fundamental reason for the success or failure of information system",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "is a fundamental reason for the success or failure of information system adoption, including E-payment adoption [62], online purchasing [63], adoption of crypto currencies [64] and internet banking [65]. Trusting BDA is not the only issue, because mistrusting the capa - bilities of technology to deliver valuable services without interruption and data loss also reduce its adoption intentions [66]. Thus, based on the above cited literature, we assume that perceived trust will also affect BDA adoption. H1 Perceived trust has a significant relationship with BIs to use BDA. Perceived security Perceived security refers to the degree to which a person believes that use of a specific system is safe and sound for transmitting and recording sensitive information [31, 61]. Perceived security of information is an important concern for",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "information [31, 61]. Perceived security of information is an important concern for healthcare, which con - tains sensitive patient information [39]. Perceived security is the factor that restricts user thinking about the benefits of a system and convinces him to use a system that is negatively perceived [40]. In the previous literature, many studies described the value of perceived security for the adoption of different analytical information systems (i.e., cloud computing [67], B2C electronic commerce [68], online markets [69], and elec - tronic health record systems [70]). The use of BDA systems has some reservations in the minds of users regarding information security in the form of informational fraud, misuse of sensitive information and use by various unconcerned departments [71, 72]. Prior lit - erature on BDA and",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "various unconcerned departments [71, 72]. Prior lit - erature on BDA and its adoption has not emphasized security of information sufficiently [72, 73]. Based on the prior literature, one key feature that can affect BDA adoption by healthcare organizations is the security of the analytical tools used to generate useful information. Employees’ perceptions of the security level of technology possibly affect its adoption rate. Organizations that have high-level capabilities for dealing with infor - mation security will possibly develop the intention to use BDA. H2 Perceived security has a significant relationship with BIs to use BDA. Task‑technology fit The effective adoption of an information system relies on identification of the task for which the technology is used and whether a suitable match exists between the task and technology.",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "used and whether a suitable match exists between the task and technology. As described by Goodhue [25], decomposition of TTF investigates the user’s requirements for the information system, which ultimately impact the individual perfor - mance. The study of Lin and Huang [74] defines the task as activities performed to create valued outputs that satisfy human wants. Technology refers to the combination of vari - ous supportive activities to perform such tasks (i.e., computers, software, and others). Prior studies by Benford, D’Ambra and Khan et al. [50, 75] described the various dimen - sions of tasks (e.g., nonroutines, interdependence, data access, and quantitative data analy - sis), which were related to several technical aspects that fulfilled individuals’ needs. Task and technology both have significant impacts on the ability",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "individuals’ needs. Task and technology both have significant impacts on the ability of TTF to estimate users’ [Página 6] Page 6 of 20 Shahbaz et al. J Big Data (2019) 6:6 performances from several perspectives [76]. Innovative information technology and the system will be useless if they fail to satisfy individual requirements for performance of a specific task [27]. Therefore, recognizing the task-technology fit of a BDA system is signifi - cant, because it leads towards adoption of the system by healthcare organizations. Prior studies discussed TTF in several aspects to determine the user task fitness for technology and group decision making, which led towards the successful adoption of an information system in various organizational settings (i.e., massive open online courses [MOOCs] [77], E-commerce [78], electronic health records",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "(i.e., massive open online courses [MOOCs] [77], E-commerce [78], electronic health records [79], group decision support systems [80], high-speed data services [81], and mobile banking [27]). Therefore, based on the abovementioned literature, we believe that successful adoption of BDA by health - care organizations significantly depends on matching of technology with the user task requirements, which has not been studied by prior researchers. The study of Zhou et al. [27] designed a TTF measure that could evaluate the fit between task and technology [79], with no need to evaluate the impact of the task and technology characteristics on TTF. Therefore, we use only TTF as used by Klopping and Qiwei [78, 79], and we use the task and technology characteristics separately. H3 TTF has a significant relationship with",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "task and technology characteristics separately. H3 TTF has a significant relationship with BIs to use BDA. Technology acceptance model This paper applied the TAM to observe the adoption of BDA because it satisfactorily determined users’ perceptions of the ease of use and usefulness of a new system [82]. TAM model determinants are optimal for determining user perceptions about its adop - tion [54]. TAM has been adopted by many researchers to successfully check and man - age new system adoptions [54, 57, 83, 84]. Since the invention of TAM, an abundance of studies have produced different research models to effectively predict user attitudes and behavioral intentions towards technology adoption. Interestingly, all of these studies used approximately similar attributes to evaluate technology adoption [85]. Many stud - ies have",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "similar attributes to evaluate technology adoption [85]. Many stud - ies have concluded that TAM is one of the best models in different contexts (e.g., fore - casting general buyer behavioral intentions [86], telemedicine adoption [87] and radio frequency identification (RFID) integrated system adoption [85]). Previous literature has witnessed the adoption of innovations with massive implementation of TAM in the eval - uation of user intentions regarding new technology diffusion [88]. The literature widely backs the use of TAM constructs [i.e., perceived usefulness (PU) and perceived ease of use (PEOU)] in measuring adoption solutions for new technology [24, 39, 89, 90]. The study proposes that TAM will provide superior understanding of BDA adoption, because BDA is a heavily technology-driven research area that is also user-oriented and inno -",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "a heavily technology-driven research area that is also user-oriented and inno - vation-focused [23]. Previous studies by Esteves, Rahman, and Shin [21, 23, 91] also concluded that TAM constructs (i.e., PEOU and PU) were significant predictors of BDA adoption/usage. Perceived ease of use The study of Davis [82] defined PEOU as the degree of ease involved when using an information system. Subsequently, Soon [54] clarified that ease in using the information system and technology would help enhance its acceptability among users. The use of an [Página 7] Page 7 of 20 Shahbaz et al. J Big Data (2019) 6:6 effortless system will help enhance individual and organizational performances [24, 92]. BDA potentially generates benefits for organizations, including cutting costs, control - ling risk factors and helping with efficient",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "including cutting costs, control - ling risk factors and helping with efficient decision making. Adoption of BDA depends on user considerations in terms of its convenience of use, which employs processing of large-scale and heterogeneous data [23]. However, the intensity of difficulty and ease of using BDA vary from person to person. The prior literature has proven the existence of a significant direct relationship between the PEOU and a user’s intentional behavior to adopt the system in various fields of study [54, 93, 94]; this relationship was also discussed by Esteves, Shin, and Weerak - kody [21, 23, 39] when studying user intentional behaviors regarding the adoption of big data, although these studies were less focused on this study context. For example, meas - uring the relationship of",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "on this study context. For example, meas - uring the relationship of PEOU with the intention to use BDA is essential for healthcare organizations. The idea of BDA is not much older and can create much confusion in the user’s mind about its ease of use, which may decrease user intentions towards its use in healthcare organizations. Based on the aforesaid opinion, we propose the following hypothesis. H4 PEOU has a significant relationship with BIs to use BDA. Perceived usefulness PU can be defined as whether an individual considers that their job performance will be improved by using the system [24, 82]. PU is the most commonly used variable and the primary driver of technology adoption [92, 95]; PU is also expected to be the primary driver of",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "[92, 95]; PU is also expected to be the primary driver of intention to use BDA in healthcare in this study. PU is an essential variable for studies of the adoption intentions of innovation from the user’s perspective [54]. The previous literature has provided evidence of a positive relationship with intention to use many times, which has been successfully measured in various fields [26, 54, 94], includ - ing the field of big data [21, 23, 57], but BDA not has not been a focus in healthcare. The user cannot shape a positive perception of usefulness until he or she feels any practi - cal worth of BDA in healthcare organizations. According to the theoretical principle of the TAM model, the study hypothesized that PU had a positive",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "of the TAM model, the study hypothesized that PU had a positive relationship with BIs. H5 PU has a significant relationship with BIs to use BDA. Behavioral intention to use A BI is an intention to achieve some specified future behavior [21] and is a key predictor of an individual’s actual use of technology [96, 97]. BI is an essential first step towards actual adoption of any system [24]. According to Venkatesh and Davis [92], BI was an important mediator in the relationship between predictors and specific system adoption. The prior literature has proven that a person is more likely to adopt a technology if he has an intention to use it [98]. The social sciences literature has provided evidence that BIs have a direct impact on actual",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "literature has provided evidence that BIs have a direct impact on actual use [77, 82], and various studies have concluded that BIs have a significant influence on adoption of BDA [21, 23]. This study suggests that individuals with the intention to use BDA will lead to the actual use of BDA. [Página 8] Page 8 of 20 Shahbaz et al. J Big Data (2019) 6:6 H6 BIs have a significant relationship with actual use of BDA. The moderating role of resistance to change According to French [99], change is a state in which differences exist between new and old ways of thinking. The behavior of individuals who protects them from the conse - quences of either real or imagined change is called RTC [100]. Oliver [101] defines RTC",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "real or imagined change is called RTC [100]. Oliver [101] defines RTC as protection of the status quo via individuals creating resistance against the innovative system. Every innovative system is commonly a source of panic and bears RTC due to its perception as a possible threat to the solidity of old habits [43, 101]. To avoid RTC in organizations when implementing new systems or ways of working, Oreg [102] pro - posed that the organization should encourage employees to learn new skills, tasks, and programs. One aspect of personality is that RTC is very important for technology adop - tion [103]. The previous literature has shown that RTC is a demotivator and negatively influences the acceptance of information technology [104–106]. The study of Alomari [107] concluded that RTC",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "of information technology [104–106]. The study of Alomari [107] concluded that RTC was one factor that caused non-adop - tion and failure of new information systems. Lallmahomed et al. [33] investigated the adoption behavior of an e-government system by focusing on RTC and established a significant negative relationship between RTC and adoption of the system. Many other researchers also proved the importance and negative associations of RTC with accept - ance of information and communication technology systems [42, 43, 108]. A study of green supply chain management (GSCM) adoption [41] investigated RTC as a moderator between BIs and adoption of GSCM and concluded that higher RTC among employees would lead to non-implementation of GSCM. Similarly, Bral III et al. [109] investigated the moderating role of RTC between psychological",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "III et al. [109] investigated the moderating role of RTC between psychological capital and organizational citizenship behavior. Regarding BDA in healthcare organizations, RTC is likely to moderate between intentional behaviors and actual usage of the BDA system by employees. The previous literature also reported that RTC either did not significantly affect or had a minor direct impact on the actual use of technology [41, 109]. Therefore, this study also focuses on the moderating role of RTC rather than on the direct influence of RTC on actual use. The American Health and Human Services Department faced huge resistance to change from physicians when they were adopting an electronic health records system, especially during the initial phase [110]. The study of Bhattacherjee [111] empirically investigated and concluded that RTC was",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "The study of Bhattacherjee [111] empirically investigated and concluded that RTC was a key barrier to actual adoption of information technology in the healthcare sector in both the initial and post-adoption phases but was more of an obsta - cle during the initial phase. Other researchers also found that RTC was often regarded as the cause of failure of the actual use of information technology in healthcare organiza - tions [112, 113]. Here, we focus on RTC because we also investigate the initial adoption of BDA. However, although RTC has a key impact on the adoption of information systems in healthcare, previous research has not paid attention to RTC during this process. Therefore, to bridge this big gap in the literature, we considered the following hypothesis: H7 RTC",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "big gap in the literature, we considered the following hypothesis: H7 RTC moderates the relationship between BIs to use and actual use of BDA. [Página 9] Page 9 of 20 Shahbaz et al. J Big Data (2019) 6:6 Methodology Here, the researchers describe the methodology followed by this study. Figure 2 rep- resents the flow diagram of the research progress. Development of measures In this study, we adopted all of the measures from previous studies with the same con - text to preserve the content validity. All constructs were measured on a 7-point Likert scale in which the agreement of the participants to a given statement was assigned a score ranging from 1 (strongly disagree) to 7 (strongly agree). The 3-item perceived trust scale was adapted from [114],",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "7 (strongly agree). The 3-item perceived trust scale was adapted from [114], the four-item perceived security scale was adapted from [31], and the three-item task-technology fit scale was adapted from [27, 74]. Similarly, the three-item scales for both PU and PEOU were adapted from [39]. The three-item scale for BI to use was adapted from [98], and the three-item scale for actual use was adapted from [98]. The four-item scale for RTC was adapted from [104]. Sampling and data collection The structured questionnaire-based survey method was used in this empirical study to measure the proposed model. The survey method is helpful for measurement of behav - iors and the relationships among constructs [115]. Typically, the survey method has been used in previous studies in which researchers assess adoption",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "method has been used in previous studies in which researchers assess adoption or user intention-behav - ior [116]. First, a pilot study was conducted with 20 prospective users of the BDA sys - tem. The results of the pilot study were discussed with four senior professors with great command of construct building, and then the measurement items were refined based on the discussions, which also confirmed the face validity of the measurement scale. All Fig. 1 Proposed research framework. It is graphical presentation of all variables and relationship among them in the proposed research model [Página 10] Page 10 of 20 Shahbaz et al. J Big Data (2019) 6:6 prospective BDA system end users from 25 hospitals in four big cities of Punjab Paki - stan (Lahore, Faisalabad,",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "hospitals in four big cities of Punjab Paki - stan (Lahore, Faisalabad, Rawalpindi, and Multan) and the Islamabad capital territory of Pakistan were selected for data collection using a convenience sampling technique. A convenience sampling technique is a suitable sampling technique by virtue of the accessibility of the researcher to the participants [117]. A refined questionnaire with a cover letter that assured the respondents that the data would be used only for academic research purposes and kept confidential was distributed to all participants through an online survey. The online survey ensures research consistency with data collection [118]. Fig. 2 Flow diagram of research progress. In this figure, we graphically present the flow of progress in the research [Página 11] Page 11 of 20 Shahbaz et al. J Big",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "research [Página 11] Page 11 of 20 Shahbaz et al. J Big Data (2019) 6:6 A total of 400 questionnaires were distributed among the participants, and 260 filled questionnaires were received, of which 36 questionnaires that contained missing values or biased responses were not included in the study. The remaining 224 responses were selected for the analysis. Furthermore, the study considered age and gender as control variables to ensure that the results of the model were not influenced adversely by covari - ance, because the previous literature suggested that age and gender might influence the intention to use [63, 118]. Results and discussion We employed structural equation modeling (SEM) using the IBM-AMOS (v21) software to evaluate the proposed research model. It is evident from the previous literature that",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "the proposed research model. It is evident from the previous literature that the AMOS software is a powerful tool for performing confirmatory factor analysis (CFA) and SEM [119]. AMOS is also a complete package for evaluation of formative measures and moderating relationships [120]. Because AMOS is user-friendly and provides a graphical interface that enables easier handling, we used this software for the CFA and SEM and SPSS (v 21) to measure the validity and reliability and conduct an exploratory factor analysis (EFA) prior to the CFA and SEM. We also used SPSS for the demographic statistics. Table 2 presents the demographic information for the respondents, of whom 128 out of 224 were male, 82% were in the age bracket of 25–45 years, and 98% had a bachelor’s or",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "the age bracket of 25–45 years, and 98% had a bachelor’s or higher degree; the remaining 2% had a high school certificate or relevant diploma as well as knowledge and experience with using technological systems. Measurement model Kaiser–Meyer–Olkin (KMO) and Bartlett’s test of sphericity were used to measure sam - ple adequacy [121]. The KMO value was 0.857, which was within the cutoff range of 0.8–1 that showed sample adequacy. Data were collected without regard to differences in time; we checked the common method bias (CMB) in the data using Harman’s single- factor test [122]. After categorizing the items into eight subgroups, the results indicated that the first factor explained 31.1% of the variance, which was below the 40% cutoff rate. Thus, CMB is not an issue in",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "below the 40% cutoff rate. Thus, CMB is not an issue in the study. Prior to the path analysis, the reliability and validity were measured. The value of Cronbach’s alpha was greater than 0.7 for all factors. The composite reliability (CR) and AVE values ranged from 0.847 to 0.962 and from 0.652 to 0.864, respectively, which were within the accepted ranges [123, 124]. Table 3 shows all of the Cronbach’s alpha, CR and AVE values. Table 4 proves that no issue existed with the discriminant validity of the constructs in the study, because the square root of AVE was higher than all inter-construct correlations [125]. The study conducted EFA using SPSS to ensure that the measures in the study were correct with respect to the concerned variables. The",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "in the study were correct with respect to the concerned variables. The factor loading values shown in Table 3 ranged from 0.744 to 0.953, which proved that no issue existed regarding cross loading of the constructs [126]. CFA was conducted through AMOS to check the consistency and validity of the con - structs for the proposed study framework. The CFA values are CMIN/DF = 1.545, root mean square error of approximation (RMSEA) = 0.049 with a PCLOSE value of 0.529, comparative fit index (CFI) = 0.967 and Tucker-Lewis fit index (TLI) = 0.960. All values [Página 12] Page 12 of 20 Shahbaz et al. J Big Data (2019) 6:6 are in accordance with the threshold values [126]. These results proved the good fitness of the model. Structural model",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "[126]. These results proved the good fitness of the model. Structural model The above results proved good fitness of the model and that the data were highly reliable and valid. Therefore, we continued with the path analysis. In the path analy - sis, CMIN/DF = 1.748, RMSEA = 0.058 with a PCLOSE value of 0.278, CFI = 0.939, and TLI = 0.901, which proved that the model was a good fit. Then, the study meas - ured the path coefficients and found that all coefficients were significant. The results shown in Fig. 3 demonstrate that perceived trust (β = 0.124, p < 0.05), perceived secu - rity (β = 0.209, p < 0.001), TTF (β = 0.263, p < 0.001), PEOU (β = 0.240, p < 0.001), and",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "0.263, p < 0.001), PEOU (β = 0.240, p < 0.001), and PU (β = 0.118, p < 0.05) have significant positive relationships with BIs to use BDA. Thus, H1, H2, H3, H4, and H5 are supported. In addition, BIs to use BDA (β = 0.412, p < 0.001) have a significant positive relationship with the actual use of BDA, and thus H6 is also accepted. The model also demonstrates that 45% of the variance exists in the BIs to use BDA and 25% of the variance represents the actual use of BDA. The control variables age and gender did not have a significant relationship with the actual use of BDA. Therefore, we concluded that the hypothesized study model was accepted. Moderating the effect of RTC The study",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "hypothesized study model was accepted. Moderating the effect of RTC The study proved that RTC moderated the relationship between BIs to use the BDA sys - tem and actual use of the BDA system. The interaction term (behavioral intentions to use × resistance to change) (β = − 0.201, p < 0.001) had a significant and negative effect on the actual use of the BDA system. Thus, a higher RTC weakened the relationship between BIs to use and actual use of the BDA system, which accepted H7.Table 2 Demographical information of respondents Category Frequency Percentage (%) Gender Male 128 57.1 Female 96 42.9 Total 224 100.0 Age 18–25 29 12.9 25–35 89 39.7 35-45 94 42.0 45 and above 12 5.4 Total 224 100.0 Education High school/diploma 4",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "45 and above 12 5.4 Total 224 100.0 Education High school/diploma 4 1.8 Bachelor 109 48.7 Master 101 45.1 Doctoral 10 4.5 Total 224 100.0 [Página 13] Page 13 of 20 Shahbaz et al. J Big Data (2019) 6:6 BDA change the decision-making style in the healthcare sector. An interactive and task-oriented system can make possible use of BDA in any sector, especially in health - care organizations. With these objectives in mind, this study provided useful findings and a framework for practical adoption of BDA systems and future research. The study focused on security and trust of information, which were observed to be big concerns of system users. Similar to the results of [23], perceived trust and perceived security had a Table 3 Results of factor loadings,",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "trust and perceived security had a Table 3 Results of factor loadings, validity, and reliability Variables Items Loadings Cronbach’s alpha CR AVE Perceived trust PT1 0.849 0.834 0.847 0.654 PT2 0.879 PT3 0.778 Perceived security PS1 0.838 0.903 0.905 0.705 PS2 0.849 PS3 0.806 PS4 0.744 Task-technology fit TTF1 0.864 0.898 0.900 0.751 TTF2 0.797 TTF3 0.768 Perceived ease of use PEOU1 0.857 0.877 0.877 0.704 PEOU2 0.859 PEOU3 0.805 perceived usefulness PU1 0.816 0.838 0.847 0.652 PU2 0.881 PU3 0.841 Behavioral intention to use BDA BI1 0.778 0.909 0.911 0.773 BI2 0.796 BI3 0.809 Resistance to change RTC1 0.939 0.962 0.962 0.864 RTC2 0.937 RTC3 0.953 RTC4 0.944 Actual use of BDA AU1 0.872 0.918 0.919 0.792 AU2 0.889 AU3 0.879 Table 4 Correlations matrix and square",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "0.919 0.792 AU2 0.889 AU3 0.879 Table 4 Correlations matrix and square root of AVE Inclined italic lines represent the square root of the AVE of each variable Significance of correlations: *p < 0.050, **p < 0.010, ***p < 0.001RTC PS AU PEOU TTF PT PU IB RTC 0.929 PS 0.092 0.840 AU 0.168* 0.396*** 0.890 PEOU 0.111 0.433*** 0.403*** 0.839 TTF − 0.002 0.619*** 0.429*** 0.448*** 0.866 PT − 0.041 0.413*** 0.075 0.152* 0.357*** 0.809 PU − 0.034 0.291*** 0.047 0.270*** 0.258*** 0.337*** 0.808 IB 0.004 0.556*** 0.490*** 0.524*** 0.616*** 0.370*** 0.348*** 0.879 [Página 14] Page 14 of 20 Shahbaz et al. J Big Data (2019) 6:6 positive effect on BIs to adopt BDA. The study also investigated TTF, which had a posi - tive impact on",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "study also investigated TTF, which had a posi - tive impact on the behavioral intentions to adopt BDA, indicating that features of the system must be matched to the specific task of the user for successful adoption of a BDA system. Our results are consistent with the results of Afshan and Brock [127]. Similar to previous studies by Shin and Soon et al. [23] concerning BDA adoption, this study also demonstrated the positive effects of PEOU and PU on BIs to adopt BDA, which suggested that the system should be easy to use and have attractive features that make it look useful to the user. Furthermore, the study investigated RTC in employees, which played a key role in the convergence of BIs to put the system into actual",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "role in the convergence of BIs to put the system into actual use. The results of this study illustrate that as RTC from employees increases, actual use of BDA systems will decrease. The RTC result is consistent with that of Beal III and Nejati [41, 109]. Conclusion The adoption of BDA is in the initial stage, in which many healthcare organizations are thinking about adopting BDA systems. The present is an optimal time to adopt/implement BDA systems, especially in healthcare organizations, with an aim of providing better health - care facilities by maintaining patients’ health records and formulating better strategies. This study contributes to the literature by showing the main factors that are important when adopting the BDA system. This study results are also imperative for strategy makers",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "the BDA system. This study results are also imperative for strategy makers who want to implement a BDA system by demonstrating factors that are important initially. In contrast to existing studies, this study also expressed the huge positive combined effect of the TTF and TAM theories on behavioral intentions to adopt BDA. Combining TAM and TTF gives more effective results than use of TAM or TTF individually [19, 26]. The prior literature demonstrates use of the TAM model alone in the adoption of a BDA system. The current study also incorporated important concerns from users regarding adoption of any innovative system, such as perceived security and perceived trust. These factors provide an additional significant aspect to the literature regarding BDA. Our sampling territory is Pakistan, which Fig. 3",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "the literature regarding BDA. Our sampling territory is Pakistan, which Fig. 3 SEM results for hypotheses testing. In the figure, we graphically present the SEM results (i.e. path coefficient and significance level) of proposed model [Página 15] Page 15 of 20 Shahbaz et al. J Big Data (2019) 6:6 is a developing country. RTC is the largest barrier in the adoption of innovative systems, particularly in developing countries but also in developed countries. According to our best information, this study is the first to enrich the literature by linking resistance to change of employees with BDA adoption as a moderator. This moderation result will help implement - ers control this factor at the time of adopting the BDA system. Theoretical contribution This study contributes noteworthy research insights into",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "the BDA system. Theoretical contribution This study contributes noteworthy research insights into BDA system implementation. The study fills the main gap in the literature concerning the empirical evidence for BDA in Paki - stani healthcare organizations for the first time. Second, the majority of previous studies only highlighted the importance, challenges, and opportunities of BDA, because BDA was in the initial stage of adoption and was a comparatively new topic. Second, few researchers have investigated the adoption of BDA, and the existing studies have focused on a specific per - spective (i.e., an economic or financial perspective) or have simply emphasized TAM the - ory. This study is probably the first on BDA adoption to propose a model that combines the TAM and TTF theories as predictors of",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "a model that combines the TAM and TTF theories as predictors of behavioral intentions to use BDA. Thus, integration and implementation of the TAM theory with the TTF theory for BDA adoption is a new perspective that enhances the literature. Second, to switch from the previous healthcare sys - tem to a BDA system, the literature needs a strong theoretical basis for further research and a broad and general research model that is not specific to one aspect of the business. This study model will be helpful and will advance a theory for future BDA research. Furthermore, the study included security and trust aspects of information in the model to elucidate their impacts on BI. Our results will contribute to the security and trust perspectives in the tech",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "results will contribute to the security and trust perspectives in the tech - nology acceptance literature and provide security and trust grounds for further research. In addition, the results obtained for resistance to change represent an immense theoretical con - tribution for researchers, because the current study has highlighted this important barrier in the implementation of BDA. This investigation can be used as a reference for future research and to increase understanding of the adoption of BDA research. Practical contribution The study also contributed practically in several ways similar to its theoretical contribu - tions. The findings of the study propose salient guidelines and important implications for practitioners and implementers of BDA systems that can assist with successful adoption of BDA systems. First, connecting system functions with the",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "with successful adoption of BDA systems. First, connecting system functions with the required tasks of the organi - zation as well as the PU and PEOU of the system are important. This approach will pro - vide results that are more fruitful for practitioners when implementing BDA systems. Second, the findings of the study also indicate that perceived security and perceived trust are the key predictors of intentions regarding acceptance of a BDA system. Third, this study explores the moderating effect of RTC, which reduces the adoption of BDA systems in developing countries. This study will provide broad insights for implementers of BDA systems in developing countries and allow the design of strategies to ultimately reduce employees’ resistance levels. Finally, this study provides an initial platform for practitioners",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "employees’ resistance levels. Finally, this study provides an initial platform for practitioners for adoption and promotion of BDA practices within the organization to obtain maximum advantages of innovative technology, especially in developing countries. [Página 16] Page 16 of 20 Shahbaz et al. J Big Data (2019) 6:6 Limitations and future directions The authors acknowledge some limitations of the current study. First, the focus of this study is on healthcare organizations in Pakistan regarding adoption of a BDA system. The impact of organizational culture was ignored by this study, which might have an effect on the level of adoption of this system. Future researchers may test the same research model in other organizations considering different cultural setups, because the organizational setup and culture vary from industry to industry; therefore,",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "because the organizational setup and culture vary from industry to industry; therefore, the findings of this study may vary when applied to different sector organizations. This study pro - vided understanding of BDA system adoption in developing countries in particular and developed countries in general. Thus, future researchers can test this model in devel - oped countries to increase the generalization of the study, because the severity of resist - ance to change from employees is greater in developing than in developed countries [41]. The research model can be tested in different cultural settings with a focus on adoption of BDA systems. This study was aimed to investigate the user adoption factors of BDA, which neglect the other side of system implementation. Therefore, future researchers can identify the",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "the other side of system implementation. Therefore, future researchers can identify the developers/architects intentions for development and implementation of BDA system. Finally, this study was based on cross-sectional settings, which restricted measurement of the consistency in respondent behavior; this gap should be tested in a longitudinal setup to improve the significant contribution to knowledge. Abbreviations BDA: big data analytics; TTF: task-technology fit; TAM: technology acceptance model; PEOU: perceived ease of use; PU: perceived usefulness; RTC : resistance to change; BI: behavioral intentions. Authors’ contributionsMS conceptualized the idea, prepared the literature and build theory, designed research framework and collect data, analyzed results, drafted and proof-read the manuscript, GCY and ZL provided supervision and guide throughout the process. FS and YH contribute in analysis and results writing. All authors read",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "FS and YH contribute in analysis and results writing. All authors read and approved the final manuscript. Acknowledgements This study has been supported by “National Natural Science Foundation of China (NSFC)” Grant Numbers: 71774044, 71672050, and 71272191. Competing interests The authors declare that they have no competing interests. Availability of data and materials Data can be available on demand. Consent for publication We, the authors, consent to the publication of this manuscript in the Journal of Big Data. Ethics approval and consent to participate Not applicable. Funding Not applicable. Publisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Received: 22 October 2018 Accepted: 8 January 2019 References 1. Müller O, Junglas I, Vom Brocke J, Debortoli S. Utilizing big data",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "Müller O, Junglas I, Vom Brocke J, Debortoli S. Utilizing big data analytics for information systems research: chal-lenges, promises and guidelines. Eur J Inf Syst. 2016;25:289–302. [Página 17] Page 17 of 20 Shahbaz et al. J Big Data (2019) 6:6 2. Cottle M, Hoover W. Transforming health care through big data. Washington DC: Institute for Health Technology Transformation; 2013. p. 6–19. 3. Lavalle S, Lesser E, Shockley R, Hopkins MS, Kruschwitz N. Big data, analytics and the path from insights to value. MIT Sloan Manag Rev. 2011;52:21–32. 4. Raghupathi W, Raghupathi V. Big data analytics in healthcare: promise and potential. Health Inf Sci Syst. 2014;2:3. https ://doi.org/10.1186/2047-2501-2-3. 5. Nannetti P . The deciding factor : big data & decision making. Capgemini Consulting Technology outsourcing. 2012;1–5. https ://www.capge mini.com/resou",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "data & decision making. Capgemini Consulting Technology outsourcing. 2012;1–5. https ://www.capge mini.com/resou rces/the-decid ing-facto r-big-data-decis ion-makin g/. 6. Connolly S, Wooledge S. Harnessing the value of big data analytics. Big DataAnalytics. 2012; p. 1–14. 7. Wang Y, Hajli N. Exploring the path to big data analytics success in healthcare. J Bus Res. 2017;70:287–99. 8. Maria VMF. Big data services based on mobile data and their strategic importance. In: 7th International Confer - ence on Computers Communications and Control. 2018;276–81. http://ieeex plore .ieee.org/lpdoc s/epic0 3/wrapp er.htm?arnum ber=67580 26. 9. Sandhya Kumari S, Sandhya Rani K. Big data analytics for healthcare system. In: 2018 IADS international conference on computing, communications & data engineering (CCODE), 7–8 February 2018. 10. Gharajeh MS. Biological big data analytics. Adv Comput. 2018;109:321–55. 11. Wang Y,",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "Gharajeh MS. Biological big data analytics. Adv Comput. 2018;109:321–55. 11. Wang Y, Kung LA, Byrd TA. Big data analytics: understanding its capabilities and potential benefits for healthcare organizations. Technol Forecast Soc Change. 2018;126:3–13. 12. Groves P , Knott D. The ‘big data’ revolution in healthcare. New York: McKinsey & Company; 2013. 13. Kim MK, Park JH. Identifying and prioritizing critical factors for promoting the implementation and usage of big data in healthcare. Inf Dev. 2017;33:257–69. 14. Braunstein ML. Practitioner’s guide to health informatics. Switzerland: Springer International Publishing; 2015. p. 133–49. https ://doi.org/10.1007/978-3-319-17662 -8. 15. Huang T, Lan L, Fang X, An P , Min J, Wang F. Promises and challenges of big data computing in health sciences. Big Data Res. 2015;2:2–11. 16. Kim MK, Cho YW, Park JH.",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "sciences. Big Data Res. 2015;2:2–11. 16. Kim MK, Cho YW, Park JH. The prospects and development directions for healthcare big data industry. Electronics and Telecommunications Research Institute. 2013. 17. Heitmueller A, Henderson S, Warburton W, Elmagarmid A, Pentland AS, Darzi A. Developing public policy to advance the use of big data in health care. Health Aff (Millwood). 2014;33:1523–30. 18. Bughin J. Reaping the benefits of big data in telecom. J Big Data. 2016;3:14. 19. Dishaw MT, Strong DM. Extending the technology acceptance model with task-technology fit constructs. Inf Manag. 1999;36:9–21. 20. Petersen Glen S. High impact sales force automation. Boca Raton: FL St Lucie Press; 1997. 21. Esteves J, Curto J. A risk and benefits behavioral model to assess intentions to adopt big data. J Intell Stud Bus.",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "model to assess intentions to adopt big data. J Intell Stud Bus. 2013;3:37–46. https ://www.scopu s.com/inwar d/recor d.uri?eid=2-s2.0-84905 68971 7&partn erID=40&md5=53271 4767e 19a33 35cb1 ef08d e04d6 6c . 22. Rahman N. Factors affecting big data technology adoption. Student Res Symp 2016. 2016;0–29. http://pdxsc holar .libra ry.pdx.edu/stude ntsym posiu m%5Cn, http://pdxsc holar .libra ry.pdx.edu/stude ntsym posiu m/2016/Prese ntati ons/10. 23. Shin DH. Demystifying big data: anatomy of big data developmental process. Telecomm Policy. 2016;40:837–54. 24. Brock V, Khan HU. Big data analytics: does organizational factor matters impact technology acceptance? J Big Data. 2017;4:21. 25. Goodhue DL, Thompson RL. Task-technology fit and individual performance. MIS Q. 1995;19:213. http://www.jstor .org/stabl e/24968 9?origi n=cross ref . 26. Wu B, Chen X. Continuance intention to use MOOCs: integrating the technology acceptance model (TAM)",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "X. Continuance intention to use MOOCs: integrating the technology acceptance model (TAM) and task technology fit (TTF) model. Comput Human Behav. 2017;67:221–32. 27. Zhou T, Lu Y, Wang B. Integrating TTF and UTAUT to explain mobile banking user adoption. Comput Human Behav. 2010;26:760–7. 28. Junglas IA, Watson RT. Location-based services. Commun ACM. 2008;51:65–9. http://porta l.acm.org/citat ion.cfm?doid=13255 55.13255 68. 29. Lee C-C, Cheng HK, Cheng H-H, Cheng HK. An empirical study of mobile commerce in insurance industry: task–technology fit and individual differences. Decis Support Syst. 2007;43:95–110. www.elsev ier.com/locat e/dss . 30. Damghanian H, Zarei A, Siahsarani Kojuri MA. Impact of perceived security on trust, perceived risk, and accept - ance of online banking in Iran. J Internet Commer. 2016;15:214–38. 31. Arpaci I, Yardimci Cetin Y, Turetken O. Impact of",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "Commer. 2016;15:214–38. 31. Arpaci I, Yardimci Cetin Y, Turetken O. Impact of perceived security on organizational adoption of smartphones. Cyberpsychol Behav Soc Netw. 2015;18:602–8. https ://doi.org/10.1089/cyber .2015.0243. 32. Fife E, Orjuela J. The privacy calculus: mobile apps and user perceptions of privacy and security. Int J Eng Bus Manag. 2012;4:1–10. 33. Lallmahomed MZI, Lallmahomed N, Lallmahomed GM. Factors influencing the adoption of e-government ser - vices in Mauritius. Telemat Inform. 2017;34:57–72. 34. Wang EST, Lin RL. Perceived quality factors of location-based apps on trust, perceived privacy risk, and continuous usage intention. Behav Inf Technol. 2017;36:2–10. 35. Liao C, Liu C-C, Chen K. Examining the impact of privacy, trust and risk perceptions beyond monetary transac-tions: An integrated model. Electron Commer Res Appl. 2011;10:702–15. http://linki nghub .elsev ier.com/retri eve/ pii/S1567",
        "start_idx": 8236,
        "end_idx": 8364
      },
      {
        "text": "model. Electron Commer Res Appl. 2011;10:702–15. http://linki nghub .elsev ier.com/retri eve/ pii/S1567 42231 10004 08. 36. Nguyen T, Zhou L, Spiegler V, Ieromonachou P , Lin Y. Big data analytics in supply chain management: a state-of-the-art literature review. Comput Oper Res. 2018;98:254–64. https ://doi.org/10.1016/j.cor.2017.07.004. 37. Jain P , Gyanchandani M, Khare N. Big data privacy: a technological perspective and review. J Big Data. 2016;3:25. 38. Heart T. Who is out there? Exploring the effects of trust and perceived risk on SaaS adoption intentions. DATA BASE Adv Inf Syst. 2010;41:49–67. [Página 18] Page 18 of 20 Shahbaz et al. J Big Data (2019) 6:6 39. Sivarajah U, Kamal MM, Irani Z, Weerakkody V. Critical analysis of big data challenges and analytical methods. J Bus Res. 2017;70:263–86. 40. Malaka I,",
        "start_idx": 8352,
        "end_idx": 8480
      },
      {
        "text": "data challenges and analytical methods. J Bus Res. 2017;70:263–86. 40. Malaka I, Brown I. Challenges to the organisational adoption of big data analytics : a case study in the South African telecommunications industry. In: Proceedings of the 2015 Annual Research Conference on South African Institute of Computer Scientists and Information Technologists. 2015; p. 27. 41. Nejati M, Rabiei S, Chiappetta Jabbour CJ. Envisioning the invisible: understanding the synergy between green human resource management and green supply chain management in manufacturing firms in Iran in light of the moderating effect of employees’ resistance to change. J Clean Prod. 2017;168:163–72. 42. Sharma M, Gupta R, Acharya P . Prioritizing the critical factors of cloud computing adoption using multi-criteria decision-making techniques. Glob Bus Rev. 2017. https ://doi.org/10.1177/09721 50917 74118 7. 43.",
        "start_idx": 8468,
        "end_idx": 8596
      },
      {
        "text": "decision-making techniques. Glob Bus Rev. 2017. https ://doi.org/10.1177/09721 50917 74118 7. 43. Reginato E, Fadda I, Paglietti P . The influence of resistance to change on public-sector reform implementation: the case of Italian municipalities’ internal control system. Int J Public Adm. 2016;39:989–99. 44. Lorenzi NM, Kouroubali A, Detmer DE, Bloomrosen M. How to successfully select and implement electronic health records. BMC Med Inform Decis Mak. 2009;9:15. http://uvic.summo n.seria lssol ution s.com/2.0.0/link/0/eLvHC XMwY2 BQsEg 1TgWd _JSaZG GWbJ6 anGhg nmRqa p5qam JmYpg E3iCG 2DqGV Jq7CT Ewpea JMsi6 uYY4e -iCSsj 4AsiZ C_GgU1y AlY2F oYEYA 28iaP F3Xgl 4k1gK AK4dG 90. 45. Bates DW. Physicians and ambulatory electronic health records. Health Aff. 2005;24:1180–9. 46. Bartos CE, Butler BS, Crowley RS. Ranked levels of influence model: selecting influence techniques to minimize IT resistance. J",
        "start_idx": 8584,
        "end_idx": 8712
      },
      {
        "text": "levels of influence model: selecting influence techniques to minimize IT resistance. J Biomed Inform. 2011;44:497–504. 47. Guo X, Sun Y, Wang N, Peng Z, Yan Z. The dark side of elderly acceptance of preventive mobile health services in China. Electron Mark. 2013;23:49–61. 48. Groves P , Kayyali B, Knott D, Van Kuiken S. The “big data” revolution in healthcare: accelerating value and innova-tion. McKinsey Glob Inst. 2013;1–22. http://www.image s-et-resea ux.com/sites /defau lt/files /media s/blog/2013/12/mckin sey_13120 4_-_the_big_data_revol ution _in_healt hcare .pdf. 49. Yan X, Song T, Wu Q. An improved cultural algorithm and its application in image matching. Multimed Tools Appl. 2017;76:14951–68. 50. Latif Z, Tunio MZ, Pathan ZH, Jianqiu Z, Ximei L, Sadozai SK. A review of policies concerning development of big data industry in Pakistan: Subtitle: Development",
        "start_idx": 8700,
        "end_idx": 8828
      },
      {
        "text": "of policies concerning development of big data industry in Pakistan: Subtitle: Development of big data industry in Pakistan. In: 2018 international conference on computing, mathematics and engineering technologies (iCoMET), 2018; 2018; p. 1–5. 51. Mahmood T, Afzal U. Security analytics: big data analytics for cybersecurity. In: 2013 2nd national conference on Information assurance (ncia). 2013;129–34. http://ieeex plore .ieee.org/stamp /stamp .jsp?arnum ber=67253 37. 52. Tsai CW, Lai CF, Chao HC, Vasilakos AV. Big data analytics: a survey. J Big Data. 2015;2:21. 53. Archenaa J, Anita EAM. A survey of big data analytics in healthcare and government. Procedia Comput Sci. 2015;50:408–13. 54. Soon KWK, Lee CA, Boursier P . A study of the determinants affecting adoption of big data using integrated technology acceptance model (TAM) and diffusion of innovation (DOI)",
        "start_idx": 8816,
        "end_idx": 8944
      },
      {
        "text": "data using integrated technology acceptance model (TAM) and diffusion of innovation (DOI) in Malaysia. Int J Appl Bus Econ Res. 2016;14:17–47. 55. LaBrie RC, Steinke GH, Li X, Cazier JA. Big data analytics sentiment: US-China reaction to data collection by busi-ness and government. Technol Forecast Soc Change. 2018;130:45–55. 56. Memon MA, Soomro S, Jumani AK, Kartio MA. Big data analytics and its applications. Ann Emerg Technol Comput. 2017;1. www.aetic .theia er.org . 57. Weerakkody V, Kapoor K, Balta ME, Irani Z, Dwivedi YK. Factors influencing user acceptance of public sector big open data. Prod Plan Control. 2017;28:891–905. 58. Arunachalam D, Kumar N, Kawalek JP . Understanding big data analytics capabilities in supply chain manage - ment: unravelling the issues, challenges and implications for practice. Transp Res Part E",
        "start_idx": 8932,
        "end_idx": 9060
      },
      {
        "text": "unravelling the issues, challenges and implications for practice. Transp Res Part E Logist Transp Rev. 2018;114:416–36. 59. Gupta S, Kar AK, Baabdullah A, Al-Khowaiter WAA. Big data with cognitive computing: a review for the future. Int J Inf Manag. 2018;42:78–89. 60. Pavlou PA, Fygenson M. Understanding and prediction electronic commerce adoption: an extension of the theory of planned behavior. MIS Q. 2006;30:115–43. http://searc h.ebsco host.com/login .aspx?direc t=true&db=buh&AN=19754 863&site=ehost -live%0A, http://www.jstor .org/stabl e/25148 720. 61. Cui F, Lin D, Qu H. The impact of perceived security and consumer innovativeness on e-loyalty in online travel shopping. J Travel Tour Mark. 2018;35:819–34. https ://doi.org/10.1080/10548 408.2017.14224 52. 62. Nguyen TD, Huynh PA. The roles of perceived risk and trust on e–payment adoption. In: International Econometric Conference of Vietnam. 2018; p. 926–40. 63.",
        "start_idx": 9048,
        "end_idx": 9176
      },
      {
        "text": "e–payment adoption. In: International Econometric Conference of Vietnam. 2018; p. 926–40. 63. Fang Y, Qureshi I, Sun H, McCole P , Ramsey E, Lim KH. Trust, satisfaction, and online repurchase intention: the moderating role of perceived effectiveness of e-commerce institutional mechanisms. MIS Q. 2014;38:407–27. https ://misq.org/trust -satis facti on-and-onlin e-repur chase -inten tion-the-moder ating -role-of-perce ived-effec tiven ess-of-e-comme rce-insti tutio nal-mecha nisms .html . 64. Shahzad F, Xiu GY, Wang J, Shahbaz M. An empirical investigation on the adoption of cryptocurrencies among the people of mainland China. Technol Soc. 2018. http://www.scien cedir ect.com/scien ce/artic le/pii/S0160 791X1 83002 04. 65. Kim KK, Prabhakar B, Park SK. Trust, perceived risk, and trusting behavior in internet banking. Asia Pacific J Inf Syst. 2009;19:1–23. 66. Alkhater N, Walters R, Wills G. An",
        "start_idx": 9164,
        "end_idx": 9292
      },
      {
        "text": "J Inf Syst. 2009;19:1–23. 66. Alkhater N, Walters R, Wills G. An empirical study of factors influencing cloud adoption among private sector organisations. Telemat Inform. 2018;35:38–54. http://linki nghub .elsev ier.com/retri eve/pii/S0736 58531 73030 88. 67. Ackermann T, Widjaja T, Benlian A, Buxmann P . Perceived IT security risks of cloud computing: conceptualization and scale development. ICIS. 2012;1–20. http://aisel .aisne t.org/icis2 012/proce eding s/ISSec urity /3/. 68. Hartono E, Holsapple CW, Kim KY, Na KS, Simpson JT. Measuring perceived security in B2C electronic commerce website usage: a respecification and validation. Decis Support Syst. 2014;62:11–21. 69. Mekovec R, Hutinski Ž. The role of perceived privacy and perceived security in online market. In: MIPRO, 2012 Proceedings of the 35th International Convention. 2012; p. 1883–8. [Página 19] Page 19 of 20 Shahbaz",
        "start_idx": 9280,
        "end_idx": 9408
      },
      {
        "text": "International Convention. 2012; p. 1883–8. [Página 19] Page 19 of 20 Shahbaz et al. J Big Data (2019) 6:6 70. Zandieh SO, Yoon-Flannery K, Kuperman GJ, Langsam DJ, Hyman D, Kaushal R. Challenges to EHR implementation in electronic- versus paper-based office practices. J Gen Intern Med. 2008;23:755–61. 71. Ferguson AG. Policing predictive policing. Wash Univ Law Rev. 2017;211–68. https ://paper s.ssrn.com/sol3/paper s.cfm?abstr act_id=27655 25. 72. Broeders D, Schrijvers E, van der Sloot B, van Brakel R, de Hoog J, Hirsch Ballin E. Big Data and security poli-cies: towards a framework for regulating the phases of analytics and use of big data. Comput Law Secur Rev. 2017;33:309–23. 73. Abouelmehdi K, Beni-Hessane A, Khaloufi H. Big healthcare data: preserving security and privacy. J Big Data. 2018;5:1. 74. Lin TC, Huang",
        "start_idx": 9396,
        "end_idx": 9524
      },
      {
        "text": "preserving security and privacy. J Big Data. 2018;5:1. 74. Lin TC, Huang CC. Understanding knowledge management system usage antecedents: an integration of social cognitive theory and task technology fit. Inf Manag. 2008;45:410–7. 75. D’Ambra J, Wilson CS, Akter S. Application of the task-technology fit model to structure and evaluate the adoption of e-books by academics. J Am Soc Inf Sci Technol. 2013;64:48–64. 76. Aljukhadar M, Senecal S, Nantel J. Is more always better? Investigating the task-technology fit theory in an online user context. Inf Manag. 2014;51:391–7. 77. Khan IU, Hameed Z, Yu Y, Islam T, Sheikh Z, Khan SU. Predicting the acceptance of MOOCs in a developing country: application of task-technology fit model, social motivation, and self-determination theory. Telemat Informat. 2018;35:964–78. 78. Klopping IM, Mckinney E. Extending the",
        "start_idx": 9512,
        "end_idx": 9640
      },
      {
        "text": "self-determination theory. Telemat Informat. 2018;35:964–78. 78. Klopping IM, Mckinney E. Extending the technology acceptance model and the task-technology fit model to consumer e-commerce. Inf Technol Learn Perform J. 2004;22:35–48. 79. Gan Q, Cao Q. Adoption of electronic health record system: multiple theoretical perspectives. In: 2014 47th Hawaii International Conference on System Sciences (HICSS). 2014;2716–24. http://ieeex plore .ieee.org/docum ent/67589 42/. 80. Dennis AR, Wixom BH, Vandenberg RJ. Understanding fit and appropriation effects in group support systems via meta-analysis understanding fit and appropriation effects in group support systems via meta-analysis. Source MIS Q MIS Q. 2001;25:167–93. http://www.jstor .org/stabl e/32509 28%5Cn, http://about .jstor .org/terms . 81. Pagani M. Determinants of adoption of high speed data services in the business market: evidence for a combined technology acceptance model with task technology fit",
        "start_idx": 9628,
        "end_idx": 9756
      },
      {
        "text": "market: evidence for a combined technology acceptance model with task technology fit model. Inf Manag. 2006;43:847–60. 82. Davis FD. Perceived usefulness, perceived ease of use, and user acceptance of information technology. MIS Q. 1989;13:319. http://www.jstor .org/stabl e/24900 8?origi n=cross ref . 83. Dillon A, Morris MG. User acceptance of new information technology: theories and models. Annu Rev Inf Sci Technol. 1996;31:3–32. http://arizo na.openr eposi tory.com/arizo na/handl e/10150 /10558 4. 84. Shahzad F, Xiu GY, Khan I, Wang J. m-Government security response system: predicting citizens’ adoption behavior. Int J Hum Comput Interact. 2018. https ://doi.org/10.1080/10447 318.2018.15168 44. 85. Kapoor K, Dwivedi Y, Piercy CN, Lal B, Weerakkody V. RFID integrated systems in libraries: extending TAM model for empirically examining the use. J Enterp Inf Manag. 2014;27:731–58. https ://doi.org/10.1108/JEIM-10-2013-0079. 86.",
        "start_idx": 9744,
        "end_idx": 9872
      },
      {
        "text": "empirically examining the use. J Enterp Inf Manag. 2014;27:731–58. https ://doi.org/10.1108/JEIM-10-2013-0079. 86. Gentry L, Calantone R. A comparison of three models to explain shop-bot use on the web. Psychol Mark. 2002;19:945–56. 87. Chau PYK, Hu PJ-H. Information technology acceptance by individual professionals: a model comparison approach. Decis Sci. 2001;32:699–719. https ://doi.org/10.1111/j.1540-5915.2001.tb009 78.x. 88. Park SY, Nam M-W, Cha S-B. University students’ behavioral intention to use mobile learning: evaluating the tech-nology acceptance model. Br J Educ Technol. 2012;43:592–605. https ://doi.org/10.1111/j.1467-8535.2011.01229 .x. 89. Pei Y, Xue W, Su Y, Li D. Discussion on influence factors and evaluation of customer experience for B2C E-commerce enterprises. In: 2015 International Conference on Logistics, Informatics and Service Sciences (LISS), 2015. 2015. 90. Prieto JCS, Migueláñez SO, García-Peñalvo FJ. ICTs integration in education: mobile learning",
        "start_idx": 9860,
        "end_idx": 9988
      },
      {
        "text": "Prieto JCS, Migueláñez SO, García-Peñalvo FJ. ICTs integration in education: mobile learning and the technology acceptance model (TAM). In: Proceedings of the second international conference on technological ecosystems for enhancing multiculturality. 2014;683–7. https ://doi.org/10.1145/26697 11.26699 74. 91. Rahman N. Factors affecting big data technology adoption Nayem Rahman department of engineering and technology. In: Student Res Symp Pap 10. 2016;0–29. http://pdxsc holar .libra ry.pdx.edu/stude ntsym posiu m/2016/ Prese ntati ons/10. 92. Venkatesh V, Davis FD. A theoretical extension of the technology acceptance model: four longitudinal field stud-ies. Manag Sci. 2000;46:186–204. https ://doi.org/10.1287/mnsc.46.2.186.11926 . 93. Ibrahim R, Leng NS, Yusoff RCM, Samy GN, Masrom S, Rizman ZI. E-learning acceptance based on technology accept - ance model (TAM). J Fundam Appl Sci. 2017;9:871. https ://www.ajol.info/index .php/jfas/artic le/view/16545 1. 94. Ambak K,",
        "start_idx": 9976,
        "end_idx": 10104
      },
      {
        "text": "Fundam Appl Sci. 2017;9:871. https ://www.ajol.info/index .php/jfas/artic le/view/16545 1. 94. Ambak K, Harun NE, Rosli N, Daniel BD, Prasetijo J, Abdullah ME, et al. Driver intention to use electric cars using technology acceptance model. ARPN J Eng Appl Sci. 2016;11:1–4. 95. Claes V, Devriendt E, Tournoy J, Milisen K. Attitudes and perceptions of adults of 60 years and older towards in-home monitoring of the activities of daily living with contactless sensors: an explorative study. Int J Nurs Stud. 2015;52:134–48. 96. Castañeda JA, Muñoz-Leiva F, Luque T. Web acceptance model (WAM): moderating effects of user experience. Inf Manag. 2007;44:384–96. 97. Brock C, Blut M, Linzmajer M, Zimmer B. F-commerce and the crucial role of trust. In: Thirty Second International Conference on Information Systems. 2011; p. 1–11. 98. Venkatesh V,",
        "start_idx": 10092,
        "end_idx": 10220
      },
      {
        "text": "Second International Conference on Information Systems. 2011; p. 1–11. 98. Venkatesh V, Morris MG, Davis GB, Davis FD. User acceptance of information technology: toward a unified view. MIS Q. 2003;27:425–78. 99. French WL. CHB. Organization development: behavioral science interventions for organization improvement. 6th ed. Upper Saddle River: Prentice Hall; 1999. http://ezpro xy.yorks j.ac.uk/login ?url=, http://searc h.ebsco host.com/ login .aspx?direc t=true&db=edb&AN=62665 11&site=eds-live&scope =site . 100. Zander A. Resistance to change—its analysis and prevention. Adv Manag J. 1950;15:9–11. http://psycn et.apa.org/psyci nfo/1950-06096 -001. 101. Oliver C. Strategic responses to institutional processes. Acad Manag Rev. 1991;16:145–79. https ://doi.org/10.5465/AMR.1991.42790 02. [Página 20] Page 20 of 20 Shahbaz et al. J Big Data (2019) 6:6 102. Oreg S. Resistance to change: developing an individual differences measure. J Appl Psychol. 2003;88:680–93. 103. Venkatesh V, Morris",
        "start_idx": 10208,
        "end_idx": 10336
      },
      {
        "text": "an individual differences measure. J Appl Psychol. 2003;88:680–93. 103. Venkatesh V, Morris M, Ackerman P . A longitudinal field investigation of gender differences in individual technol- ogy adoption decision-making processes. Organ Behav Hum Decis Process. 2000;83:33–60. 104. Huang RT. Exploring the moderating role of self-management of learning in mobile english learning. Educ Tech-nol Soc. 2014;17:255–67. 105. Kim H-W, Kankanhalli A. Investigating user resistance to information systems implementation: a status quo bias perspective. MIS Q. 2009;33:567–82. http://www.jstor .org/stabl e/20650 309. 106. Nov O, Ye C. Users’ personality and perceived ease of use of digital libraries: the case for resistance to change. J Am Soc Inf Sci Technol. 2008;59:845–51. 107. Alomari MK, Sandhu K, Woods P . Exploring citizen perceptions of barriers to e-government adoption in a develop - ing",
        "start_idx": 10324,
        "end_idx": 10452
      },
      {
        "text": "citizen perceptions of barriers to e-government adoption in a develop - ing country. Transform Gov People Process Policy. 2014;8:131–50. https ://doi.org/10.1108/TG-05-2013-0013. 108. Nov O, Ye C. Resistance to change and the adoption of digital libraries: an integrative model. J Am Soc Inf Sci Technol. 2009;60:1702–8. 109. Beal III L, Stavros JM, Cole ML. Effect of psychological capital and resistance to change on organisational citizen-ship behaviour. SA J Ind Psychol. 2013;39. http://sajip .co.za/index .php/sajip /artic le/view/1136. 110. Ford EW, Menachemi N, Peterson LT, Huerta TR. Resistance is futile: but it is slowing the pace of EHR adoption nonetheless. J Am Med Informat Assoc. 2009;16:274–81. 111. Bhattacherjee A, Hikmet N. Physicians’ resistance toward healthcare information technology: a theoretical model and empirical test. Eur J Inf Syst. 2007;16:725–37. 112. Di Fabio",
        "start_idx": 10440,
        "end_idx": 10568
      },
      {
        "text": "model and empirical test. Eur J Inf Syst. 2007;16:725–37. 112. Di Fabio A, Bernaud J-L, Loarer E. Emotional intelligence or personality in resistance to change? Empirical results in an Italian health care context. J Employ Couns. 2014;51:146–57. https ://doi.org/10.1002/j.2161-1920.2014.00048 .x. 113. Alkraiji A, Jackson T, Murray I. Barriers to the widespread adoption of health data standards: an exploratory qualita-tive study in tertiary healthcare organizations in Saudi Arabia. J Med Syst. 2013;37:9895. 114. Jang SH, Kim RH, Lee CW. Effect of u-healthcare service quality on usage intention in a healthcare service. Technol Forecast Soc Change. 2016;113:396–403. 115. Newsted PR, Huff SL, Munro MC. Survey instruments in information systems. MIS Q. 1998;22:553. 116. Cheung CMK, Lee MKO. Understanding the sustainability of a virtual community: model development and empirical test. J",
        "start_idx": 10556,
        "end_idx": 10684
      },
      {
        "text": "the sustainability of a virtual community: model development and empirical test. J Inf Sci. 2009;35:279–98. https ://doi.org/10.2307/30036 540. 117. Saunders M, Lewis P , Thornhill A. Research methods for business students. Res methods Bus students. 2016;649. http://lib.myili brary .com/Open.aspx?id=81948 7#. 118. Liu H, Chu H, Huang Q, Chen X. Enhancing the flow experience of consumers in China through interpersonal interaction in social commerce. Comput Human Behav. 2016;58:306–14. 119. Byrne BM. Structural equation modeling with AMOS: basics concepts, applications, and programming. Struct Equ Model. 2016. http://www.uta.fi/aktkk /lectu res/sem_en/pdf/sem_exerc ise_v2.4.pdf . 120. Von Der Heidt T, Scott D. Rethinking the role of external collaboration in product innovation. Int J Entrep Innov Manag. 2012;15:59–90. http://www.scopu s.com/inwar d/recor d.url?eid=2-s2.0-84857 30684 7&partn erID=40&md5=66b5e 12dd2 8c6cc f46ea 9bca9 8bfbe 54. 121. Bartlett MS. A",
        "start_idx": 10672,
        "end_idx": 10800
      },
      {
        "text": "7&partn erID=40&md5=66b5e 12dd2 8c6cc f46ea 9bca9 8bfbe 54. 121. Bartlett MS. A note on the multiplying factors for various X2 approximations. J R Stat Soc. 1954;16:296–8. http://www.jstor .org/stabl e/29840 57. 122. Podsakoff PM, MacKenzie SB, Lee JY, Podsakoff NP . Common method biases in behavioral research: a critical review of the literature and recommended remedies. J Appl Psychol. 2003;88:879–903. 123. Flynn B. Empirical research methods in operations management. J Oper Manag. 1990;9:250–84. 124. Hair JF, Anderson RE, Tatham RL, Black WC. Multivariate data analysis with readings. 5th ed. Prentice-Hill: Up. Sad-dle River; 1998. 125. Fornell C, Larcker D. Evaluating structural equation models with unobservable variables and measurement error. J Mark Res. 1981;18:39–50. https ://doi.org/10.2307/31513 12. 126. Hair JF, Black WC, Babin BJ, Anderson RE. Multivariate data analysis. Vectors.",
        "start_idx": 10788,
        "end_idx": 10916
      },
      {
        "text": "Hair JF, Black WC, Babin BJ, Anderson RE. Multivariate data analysis. Vectors. Upper Saddle River: Prentice Hall; 2010. p. 816. 127. Afshan S, Sharif A. Acceptance of mobile banking framework in Pakistan. Telemat Inform. 2016;33:370–87.",
        "start_idx": 10904,
        "end_idx": 10939
      }
    ],
    "23c7b594-d82f-4664-8dfc-c9cf484578c3": [
      {
        "text": "[Página 1] RESEARCH Open Access Big data security access control algorithm based on memory index acceleration in WSNs Jianhua Peng1,2*, Hui Zhou1, Qingjie Meng1and Jingli Yang1 * Correspondence: pengjh@niit.edu. cn 1College of Computer and Software, Nanjing Institute of Industry Technology, Nanjing 210046, People ’s Republic of China 2Nanjing Shendi Intelligent Construction Technology Research Institute, Nanjing 210019, People ’s Republic of ChinaAbstract The access control is used to ensure these data security when WSN (wireless sensor network) with a large number of base stations transmits huge amount of data to a data center server. Meanwhile big data systems are used to efficiently store, manage, and use data from large-scale WSNs. In big data systems for WSNs, the traditional access control technology will greatly affect the system performance. This paper",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "traditional access control technology will greatly affect the system performance. This paper first analyzes the data processing flow of the traditional access control strategy in big data systems, analyzes its time complexity, and explores how it affects system performance. Then, we propose the big data security access control algorithm based on memory index acceleration in WSNs which has better performance over the traditional ones. In our experiments, under the same test environment and security strategy, the performance has been greatly improved with the proposed algorithm. Keywords: Wireless sensor networks, Security, Big data, Access control, Memory index 1 Introduction A wireless sensor network (WSN) is an autonomous wireless communication system composed of a large number of micro-sensor nodes with limited computing capacity, storage capacity, and communication capability [ 1].",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "nodes with limited computing capacity, storage capacity, and communication capability [ 1]. The advancement of mass data collection technology in wireless sensor networks has led to the emergence of a large number of wireless sensor applications. It is becoming increasingly important to ensure the data security of big data systems for WSNs [ 2]. The security is the cornerstone of the big data systems. A data breach could result in serious harm to any of the individ- uals to whom the information relates. Access control is an effective method to ensure data security. It is based on authentication and authorization and is the most widely used strategy for data security prevention and protection in big data systems. It can re- strict access to key resources and prevent intrusion",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "systems. It can re- strict access to key resources and prevent intrusion of illegal users or inadvertent oper- ation of legitimate users [ 3]. The focus of access control is on authorization. In a distributed system, nodes need to be coordinated and the access rights are synchro- nized between nodes. After various security technologies applied to distributed © The Author(s). 2020 Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .Peng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 https://doi.org/10.1186/s13638-020-01725-1 [Página 2] systems, each technology faces its own security challenges. The distributed systems must not only implement access control policies for data leaving each collaboration system, but must also control access to local resources. Depending on the sensitivity of the data,",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "control access to local resources. Depending on the sensitivity of the data, it needs to ensure that distributed applications on other coordination systems have access to the data they are processing. With all these factors considered, the imple- mentation of the access control functions for distributed systems is very complicated [ 4]. The access control models can be divided into 5 categories: the discretionary access control (DAC) [ 5], the mandatory access control (MAC) [ 6], the role-based access con- trol (RBAC) [ 7–9], the attribute-based access control (ABAC) [ 10], the policy-based ac- cess control (PBAC) [ 11,12]. Various data security algorithms are used to improve the security of the access control model. These algorithms severely degrade the perform- ance of big data systems. There are three",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "severely degrade the perform- ance of big data systems. There are three ways to improve the performance of access control in big data systems. One is to improve the performance of the algorithm itself, the other is to improve the big data security model, and the third is to study the methods that affect the performance of big data systems after the algorithm is applied to the model. It is the focus of access control research from rights rules and control policy to get rights. Due to the large amount of data and the large number of access users, the wire- less sensor big data system still has major shortcomings in the way of getting and con- trolling access control rights. Separation of duty (SOD) is used for",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "con- trolling access control rights. Separation of duty (SOD) is used for enforcing least privilege concept in access control model [ 13]. In the RBAC model, rights are assigned to roles, and roles are associated with user to form rights relationships in the access control model [ 14]. In the ABAC model, each attribute has a set of attribute and value definitions. The defined relationship is combined with the user to form the rights rela- tionship of the access control model [ 15]. In the access control system, these rights re- lationships are stored in metadata files. The system gets the user ’s rights to access resources by parsing the metadata. The parsing process will affect the performance of access control. Binary tree method [ 16,17] can reduce",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "the performance of access control. Binary tree method [ 16,17] can reduce the parsing time. Moreover, we can also improve the performance of getting rights by including flexible authentication based on user context information in the attributes [ 18]. Big data systems for wireless sensor networks have lot of metadata because of the large amount of data and users. Therefore, it takes long time for access control to get rights from the metadata and judge rights. To improve its performance, the current main solution is to parameterize user function attributes to reduce the number of rights policies [ 19]. The method will restrict the design and application scope of access control policies. The Hadoop ecosys- tem provides a complete solution for big data. Therefore, it can provide all",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "provides a complete solution for big data. Therefore, it can provide all round func- tions such as data storage, data processing, data analysis, and data security for WSNs in the context of big data [ 20]. Apache Ranger is a data security management framework for the Hadoop ecosystem. It performs unified data authorization, management, and auditing for the Hadoop ecosystem. Various algorithms and methods for big data se- curity can be easily applied to the Hadoop ecosystem through it [ 21]. During the devel- opment of the Apache Ranger project, we found that after enabling access control, the performance of big data systems will be greatly reduced. The binary tree is the main method to improve the performance of access control. The binary search tree has no performance",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "the performance of access control. The binary search tree has no performance advantage when it is a single branch tree. It also takes a relatively long time when the binary tree is used to obtain permissions fromPeng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 2 of 17 [Página 3] a large amount of metadata in big data systems. It can improve the performance of per- mission acquisition in the big data access control process by parameterizing user func- tion attributes to reduce the number of permission policies [ 22]. However, this method will limit the design and application scope of access control policies. In top-level big data security engineering applications such as Apache Ranger, access control storage structured policy data. The system first",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "as Apache Ranger, access control storage structured policy data. The system first loads the access control policy into memory during the access control process. The system queries the policies from the memory, obtains permissions, and judges permissions in a conditional loop when user accesses data. So this method has serious performance problems. The method which uses L2 cache to build indexes can solve current problems and significantly improve the per- formance of access control for big data systems. There are three main contributions of this paper. The first contribution is to build a second-level cache to reduce the number of cycles during policy extraction. The second contribution is to build a memory index to shorten the access time of the security pol- icy. The third contribution is to",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "access time of the security pol- icy. The third contribution is to update the second-level cache and index content to en- sure the effectiveness of policy changes. In this way, the access control authority efficiency will be greatly improved without affecting the security of access control. The rest of the papers are organized as follows. Section 2 discusses the related work of the access control methods in big data system. Section 3 theoretically analyses the cause of performance degradation and proposes the big data security access control al- gorithm based on memory index acceleration in WSNs. Section 4 performs experimen- tal verification and results analysis. And section 5 concludes the paper with summary. 2 Related work 2.1 Data security for WSNs Data fusion can effectively reduce the data",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "2.1 Data security for WSNs Data fusion can effectively reduce the data transmission volume and network energy consumption in wireless sensor networks (WSNs). At the same time, it can be used to improve the security of wireless sensor network data. An intelligent data fusion algo- rithm was proposed in [ 23] for wireless sensor network based on hybrid delayed per- ception clustering. This algorithm combines the advantages of single-layer cluster structure and multi-layer cluster structure. It effectively improves the security of data transmission while reduces network delay and network energy consumption. Haomeng Xie [ 24] classifies various wireless sensor networks attack detection methods based on the protocol stack layer, explains the advantages and disadvantages of those methods, and measures the security of wireless sensor networks. A game theory-based",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "methods, and measures the security of wireless sensor networks. A game theory-based DSA algo- rithm was proposed in [ 25] to implement spectrum leasing and interference mitigation between SUs in network channels which reduces data loss in wireless sensor network data transmission and improves data security confidence. In order to improve the se- curity of data transmission in wireless sensor networks, a decision transmission scheme was proposed in [ 26] to enhance collaborative spectrum sensing in industrial IoT and established a cooperative spectrum sensing mathematical model based on decision transmission, which was added packet error and packet loss factors. With the further development of wireless communication and sensor technology, large-scale wireless sensor networks are being applied in more and more industries. The Internet of Vehicles is a typical",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "in more and more industries. The Internet of Vehicles is a typical large-scale wireless sensor network. It has thePeng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 3 of 17 [Página 4] characteristics of large data volume and high data security requirements. In order to improve the data security of large-scale wireless sensor networks, Jiliang Li [ 27] de- signed the CL-CPPA protocol which can effectively protect the data security of large- scale wireless sensor networks. Wearable devices are also a typical large-scale wireless sensor network. Hong Liu [ 4] designed a collaborative privacy protection scheme for wearable devices. This solution has authentication and data access control consider- ations in the context of space awareness and time awareness. The experimental results prove that this",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "of space awareness and time awareness. The experimental results prove that this scheme can better protect the security of the sensor ’s big data. 2.2 Data security for big data system In big data systems, storing cipher text in the cloud is one of the safest ways to store and access big data. However, verifying the user ’s access legitimacy and securely updat- ing the cipher text in the cloud based on the access policy specified by the data owner are two key challenges for making cloud-based big data storage effective. Traditional approaches either completely ignore the issue of access policy updates or delegate up- dates to third-party agencies. Access policies update is vital to enhance security and handle the dynamics caused by user joins and leaves. Based",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "security and handle the dynamics caused by user joins and leaves. Based on this, Chunqiang Hu [ 28] proposed a secure and verifiable access control scheme based on NTRU (Number The- ory Research Unit) cryptosystem for big data storage in the cloud. This solution allows the cloud server to effectively update cipher text when the data owner specifies a new access policy. Data owner can verify the update to resist the spoofing behavior of the cloud. The test results show that the big data system using the solution can effectively prevent users from cheating and has ability to resist various attacks. In big data systems, unstructured and semi-structured data account for the vast major- ity. The complicated data types make the traditional authorization mode difficult to meet the",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "complicated data types make the traditional authorization mode difficult to meet the minimum authorization principle. In big data systems, fine-grained access control can meet the minimum authorization principle. With the rapid development of big data sys- tems, the fine-grained access control model has opened up a new wave of access control research in the field of big data. After a large amount of research on the canonical system, Julian A. Padget [ 29] proposed a first-order logic based on deontic to represent and infer data access strategies. Shangping Wang [ 17]u s e db i n a r yt r e et e c h n o l o g yt od e a lw i t ha t t r i - bute revocation and grant, proposed",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "t ha t t r i - bute revocation and grant, proposed an effective RABE (revocable and grantable attribute-based encryption) scheme. Under the assumption of error hypothesis, the secur- ity of the scheme has selective security features in the standard model. The value of the access control can only be maximized when it is applied to big data systems. Yuqing Mo [ 30] discussed the security requirements of big data and extracted key technologies for big data security from authentication, authorization, access con- trol, data hiding and encryption, network security, and system security. Based on the key technology of extraction, he designed a security management system for the Hadoop platform. Apache Hadoop is an important framework for fault-tolerant distrib- uted big data storage and processing. The Hadoop core",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "for fault-tolerant distrib- uted big data storage and processing. The Hadoop core platform and other open source tools such as Apache Hive, Storm, and HBase provide an ecosystem that enables users to take full advantage of the potential of big data. Apache Ranger and Apache Sentry provide centralized policy management and implementation through plugins, providing fine-grained access control for components of this ecosystem [ 31,32].Peng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 4 of 17 [Página 5] The access control involves various algorithms and models. In a traditional system, the impact of access control technology on system performance is not obvious. How- ever, in big data systems, the impact is very serious because massive data is stored and processed in different nodes. In",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "serious because massive data is stored and processed in different nodes. In the application of access control to ensure the security of big data, especially the fine-grained access control, the specific implementation method of access control technology has a decisive impact on the performance. Through the verification of HBase, the Apache big data top-level project, the fine- grained big data access control method implemented by Apache Ranger seriously af- fects the performance of HBase. Based on the analysis of the impact of access control on the performance of HBase, this paper improves the implementation algorithm of Apache Ranger. The improvement algorithm significantly improved the performance of big data system. 3 Proposed method 3.1 Access control analysis in WSNs big data system The purpose of access control is to",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "in WSNs big data system The purpose of access control is to ensure the security of data. In the WSNs big data system, the access control will perform the access control operation according to the access control policy, access control information, access control decision, and access control implementation information to obtain the access right to the access data when the visitor accesses the data. As shown in Fig. 1, the execution of access control is a complex process. It is executed in the data storage, access, and processing stages in the WSNs big data system. The data storage structure is the basic concern of access control. It is determined by the characteristics of the data of WSNs. The resources in big data system can be deter- mined by:",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "WSNs. The resources in big data system can be deter- mined by: r¼dmr datadesðÞ ð 1Þ where ris the resource and datades is the characteristics of the data of WSNs. Get strategyAccessor OperateSecurity view (Filter rule)DataNode DataNode DataNode. . .FilterAccess request (Accessor,Operation,Data)DataData Access control decision Access control policyAccess control InformationAccess control implementationAccess control decision Acc esscontr olimp leme nta tion Judgement result FeedbackGetInformat ion MetaData CollectionStorage Real-time data analysis and processingData analysis and processingApplication Sensing Networking LayerData Collection LayerData Processing LayerApplicatio n Layer Data Storage Layer Fig. 1 Big data access control framework in WSNsPeng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 5 of 17 [Página 6] Big data system gets rights policies by loading rights resources (metadata) into mem- ory. Its formula",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "rights policies by loading rights resources (metadata) into mem- ory. Its formula is as follows: arp¼frðÞ ð 2Þ where ris the resource and arpis the rights policy for access control. The system can get the rights policy of the user through the user and policy, the for- mula is as follows: uap¼getUserPolicy u ;arpðÞ ð 3Þ where uap is the all rights policy of user for access control. The accessed object, user, and the rights policy to which the user belongs determine the access control policy when a user accesses the WSNs big data system. The formula is as follows: ucp¼getCurAP ao ;u;uap ðÞ ð 4Þ where aois the accessed object, uis the access user, uap is the all rights policy of user for access control, and ucpis",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "is the all rights policy of user for access control, and ucpis the access related rights policy. When a user accesses the WSNs big data system, he will be granted to access if the accessed object is in the access rights policy. The logic can be expressed by: up¼getAP ao ;ucpðÞ ð 5Þ where upis the rights. The process for users to get rights is as follows: up¼getAP ao ;ucpðÞ ¼>upower ¼getAP ao ;getCurAP ao ;u;uap ðÞ ðÞ ¼>upower ¼getAP ao ;getCurAP ao ;u;getUserPolicy u ;arpðÞ ðÞ ðÞ ¼>upower ¼getAP ao ;getCurAP ao ;user;getUserPolicy u ;frðÞ ðÞ ðÞ ðÞ ¼>upower ¼getAP ao ;getCurAP ao ;u;getUserPolicy u ;f dmp data pro ðÞ ðÞ ðÞ ðÞ ðÞ ð6Þ 3.2 Problem analysis As shown in Fig. 1, the access control for",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "3.2 Problem analysis As shown in Fig. 1, the access control for big data systems is very complex. So a unified big data security framework is needed to ensure the realization of big data access con- trol. Apache Ranger is a data security management framework for the Hadoop ecosys- tem. It performs unified data authorization, management, and auditing for the Hadoop ecosystem. Various algorithms and methods for big data security can be easily applied to the Hadoop ecosystem through it [ 18–21]. During the development of the Apache Ranger project, we found that after enabling big data security functions in big data sys- tems, the performance of big data systems will be greatly reduced, which will have a great impact on the application of big data systems. Apache",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "have a great impact on the application of big data systems. Apache Ranger uses a policy-based model to express the “User-Resource-Rights ” logic. It is also a fine-grained security control model that effectively manages data se- curity on the Hadoop platform. The “User-Resource-Rights ”logic has the following characteristics: /C15A user is an object that accesses a resource and belongs to a group or role.Peng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 6 of 17 [Página 7] /C15Different components correspond to different business resources. /C15AllowACL andDenyACL expressed rights. The AllowACL is an access control list that describes the conditions that allowed to access. The DenyACL is a negative access control list that describes the case of denying access. Definition 1 : Access control item",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "describes the case of denying access. Definition 1 : Access control item (ACI): A collection of visitors (users, user groups, roles) and access types, represented by AccessItem. AccessItem ¼List<User=Group =Role>þList<AccessType > ð7Þ where User/Group/Role is the object which accesses the big data resource; AccessType is the type of access object requires, i.e. access rights. Definition 2 : Allow access control item (ACCI): A control that allows access to big data resources, represented by allow. Definition 3 : Exceptions allow access control items (EAACI): Allow access to excep- tion access items in control items, represented by allowException . Definition 4 : Allow access control list (AACL): Allows the data set of the access con- trol item set plus the exception allowed access control set, represented by AllowACL . AllowACL",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "plus the exception allowed access control set, represented by AllowACL . AllowACL ¼List<AccessItem >allow þList<AccssItem >allowException ð8Þ Definition 5 : Negative controls item (NCI): Control that do not allow access to big data resources, expressed in deny. Definition 6 : Abnormal negative control item (ANCI): Exception accesses in access control entries did not allow, expressed as denyException . Definition 7 : Negative control list (NCL): A data set in the set of negative access control items that excludes the set of abnormal negative control items, represented by DenyACL . DenyACL ¼List<AccessItem >deny þList<AccssItem >denyException ð9Þ Definition 8 : Access control policy (ACP): A logical rule for accessing resources by a large data resource access object, consisting of a resource, an allowable control list, and a negative control list,",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "of a resource, an allowable control list, and a negative control list, represented by a Policy . Policy ¼List<Resource >þAllowACL þDenyACL ð10Þ Definition 9 : Access control service (ACS): A collection of access control policies, represented by a service. Figure 2shows the current policy-based access control process. As can be seen from Fig. 2, when a user AU requests access to resources Ares, Apa- che Ranger obtains all the rights policies of the user related to requested resources. The access right of AU will be determined by the following logic: If the ARes is in the NACI and not in ANAC, AU cannot access it. If the ARes is not in ACCI, AU cannot access it when it is in NACI and ANAC. If the ARes is not",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "when it is in NACI and ANAC. If the ARes is not in EAACI, AU cannot access it when it is in NACI, ANAC, and ACCI. If the ARes is in EAACI, AU can access it when it is in NACI, ANAC, and ACCI. If the ARes is not in ACCI, AU cannot access it when it is not in NACI.Peng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 7 of 17 [Página 8] If the ARes is not in EAACI, AU cannot access it when it is not in NACI but in ACCI. If the ARes is in EAACI, AU can access it when it is not in NACI but in ACCI. Figure 2contains the query of the strategy, the analysis of the",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "ACCI. Figure 2contains the query of the strategy, the analysis of the strategy, and the logical judgment based on the parsed strategy. Its logic can be decomposed to formulas (7)–(10) in the following order: AccessItem ¼List<User=Group =Role>þList<Access Type > DenyACL ¼List<AccessItem >deny þList<AccssItem >denyException AllowACL ¼List<AccessItem >allow þList<AccssItem >allowException Policy ¼List<Resource >þAllowACL þDenyACL2 664 ð11Þ It can be seen from formula ( 11) that the user can get rights only after a quadruple loop. Since the time complexity of the nested loop is equal to the number of times the innermost statement of the line is executed, the time complexity of the algorithm is T(n)=O(n4). In big data systems, due to the large amount of data and resources being distributed in many different cluster nodes, the time to",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "and resources being distributed in many different cluster nodes, the time to judge whether an access object has the right to access a certain resource right will be very long. Therefore, the current implementation of access control based on the “user-resource-priv~ ilege ”policy will seriously affect the performance of big data systems. 3.3 Security access control algorithm based on memory index acceleration Definition 10 : Security access control algorithm based on memory index acceleration (SACABMIA): Using the principle of second-level cache to build keys, establish in- dexes, and place frequently accessed resources and rights on the memory acceleratorUser access Match deny AccessItem Match denyException AccessItemYes Refuse to access resourcesNoMatch allow AccessItemNo YesMatch AllowException AccessItemYes NoNoAllow access to resources YesGet associated permissions Fig. 2 The current policy-based access control",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "to resources YesGet associated permissions Fig. 2 The current policy-based access control processPeng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 8 of 17 [Página 9] through the index. When a user requests access to a resource, system first checks the index. If there are no objects in the index, the index is then extracted and updated from the configuration resource. When the rights resource is changed, the algorithm updates the indexes in the secondary cache synchronously. Figure 3shows the execution flow of the algorithm. The key is constructed based on the user, the accessed object, and the type of access. The formula is as follows: pk¼generateKey ao ;u;at ðÞ ð 12Þ where aois an accessed object, uis the access user, atis the access",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "where aois an accessed object, uis the access user, atis the access type, and pkis the constructed key. The key can be parsed to obtain the accessed object, access user, and access type. The formula is as follows: ao;u;at hi ¼analyseKey pk ðÞ ð 13Þ A memory accelerated index is built using the key and the rights obtained when a user accesses a WSN big data system. indexMap ¼indexMap pk ;upðÞ ð 14Þ where pkis the constructed key, upis the rights and indexMap is the memory acceler- ated index. The memory accelerated index can be parsed to a list of keys. The formula is as follows: List<pk>¼getAllKeyInMemoryIndex indexMap ðÞ ð 15Þ where pkis the constructed key and indexMap is the memory accelerated index. The system updates the policies",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "and indexMap is the memory accelerated index. The system updates the policies corresponding to all keys in the memory acceleration index when the system policy is changed. The pseudo code for the update algorithm is as follows: Input: void Output: void 1. funcation updateIndexMap() 2. arp←f(r) /*Get new rights policies*/ 3. uap←getUserPolicy (u,arp) /* Assign policy to users*/ 4. listPowerkey ←getAllKeyInMemoryIndex (indexMap ) /* Obtain a list of keys */ 5. for k to listPowerkey.size 6. pk←listPowerkey.get(k) /*Get ao, user from Powerkey*/ 7. <ao, u, at> ←analyseKey (pk) /*Get ao, user from Powerkey*/ 8. ucp←getCurAP (ao,u,uap) /*Get user's access control policy*/ 9. up←getAP (ao,ucp) /* Get access control rights */ 10. indexMap ←indexMap (pk,up)/* Update the memory acceleration index */ 11. end for 12. return 13. end funcation",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "memory acceleration index */ 11. end for 12. return 13. end funcation The process of getting permissions is as follows:Peng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 9 of 17 [Página 10] up¼indexMap pk ðÞ ¼power ;indexMap pk ðÞ≠null getAP ðao;getAP ao ;getCurAP ao ;u;getUserPolicy u ;f dmp data pro ðÞ ðÞ ðÞ ðÞ ðÞ ;indexMap pk ðÞ ¼ null/C26 ð16Þ In a WSNs big data system, user rights policy rarely changes once it is successfully configured [ 33,34]. According to formula ( 16), the index relationship is relatively stable after the memory index is established, which greatly accelerates the efficiency and speed of access control in WSNs big data systems. It can be seen from formula ( 16) that the system can",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "It can be seen from formula ( 16) that the system can get rights when it only reads once from the memory index. So, the time complexity of the algorithm is O(n).User access resources Build Key (user-resource- rights) Match negative access control items? Matching abnormal negative access control items?Yes NoMatch allowed access control itemsNo YesMatch abnormal permission aCcess control?Yes NoNo YesMemory index accelerator Have permission object?FalseGet access to resourcesYes Have access?Allow access to resourcesYesRefuse to access resources End Build memory index accelerator Refuse to operate resourcesAllow operation resources End Policy changeUpdate memory index acceleratorParsing policy resources as user-resource-policy objectsRead completed? YesNoGet users, resources, permissionsGet strategic resources Parsing a policy resource as a policy resource object Read policy resource fileRead the memory index accelerator to get the Key Fig.",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "policy resource fileRead the memory index accelerator to get the Key Fig. 3 The execution flow based on the memory index accelerator algorithmPeng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 10 of 17 [Página 11] As can be seen from Fig. 3, the system builds a key based on User-Resource-Rights and accesses the memory index accelerator when the user accesses resources. If there is a User- Resource-Rights object corresponding to the key in the accelerator, the user will obtain the requested resources. Otherwise, the system obtains rights according to the logic of Fig. 1. The system re-reads the policy resource file, which is in json format, and converts it into a policy resource object when access control strategy is changed. The system also",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "policy resource object when access control strategy is changed. The system also updates the memory index accelerator. The specific method of building a key is as follows: After receiving the user ’s access request, the system parses the request to obtain the user who visited this time, the resources to be accessed this time, and the rights of this access. Adds the user of this visit, the resources to be accessed this time, and the rights of this visit to get the key. The main features of the “value ”object of the in-memory index accelerator include service information, service definition list, policy details related to the user, resource, and rights status of the current access. The first layer is service information, service definition list, and policy detail list.",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "first layer is service information, service definition list, and policy detail list. The service definition includes the name of the ser- vice, the basic configuration related to the service, the resource details, the access type, and the policy conditions. The policy details mainly include information about policy- related resources, policy visitor objects, negative policy details, access policies, and the Exception Access Policy details. 4 Experimental results and analysis The experiment was carried out on a four-node HBase cluster with Apache Ranger for test strategy. The hardware configuration of the test cluster node is listed in Table 1. A test table was created in HBase as listed in Table 2. The YCSB (Cloud Serving Benchmark) tool was used for testing in this experiment. It is a tool developed by",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "used for testing in this experiment. It is a tool developed by Yahoo to test the performance of cloud services. We generatedTable 1 Test cluster node hardware configuration details Node Configuration Node1 CPU: 32 core, Intel(R) Xeon(R) CPU E5-2650 v2 2.60GHz Node2 Memory: 128GB Node3 CPU: 48 core, Intel(R) Xeon(R) CPU E5-2670 v3 2.30GHz Node4 Memory: 128GB Table 2 Test table Test Info Score Name Age Math Physical Political 00001 John 15 90 93 95 00002 Paul 15 91 92 97 00003 Carly 16 90 94 91 00004 Scott 14 92 93 96Peng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 11 of 17 [Página 12] 102.4 GB of data using this tool and tested the current access control function based on the “User-Resource-Rights",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "tool and tested the current access control function based on the “User-Resource-Rights ”policy. We applied a security policy in HBase and tested performance degradation. The test results are listed in Table 3. As shown in Table 3, the performance is reduced by 10.10% with a security policy ap- plied for access control in HBase system. This result shows that the traditional access control based security technology greatly affects the performance of big data systems for WSN. Our previous analysis was proved by this result. We also applied a different number of security policies in HBase to analyze their im- pact on system performance. The test results are listed in Table 4. Figure 4shows the correlation between execution time and number of strategies. The number of policies is set",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "execution time and number of strategies. The number of policies is set from 1 to 25 with an interval of 5. The execution time in- creases from 642.01 to 732.46 s using the old algorithm with the increase of the number of strategies. This indicates that the impact of access control security on system per- formance is high and the increase in the policy has also big effect on the performance when the old method is used. Figure 5shows the correlation between performance degradation and number of strategies. The number of policies is set from 1 to 25 with an interval of 5. The per- formance degradation rate decreased from 0 to 14.09% using the old algorithm with the increase of the number of strategies. This indicates that",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "algorithm with the increase of the number of strategies. This indicates that the impact of access con- trol security on system performance is high and the increase in the policy has also big effect on the performance degradation rage when the old method is used. As shown in Table 4, the execution time of HBase system gradually increased and its performance gradually degraded as the number of strategies increases. When the num- ber of strategies reaches 25, the system performance drops by 14.09%. These results prove that the impact of access control algorithms on the performance of big data sys- tems is gradually increasing with the number of policies increases. The results alsoTable 3 Current access control function test results based on the User-Resource-Rights policy User Policy numberNodes",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "control function test results based on the User-Resource-Rights policy User Policy numberNodes The amount of dataExecution time (s)Data processing speed (ops/s)Performance degradation rate Hbase_test 0 4 100 million(102.4GB) 642.01 155687 base Hbase_test 1 4 100 million(102.4GB) 706.26 141520 10.01% ↓ Table 4 Test results of different number of security policies based on the User-Resource-Rights policy User Policy numberNodes The amount of dataExecution time (s)Data processing speed (ops/s)Performance degradation rate Hbase_test 0 4 100 million(102.4 GB) 642.01 155687 base Hbase_test 5 4 100 million(102.4 GB) 712.81 140221 11.03% ↓ Hbase_test 10 4 100 million(102.4 GB) 719.1 138994 12.01% ↓ Hbase_test 15 4 100 million(102.4 GB) 725.52 137764 13.01% ↓ Hbase_test 20 4 100 million(102.4 GB) 731.88 136578 14.00% ↓ Hbase_test 25 4 100 million(102.4 GB) 732.46 136460 14.09% ↓Peng",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "14.00% ↓ Hbase_test 25 4 100 million(102.4 GB) 732.46 136460 14.09% ↓Peng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 12 of 17 [Página 13] verified that our analysis and judgment that the access control technology seriously af- fects the performance of big data systems is correct. As a comparison, we performed experiments on the proposed new algorithm on HBase. Test results are listed in Table 5. From the results in Table 5, we can see that with the new algorithm the performance of HBase reduced only by 2.74% when a security policy is applied for access control. A different number of security policies in HBase is applied to test the performance of the new algorithm. The test results are listed in Table 6.",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "of the new algorithm. The test results are listed in Table 6. Figure 6presents the correlation between execution time and number of strategies. The graph shows that the time for users to access 102.4 GB data is 652.40 s in the case of one security policy, the time for users to access 102.4 GB data is 652.60 s in the case of five security policies, the time for the user to access the 102.4 GB data is 652.78 s in the case of ten security policies, and the time for the user to access the 102.4 GB data is 653.29 s in the case of twenty-five security policies. It shows that the increase in the policy has little effect on the time spent in parsing the data. Fig.",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "has little effect on the time spent in parsing the data. Fig. 4 Execution time trend Fig. 5 Performance degradation rate trendPeng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 13 of 17 [Página 14] As shown in Table 6, when the number of strategies increases, execution time of HBase with new algorithm increases much slower than same system with traditional al- gorithm. System performance degradation is also much slower with new algorithm. When the number of strategies reaches 25, the system performance drops only 2.88%. The system performance is much less affected. Figure 7reveals the correlation between performance degradation and number of strategies. The graph shows that the performance degradation rage for users to access 102.4 GB data is 2.74% in the case",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "for users to access 102.4 GB data is 2.74% in the case of two security policy, the performance degradation rage for users to access 102.4 GB data is 2.77% in the case of three security policies, the performance degradation rage for the user to access the 102.4 GB data is 2.80% in the case of four security policies, and the performance degradation rage for the user to ac- cess the 102.4 GB data is 2.88% in the case of seven security policies. This indicates that the impact of access control security on system performance is low and the increase in the policy has little effect on the performance degradation rage. Figure 8shows the performance degradation trend of the new algorithm compared with the traditional algorithm. The number of",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "of the new algorithm compared with the traditional algorithm. The number of policies is set from 1 to 25 with an interval of 5. The performance degradation rate decreased from 0 to 14.09% using the old algorithm, and the performance degradation rate only decreased from 0 to 2.88% using the new algo- rithm with the increase of the number of strategies. This indicates that the impact of ac- cess control security on system performance is low, and the increase in the policy has also little effect on the performance degradation rage when the method is used. From the experimental results, we can get the conclusions: The new algorithm can significantly improve the performance of big data systems where the fine-grained security policy-based access control model is applied. Big",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "systems where the fine-grained security policy-based access control model is applied. Big data security technologies mainly include data asset grooming, data encryption, data security operation and maintenance, data desensitization, and data leakage scan- ning. These security technologies can only exert their value if applied to big data sys- tems. The performance impact of big data systems using big data security includes twoTable 5 Access control function test results using the new algorithm User Policy numberNodes The amount of dataExecution time (s)Data processing speed (ops/s)Performance degradation rate Hbase_ test0 4 100 million102.4GB)635 157484 base Hbase_ test1 4 100 million102.4GB)652 153284 2.74% ↓ Table 6 Test results of different number of security policies using the new algorithm User Policy numberNodes The amount of dataExecution time (s)Data processing speed (ops/s)Performance degradation",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "Policy numberNodes The amount of dataExecution time (s)Data processing speed (ops/s)Performance degradation rate Hbase_test 0 4 100 million(102.4 GB) 635.02 157484 base Hbase_test 5 4 100 million(102.4 GB) 652.60 153239 2.77% ↓ Hbase_test 10 4 100 million(102.4 GB) 652.78 153195 2.80% ↓ Hbase_test 15 4 100 million(102.4 GB) 652.91 153165 2.82% ↓ Hbase_test 20 4 100 million(102.4 GB) 653.10 153120 2.85% ↓ Hbase_test 25 4 100 million(102.4 GB) 653.29 153075 2.88% ↓Peng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 14 of 17 [Página 15] aspects, one is the impact of the security algorithm itself on the system, and the other is the impact of the method of applying the security algorithm on the system perform- ance. This paper mainly studies from the second",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "on the system perform- ance. This paper mainly studies from the second perspective to improve the perform- ance of big data systems. The method proposed in the paper will not improve the performance of big data security algorithms. By analyzing the results of RANGER-1729 and this experiment, we can draw the con- clusion that the new algorithm can significantly reduce the impact of access control technology on the performance of big data systems. RANGER-1729 is an issue of Apa- che Ranger project, and its link address is https://issues.apache.org/jira/browse/RAN- GER-1729 . 5 Conclusions Performance is the life of big data systems and security is the cornerstone of big data systems. When big data systems apply access control technology to ensure the security of data, existing methods will seriously affect",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "technology to ensure the security of data, existing methods will seriously affect the system performance. This paper first Fig. 6 The execution time trend Fig. 7 The performance degradation rate trendPeng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 15 of 17 [Página 16] analyses the data processing flow of the existing access control technology in the big data system and its time complexity. Then it points out that the system performance will be greatly affected by this existing technology. We proposed a big data security ac- cess control algorithm based on memory index acceleration for WSNs in this article. We walked through its data processing flow and analyzed its time complexity. We also theoretically proved that the new algorithm has better performance. Through",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "We also theoretically proved that the new algorithm has better performance. Through experi- ments, we further proved that compared with the traditional access control technology, the new algorithm has less impact on the performance of big data systems. Abbreviations YCSB: Cloud Serving Benchmark; NTRU: Number Theory Research Unit; DAC: Discretionary access control; MAC: Mandatory access control; RBAC: Role-based access control; ABAC: Attribute-based access control; PBAC: Policy- based access control; RABE: Revocable and grantable attribute-based encryption Acknowledgements The authors acknowledged the anonymous reviewers and editors for their efforts in valuable comments and suggestions. Authors ’contributions J. Peng proposes the innovation ideas and theoretical analysis, and H. Zhou carries out experiments and data analysis. Q. Meng and J. Yang conceived of the study, and participated in its design and coordination",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "Yang conceived of the study, and participated in its design and coordination and helped to draft the manuscript. All authors read and approved the final manuscript. Funding The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: This work was supported by the school research fund of Nanjing Institute of Industry Technology (Grant No. YK18-05-03). Availability of data and materials Data sharing is not applicable to this article as no datasets were generated or analyzed during the current study. Competing interests The authors declare that they have no competing interests. Received: 14 January 2020 Accepted: 23 April 2020 References 1. Z. Huang, X. Xu, J. Ni, H. Zhu, C. Wang, Multimodal representation learning for recommendation in Internet of Things. IEEE",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "C. Wang, Multimodal representation learning for recommendation in Internet of Things. IEEE Internet Things J. 6(6), 10675 –10685 (2019) 2. B. Wu, T.L. Yip, X. Yan, C. Guedes Soares, Fuzzy logic based approach for ship-bridge collision alert system. Ocean Eng. 187, 106152 (2019) 3. P. Alexander, L. Pike, P. Loscocco, G. Coker, Model checking distributed mandatory access control policies. ACM Transactions on Information and System Security (TISSEC) 18(2), 1 –25 (2015) Fig. 8 Comparison of performance degradation trendsPeng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 16 of 17 [Página 17] 4. H. Liu, X. Yao, T. Yang, H. Ning, Cooperative privacy preservation for wearable devices in hybrid computing-based smart health. IEEE Internet Things J. 6(2), 1352 –1362 (2018) 5. Terzis, S., Wagealla, W.,",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "Internet Things J. 6(2), 1352 –1362 (2018) 5. Terzis, S., Wagealla, W., English, C., & Nixon, P. Trust lifecycle management in a global computing environment. In International Workshop on Global Computing (pp. 291-313). Springer, Berlin, Heidelberg (2004). 6. Chang, R., Jiang, L., Chen, W., He, H., Yang, S., Jiang, H., & Liu, Y. (2018). Towards a multilayered permission-based access control for extending Android security. Concurrency and Computation: Practice and Experience, 30(5), e4180 (2018). 7. N. Mundbrod, M. Reichert, Object-specific role-based access control. International Journal of Cooperative Information Systems 28(01), 1950003 (2019) 8. M.U. Aftab, Z. Qin, N.W. Hundera, O. Ariyo, N.T. Son, T.V. Dinh, Permission-based separation of duty in dynamic role- based access Control Model. Symmetry 11(5), 669 (2019) 9. C.M. Subramanian, A.K. Cherukuri, C. Chelliah, Role based",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "11(5), 669 (2019) 9. C.M. Subramanian, A.K. Cherukuri, C. Chelliah, Role based access control design using three-way formal concept analysis. Int. J. Mach. Learn. Cybern. 9(11), 1807 –1837 (2018) 10. V. Hu, C. Kuhn, D. Richard, D.F. Ferraiolo, Attribute-based access control. Computer 48(2), 85 –88 (2015) 11. M. Uriarte, J. Astorga, E. Jacob, M. Huarte, M. Carnerero, Expressive policy-based access control for resource-constrained devices. IEEE Access 6,1 5 –46 (2017) 12. J.P. Cruz, Y. Kaji, N. Yanai, RBAC-SC: role-based access control using smart contract. Ieee Access 6, 12240 –12251 (2018) 13. S. Aditham, N. Ranganathan, A system architecture for the detection of insider attacks in big data systems. IEEE Transactions on Dependable and Secure Computing 15(6), 974 –987 (2017) 14. Q. Xia, E.B. Sifah, K.O.B.O. Agyekum, H. Xia,",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "974 –987 (2017) 14. Q. Xia, E.B. Sifah, K.O.B.O. Agyekum, H. Xia, K.N. Acheampong, A. Smahi, M. Guizani, Secured fine-grained selective access to outsourced cloud data in IoT environments. IEEE Internet Things J. 6(6), 10749 –10762 (2019) 15. Y. Zhu, D. Huang, C.J. Hu, X. Wang, From RBAC to ABAC: constructing flexible data access control for cloud storage services. IEEE Trans. Serv. Comput. 8(4), 601 –616 (2014) 16. S. Wang, X. Zhang, Y. Zhang, Efficient revocable and grantable attribute-based encryption from lattices with fine-grained access control. IET Inf. Secur. 12(2), 141 –149 (2018) 17. M. Zhang, D. Zhang, F. Goerlandt, X. Yan, P. Kujala, Use of HFACS and fault tree model for collision risk factors analysis of icebreaker assistance in ice-covered waters. Saf. Sci. 111, 128 –143 (2019)",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "of icebreaker assistance in ice-covered waters. Saf. Sci. 111, 128 –143 (2019) 18. M. Alam, N. Emmanuel, T. Khan, Y. Xiang, H. Hassan, Garbled role-based access control in the cloud. J. Ambient. Intell. Humaniz. Comput. 9(4), 1153 –1166 (2018) 19. S. Pal, M. Hitchens, V. Varadharajan, T. Rabehaja, Policy-based access control for constrained healthcare resources in the context of the Internet of Things. J. Netw. Comput. Appl. 139,5 7 –74 (2019) 20. M. Babar, F. Khan, W. Iqbal, A. Yahya, F. Arif, Z. Tan, J.M. Chuma, A secured data management scheme for smart societies in industrial internet of things environment. IEEE Access 6, 43088 –43099 (2018) 21. D. Chattaraj, M. Sarma, A.K. Das, N. Kumar, J.J. Rodrigues, Y. Park, HEAP: an efficient and fault-tolerant authentication and key exchange",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "Rodrigues, Y. Park, HEAP: an efficient and fault-tolerant authentication and key exchange protocol for Hadoop-assisted big data platform. IEEE Access 6, 75342 –75382 (2018) 22. Huang Z., Tang J., G. Shan, Ni J., Chen Y., & Wang C. An efficient passenger-hunting recommendation framework with multi-task deep learning. IEEE Internet of Things Journal . DOI: https://doi.org/10.1109/JIOT.2019.2901759(2019) . 23. X. Liu, R. Zhu, A. Anjum, J. Wang, H. Zhang, M. Ma, Intelligent data fusion algorithm based on hybrid delay-aware adaptive clustering in wireless sensor networks. Futur. Gener. Comput. Syst. 104,1–14 (2020) 24. H. Xie, Z. Yan, Z. Yao, M. Atiquzzaman, Data collection for security measurement in wireless sensor networks: a survey. IEEE Internet Things J. 6(2), 2205 –2224 (2018) 25. X. Liu, R. Zhu, B. Jalaian, Y. Sun, Dynamic spectrum",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "(2018) 25. X. Liu, R. Zhu, B. Jalaian, Y. Sun, Dynamic spectrum access algorithm based on game theory in cognitive radio networks. Mobile Networks and Applications 20(6), 817 –827 (2015) 26. R. Zhu, X. Zhang, X. Liu, W. Shu, T. Mao, B. Jalaian, ERDT: Energy-efficient reliable decision transmission for intelligent cooperative spectrum sensing in industrial IoT. IEEE Access 3, 2366 –2378 (2015) 27. J. Li, Y. Ji, K.K.R. Choo, D. Hogrefe, CL-CPPA: certificate-less conditional privacy-preserving authentication protocol for the Internet of Vehicles. IEEE Internet Things J. 6(6), 10332 –10343 (2019) 28. C. Hu, W. Li, X. Cheng, J. Yu, S. Wang, R. Bie, A secure and verifiable access control scheme for big data storage in clouds. IEEE Transactions on Big data 4(3), 341 –355 (2017) 29. J.A. Padget,",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "IEEE Transactions on Big data 4(3), 341 –355 (2017) 29. J.A. Padget, W.W. Vasconcelos, Fine-grained access control via policy-carrying data. ACM Transactions on Internet Technology (TOIT) 18(3), 1 –24 (2018) 30. Y. Mo, A data security storage method for IoT under Hadoop cloud computing platform. Int. J. Wireless Inf. Networks 26(3), 152 –157 (2019) 31. X. Fu, Y. Gao, B. Luo, X. Du, M. Guizani, Security threats to Hadoop: data leakage attacks and investigation. IEEE Netw. 31(2), 67 –71 (2017) 32. X. Min, Q. Yong, W. Kui, Z. Jizhong, L. Mo, Using potential to guide mobile nodes in wireless sensor networks. Ad Hoc & Sensor Wireless Networks 12(3-4), 229 –251 (2011) 33. Y. Yang, X. Zheng, W. Guo, X. Liu, V. Chang, Privacy-preserving smart IoT-based healthcare big data",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "W. Guo, X. Liu, V. Chang, Privacy-preserving smart IoT-based healthcare big data storage and self- adaptive access control system. Inf. Sci. 479, 567 –592 (2019) 34. K. Yang, Q. Han, H. Li, K. Zheng, Z. Su, X. Shen, An efficient and fine-grained big data access control scheme with privacy-preserving policy. IEEE Internet Things J. 4(2), 563 –571 (2016) Publisher ’sN o t e Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.Peng et al. EURASIP Journal on Wireless Communications and Networking (2020) 2020:90 Page 17 of 17",
        "start_idx": 7656,
        "end_idx": 7749
      }
    ],
    "ce755c85-2d8c-46b5-821f-4f3090e19a0c": [
      {
        "text": "[Página 1] (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 1 4, No. 9, 202 3 990 | P a g e www.ijacsa.thesai.org Next -Generation Intrusion Detection and Prevention System Performance in Distributed Big Data Network Security Architectures Michael Hart1, Rushit Dave2, Eric Richardson3 College of Science, Engineering, & Technology , Minnesota State Uni versity, Mankato, United States1, 2 College of Health and Human Services , University of North Carolina Wilmington , United States3 Abstract—Big data systems are expanding to support the rapidly growing needs of massive scale data analytics. To safeguard user data, the design and placement of cybersecurity systems is also evolving as organizations to increase their big data portfolios. One of several challenges presented by these changes is benchmarking real -time big data",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "several challenges presented by these changes is benchmarking real -time big data systems that use different network security architectures. This work introduces an eight - step benchmark process to evaluate big data systems in varying architectural environments. The benchmark is tested on real - time big data systems running in perimeter -based and perimeter - less network environments. Findings show that marginal I/O differences exist on distributed file systems between network architectures. However, during various types of cyber incidents such as distributed denial of service (DDoS) attacks, certain security ar chitectures like zero trust require more system resources than perimeter -based architectures . Results illustrate the need to broaden research on optimal benchmarking and security approaches for massive scale distributed computing systems. Keywords —Big data systems; zero trust",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "for massive scale distributed computing systems. Keywords —Big data systems; zero trust architecture; benchmarking; distributed denial of service attacks I. INTRODUCTION Big data systems are u nified environments designed for massive -scale data analytics . Systems capable of handling large amounts of data are becoming more important as the volume of data created and communicated over the Internet increases [1]. Cybersecurity systems play an important role in ensuring the large quantities of data on the Internet remains safe. One dimension of several necessary to accomp lish the latter are next -generation security devices. I ntrusion detection and prevention systems (ID PSs) properly manage data accessibility, privacy, and safety. IDPS algorithms are able to identify cyber threats using several mechanisms. This includes using prior information from previous attacks, anomalies",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "using several mechanisms. This includes using prior information from previous attacks, anomalies in network packets [1], and machine learning [ 2]. As big data systems become more common, their roles will continue to expand. This includes the capability to analyze and detect information security vul nerabilities at scale. For example, several big data frameworks exist that discover distributed denial of service (DDoS) attacks [3]. This expansion of roles offers many exciting opportunities for organizations . However, as the use of big data systems grow s, the capability of attackers to leverage associated parallel computing power for nefarious reasons also increases [3]. A systematic review of 32 papers pertaining to securing big data found that a critical need in future research is building more secure big data infrastructure [4].",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "need in future research is building more secure big data infrastructure [4]. Contributing to the latter objective , the researchers demonstrate how varying network architectures impact the security and p erformance of big data systems. Organization of the paper is as follows. Section II reviews literature on intrusion detection and prevention methods for big data systems. Section III outlines the research design and methodologies used to test perimeter -based security and perimeter -less security applied to a big data system environment. Section IV describes the research results. Section V concludes the study by discussing the limitations and future outlook. II. LITERATURE REVIEW Work is necessary to optimize both the information security and performance of distributed systems. Today , several open - source big data frameworks provide remarkable potential",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "Today , several open - source big data frameworks provide remarkable potential for solving challenging data science and related problems by leveraging powerful parallel and distributed data processing . However, securing these systems often carries performance penalties. The review of literature that follows explores research on the impact of various IT infrastructure security strategies and their influence on big data environments. It begins by reviewing comprehensive surveys most closely related to information security and big data systems . A. Surveys of Big Data and Intrustion Detection Previous s ystematic reviews of literature focused on information security and big data provide a vast array of objectives. A prominent theme is using deep learning [1] and machine learning [2] to assist in detecting or preventing cybersecurity attacks. This line of",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "[2] to assist in detecting or preventing cybersecurity attacks. This line of research often utilizes deep learning or machine learning algorithms for near real -time data protection . A recent and well cited comprehensive survey in [1] evaluates how deep learning is used for intrusion detection systems in the cybersecurity domain. It found notable contrast s between machine learning approaches in cybersecurity and deep learning. Conventional machine learning approaches utilized in cybersecurity were classified by approach es such as artificial neural networks (ANNs), Bayesian networks , decision trees, fuzzy logic , k-means clustering , k-nearest neighbor (kNN) algorithm, and support vector machines (SVMs). The [Página 2] (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 1 4, No. 9, 202 3 991 | P a g e",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "1 4, No. 9, 202 3 991 | P a g e www.ijacsa.thesai.org survey centered on deep learning focal intrusion detection methods that included autoencoders (AEs), convolutional neural networks (CNNs), deep belief networks (DBNs), generative adversarial networks (GANs), and long short -term memory (LSTM) recurrent neural networks [1]. AEs, DBNs , and GANs were highlighted in [1] for their unsupervised learning strengths. In the absence of gradient estimation, AEs can use gradient descent to train data. A strength of LSTM is its capabilities in analyzing time -series data. CNNs do not need as much data processing prior to evaluation as c ertain algorithms and is able to classify cyber - attacks using multiple characteristics well. Combined, the survey of literature finds that AEs, CNNs, DBNs, GANs, and LSTM",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "the survey of literature finds that AEs, CNNs, DBNs, GANs, and LSTM networks each have potential to improve intrusion detection methods. Furthermore, the survey [1] outlined the importance of dataset reliability when evaluating deep learning intrusion detection effectiveness. Variance in cybersecurity attack datasets can introduce model bias when comparing multiple deep learning methods. Thus, any biases i n attack datasets or data from live systems could increase spurious results [1]. A subsequent theme in the literature concentrate s on cybersecurity and privacy prevention in big data applications. While this research again employs various data science meth ods to detect or prevent data breaches, it also illustrate s how big data techniques can prevent information privacy issues . Research in [4] led to a proposed model for enhancing information",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": ". Research in [4] led to a proposed model for enhancing information privacy. The model highlights people, organizations, society, and government roles . It leverages IDS, IPS, and encryption as its primary techniques to prevent data breaches [4]. B. Big Data Architectures and Information Security As big data evolves, the supporting infrastructures will require proper encryption, intrusion detection , and intrusion prevention . Changing architectures within computer networks, messaging techniques, and undefined communication metho ds introduce numerous challenges . In a 2014 study Mitchel and Chen [5] recognized this paradigm. Their emphasis on cyber - physical systems (CPS) ranging from smart grids to unmanned aircraft systems led to the classification of four primary intrusion detect ion categories. These include legacy technologies, attack sophistication, closed control loops, and physical",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "categories. These include legacy technologies, attack sophistication, closed control loops, and physical process monitoring. Each of the latter is narrow concepts as they relate to the broader field of intrusion detection, underlying the unique customi zation of IDSs for cyber -physical systems [5] . Three years later Zarpelo et al. [6] outlined a similar but distinct paradigm; intrusion detection focal to the Internet of things (IoT). The researchers stated that IoT has similar information security matter s as the Internet, cloud services, and wireless sensor networks (WSNs). Despite similarities, IoT information security approaches are distinct , according to the authors due to concepts such as data sharing between users, the volume of interconnected object s, and the amount of computational power of the associated devices. Like cyber -",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "the amount of computational power of the associated devices. Like cyber - physical systems, IoT presents diverse challenges to the design of instruction detection systems [6]. Designing secure cloud computing environments poses several novel problems at multiple infrastructure layers. As an example, cloud resources can be leased by numerous vendors focused on varying as -a-service models such as infrastructure as a service (Iaas), platform as a service (PaaS), and/or software as a service (SaaS). Multi -cloud applications rely upon the seamless integration of cloud resources from providers focused on one or many as -a-service types, which continue to expand. In Casola et al. [7] a model is outlined for designing, creating, and implementing multi -cloud applications. The flexible approach accounts for varying as -a- service components. Security -by-design",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "The flexible approach accounts for varying as -a- service components. Security -by-design is a primary objective of the process lifecycle between the functional design of multi - cloud applications and the security design. The functional design phase defines the application logic, interconnections of services, and resource requirements. In the security design phase, each cloud element is assessed in terms of security risks and security needs. Security policies and controls are designed based on the lat ter requirements. Similar to CPS [5] and IoT [6], the multi -cloud application model is a subsequent example of how information security solutions play a prominent role due to the systems’ distinct architectural and infrastructure layers. Securing big data environments or leveraging associated techniques like machine learning to enhance information security intertwines numerous",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "leveraging associated techniques like machine learning to enhance information security intertwines numerous fields include but not limited to CPS, IoT, and cloud computing. Like big data systems, CPS requires cybersecurity protection [8] of private data [9]. Big data, IoT, and CPS often overlap through the ad hoc interfaces of systems such as smart vehicles, buildings, factories, transportation systems, and grids [10]. As a vulnerable attack surface, IoT advances the need for in telligent information security. Machine learning [11], including ensemble intrusion detection [12], and IDS design [13] are proposed techniques to mitigate malicious cyber security attacks. Due in part to porous attack surfaces in cloud centric big data, IDSs may require collaborative frameworks [14]. In [15], fuzzy c means cluster (FCM) and support vector machine (SVM) were proposed",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "fuzzy c means cluster (FCM) and support vector machine (SVM) were proposed as a collaborative technique for IDS detection rates. Compared to other mechanisms, the proposed hybrid FCM -SVM showed lower false alarm ratios and higher detection accuracy [15]. Furthermore, [16] illuminates the need for scaling IDS detection algorithms using the resources of parallel computing in the cloud. In [17] the res earchers propose the BigCloud security -by- design framework. The framework draws from the need to integrate big data security into the system development lifecycle. Its primary cloud application domain is focal to infrastructure as a service. It notes IaaS as one of the faster growing as -a-service options for big data. The model helps design and enforce secure authentication, authorization, data auditability, availability, confidentiality, integrity, and",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "design and enforce secure authentication, authorization, data auditability, availability, confidentiality, integrity, and privacy. However, its IaaS concentration could provid e greater benefits to as -a-service components specific to host operating systems, hypervisors, networking, and hardware [17]. Similar to IaaS, the evolution of serverless platforms and Function -as-a-service (FaaS) applications requires careful security des ign to overcome security threats that new services often suffer [18]. [Página 3] (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 1 4, No. 9, 202 3 992 | P a g e www.ijacsa.thesai.org While distinct, CPS, IoT, cloud computing, and big data are merely a few examples of why designing intrusion detection and prevention systems remains highly elastic in modern computation al architectures . As the information technology landscape changes, information security",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "computation al architectures . As the information technology landscape changes, information security bends to meet the evolving needs of the complete environment. To conclude the literature review , the authors will outline several relevant studies introduc ing potential solutions to design stronger information security controls for big data systems . C. Encryption An ongoing challenge in distributed big data systems is securing communication between multiple systems operating across various computer networks . Apache Hadoop a nd Apache Spark are examples of big data frameworks that present several opportunities for attackers to access the data they facilitate. Central to big data frameworks is the ability to use parallel processing to analyze massive amounts of data. MapReduce is one of many programming paradigms that leverages Hadoop to extract valuable knowledge",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "one of many programming paradigms that leverages Hadoop to extract valuable knowledge from large volumes of data . However, like most application or service modules within big data frameworks, MapReduce highlights the vast attack vectors that exist in di stributed big data systems. MapReduce examples in literature include side channel attacks [19], job composition attacks [20], and malicious worker compromises in the form of distributed denial -of-service (DDoS) or replay attacks [21], Eaves dropping and data tampering [22]. Encryption is a primary countermeasure to secure transmissions and prevent data leaks between big data servers [19]. A primary objective in addressing cybersecurity attacks on parallel processing services is identifying and preventing leaks that often occur during data transmission between distributed worker nodes , also referred to as DataNodes in",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "transmission between distributed worker nodes , also referred to as DataNodes in Apache Hadoop . These unique yet integrated servers work in parallel to complete MapReduce jobs . Often in Hadoop, data is stored and retrieved from the Hadoop Distributed File System (HDFS) . In [19] side -channel attacks are addressed that can occur between MapReduce workers that utilize HDFS for data storage. These types of cybersecurity attacks can target worker nodes to extract valuable inform ation pertaining to MapReduce jobs such as the amount of packet bandwidth. This further contributes to successful pattern attacks. The authors proposed a solution to this vulnerability labeled Strong Shuffle that enforces strong data hiding between workers [19]. In contrast to alternative countermeasures such as correlation hiding in [20], Strong Shuffle avoids",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "to alternative countermeasures such as correlation hiding in [20], Strong Shuffle avoids leaking the number of records accepted by each reducer during MapReduce runtime . Secure plaintext communications is a function of semantically secur e encryption in the Strong Shuffle solution [19]. In [19] data communicated between Hadoop DataNodes and stored in HDFS is encrypted with semantically secure AES -128-GCM encryption. Although the latter helps prevent clear text leakage between MapReduce job s in Hadoop, encryption in big data environments has limitations. For example, encrypted databases can still reveal certain information during operations that include table queries. Deterministic encryption and order -preserving encryption can leak the equa lity relationship and the order between records. One proposed solution is semantically secure encryption. In [23] the authors propose a semantically",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "solution is semantically secure encryption. In [23] the authors propose a semantically secure database system named Arx . Alternative to order -preserving encryption, semantic security within Arx only allows an attacker to extract order relationships and frequency of the direct database query in use in contrast to the entire database. The authors note that worst -case attackers would gain as much information from a data leak as deterministic or orde r-preserving encryption over time [23]. While methods such as encryption and authentication help with cross -node data leaks, they do not prevent other attacks , such as DDoS and passive network eavesdropping [21] . A subsequent countermeasure is the effective design and implementation of intrusion detection and prevention systems [14]. D. Next-Generation Security and Big Data Systems Next",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "and prevention systems [14]. D. Next-Generation Security and Big Data Systems Next -generation security at a high level can detect and prevent malicious cybersecurity attacks. Much of the literature focuses on identifying malicious network packets in real -time. The comprehensive survey in [24] reviews how modern data mining techniques are evolving to meet real -time detection needs. The review classifies intrusion detection systems by architecture, imple mentation, and detection method s. Detection methods are categorized as anomaly -based, signature based, and hybrids. Signature based methods or misuse often rely upon a database that defines patterns or existing malicious attack signatures. Anomaly detectio n can detect non-normal network traffic behavior that has yet to be defined in a signature database. Data mining methods including supervised, unsupervised, and hybrid learning",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "a signature database. Data mining methods including supervised, unsupervised, and hybrid learning are being used to improve anomaly -based intrusion detection systems [24]. While supervised , unsupervised , and hybrid learning IDS research continues to progress [24], the ongoing need to improve existing big data implementations remains. In several systematic literature reviews [1, 2, 3, 24], IDSs are known to have limitations that c ontradict the performance benefits of parallel processing and distributed computing. For example, large signature based systems drain CPU and memory resources [24]. While researchers continue to advance areas of intrusion detection such as packet anomalies and encryption , only a few studies are advancing security by design and its effects on varying big data architectures [1]. To address this need, the authors of this",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "big data architectures [1]. To address this need, the authors of this study designed a distributed big data system over a wide area network to explore the perfo rmance of distributed nodes under different network traffic loads . III. METHODS This research methodology follows the design science approach in [25 ] and [26]. Design science is based on a scientific framework for IT research. As March and Smith [25] outline, IT research should consider natural and design science as a method to build and evaluate tangible objects . Within this philosophy, objects often have output s in the form of models or instantiations . Instantiations associate with new artifacts in the design science methodology and the understanding of the artifact in its environment [25]. IT artifacts can be",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "understanding of the artifact in its environment [25]. IT artifacts can be realized in many forms such as through the design of an object that helps solve business problems [26]. [Página 4] (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 1 4, No. 9, 202 3 993 | P a g e www.ijacsa.thesai.org A. Organizational Problem Central to the organizational problem in this study is the need to architect a real-world or simulated big data environment that generate s important inputs and outputs. In the case of this study, several architectural layers require design, configuration, benchmarking, and evaluation that accurately represent industry big data system implementations . These research activities could establish a more mature model for IDPS placement in evolving network architectures . Design science",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "mature model for IDPS placement in evolving network architectures . Design science methods guide the latter activities [26]. Big data clusters can have thousands of nodes. Attempting to secure individual servers poses several issues ranging from significant costs to lost computational resources. Important to the artifact design process is the creation of an IDS and IPS testing environment that result s in minimal disruption to existing big data infrastructures. Additionally , the authors constructed an experimental setup similar to several local small business environment s that are readily available , relatively inexpensive, and relevant to a broad audience. Therefore, the testing environment is limited to several small commodity virtual machines (VM s) operating in physically distanced data centers . The authors will briefly outline the network architecture, hardware",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "data centers . The authors will briefly outline the network architecture, hardware , software used in the experimental environment. B. Network Architecture Fig. 1 depicts the baseline network architecture used in this study. The experimental network emulates a small to medium - sized business with a 200 Mbps dedicated lease line between four distinct physical locations. Connections are 1 Gbps copper from the demarc ation point to the LAN nodes. Each server is connected to layer 2 switches followed by a layer 3 Cisco Systems enterprise class router. Fig. 1. Perimeter -based security network architecture. The cybersecurity servers labeled “CyberOne” to “CyberFour” illustrate the systems used to attack the big data cluster. The big data cluster includes four servers labeled “SparkOne” to “SparkFour.” One streaming server is depicted",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "includes four servers labeled “SparkOne” to “SparkFour.” One streaming server is depicted as the data strea m located in the same local area network (LAN) as SparkOne. Four intrusion dete ction and prevention systems are situated between each big data server and its extrinsic networks. C. Hardware The big data servers r un on parallel Dell hardware [2 7]. The hardware is manufactured on the same date and shipped in the same container. The testin g server used the same single Intel CPU with 16 logical cores and 32 GBs of physical random -access memory. The baseline Intel CPU benchmark average results from the Pass Mark version 10 performance test [29] are 2,799 M Ops per s econd for a single thread and 5,443 mega bytes per second",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "s econd for a single thread and 5,443 mega bytes per second for data encryption. Cisco RV series routers with integrated firewalls exist between each Apache Spark node and the external network. Cisco Firmware 1.0.3.55 is in use with the default firewall ruleset . The authors added customized rules that allow the internal LAN IP addresses to communicate on the necessary Apache HDFS and Spark ports. Subsequent ports are blocked [28]. D. Big Data Systems Each big data server and streaming server used equivalent softwar e and versions. Systems ran on the Ubuntu server 20.04.3 LTS operating system. Installed software included Java 11, Python 3.8, Apache Hadoop 3. 2, and Apache Spark 3. 2. The big data environment is comprised of five servers . This includes one primary",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "data environment is comprised of five servers . This includes one primary cluste r manager labeled SparkOne and three secondary work nodes labeled SparkTwo , SparkThree , and SparkFour . Apache Spark is tuned using optimal parameters such as those specified in [30] and [31]. HDFS disks are balanced between nodes with DFS replicating three blocks. The data stream denotes the indepe ndent Spark streaming instance. SparkOne is the primary node in the testing environment used in this study . It is comprised of the driver program. The driver program executes the big data application’s main() clas s and generates the SparkContext [3 2]. SparkContext is capable of using various big data resource managers. Tests in this study use Yet Another Resource Negotiator (YARN) as the distributed cluster",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "this study use Yet Another Resource Negotiator (YARN) as the distributed cluster manager [33]. SparkContext helps communicate application jobs containing code in various forms such as Python and JAR files to the executors on the worker or secondary nodes in the cluster. YARN has two primary high -level components labeled the NodeManager and ResourceM anager. Secondary nodes in a big data cluster managed by YARN each have a NodeManager. Its function is to manage containers on each server. Containers encompass resources such as network, disk, CPU , and memory. These are allocated properly to facilitate ta sk execution . The YARN ResourceManager consists of the ApplicationsManager and the Scheduler . While the Scheduler determines the necessary resources for each application the [Página 5] (IJACSA) International Journal of Advanced",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "resources for each application the [Página 5] (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 1 4, No. 9, 202 3 994 | P a g e www.ijacsa.thesai.org ApplicationsManager identifies which container the application will use and subseq uently monitors their task execution [33]. Apache Spark and HDFS replicate between three secondary big data servers. The secondary or worker nodes labeled SparkTwo, SparkThree, and SparkFour contain executor processes. An executor process remains throughou t the runtime of tasks that each worker is allocated by the cluster manager. Every application receives it s own executor process and/or processes as necessary. The driver program on SparkOne is configured to listen for executor process communications from the secondary nodes until the job is completed . Per Apache Spark documentation",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "secondary nodes until the job is completed . Per Apache Spark documentation in [32], when possible, the driver program should be on the same local area network as the worker nodes due to the latter communication . In the experimental network design, the worker nodes are physically distanced. Therefore, Spark is optimized to open local remote procedure calls on the worker LANs [3 2]. E. Attack Systems Although the cybersecurity servers ran on the same hardware as the big data servers, they used different softw are. CyberOne, Cyber Two, CyberThree, and CyberFour each delineate a server used to carry out cyber -attacks on the big data cluster. The s oftware includes the Kali Linux operating system running the 5.14 kernel. Kali Linux is an open -source operating system",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "running the 5.14 kernel. Kali Linux is an open -source operating system based on Debian Linux. It is designed for numerous information security objectives such as reverse engineering, forensics, pen testing, and research [3 4]. F. Intrustion Detection and Prevention Systems Consistent with Fig . 1, the baseline IDS and IPS systems are located between the cyber -attack and big data systems. Regardless, the authors manipulate the placement of these systems throughout each experimentation. As a simulated construct in the research methodology, the authors propose that IDS and IPS archit ecture placement predicts data streaming performance between worker nodes. Performance evaluation of this potential construct is an important step toward advancing a future ID PS placement framework for physically distanced big data systems. The authors imp lemented Snort",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "framework for physically distanced big data systems. The authors imp lemented Snort and Suricata, two popular open -source IDS and IPS systems. Snort is developed by Cisco Systems. It serves as a leading intrusion detection engine and rule set for Cisco next -generation firewalls and IPSs . Its mechanisms for detecting and pre venting security threats continue to evolve . However, a fundamental capability during this writing is the formation of rules. In contrast to traditional methods such as signature -based detection, rules focus on vulnerability detection [35]. Suricata is dev eloped by the Open Information Security Foundation (OISF). Similar to Snort, Suricata can use rules to detect and block cyber -attacks [36]. Version 2.9.7 of Snort ran with libpcap version 1.9.1 and version 8.39 of the payload",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "Snort ran with libpcap version 1.9.1 and version 8.39 of the payload detection rules. Suricata test ing uses version 6.0.6 with the emerging threats open ruleset. The authors customized the latter default Snort and Suricata rulesets to secure the distributed nodes. The rulesets are parallel in count and type (e.g. alert, drop) to control significant vari ations in resource contention. Suricata and Snort use the same rules in the tests , except for minor incompatibilities. Where incompatible, the rules are adjusted to perform the same action in both IDSs at parallel throughput rates. Snort and Suricata run o n the same server hardware and operating system s as the big data servers. A second NIC allows the servers to act as gateways between trusted and untrusted networks. The",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "the servers to act as gateways between trusted and untrusted networks. The servers communicate between the local area networks using Transport Layer Security (TLS ) and Secure Shell (SSH) Protocols . Ubuntu server 20.04.3 LTS is configured using OpenSSH version 8.2 and OpenSSL version 1.1.1. G. Benchmarks The authors developed custom benchmarks to identify how big data clusters perform under various IDS physically distanced network architectures . The benchmarks perform two significant network load functions, 1) stream ing unstructured data to the Spark big data cluster and 2) flooding the Spark nodes via DDoS attacks. Network and system benchmarking uses version 16m of the nmon source code to measure network performance . Originally developed by IBM, nmon is an open - source Linux project that monitors system resource",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "nmon is an open - source Linux project that monitors system resource utilization . Performance metrics include CPU, disk, memory, and networking [37]. The authors follow the design science methodology [25] to design and implement an IDS placement experiment for physically distanced big data systems. Next, the authors construct a series of tests to determine how IDS locations influence real -world distributed worker nodes . IV. RESULTS Each of the tests followed a n eight -step process, 1) network architecture is determined and implemented, 2) IDPS locations are identified and configured, 3) IDPS customized rulesets are implemented, 4) the big data system cluster is started and tested as operational, 5) data stream s to the cluster are invoked, 6) DDoS attacks are executed, 7) the benchmarks are run,",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "are invoked, 6) DDoS attacks are executed, 7) the benchmarks are run, and 8) the researchers maintain and monitor the testing environment for anomalies. Each of the t ests was repeated three times to ensure saturation existed in the results . A. Test 1 Perimeter -Based Security Results Fig. 1 illustrates the IDPS placement location for the first test. The cloud represents the leased line between the geographical sites. Below the cloud icon is the selected IDPS solution followed by the Apache Spark cluster. Network architecture in the first test follows Cisco Systems ’ best practices for a collapsed data center and LAN core [38]. Within this design, a hardware -based IDPS is situated between the public untrusted and private trusted network s. Test one includes a traditional",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "public untrusted and private trusted network s. Test one includes a traditional perimeter Cisco Systems IDPS . Individual Spark nodes are networked in a single VLAN connected through the collapsed core. In contrast to the network architecture in Fig. 1, CyberOne through CyberFou r servers are not deployed for tests 1 -3. In each of these tests, typical network traffic is present void of any DDoS attacks. Benchmark metrics are specific to the big data systems unless otherwise specified. During the data stream, HDFS is [Página 6] (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 1 4, No. 9, 202 3 995 | P a g e www.ijacsa.thesai.org writing 128 MB block s to disk on all three Spark worker nodes at a constant rate. Inconsequential",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "disk on all three Spark worker nodes at a constant rate. Inconsequential wait time exists on disk reads and writes. Average CPU utilization per thread or “CPU%” on the big data worker nodes is 4.3% during the first test. The average time a proce ss waits for an input -output (I/O) to complete or “wait%” is 0.3. The a verage number of processor context switches per second is 1,728 , identified as “PWps ” hereafter . The authors measured network performance between each of the Spark nodes using four metrics. Metrics are captured on the worker node network interface cards. The first performance variable measure s the average number of all network packet reads per second (APRps) . The second variable captures the average number of all network",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "(APRps) . The second variable captures the average number of all network packet writ es per second (APWps). The measure “APIORk Bs” refers to the amount of network I/O read traffic in kB per second sent between the servers. The fourth metric, “APIOW kBs,” indicates the amount of network I/O write traffic in kB per second sent between the servers. Fig. 3 illustrates the average n etwork I/O (KB/s) on each Apache Spark node in tests 1 -3 while Fig . 4 demonstrate the average n etwork I/O (KB/s) on each Apache Spark node in tests 3 -6. In the perimeter -based network architecture , the average APRps re ads per second are 637 across all Spark worker nodes. The a verage APWps writes per second are 620.",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "Spark worker nodes. The a verage APWps writes per second are 620. The a verage APIORkBs read traffic between all Spark worker nodes is 80 while APIOWkBs is 78. The authors reconfigured the network architecture in the subsequ ent test to provide further insight into IDPS placement impact on distributed big data systems . Fig. 2. Perimeter -less security network architecture. B. Tests 2 -3 Perim eter-less Security Results Fig. 2 demonstrates the big data network designed for tests two and three. Network architecture uses a modified perimeter -less design proposed by Kotantoulas [39]. In contrast to the traditional perimeter IDPS location in Fig. 1, every big data worker node is in a zero trust network . The authors designed an SD-WAN trust boundary to secure each big",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": ". The authors designed an SD-WAN trust boundary to secure each big data node . The boundary consists of Snort and Suricata intrusion detection and prevention security gateways . Similar to the virtual software defined perimeter (vEPC) proposed by Bello et al. [40] , this study’s zero trust software -based system acts as a security gateway for all distributed servers. Sparkone through Sparkfour are designed to operate securely in most cloud architectures in this model by integrating an SDN security stack on each physically distanced serv er. The integrated IDPS gateways control and authorize incoming and outgoing network communication . The design emulates the trust boundary surrounding the cloud edge in [39] using the SSH and TLS protocol s. Gateways authenticate and connect the distributed systems using a",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "TLS protocol s. Gateways authenticate and connect the distributed systems using a 3072 -bit key generated by the Rivest –Shamir –Adleman (RSA) algorithm . Benchmark results for test 2 with Snort SDN gateways show the wait% is 0.413% and CPU% is 12.54%. Results from this study show that CPU resource consumption is over two times greater in the zero trust architecture than the perimeter network design. Test 3 with Suricata SDN gateways results in 11.05% CPU% and 0.342% wait%. Similar to the perimeter - less design in test 2, test 3 used considerably more CPU resources than test 1. Despite similar rulesets, Suricata SDN gateways used slightly less CPU than Snort. In the test 2 perimeter -less network architecture the average APRps reads per second are 2,198 across",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "-less network architecture the average APRps reads per second are 2,198 across all Spark worker nodes. The a verage APWps writes per second are 653. The average APIORkBs read traffic between all Spark worker nodes is 298 in test 2 , APIOWkBs is 82. Fig. 3. Tests 1 -3 spark per node network I/O in KB/s . The t est 3 network architecture had similar results to test 2 . The average APRps reads per second are 2,120 across the distributed Spark systems. The a verage APWps is 611. APIORkBs between the big data servers is 289 and APIOWkBs is 77. Fig. 3 illustrates the average network I/O (KB/s) on each Apache Spark node in tests 1 -3. These results indicate that network traffic and network I/O are",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "1 -3. These results indicate that network traffic and network I/O are nominal when writing to HDFS in all network architectures within this study. In contrast, the number of packets the systems have to read is higher in the perimeter -less network architectures. APRps is over three times higher in tests 2 and 3 than in test 1. 020040060080010001200 1234567891011121314151617181920Kilobits Seconds [Página 7] (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 1 4, No. 9, 202 3 996 | P a g e www.ijacsa.thesai.org C. Test 4 Perimeter -Based DDoS Attack Results Test 4 uses the network architecture (Fig. 1), parallel to test 1. Perimeter -based intrusion detection and prevent ion systems protect the internal LANs of the Spark nodes. CyberOne through CyberFour are active in",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "internal LANs of the Spark nodes. CyberOne through CyberFour are active in test 4. The cyber servers are configured to flood the big data cluster with unlimited TCP SYN handshakes. Benchmark results for the big data servers during the DDoS attacks parallel test 1 i n test 4. In test 4, the IDPS s prevented additional CPU load and network load on the big data servers. In the test case, the hardware IPSs successfully blocked the DDoS attacks. D. Tests 5 -6 Perimeter -less DDoS Attack Results Tests 5 and 6 are similar to test s 3 and 4 . However, DDoS attacks are administered on the big data cluster. Tests 5 -6 use the (Fig. 2) perimeter -less security network architecture. Test 5 uses the Snort -based",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "2) perimeter -less security network architecture. Test 5 uses the Snort -based SDN security boundary , while test 6 uses Suricata. CyberOne through CyberFour ar e active in tests 5 and 6. The cyber servers execute DDoS attacks on the big data cluster by flooding the servers with unlimited TCP SYN handshakes. Snort and Suricata security gateways successfully protect the big data systems from DDoS attacks in a zero trust network in tests 5 and 6 ; however, at the expense of local computational resource increases. Results for test 5 with Snort SDN gateways show the wait% is 0.308% and CPU% is 13.8%. CPU resource consumption increases on average over 1% on the big data servers during the DDoS attacks. Test 6 with Suricata SDN gateways results in",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "during the DDoS attacks. Test 6 with Suricata SDN gateways results in 11.95% CPU% and 0.337% wait%. DDoS attacks increased average CPU% by 0.9% across big data systems. Suricata SDN gateways used slightly less CPU than Snort SDN gateways during the DDoS attacks. Within the test 5 perimeter -less network architecture the average APRps reads per second are 4,762 across all distributed by data secondary nodes. The average APWps writes per second are 626. The average APIORkBs traffic between the distributed systems is 425. APIOWkBs is 79. Fig. 4. Tests 4 -6 spark per node network I/O in KB/s . The Suricata gateways in test 6 have average APRps reads per second of 4,311 across the distributed Spark systems. Average APWps is 6 61. APIORkBs between the big",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "distributed Spark systems. Average APWps is 6 61. APIORkBs between the big data servers is 416 and APIOWkBs is 81. Fig. 4 demonstrates the average network I/O (KB/s) on each Apache Spark node in tests 3 -6. E. Test 7 Perimeter -Based DDoS Attack Results Test 7 shares the same network architecture as test 1 and test 4, illustrated in Fig. 1. To decipher how the DDoS attacks affect the big data servers in the perimeter -based network architecture without IDPS protection, test 7 repeats test 4 but allow all network traffic from CyberOne through CyberFour to the big data cluster. When the DDoS attacks are allowed through the perimeter IPSs in the Fig. 1 network architecture, results show an average CPU% of 17.9% across all distributed big",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "architecture, results show an average CPU% of 17.9% across all distributed big data systems. Predictably, network packets increase in test 7 compared to tests 1 and 4. APRps is 2,895 while APIORkBs is 518. Test 7 has the highest APIORkBs of all network benchmarks performed in this study. F. Discussion of the Results The results illustrate that network traffic and network I/O have marginal differences when writi ng to HDFS in the network architectures studied . CPU resources and network traffic read by the operating systems increased in zero trust network architectures. The most substantial differences were between tests 4 and 5. During the DDoS attacks, the big da ta servers required more CPU resources in the perimeter -less security network architecture. In test 5, APIORkBs are considerably",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "the perimeter -less security network architecture. In test 5, APIORkBs are considerably higher at 425 than test 4 at 80. This additional traffic is partly due to the SDN security boundaries necessary to protect the systems in a zero trust network environment. Shifting compute resources closer to individual devices may be necessary as network security perimeters dissipate . However, zero trust architectures in the experimental environment reduced cluster performance. T herefore, additional research is beneficial to optimize the design of perimeter -less network environments . G. Limitations Several environmental factors limit the results. Site -to-site networks were on leased 200 Mbps connections. Future studies might consider leased lines capable of establishing more robust data streams to the distributed nodes. A subsequent restriction is the number of architectures and",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "the distributed nodes. A subsequent restriction is the number of architectures and communication technologies tested. Similar to the architecture in [40], gateways allow for IP Security ( IPsec) or Transport Layer Security (TLS) protocols. Future IDPS SDN gateways could add this layer of encryption in a software -defined security boundary between geo- distributed big data systems. The outlined limitations emphasize the need for future research t o investigate more extensive network architectures and IDPS technologies for big data system security. V. CONCLUSION As the volume of data expand s, organizations require big data systems to perform large -scale data analytics. One of several needs for these sy stems is effective intrusion detection and prevention strategies. This paper builds a review of the 020040060080010001200140016001800 1234567891011121314151617181920Kilobits Seconds [Página 8] (IJACSA)",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "paper builds a review of the 020040060080010001200140016001800 1234567891011121314151617181920Kilobits Seconds [Página 8] (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 1 4, No. 9, 202 3 997 | P a g e www.ijacsa.thesai.org literature on methods used to reduce cybersecurity threats in a range of network architectures that big data systems operate. Findings from literature sugge st intrusion detection and prevention systems can respond to certain security attacks . However, a potential disadvantage of capable security systems is the impact on big data system cluster performance. Using a design science approach, the authors develop an eight -step process to benchmark big data systems in varying network architectural environments. The new benchmark process is tested on real -time big data systems running in perimeter -based and perimet er-less",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "real -time big data systems running in perimeter -based and perimet er-less network environments. During DDoS cyber -attacks, perimeter -based network architectures outperformed perimeter -less network architectures. This underlines the importance of optimizing the design of zero trust architectures for distributed big data s ystems. REFERENCES [1] D. Gümüşbaş, T. Yıldırım, A. Genovese, and F. Scotti, “A comprehensive survey of databases and deep learning methods for cybersecurity and intrusion detection systems ,” IEEE Systems Journal , vol. 15, no. 2, pp. 1717 –1731, Jun. 2021, doi: 10.1109/JSYST.2020.2992966 . [2] I. D. Aiyanyo, S. Hamman, and H. Lim, “A systematic review of defensive and offensive cybersecurity with machine learning,” Applied Sciences, vol. 10, no. 17, p. 5811, 2020, doi: 10.3390/app10175811. [3] N. V. Patil, C. Rama Krishna, and K.",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "2020, doi: 10.3390/app10175811. [3] N. V. Patil, C. Rama Krishna, and K. Kumar, “Distributed frameworks for detecting distributed denial of service attacks: A comprehensive review, challenges and future directions,” Concurrency and Computation: Practice and Ex perience , vol. 33, no. 10, pp. 1 -21, May 2021, doi: 10.1002/cpe.6197 . [4] R. Rafiq, M. J. Awan, A. Yasin, H . Nobanee , A. M. Zain, and S. A. Bahaj, “Privacy prevention of big data applications: A systematic literature review ,” Sage Open , vol. 12, no. 2, Apr. 2022, doi: 10.1177/21582440221096445 . [5] R. Mitchell and I. R. Chen, “A survey of intrusion detection techniques for cyber -physical systems,” ACM Com put. Surv. , vol. 46, no. 4, Mar. 2014, doi: 10.1145/2542049 . [6] B. B. Zarpelão, R. S.",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "4, Mar. 2014, doi: 10.1145/2542049 . [6] B. B. Zarpelão, R. S. Miani, C. T. Kawakani, and S. C. de Alvarenga, “A survey of intrusion detection in Internet of Things,” Journal of Network an d Computer Applications , vol. 84, pp. 25 –37, Apr. 2017, doi: 10.1016/j.jnca.2017.02.009 . [7] V. Casola, A. De Benedictis, M. Rak, and U. Villano, “Security -by- design in multi -cloud applications: An opt imization approach,” Information Sciences , vol. 454 –455, pp. 344 –362, Jul. 2018, doi: 10.1016/j.ins.2018.04.081 . [8] R. Atat, L. Liu, J. Wu, G. Li, C. Ye, and Y. Yang, “Big data meet cyber - physical systems: a panoramic survey,” IEEE Access , vol. 6, pp. 73603 – 73636, 2018, doi: 10.1109/ACCESS.2018.2878681 . [9] R. Gift y, R. Bharathi, and",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "73636, 2018, doi: 10.1109/ACCESS.2018.2878681 . [9] R. Gift y, R. Bharathi, and P. Krishnakumar, “Privacy and security of big data in cyber physical systems using Weibull distribution -based intrusion detection,” Neural Computing and Applications , vol. 31, no. 1, pp. 23 –34, Jan. 2019, doi: 10.1007/s00521 -018-3635 -6. [10] S. F. Ochoa, G. Fortino, and G. Di Fatta, “Cyber -physical systems, internet of things and big data,” Future Generation Computer Systems , vol. 75, pp. 82 –84, Oct. 2017, doi: 10.1016/j.future.2017.05.040 . [11] K. A. P. da Costa, J. P. Papa, C. O. Lisboa, R. Munoz, and V. H. C. de Albuquerque, “Internet of Things: A survey on machine learning -based intrusion detection approaches,” Computer Network s, vol. 151, pp. 147 – 157, Mar. 2019, doi: 10.1016/j.comnet.2019.01.023 .",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "s, vol. 151, pp. 147 – 157, Mar. 2019, doi: 10.1016/j.comnet.2019.01.023 . [12] N. Moustafa, B. Turnbull, and K. R. Choo, “An ensemble intrusion detection technique based on proposed statistical flow features for protecting network traffic of Internet of Things,” IEEE Internet of Things Journal , vol. 6, no. 3, pp. 4815 –4830, Jun. 2019, doi: 10.1109/JIOT.2018.2871719 . [13] A. Yang, Y. Zhuansun, C. Liu, J. Li, and C. Zhang, “Design of intrusion detection system for Internet of Things based on im proved BP neural network,” IEEE Access , vol. 7, pp. 106043 –106052, 2019, doi: 10.1109/ACCESS.2019.2929919 . [14] Z. Tan et al. , “Enhancing big data security with collaborative intrusion detection,” IEEE Cloud Computing , vol. 1, no. 3, pp. 27 –33, Sep. 2014, doi: 10.1109/MCC.2014.53 .",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "vol. 1, no. 3, pp. 27 –33, Sep. 2014, doi: 10.1109/MCC.2014.53 . [15] A. N. Jaber and S. U. Rehman, “FCM –SVM based intrusion detection system for cloud computing environment,” Cluster Computing , vol. 23, no. 4, pp. 3221 –3231, Dec. 2020, doi: 10.1007/s10586 -020-03082 -6. [16] M. Hafsa and F. Jemili, “Comparative study between big data analysis techniques in intrusion detection,” Big Data and Co gnitive Computing , vol. 3, no. 1, pp. 1 -13, Dec. 2018, doi: 10.3390/bdcc3010001 . [17] F. M. Awaysheh, M. N. Aladwan, M. Alazab, S. Alawadi, J. C. Cabaleiro, and T. F. Pena, “Security by design for big data frameworks over cloud computing,” IEEE Transactions on Engineering Management , pp. 1 –18, Feb. 2021, doi: 10.1109/TEM.2020.3045661 . [18] A. Bocci, S. Forti,",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "1 –18, Feb. 2021, doi: 10.1109/TEM.2020.3045661 . [18] A. Bocci, S. Forti, G. L. Ferrari, and A. Brogi, “Secure FaaS orchestration in the fog: How far are we?” Computing, vol. 103, no. 5, pp. 1025 –1056, May 2021, doi: 10.1007/s00607 -021-00924 -y. [19] Y. Wang, X. Z hang, Y. Wu, and Y. Shen, “Enhancing leakage prevention for mapreduce,” IEEE Transactions on Information Forensics and Security , vol. 17, pp. 1558 –1572, 2022, doi: 10.1109/TIFS.2022.3166641 . [20] O. Ohri menko, M. Costa, C. Fournet, C. Gkantsidis, M. Kohlweiss, and D. Sharma, “Observing and preventing leakage in MapReduce,” in Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security , New York, NY, USA, 2015, pp. 1570 –1581. doi: 10.1145/2810103.2813695 . [21] A. M. Sauber, A. Awad, A.",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "1570 –1581. doi: 10.1145/2810103.2813695 . [21] A. M. Sauber, A. Awad, A. F. Shawish, and P. M. El -Kafrawy, “A novel hadoop security model for addressing malicious collusive wo rkers,” Computational Intelligence and Neuroscience , vol. 2021, pp. 1 -10, 2021, doi: 10.1155/2021/5753948 . [22] P. Derbeko, S. Dolev, E. Gudes, and S. Sharma, “Security and privacy aspects in MapReduce on clo uds: A survey,” Computer Science Review , vol. 20, pp. 1 –28, May 2016, doi: 10.1016/j.cosrev.2016.05.001 . [23] R. Poddar, T. Boelter, and R. Popa, “Arx: An encrypted database using semantically secure encryption,” Proceedings of the VLDB Endowment , vol. 12, pp. 1664 –1678, Jul. 2019, doi: 10.14778/3342263.3342641 . [24] A. Nisioti, A. Mylonas, P. D. Yoo, and V. Katos, “From intrusion detection to attacker attribution:",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "P. D. Yoo, and V. Katos, “From intrusion detection to attacker attribution: A comprehensive survey of unsupervised methods,” IEEE Communications Surveys & Tutorials , vol. 20, no. 4, pp. 3369 –3388, Fourthquarter 2018, doi: 10.1109/COMST.2018.2854724 . [25] S. T. March and G. F. Smith, “Design and natural science research on information technology,” Decision Support Systems , vol. 15, no. 4, pp. 251–266, Dec. 1995, doi: 10.1016/0167 -9236(94)00041 -2. [26] A. R. Hevner, S. T. March, J. Park, and S. Ram, “Design science in information systems research,” MIS Quarterly , vol. 28, no. 1, pp. 75 – 105, 2004, doi: 10.2307/25148625 . [27] “Dell technology,” Dell Inc , June, 2022 . [Online]. Available: https://www.dell.com. [28] “Cisco routers and SD -WAN,” Cisco Systems , June, 2022. [Online]. Available: https://www.cisco.com/site/us/en/products/networking/sdwan -",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "and SD -WAN,” Cisco Systems , June, 2022. [Online]. Available: https://www.cisco.com/site/us/en/products/networking/sdwan - routers/index.html. [29] “Benchmarking & Diagnostic Software,” Passmark Software , June, 2022. [Online]. Available: https://www.passmark.com. [30] “Spark tuning guide on 3rd generation Intel® Xeon® scalable processors based platform,” Intel Corporation , August, 2021, [Online]. Available: https://www.intel.cn/content/www/cn/zh/developer/articles/guide/spark - tuning -guide -on-xeon -based -systems.html. [31] “Tuning Spark,” The Apache Software Foundation , July, 2022. [On line]. Available: https://spark.apache.org/docs/3.2.2/. [32] “Cluster Mode Overview,” The Apache Software Foundation , June, 2022. [Online]. Available: https://spark.apache.org/docs/latest/cluster - overview.html. [33] “Apache Hadoop YARN,” The Apache Software Foundati on, June, 2022. [Online]. Available: [Página 9] (IJACSA) International Journal of Advanced Computer Science and Applications, Vol. 1 4, No. 9, 202 3 998 | P a g e www.ijacsa.thesai.org https://hadoop.apache.org/docs/stable/hadoop -yarn/hadoop -yarn- site/YARN.html. [34] “Kali linux",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "P a g e www.ijacsa.thesai.org https://hadoop.apache.org/docs/stable/hadoop -yarn/hadoop -yarn- site/YARN.html. [34] “Kali linux features,” OffSec Services Limited , June, 2022. [Online]. Available: https://www.kali.org/features. [35] “Snort FAQ/Wiki,” Cisco Systems , July, 2022. [Online]. Available: https://www.snort.org/faq . [36] “Suricata user guide,” Open Information Security Foundation , July, 2022. [Online]. Available: https://suricata.readthedocs.io/en/suricata - 6.0.6 . [37] “nmon for Linux ,” IBM, June, 2022. [Online]. Available: http://nmon.sourceforge.net. [38] “Collapsed data center and campus core deployment guide,” Cisco Systems , June, 2022. [Online]. Available: https://www.cisco.com/c/dam/global/en_ca/solutions/strategy/docs/sbaG ov_nexus7000Dguide_new.pdf. [39] J. Kotantoulas, “Zero trust for government networks ,” Cisco Systems , June, 2022. [Online]. Available: https://blogs.cisco.com/government/zero -trust-for-government - networks -6-steps -you-need -to-know. [40] Y. Bello, A. R. Hussein, M. Ulema, and J. Koilpillai, “On sustained zero trust conceptualization security for mobile core ne tworks in 5G and",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "zero trust conceptualization security for mobile core ne tworks in 5G and beyond,” IEEE Transactions on Network and Service Management , vol. 19, no. 2, pp. 1876 –1889, Jun. 2022, doi: 10.1109/TNSM.2022.3157248 . [Página 10] © 2023. This work is licensed under http://creativecommons.org/licenses/by/4.0/ (the “License”). Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.",
        "start_idx": 7656,
        "end_idx": 7720
      }
    ],
    "e473384f-6f46-40a1-8e91-8b965a349b2e": [
      {
        "text": "[Página 1] DOM: A big data analytics framework for mining Thai public opinions Santitham Prom-on, Sirapop Na Ranong, Patcharaporn Jenviriyakul, Thepparit Wongkaew, Nareerat Saetiew and Tiranee Achalakul Department of Computer Engineering, Faculty of Engineering King Mongkut’s University of Technology Thonburi Bangkok, Thailand Abstract—This paper presents the development of DOM, a mobile big data analytics engine for mining Thai public opinions. The engine takes in data from multiple well-known social network sources, and then processes them using MapReduce, a keyword-based sentiment analysis technique, and an influencer analysis algorithm to determine public opinions and sentiments of certain topics. The system was evaluated its sentiment prediction accuracy by matching the predicted result with the human sentiment and tested on various case studies. The effectiveness of the approach demonstrates the practical applications",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "various case studies. The effectiveness of the approach demonstrates the practical applications of the engine. Keywords—opinion mining; big data analytics; MapReduce; public sentiment I. INTRODUCTION We, human being, have never been more connected through the emergence of social networks. Social networks, in terms of both data and users, have been exponentially growing and connect our lives together in various dimensions. We can connect with people across the planet with a touch of a finger. In every second, hundred thousands of messages are shared through social media such as Facebook, Twitter, Foursquare, Pantip, etc. They are about our life, feeling, experience and opinion. This practically represents the 21st century of our civilization, “The era of social network”. Social media networks generated huge volumes of data. They have been use",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "Social media networks generated huge volumes of data. They have been use in various types of applications including public health [1], emergency coordination [2], news recommendation [3], and stock market prediction [4]. The data from social media networks gathered under the catch-all term, “big data”. However, as much as 90% of the data stored is \"unstructured,\" meaning that it is spontaneously generated and not easily captured and classified. Big data is only valuable if it tells a story. The fuller the story your data tells, the better you’ll be able to take advantage of that data. While recognizing a trend can help you make better decisions, understanding the cause behind that trend is even more valuable. The organizations that can use stories to make sense of big data",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "The organizations that can use stories to make sense of big data are going to excel. In this paper, we presents the developments of DOM (Data and Opinion Mining), a big data analytics engine that is capable of mining Thai public opinions regarding specific issues discussed on the social network sites, and its corresponding mobile solution for answering public opinions about events and locations. Software features and design will be discussed in Section II. Section III explains how the software was implemented using cloud-based technology. Section IV shows the evaluation of the DOM effectiveness in predicting the sentiment score of public opinions. Usages of DOM for different tasks are presented in Section V. Comparisons of DOM with respect to others and the future steps in the development are",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "with respect to others and the future steps in the development are discussed in Section VI. II. DOM A. Data Sources We collected data from four different data sources; Twitter, Facebook, Foursquare and Pantip, as described in Table 1. These social network data, if the locations can be specified, were collected in scope of Bangkok area. For Twitter data, we used Search API [5] provided from Twitter Inc. to collect tweets without any keywords. We collected approximated 15 million tweets or about 12GB uncompressed data each month. Each tweet contains multiple data fields, including time, username, user followers, retweet, count, location, and the textual comment. For Facebook, we used Graph API [6] developed by Facebook Inc. Unlike Twitter, we can only request and collect data from Facebook fanpage",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "Unlike Twitter, we can only request and collect data from Facebook fanpage which consists of posts and comments of specific topics. We collected Facebook data about 5,000 messages, which is approximately Table 1. Sources of Social Network Data. Source Data Description Twitter Twitter messages, also known as tweets, are short 140-character text messages. Tweets are all public. Facebook Facebook data can only be retrieved if the privacy is set to public. They are in forms of status posts and Facebook Page posts. Foursquare Foursquare provides both text comments and review score of a number of places. Pantip Pantip data are in forms of webboard threads. It is one of the prominent Thailand online social communities. 2014 International Conference on Computer, Control, Informatics and Its Applications 978-1-4799-4575-7/14/$31.00 c/circlecopyrt2014 IEEE",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "International Conference on Computer, Control, Informatics and Its Applications 978-1-4799-4575-7/14/$31.00 c/circlecopyrt2014 IEEE 1 [Página 2] 4MB per fanpage each month. Graph API provides attributes including time, username, number of Like, location as well as textual comment for each message. For Foursquare, the sitation is like Graph API. Foursquare provides their API for developers to gather data named Venues and Tip search API [7]. Foursquare provides comments of places. In each month we collected approximated 500 messages or 0.4 MB per place. Foursquare data includes time, username, like count, location and the textual comment. Our last data source is Pantip.com, one of the prominent Thailand online social communities. We developed a web crawler to gather the data on this website, since they do not provide an API to gather",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "on this website, since they do not provide an API to gather data. The web crawler was designed to have features like Search API. First we simulated the browser by set user-agent to be Mozilla, and then assigned the keywords to the search form of web and submitted the request. We found that approximately 300 messages or about 0.2MB were collected for each topic. Each Pantip thread contains time, username, like count and the text comment. B. System Architecture We categorized components into two sides: server-side and client-side. The architecture design of our whole framework is illustrated in Figure 2. The components of DOM engine are classified into server-side which is cloud-based cluster. DOM engine is responsible for collecting, analyzing data and distributing the analyzed data to client-side.",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "responsible for collecting, analyzing data and distributing the analyzed data to client-side. AskDOM components are client-side. The client-side requests the analyzed data, queries and displays them to end-users. Workflow of our framework is as follows. Public messages are collected from social networks, blogs and forums using DOM’s crawler module. All collected messages are stored in MongoDB, an unstructured database. After that each message is then processed using basic Natural Language Processing (NLP) technique to parse the text data, categorize its topic, compute its sentimental score and analyze its influences. DOM also uses MapReduce technique based on Apache Hadoop framework to reduce the processing time. DOM periodically processed the data to compute their sentimental score. Finally AskDOM, the mobile application, gets the analyzed data, queries and displays the information",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "the mobile application, gets the analyzed data, queries and displays the information to users according to the inquired topics. In this paper, we focus the usage of DOM as a Thai public opinion mining framework to track social issues and provide sentiment rating and information of point of interest (POI) based on public opinions. However, the core functions of DOM engine was designed to support dynamic data. There are several features that could be added or further developed to provide additional functionality (e.g. adding more data sources, supporting other languages). Since DOM is cloud-based engine, scalability is also available. Furthermore DOM can be easily applied in various types of usage, either community side or commercial side. There are case studies in section IV that shows some potential usage",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "There are case studies in section IV that shows some potential usage of DOM. The current version of DOM consists of the following: modules: C. MapReduce Framework Since huge data are involved in this project, MapReduce [8], the high performance computing technique, is used. This is because if the data is to be processed sequentially, the processing time would be too large for the practical application. MapReduce technique on Apache Hadoop framework is therefore the best way to accelerate the analysis speed. In this paper, the MapReduce technique separates the mining process into two main steps; Map and Reduce. Map function takes the entire text input, breaks it into subsets to be evaluated for their sentiment scores and distributes them to worker nodes. Reduce function combines the resulting",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "scores and distributes them to worker nodes. Reduce function combines the resulting sentiment scores from each small worker nodes by grouping keywords of specific topics of interest and summarizing the sentiment scores into final results. D. Sentiment Analysis In this work we targeted words in which opinions are expressed in each sentence. A simple observation was that these sentences always contain sentiment words (e.g. great, good, bad, worst). To simplify the process, if the sentences do not contain any sentiment words, their sentiment values will be neutral (non-opinion). So we designed our framework to classify the sentiment of each sentence based on its sentiment words and the combination of them. Furthermore we designed the system to be able to process Thai conditional sentences, which are sentences that describe",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "be able to process Thai conditional sentences, which are sentences that describe Fig. 1 Twitter activity heatmap in Bangkok area. Fig. 2 Conceptual framework of DOM and its corresponding mobile application, AskDOM. 2 [Página 3] implications or hypothetical situations and their consequences. For example, in the sentence like ‘I like the location of this company but I do not like their staffs.’ The sentiment of ‘location’ is positive but negative on ‘staffs’. We found that most conditional sentences contain modifiers and conjunctions (e.g. but, and, or). To classify each message as positive, neutral or negative, we employed a lexicon-based algorithm to measure sentiment score of each message. We defined five corpora including positive words, negative words, modifier, conjunction as well as name of point of interest. Each word",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "modifier, conjunction as well as name of point of interest. Each word in two sentiment corpora, positive words and negative words, contains sentiment rating ranging from -5 to 5. The examples of our corpuses are shown in Table 2. DOM detects and matches words and its sentiment polarity by using these corpora. Since the nature of Thai sentence structure is continuous without any whitespace breaks between words, we need to tokenize each sentence into group of words In this process we used ‘LexTo’ [9], the opensource Thai word tokenize tool, to tokenize words in each sentence and then store them as arrays using the longest word matching algorithm [10]. The example of this procedure is shown in Figure 3. DOM generates small jobs to detect words of each",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "in Figure 3. DOM generates small jobs to detect words of each sentence in parallel. First of all, DOM filters the non-related sentences out by matching words with name of POI corpus. After that only sentences that relate to specific topics of interest (in this case is point of interest) would remain. DOM then iteratively matches sentiment keywords with remaining corpuses. If there are sentiment words in array, DOM collect its sentiment score and summarize at the end of each sentence. DOM then automatically classifies each sentence into sentiment group; positive, neutral and negative, depending on its score band (the range of distributed sentiment score). DOM not only determines keyword from sentences, but also determines context of each sentence. The positions of words, modifiers, conjunctions as well as",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "of each sentence. The positions of words, modifiers, conjunctions as well as emoticons are also determined in our framework. In some cases these words can be important clues to emphasize the mood of the sentences. Especially for the modifier keywords, they can invert the sentiment score if their positions are adjacent to the sentiment words as illustrated in Figure 4. Table 2. The examples of sentences in the corpora. # Type of Corpus Word Value 1 Positive words เท ห (smart) 3 ด (good) 3 เย ยม (best) 4 2 Negative Words เส อมโทรม (decadent) -3 แย (bad) -3 ห วยแตก (worst) -4 3 Modifiers ไม (not) -1 ค อนข าง (likely) 0.5 ท ส ด (best) 1.5 4 Conjunctions แต (but) 2 และ (and) 1 รวมไปถ ง",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "(best) 1.5 4 Conjunctions แต (but) 2 และ (and) 1 รวมไปถ ง (including) 1 5 Name of places สวนล มพ น (Lumphini Park) - สยาม (Siam) - จต จ กร (Chatuchak market) - Fig. 3 Example of Thai word tokenization Fig. 4 Example of Thai Sentiment Analysis 3 [Página 4] E. Influencer Analysis The rise of social media platforms such as Twitter, with their focus on user-generated content and social networks, has brought about the study of authority and influence over social networks to the forefront of current research. For companies and other public entities, identifying and engaging with influential authors in social media is critical, since any opinions they express can rapidly spread far and wide. For users, when presented with a vast amount of content relevant",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "wide. For users, when presented with a vast amount of content relevant to a topic of interest, sorting content by the source’s authority or influence can also assist in information retrieval. In the social network community, a variety of measures were designed for the measurement of importance or prominence of nodes in a network [11, 12]. In the following, we will briefly summarize the centrality measure that we have used to describe possible candidate indicators for the power of influential in message diffusion. For DOM engine, we have used “Degree centrality” to identify influential users in the Twitter’s networks. Degree centrality is the simplest centrality measure, as illustrated in Figure 5. The degree of a node i denoted by ki, is the number of edges that are incident",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "i denoted by ki, is the number of edges that are incident with it, or the number of nodes adjacent to it. For networks where the edges between nodes are directional, we have to distinguish between in-degree and out-degree. The out-degree centrality is defined as 𝐶!!𝑖= 𝑎!\"!!!! (1) where aij is 1 in the binary adjacency matrix A if an edge from node i to j exists, otherwise it is 0. Similarly, the in-degree centrality is defined as 𝐶!!𝑖= 𝑎!\"!!!! (2) where i describes the node i and aji is 1 if an edge from node j to i exists, otherwise it is 0. F. AskDOM: Mobile Application To utilize DOM to its fullest extent, we developed AskDOM, a mobile solution designed to use DOM to provide a",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "developed AskDOM, a mobile solution designed to use DOM to provide a means for general publics to help improving their own communities by providing reviews, feedbacks, and rating of service providers automatically analyzed from public opinions on social networks (Twitter, Facebook, Pantip and Foursquare). AskDOM comprises two important modules: (a) front-end interface with features designed to connect users to service providers such as I-Share (direct feedback), Map (traffic and incident map), Anomaly (ab-normal situations reports), and (b) DOM Engine, the back-end system that periodi-cally gathers and processes social network data, performs public sentiment analysis, determines relationship influencers, and conducts natural language processing for both Thai and English. The integration of both modules will increase the transparency of the service businesses, make the agencies more accountable for their service",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "of the service businesses, make the agencies more accountable for their service quality, and provide a means for general citizens to involve with the improvement of the public services in terms of both information availability and improvement. Such an involvement will improve not only the quality of service, but also create a sense of community to the general citizens that they have to be part of the social function. • III. IMPLEMENTATION Figure 7 shows the overall implementation architecture of DOM engine. The structure has three main components which are Server, Core Service and I/O. A. Server Server section consists of three components; Ubuntu server, MongoDB as well as Apache Hadoop. We implemented DOM engine based on Apache Hadoop MapReduce which on Ubuntu server. MongoDB, the famous unstructured",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "on Apache Hadoop MapReduce which on Ubuntu server. MongoDB, the famous unstructured database was also used in this framework. The unstructured database is often highly optimized key–value stores intended for simple retrieval and appending operations to improve the performance in terms of latency and throughput. B. Core Service Core Service, the main part of our framework, consists of three components. 1) Data Crawler: This module provides automatically raw data feed from social network and stores them in the database, MongoDB. Each crawler code is specific for each social network or websites. 2) Data Preprocessing: This component prepares raw data to be ready for the analysis part by tokenizing Thai and Fig. 5 Simulation of Influencer network graph in the Twitter’s networks Fig. 6 AskDOM Application Fig. 7 DOM",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "graph in the Twitter’s networks Fig. 6 AskDOM Application Fig. 7 DOM engine architecture 4 [Página 5] English words from sentences and removing outliers and reformatting data. Then the cleaned data will be sent to Data Analysis part. 3) Data Analysis: There are two main analyses in this component: a) Sentiment Analysis evaluates sentiment in twitter text and find people’s mood in particular topic. For example, how people think about traffic in Bangkok. b) Influencer Analysis determines people’s positions in network, which indicate how influential they are. The influential people are more likely to acquire connections and have more connections. C. I/O I/O, the web-service implemented using PHP, receives the result from Core Service and then sent them to client-side to display in JSON format. Since the number",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "sent them to client-side to display in JSON format. Since the number of data in social network is increasing every second, using the static resources (e.g. static server) may not be practical. So we designed to run DOM on the cloud. Cloud provides ability to add a blob storage depending on the size of data. Furthermore DOM has ability to scale the number of processer. In other word, DOM can increase or decrease the number of mapper and reducer for running job. IV. VALIDATION To validate the effectiveness of DOM, we conducted a subjective experiment to assess the sentiment prediction accuracy. In the following, we will describe the validation procedure and discuss on validation results. A. Validation Parameter • 184,184 messages from Facebook, Twitter and Foursquare (both positive",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "Validation Parameter • 184,184 messages from Facebook, Twitter and Foursquare (both positive and negative messages) divided into short and long messages, including 172,717 short messages (≤ 150 characters) and 11,467 long messages (> 150 characters). • 12 subjects (6 males and 6 females) were participated in the experiment. They were students at the Computer Engineering Department, King Mongkut’s University of Technology Thonburi, Thailand. B. Validation method 1. For the human end, 184,184 messages were divided into 12 parts, each of which was assigned to each subject. They classified the messages in to positive and negative classes. 2. For DOM engine, 184,184 messages were classified by the engine into positive and negative classes. 3. The results of both human and DOM were compared and analyzed together to assess the",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "both human and DOM were compared and analyzed together to assess the system prediction accuracy. C. Validation results Table 3 and 4 shows the comparision results of 12 students and DOM engine. We found that DOM engine can classified messages and do sentiment analysis with accuracy over 75%. The accuracy of DOM engine is in the standard of text classification [13], so DOM engine is practical to use in social network analysis and can be applied to many dimensions in the real word. V. CASE STUDIES In addition to the evaluation of the system effectiveness, we tested DOM engine further on various case studies that were of interest of Thai public during the time periods. Each case study aims to explore either specific social or political issue that",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "case study aims to explore either specific social or political issue that people were discussed widely on the Internet, thus offers a summary of Internet public opinions of that issue. A. Political opinion: #prayforthailand Around the end of 2013, citizen of Bangkok faced with multiple rounds of political protests, and violent acts toward both protesters and officers. Hashtag “#prayforthailand” is one that was frequently used in social media to express the concerns over the situation. Different opinions were expressed regarding this political issue. We used DOM to mine the general public opinions that were expressed in the social network to determine the political climate at that time. We collected tweets around Bangkok area that contain the hashtag “#PrayForThailand.” There were over 100K tweets collected from 29 November to",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "hashtag “#PrayForThailand.” There were over 100K tweets collected from 29 November to 7 December 2013. We implemented Naïve Bayes and Support Vector Machine (SVM) to DOM engine to classify political opinions into six predefined categories as shown in Table 5. DOM can accurately put tweets into categories with more than 85% accuracy. Table 3. Summary of prediction accuracy. Message Type Positive Comment Accuracy (%) Negative Comment Accuracy (%) Total Short 79.75 56.33 75.99 Long 86.53 38.95 81.29 Total 80.19 55.57 76.32 Table 4. Detail analysis of the system effectiveness. Msg. type TP FP TN FN Precision Accuracy (%) Short 115,643 12,103 15,613 29,358 0.905 75.99 Long 8,830 771 492 1,374 0.919 81.29 Total 124,473 12,874 16,105 30,732 0.906 76.32 5 [Página 6] B. Bangkok traffic congestion ranking Bangkok’s",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "30,732 0.906 76.32 5 [Página 6] B. Bangkok traffic congestion ranking Bangkok’s traffic problem is one of the most serious problem that urban citizen have been facing in their daily life. Knowing such information on which streets the traffic jams often occur would allow citizens to prepare to encounter the problem and allows the government to find a way to solve it. We used DOM engine to track traffic jams keywords, name of streets, intersections as well as famous places in Bangkok, Thailand that contained in public tweets, and then rank the streets that were mostly mentioned about traffic jam based on 22K tweets collected from 17 February to 8 March 2014. The results as shown in Table 6 are consistent with what Thailand’s Department of Highways hotline",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "in Table 6 are consistent with what Thailand’s Department of Highways hotline gathered the statistics from phone calls. However using DOM engine is much faster and cheaper. VI. DISCUSSION AND CONCLUSION This paper presents the development, evaluation, and case studies of DOM, a big data analytics framework for assessing public sentiments of specific social issues. DOM, which is an opinion mining and sentiment analysis engine, is encapsulated as a mobile application known as AskDOM, that allows users to interact and find information of places suggested by the sentiment ratings. We have demonstrated both accuracy, as discussed in Section IV, and generalizability, as shown in Section V, of the engine in the analysis of various topics that are relevant to public interests. Further improvements are still needed to make",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "are relevant to public interests. Further improvements are still needed to make DOM engine more adaptive and robust. First, the sentiment score associated with each keyword is currently context independent and come mainly from the manual adjustment by the administrator. A context-dependent keyword-score association study is needed for each of the task required. After obtaining these related associations from different contexts, rules can be derived so that the system can work effectively on different tasks. Second, public opinions usually contain a lot of personal messages that are irrelevant to the places under discussion. A filter that is capable of detecting the context of the message is required. ACKNOWLEDGEMENT We would like to thank for the financial support the Faculty of Engineering, King Mongkut’s University of Technology Thonburi through",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "support the Faculty of Engineering, King Mongkut’s University of Technology Thonburi through the research grant (to SP) and the Office of Higher Education Commission through the National Research University (NRU) grant, fiscal year 2011-2013 (to TA). REFERENCES [1] M. J. Paul and M Dredze, A Model for Mining Public Health Topics from Twitter. Technical Report. Johns Hopkins University. 2011. [2] H. Purohit, A. Hampton, V. L. Shalin, A. P. Sheth, J. Flach, and S. Bhatt, “What Kind of #Conversation is Twitter? Mining #Psycholinguistic Cues for Emergency Coordination,” Computers in Human Behavior, vol. 29, pp. 2438-2447, November 2013. [3] O. Phelan, K. McCarthy, and B. Smyth, “Using Twitter to recommend real-time topical news” Proceedings of the third ACM conference on Recommender systems, New York City, NY, USA, 22-25 October",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "ACM conference on Recommender systems, New York City, NY, USA, 22-25 October 2009. [4] J. Bollen, H. Mao, and X.-J. Zeng, “Twitter mood predicts the stock market,” Journal of Computer Science, vol. 2, 1-8, March 2011. [5] Twitter Search API, https://dev.twitter.com/rest/public/search [6] Facebook Graph API, https://developers.facebook.com/docs/graph-api [7] Foursqaure API, https://developer.foursquare.com [8] Sathya, S., Jose, M.V. Application of Hadoop MapReduce technique to Virtual Database system design. Emerging Trends in Electrical and Computer Technology (ICETECT), 2011 International Conference: IEEE, 2011 [9] Lexto, www.sansarn.com/lexto/ [10] Haruechaiyasak, C., Kongthon, A. LexToPlus: A Thai Lexeme Tokenization and Normalization Tool, The 4th Workshop on South and Southeast Asian NLP (WSSANLP) International Joint Conference on Natural Language Processing, Nagoya, Japan, 14-18 October 2013. [11] L. C. Freemann, Centrality in social networks: I. conceptual clarification, Social",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "[11] L. C. Freemann, Centrality in social networks: I. conceptual clarification, Social Networks 1 (215-239). [12] C. Kiss and M. Bichler, “Identification of Influencers - Measuring Influence in Customer Networks,” Decision Support Systems, vol. 46, pp. 233–253, December 2008. [13] Si, J., Mukherjee, A., Liu, B., Li, Q., Li, H., Deng, X. (2013) “Exploiting topic based twitter sentiment for stock prediction” The 51st Annual Meeting of the Association for Computational Linguistics, Sofia, Bulgaria, August 4-9, 2013. Table 5. Summary of opinions with “#prayforthailand.” Opinions Percentage Oppose to the government 29.45 Loyal to the king 20.91 Feeling depressed about the situation 15.61 Oppose to both government and protests 0.82 Oppose to protesters 0.01 Others 33.2 Table 6. . Bangkok traffic congestion ranking Rank Streets / Intersections Percentage 1 Ladprao",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": ". Bangkok traffic congestion ranking Rank Streets / Intersections Percentage 1 Ladprao - Paholyothin 19.47 2 Vibhavadi - Rangsit 11.62 3 Petchaburi 7.76 4 Sukhumvit 4.71 5 Ramkumhaeng 4.13 6 Others 52.31 6",
        "start_idx": 3944,
        "end_idx": 3977
      }
    ],
    "595ef6c6-2dcb-4304-bf72-45fcd35d46e2": [
      {
        "text": "[Página 1] DOI referen ce number: 10.18293/SEKE2016 -166 Quality Assurance for Big Data Application – Issues , Challenges, and Needs Chuanqi Tao Jerry Gao Computer Science&Engineering Department Computer Engineering Department Nanjing University of Science and Tech nology San Jose State University , San Jose, USA Nanjing, China Taiyuan University of Techno logy, Taiyuan, China taochuanqi@njust.edu.cn Corresponding to: jerry.gao@sjsu.edu Abstract —With the fast advance of big data technology and analytics solutions, building high -quality big data computing services in different application domains is becoming a very hot research and application top ic among academic and industry communities, and government agencies. Therefore, big data based applications are widely -used currently, such as recommendation, predication , and decision system. Nevertheless, there are increasing quality problems resulting i n erroneous testing costs",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "Nevertheless, there are increasing quality problems resulting i n erroneous testing costs in enterprises and businesses. Current research work seldom discusses how to effectively validate big data applications to assure system quality. This paper focuses on big data system validation and quality assurance, and includes informative discussions about essential quality parameters, primary focuses, and validation process. Moreover, the paper discusses potential testing methods for big data application systems. Furthermore , the primary issues, challenges, and needs in testin g big data application are presented. Keywords — Quality assurance, big dataapplication quality assurance , big data validation . I. INTRODUCTION According to IDC [1], the Big Data technology market will \"grow at a 27% compound annual growth rate (CAGR) to $32.4 b illion through 2017 \".Today, with the fast",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "rate (CAGR) to $32.4 b illion through 2017 \".Today, with the fast advance of big data science and analytics technologies , diverse data mining solutions, machine learning algorithms, open-source platforms & tools, and big data database technologies have been developed, and become available to be used for big data applications . This suggests that big data computing and application services bring large -scale business requirements and dema nds in people ’s daily life . Big data -based application system is widely -used nowadays , such as recommendation system, predictions, recognized patterns, statistical report applications, etc. Emergent big data computing and services can be used in many disciplines and diverse applications, including business management, library science, energy and environment, education, biomedical, healthcare and life science, social media and networking,",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "and environment, education, biomedical, healthcare and life science, social media and networking, smart city and travel, and transportation, etc .[2]. Nevertheless , due to the huge vol ume of generated data, the fast velocity of arriving data, and the large variety of heterogeneous data, the big data based applications brings new challenges and issues for QA engineer s. For instance, it is a hard job to validate the co rrectness of a big data -based prediction system due to the large scale data size and the feature of timeliness. Therefore, Big data quality validation and big data -based application system quality assurance becomes a critical concern and research subject. Although there has been a numerous of published papers [ 2-6] addressing data quality and data quality assurance in",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "published papers [ 2-6] addressing data quality and data quality assurance in the past, seldom researches focus on validation for big data application quality . There is an emergent need in research work to quality study issues and quality assurance solutions for big data applications. This paper is writ ten to provide our perspective view s on big data system validation for quality assurance . The paper is organized as follows. Section II discusses the typical types of big data systems and covers the essential quality parameters and their associated factors . Section III reviews and compares the existing testing methods for big data system validation. The major issues, challenges, and needs are presented in Section IV. Conclusions are in Section V. II. UNDERSTANDING QUALITY ASSURANCE FOR BIG",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "IV. Conclusions are in Section V. II. UNDERSTANDING QUALITY ASSURANCE FOR BIG DATA APPLICATION SYSTEM This section discusses the scope and process of quality assurance for big data application system s. Moreover, it covers th e primary quality parameters with related factors. Big data applications have the following unique features: (a) statistical computation based on multi -dimensional large -scale data sets, b) machine -learning and knowledge based system evolution , c) intelligent decision making with uncertainty , d) non-oracle function s, and e) complicated visualization. These unique features bring more interesting quality assurance and QoS requirements, challenges, and needs. Based on the recent feedbacks from engineers at Silicon Valley, how to assure the quality of big data -based application system s becomes a critical concern and research subject",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "data -based application system s becomes a critical concern and research subject currently . Figure 1 The Typical Types of Big Data Application Systems [Página 2] A. Scope and Process of Big Data Application Quality Assurance Big data applications provide services for prediction, recommendations, decisions support through large -scale data sets and complicated intelligent algorithms. Figure 1 describes the typical types of big data application s. In general, big data application quality assurance refers to the study and application of various assurance processes, methods, standards, criteria, and systems to ensure the quality of big data system in terms of a set of quality parameters. Figure 2 shows a sample scope of v alidation for quality assurance of big data applications. Figure 2 The Scope of Validation for Big",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "of big data applications. Figure 2 The Scope of Validation for Big Data Application System Quality Compared to conventional software testing, a test process of big data based applications primarily focuses on their unique features , such as oracle problems, learning capability, and timeliness testing. Figure 3 shows a sample test process (function testing) for big data application system validation . The testing process, shown in Figure 3, includes the following steps . Step1 Big data system function testing , including rich oracles, intelligent algorithms , learning capability , as well as domain -specific functions; Step 2 Big data system function testing, includ ingsystem consistency, security, robustness, and QoS; Step 3 Big data system feature testing , checks usability, system evolution, visualization , and so on ; Step",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": ", checks usability, system evolution, visualization , and so on ; Step 4 Big data system timeliness testing , targets time related feature testing, including continuous testing, real- time testing, life-time testing, and others. B. Quality factors for big data application validation Conventional system quality parameters such as performance, robustness, security, etc., can be applicable onto big data systems. They are listed below . - System Performance –This parameter indicates the performance of the system, such as availability, response time, throughout, scalability, etc. - System Data Security –This parameter could be used to evaluate the security of big data based system in different perspectives. Using this parameter , data security could be evaluated in various perspectives at the different levels. - System Reliability –This parameter is used to",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "at the different levels. - System Reliability –This parameter is used to evaluatethe durability of the system when perform ing a required function under stated conditions for a specified period of time. - System Robustness - This parameter evaluates the ability of a system to resist change without adapting its initial stable configuration . Figure 3 A Quality Test Process for Big Data Application System In addition, d ue to the special characteristics , such as oracle problems, big data applications bring some impacted factors contributing to the challenges of system quality assuranc e. There are two typical big data applications: a) recommendation system s, and b) prediction system s. We have collected a number of common quality parameters from survey . Figure 4 s ummarizes the typical",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "common quality parameters from survey . Figure 4 s ummarizes the typical quality f actors for prediction and recommendation systems in a fishbone graph respectively . Those factors are presented in taxonomy below . Quality factors for prediction system s - System Correctness , which is a quality factor used to evaluate the correctness of the big data applications . Unlike the conventional system, big data applications are hard t o validate their correctness. F or instance, prediction –related software is mainly developed to make predictions or better understand about real world activities. Hence, it is difficult to determine the correct output for those types of software. Correctness is related to the prediction pattern or model. For instance, some models are more likely used to predict point of",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "For instance, some models are more likely used to predict point of inflexion values while some other models are doing well in predicting continuity. Thus, in order to verify the correctness of the system effectively, engineers need to evaluate the capability of prediction in the specified condition s and environment s. - System Accuracy , which is used to evaluate if the system yields true (no systematic errors) , and consistent (no random errors) results. Some big data applications [Página 3] are developed to find previously unknown answers, thereby only approximate solutions might be available. This can be called uncontrollable prediction . Some prediction is used to prevent something happening in the future, and the prediction result will affect actions or behaviors. In turn, those actions can promote",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "result will affect actions or behaviors. In turn, those actions can promote the prediction result. Figure 4 Big Data Application System Quality Factors - System Stability , which reflects the stability of the system prediction while environment change or data changes. For example, if the prediction capability of a system is stable with little changes when statistical data are acquired from different timeframes. - System Consis tency, which is a quality indicator useful to evaluate the consistency of the targeted systemin different perspectives. Due to the inherent uncertainties in system models, some applications do not produce single correct output for a given set on inputs. This leads to hardly determining the expected behaviors of the software. In such situation, domain -specific experts could provide opinions to support system",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "In such situation, domain -specific experts could provide opinions to support system consistency. - Duration , which indicates the expected prediction period . It can measure how up -to-date data is, and whether it is correct despite the possibility of modifications or changes that impact time and date values [6]. For instance, commonly -used prediction duration in enterpri se management can be divided into short term, middle term, and long term. - Deviation Analysis , which is used to analyze the prediction deviation within an accepted range or confidence interval. - System usability, which is a parameter that in dicates how well the big data application service can be used . This can be very subjective due to different developers and users have diverse user experiences. The typical",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "due to different developers and users have diverse user experiences. The typical usability factors include intuitiveness, comfortability, and flexibility. - System Perfo rmance , which is a distinct quality factor for big data application service . It is useful to evaluate how well big data are structured, designed, collected, generated, stored, and managed to support large -scale prediction services. Quality factors for recommend ation system s - Correctness –This quality factor reflects if the recommended service or commodity meets the demands of customers. Correctness could be subjective between different persons. Thus, how to measure correctness is still a challenge for quality ass urance engineers. - Correlation – This quality factor evaluates the d egree of correlation of the recommended service. This involves various recommendation strategies, such as user",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "of the recommended service. This involves various recommendation strategies, such as user content -based, behavior -based, and collaboration filtering -based. - Multiplicity – This quality factor refers to the measurements for repeatability of recommended service. For instance, a poor quality system probably recommends too many repeated or similar commodities to users. [Página 4] - Category Coverage – This indicator is useful to evaluate the coverage rate for diverse categories . This factor measure s the completeness of recommendation within a selected domain. - Accountability –This quality parameter is very important and mandatory for both big data service applications and user s. This could be measured in a quantitative way, such as user rating similarity, domain trust value, domain related degree, and social intimacy degree. - Duration –This factor",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "value, domain related degree, and social intimacy degree. - Duration –This factor indicates the expected recommendation period . For instance, commonly -used recommendation duration in enterprise management can be divided into short term, middle term, and long term. - Deviation Analysis –This factor is used to analyze the recommendation deviation within acce pted range or confidence interval. - System usability –This parameter indicates how well big data application service can be used . This can be very subjective due to different developers and users have diverse user experiences. - System Performance –This is a distinct quality factor for big data application service, and it is useful to evaluate how well big data are structured, designed, collected, generated, stored, and managed to support large -scale recommendation services. In addition",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "generated, stored, and managed to support large -scale recommendation services. In addition to the two typical applications discussed above, there are more big data related applications such as machine learning system, ranking system, and search system. Due to page limits, we do not list their quality factors here. A comparison of con ventional testing and big data application testing in detail is presented in Table 1. Table 1 A comparison of conventional testing and big data application testing III. VALIDATION METHODS FOR BIG DATA APPLICATION This section discusses and reviewsthe existing research results in software testing methods which have been used to validate various types of big data applications including intelligent systems, data mining programs, bioinformatics programs , and learning based applications. Program -based software t esting –Conventional",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "programs , and learning based applications. Program -based software t esting –Conventional program - based testing methods have been used in big da ta analytics application s. Csallner et al. presents a novel technique that Conventional Testing Big Data Application T esting Primary Objectives - Validate the quality of software , including functions, programs, performance, etc. - Provide on -demand testing services for big data application systems to support software validation and quality engineering process. Testing Focuses - Diverse software errors in its structures, functions, behaviors, user interfaces, and connections to the external systems. - System non -functional requirements such as performances, reliability, availability, vertical scalability, security, and etc. - Non-oracle problem or rich oracle function problem . - Complicated algorithm s. - Large -scale data input .",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "problem . - Complicated algorithm s. - Large -scale data input . - Complicated data models and integrations . Test Input - Limited scale - Specified data fo rmats - Structured data - Large-scale data volume with diverse formats and media - Structured and non -structured data - Timeliness Testing Execution - Offline testing in a test lab before product delivery . - Testing in a cloud -based test environment . - On-demand test execution in a cloud -based virtual test environment . - Continuous testing for big data applications . Test Coverage - Function -based, data flow, structure -based , state diagram . - To be develop ed; lack able currently . Data Model - Data partition, boundary analysis, etc. - Training data; sampling data; classifier; image",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "Data partition, boundary analysis, etc. - Training data; sampling data; classifier; image data classification; pseudo -oracles Testing Environment - A pre-configured test environment in a test labwith purchased hardware/software and tools . - Provide testing environment references - Develop rapid reuse framework Testing Process - Enterprise -oriented test processes for each project . - Crowd sourcing -based process - Learning -based testing - Classification based testing Testing Techniques - Apply selected well -known white -box and black -box testing techniques at the component level (or unit level) and the system level . - Required innovative continuous , timeliness , and currency testing techniques . - New testing solutions to deal with multi -dimensional large - scale data sets , uncertainty data , learning -based system evolution , and",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "data sets , uncertainty data , learning -based system evolution , and complicated visualization . Testing Tools - Use limited testing solutions and tools with the purchased licenses . - Select and use diverse testing tools solutions which are pre-configured, installed, and deployed . - Support development process . - Construct the whole process tool chains . - Data analysis tool . - Continuous evaluation including crowdsourcing an d sampling . Tool Connectivity and Platform - Traditional test tool/solution integration and composition . - Domain -specific application . - On-demand selective solutions which support users to integrate and composite test solutions and tools . - Incremental data sets. - - [Página 5] systematicallysearches for such bugs in MapReduce applications and generates corresponding test cases [18]. The technique works",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "in MapReduce applications and generates corresponding test cases [18]. The technique works by encoding the high -level MapReduce correctness conditions as symbolic program constraints and checking them for the program under test. Shang et al. propose d an approach to uncover the different behaviors of the underlying platforms for BDA Apps using Handoop between runs with small testing data and large real -life data in a cloud environment [19]. Classification -based t esting -A classification approach to program testing usually in volves two steps: a) training a classifier to distinguish failures from successful cases on a selected subset of results, and then b) applying the trained classifier to identify failures in the main set of results. A resembling reference model is usually used to train a classifier .",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "A resembling reference model is usually used to train a classifier . More specifically, there are te chniques for applying pattern classifications to alleviate the test oracle problems. Last et al. [9] and Vanmali et al. [11] apply a data mining approach to augment the incomplete specification of legacy systems. They train classifiers to learn the casual input -output relationships of a legacy system. Podgurski et al. classify failure cases into categories [10]. However, they do not study how to distinguish correct and failure behaviors of programs. Later, t heir research group further proposes classification tree approaches to refine the results obtained from classifiers [8]. Bowring et al. use a progressive machine learning approach to train a classifier on different software behaviors [7].They apply their technique in the",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "a classifier on different software behaviors [7].They apply their technique in the regression testing of a consecutive sequence of minor revisions of a program. Metamorph ic testing (MT) - This is a classic approach to testing programs that do not have oracles. Whenever a formal oracle is not avail able or costly to apply, we run into a test oracle problem . A test oracle is a mechanism against which testers can check the output of a program and decide whether it is correct. When an oracle is not available, other means of determining whether the test result is correct are known as pseudo -oracles . MT operates by checking whether a program under test behaves according to an expected set of properties known as metamorphic relations. A",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "according to an expected set of properties known as metamorphic relations. A metamorphic relation specifies how a particular change to the input of the program should change the output . MT was used for testing scientific applications in different areas such as machine learning applications [ 12, 13], bioinformatics programs [14], programs solving p artial differential equations [ 15] and image processing applications [ 16]. When testing programs solving partial diff erential equations, MT uncovered faults that cannot be uncovered by special value testing [ 15]. Learning -based testing – This involves how to adopt the various learning approaches and mechanisms to support testing for big data system s. Meinke et al. developed a technique for automatic test casegeneration for numerical software based on learning based testing (LBT)",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "automatic test casegeneration for numerical software based on learning based testing (LBT) [ 17]. The authors first created a polynomial model as an abstraction of the program under test. Then the test cases are gener ated by applying a satisfiability algorithm to the learned model. Crowd -sourced testing –This testing approach uses freelance testers and/or contracted engineers in a crowd sourcing community . It is a cost -effective method to validate a machine -learning based application systems, such as a human face recognition system . Currently, crowd -sourced testing has been used in mobile app testing and mobile TaaS (Testing as a Service ). One good example is uTest (http://www.utest.com/company ). Data m odel-based t esting –Since big data are the input values for big data application systems,",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "–Since big data are the input values for big data application systems, diverse data models can be used to assist test case generations. Vilkomir et al. presents a method to automatically gen erate test cases for a scientific program having many input parameters with dependencies [20]. They use a directed graph to model the input data space , including parameters and values as well as their dependencies. Valid test cases can be automati cally generated based on the directed graph model. Since their model satisfies the p robability law of Markov chains, it can be used to generate random and weighted test cases according to the likelihood of taking the parameter values. Rule -based s oftware t esting –This approach could be used in testing ru le-based or",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "t esting –This approach could be used in testing ru le-based or knowledge based systems. The basic idea is to design test cases based on the rules specified in an expert system . Deason et al. in [21] proposed a rule -based test data generation method for Ada programs. They demonstrate d that rule -based test data generation is feasible. The paper shows a great promise in assisting test engineers in test generation . Andrews et al. present ed a test pattern generation approach based on VHDL specific heuristi c rules [22]. Their results indicated the rule-based approach leading to a better test coverage . Conventional black -box software t esting –To assure the system performance and other related QoS parameters of big data applications , engineers could",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "and other related QoS parameters of big data applications , engineers could use convention black -box approaches to controlling their quality. Typical examples include decision table testing , equivalence partitioning , boundary value analysis , cause -effect graph, and use case testing, and so on . IV. ISSUES , CHALLENGES , AND NEEDS There are a number of major issues and challenges in big data quality validation and assurance. Here are typical ones. Issue #1 –What are the adequate test models and test coverage criteria for big data based service applications ? With the fast advance of big data technologies and anal ytics methods, more and more big data based applications and service systems are developed to be used in many areas of our daily life, including smart",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "to be used in many areas of our daily life, including smart cars, smart city, business intelligence, environmental control, and so on. The increasing deployment of big data applications and service s raises quality assurance concerns. In the past, m any existing white -box and black -box software test models and adequate validation criteria are developed to address validation needs of software applications in functions, b ehaviors, and structures. Ho wever, these existing adequate test models only focus on program functions, state -based behaviors, and program structures. In the software testing and quality assurance community, there is a lack of research work on adequate tes t modeling and [Página 6] coverage analysis for big data application systems by considering their special features and needs in rich oracle functions,",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "systems by considering their special features and needs in rich oracle functions, machine -learning based system evolutions, knowledge based system intelligence , and multi -dimensional large -scale data sets. Hence, according to real world practitioners, there is a clear demand on establish ing well-defined test coverage criteria for big data application systems. Otherwise, engineers and big data analy sts will have difficult time to figure out when th ey should stop quality testing for big data applications. This leads to the first demand described below. Need #1 –Developing w ell-defined adequate validation models and criteria to address the special features and needs of big data applications and service s. Issue #2 –Where are the well-defined big data system quality assurance programs and standards , including processes, assessment metrics,",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "data system quality assurance programs and standards , including processes, assessment metrics, regulations, and policies ? As we discussed in [25], ISO is working on updating of existing data quality assurance standards and programs for big data quality assurance. Considering the popularity of big data applications and services, we must address the quality control and assurance of big data based applications. Here, we point out the second emergent need below . Need #2 – Establishing quality assurance program s and standards to consider the special QoS parameters and factors of big data applications and services to ensure system quality . Issue #3 – What are test automation solutions and tools support ing efficient and large -scale testing operations for big data applications and servi ces? In the past",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "testing operations for big data applications and servi ces? In the past three decades, many test automation tools and soluti ons have been developed for engineers in assisting test automation activities and oper ations. Unfortunately, m ost of these tools are only useful to validate software system functions, behaviors , program structures , and system performance and other QoS parameters . As discussed in section III, there are a few published papers addressing validation methods . However, these validation methods are not designed and developed to address the special features and needs of big data application systems. As discussed in Section III, there has been a few of publi shed research work addressing special validation needs in big data based applications . However, there is a clear lack",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "in big data based applications . However, there is a clear lack of research work on automatic validation methods and solutions for big data application services. Therefore, the third emergent need for b ig data applications is listed below. Need #3 –More innovative adequate testing methods and test automation tools to address the special needs and features of big data application system s and services. Unlike conventional software test automation tools, th ese expected test automation solutions must consider big data applications’ special features listed below : - Large -scale big data inputs with diverse formats, and structured and non -structured data ; - Learning and knowledge based system evolutions ; - Non-oracles problems and r ich oracle functions with uncertainty ; - New QoS parameters, such as",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "ich oracle functions with uncertainty ; - New QoS parameters, such as accuracy, accountability, usability, and - Data modeling V. CONCLUSIONS With the fast advance of big data management technologies and analytics solutions, how to build high -quality big data application service s becomes a very hot subjec t. Nevertheless, there are increasing quality problems resulting in erroneous data costs in enterprises and businesses [25]. Current research work seldom discusses how to effectively validate big data applications to ensure system quality . This paper provides informative discussions on big data system validation and quality assurance, including the essential concepts , focuses, and validation process. Moreover, the paper identifies and discusses some primary quality factors . In addition, it presents a comparison between conventional testing and big data application",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "addition, it presents a comparison between conventional testing and big data application testing . Furthermore , the primary issues, chall enges, and needs are pre sented . REFERENCES [1] Editor of Hosting Journalist, IDC forecast: big Data technology and services to hit $32.4 billion in 2017 , December 18, 2013. [2] A.O. Mohammed, S. A. Talab . Enhanced extraction clinical data technique to improve data quality in clinical data warehouse. International Journal of Database Theory and Appl ication, 8 (3): 333-342, 2015 . [3] M. R. Wigan , R. Clake . Big data’s big unintended consequences . IEEE Computer, 46 (6):46-53, 2013. [4] J. Alferes, P.Poirier, C. Lamaire -Chad, et al. Data quality assurance in monit oring of wastewater quality: Univariate on - line and off -line methods.",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "oring of wastewater quality: Univariate on - line and off -line methods. In Proc. of the11th IWA conference on instrumentation control and automation, pp. 18-20, September, 2013. [5] R. Clarke . Quality factors in big data and big data analytics . Xamax Consultancy Pry Ltd. 2014. [6] A. Immonen, P. Paakkonen, and E. Ovaska . Evaluating the quality of social media data in big data architecture. IEEE Access, 3: 2028 - 2043 October 16, 2015. [7] J.F. Bowring , J.M. Rehg, and M.J. Harrold . Active learning forautomatic classific ation of software behavior. In Proc. of the 2004 ACM SIGSOFT International Symposium on SoftwareTesting and Analysis (ISS TA). ACM, New York , NY, pp.195–205, 2004 . [8] P. Francis, D. Leon , M. Minch, and A. Podgurski .",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "[8] P. Francis, D. Leon , M. Minch, and A. Podgurski . Tree- basedmethods for classifying software failures. In Proc. of the 15th International Symposium on Software Reliability Engineering (ISSRE). Los Alamitos, CA, pp. 451–462. 2004 . [9] M. Last, M. Friedman, and A. Kandel . The data mining approachto automated software testing. In Proc. of the 9th ACM SIGKDD International Conference on Knowledge Discovery andData Mining (KDD). ACM, New York , NY, pp. 388 –396, 2003 . [Página 7] [10] A. Podgurski, D. Leon, P. Francis, W. Masri, M. Minch, J. Sun, and B. Wang . Automated support for classifying software failure reports. In Proc. of the 25th International Conference on Software Engineering (ICSE) , LosAlamitos, CA, pp. 465 –475, 2003 . [11] M. Vanmali, M. Last,",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "LosAlamitos, CA, pp. 465 –475, 2003 . [11] M. Vanmali, M. Last, and A. Kandel . Using a neural networkin the software testing process. Internationa l Journal of Intelligent Systems , 17 (1) : 45–62, 2002 . [12] X. Xie, J.W. Ho, C. Murphy, G. Kaiser, B. Xu, and T.Y. Chen . Testing and validating machine learning classi fiers by metamorphic testing . Journal of System and Software, 84 (4):544–558, 2011 . [13] C. Murphy, G. Kaiser, L. Hu, and L. Wu, Properties of machine learning application sfor use in metamorphic testing . In Proc. of the 20th International Conferenceon Software Engineering and Knowledge Engineering (SEKE), pp.867–872, 2008 . [14] T.Y. Chen, J.W.K. Ho , H. Liu, and X. Xie . An innovative approach for testing bioinformatics programs",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "Liu, and X. Xie . An innovative approach for testing bioinformatics programs using metamorphic testing, BMC Bioinform. 10(2009). [15] T. Chen, J. Feng, and T.H. Tse . Metamorphic testing of programs on partial differential equations: a case study . In Proc. of the 26th Annual International Computer Software and Applications Conference, (COMPSA C), pp. 327 –333, 2002 . [16] J. Mayer, R. Guderlei . On random testing of image processing applications, In Proc. of the 6th International Conference on Quality Software (QSIC ), pp. 85–92, 2006 . [17] K. Meinke, F. Niu . A learning -based approach to unit testing of numerical software, in: A. Petrenko, A. Simo, J. Maldonado (Eds.), Testing Software andSystems, Lecture Notes in Computer Science, vol. 6435, Springer, Berlin,Heidelberg, 20 10, pp. 221–235. [18]",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "in Computer Science, vol. 6435, Springer, Berlin,Heidelberg, 20 10, pp. 221–235. [18] C. Csallner , L. Fegaras , C. Li. New Ideas Track: Testing MapReduce -style programs . In Prof. o f 9th ACM SIGSOFT Symposium on the Foundations of Software Engineering (ESEC/ FSE’11), pp 1 -4, 2011. [19] W. Shang，Z.M. Jiang，H. Hemmati，B. Adams，and A.E. Hassan . Assisting developers of big data analytics applications when deploying on Hadoop clouds . In Prof. of 35th International Conference on Software Engineering (ICSE), pp 402 -411, 2013 . [20] S.A. Vilkomir, W.T. Swain, J.H. Poore, and K.T. Clarno . Modeling input space fortesting scientific compu tational software: a case study . In Prof. of the 8th International Conference on Computational Science, Part III (ICCS), pp. 291– 300, 2008 . [21] W.H.",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "Computational Science, Part III (ICCS), pp. 291– 300, 2008 . [21] W.H. Deason，D.B. Brown，and K.H. Chang . A rule -based software test data generator . IEEE Transactions on Knowledge and Data Engineering, 3(1): 108 -117, 1991. [22] A. Andrews , A.O. Fallon , and T. Chen . A Rule -Based Software Testing Method for VHDL Models. VLSI -SOC 2003 : 92. [23] W. Afzal, R. Torkar, and R. Feldt . A systematic review of search-based testing for non-functional system properties . Inform ation and Softw are Technol ogy, 51 (6) :957–976, 2009 . [24] T. Clune, R. Rood, Software testing and verification in climate model development, IEEE Softw are, 28 (6) :49–55, 2011 . [25] J. Gao, C .L. Xie, and C .Q. Tao. Quality assurance for big",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "Gao, C .L. Xie, and C .Q. Tao. Quality assurance for big data– issuses, challenges, and needs. In Prof. of IEEE 9th International Symposium on Service oriented System Engineering , OZFORD, UK, 2016. ACKNOWLEDGEMENT This paper is s upported by the National Natural Science Foundation of China under Grant No.61402229 and No.61 502233; the Open Fund of the State Key Laboratory for Novel Software Technology (KFKT2015B10), and the Postdoctoral Fund of Jiangsu Province under Grant No.1401043B . .",
        "start_idx": 4988,
        "end_idx": 5066
      }
    ],
    "3791031e-e4e6-4607-b30c-11e884efa5c1": [
      {
        "text": "[Página 1] Just can’t get enough - Synthesizing Big Data Tilmann Rabl Middleware Systems Research Group University of Toronto Canada tilmann.rabl@utoronto.caManuel Danisch, Michael Frank, Sebastian Schindler bankmark UG Passau, Germany {ﬁrst.last}@bankmark.deHans-Arno Jacobsen Middleware Systems Research Group University of Toronto Canada jacobsen@eecg.toronto.edu ABSTRACT With the rapidly decreasing prices for storage and storage systems ever larger data sets become economical. While only few years ago only successful transactions would be recorded in sales systems, today every user interaction will be stored for ever deeper analysis and richer user modeling. This has led to the development of big data systems, which offer high scalability and novel forms of anal- ysis. Due to the rapid development and ever increasing variety of the big data landscape, there is a pressing need for tools",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "of the big data landscape, there is a pressing need for tools for testing and benchmarking. Vendors have little options to showcase the performance of their systems but to use trivial data sets like TeraSort or WordCount. Since customers’ real data is typically subject to privacy regula- tions and rarely can be utilized, simplistic proof-of-concepts have to be used, leaving both, customers and vendors, unclear of the tar- get use-case performance. As a solution, we present an automatic approach to data synthetization from existing data sources. Our system enables a fully automatic generation of large amounts of complex, realistic, synthetic data. 1. INTRODUCTION Data generation is a tedious part of the daily routine of researchers testing new algorithms, database administrators testing new con- ﬁgurations, and performance engineers testing",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "new algorithms, database administrators testing new con- ﬁgurations, and performance engineers testing new optimizations. Typically, the data is generated using scripting solutions, which are written and rewritten for each individual use case. While this is manageable for simple and small data sets, databases are ever in- creasing in size and complexity. Database testing typically is done on simple scenarios, however, customers demand ever more real- istic benchmarks that match their use cases [21]. Modern enter- prise systems comprise hundreds to thousands of tables, which fre- quently have more than ﬁfty columns. System vendors that can show the performance of their systems in a most realistic setup have an advantage over vendors that present generic, simple bench- marks. To this end, multiple generic data generators have been devel- oped,",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "marks. To this end, multiple generic data generators have been devel- oped, that enable a fast description of data models and correlations [10, 9, 1, 4]. Although all of these systems feature more efﬁcient Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proﬁt or commercial advantage and that copies bear this notice and the full cita- tion on the ﬁrst page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "redistribute to lists, requires prior speciﬁc permission and/or a fee. Request permissions from permissions@acm.org. SIGMOD’15, May 31–June 4, 2015, Melbourne, Victoria, Australia. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2758-9/15/05 ...$15.00. http://dx.doi.org/10.1145/2723372.2735378.and more realistic generation of data none has reached wide adop- tion in the industry. While building custom data generators for each use case is highly inefﬁcient, generic tools are either too limited in their features or their learning curve is too steep to be accepted by developers. This has led to a dilemma, where it is often infeasible for customers to ﬁnd the best solution for their use case since they can only give metadata but not real data to vendors and vendors are not able to cover the complex schemas",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "to vendors and vendors are not able to cover the complex schemas and dependencies in their proof of concept systems. What is needed is a simple solution, that is able to automatically generate realistic data based only on meta information of the orig- inal data. Because of ever increasing data sizes and the prevalent increase of cores, processors, and physical machines in deployed systems, the solution has to be performant and scalable. The data has to be rich in features even in single values, since big data sys- tems do not stop processing at the value level. With these require- ments in mind, we have developed DBSynth, a fully automatic data generation solution that can reproduce realistic data sets based on schema information and sampling. Unlike any other",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "realistic data sets based on schema information and sampling. Unlike any other generic data generator, it can extract features on the value level and generate new relevant values. DBSynth is an extension to the Parallel Data Generation Frame- work (PDGF), a generic data generator suite [17]. PDGF is the ba- sis for the data generator of the new industry standard ETL bench- mark TPC-DI, which was released by the Transaction Processing Performance Council (TPC) in January 2014 [16] and was also used to implement the data generator for BigBench, the ﬁrst end-to-end proposal for a big data analytics [7]. Our main contributions are the following, we demonstrate DB- Synth, which is the answer to one of the fundamental challenges in the big data space today - ﬁnding the",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "the fundamental challenges in the big data space today - ﬁnding the best system for a given use case. DBSynth is the ﬁrst tool, which can generate complete data models from large complex databases on a variety of systems, sample data sets and automatically create relevant dictionaries and Markov models. Furthermore, we demonstrate PDGF, a highly ad- vanced generic data generator, which can use models created by DBSynth to create big, realistic, synthetic data. PDGF is faster and more efﬁcient than any other generic generator and can be run with perfect speedup in multi-core and multi-node environments. PDGF can write data in various formats (e.g., CSV , JSON, XML, and SQL) to ﬁles, database systems, streaming systems, and mod- ern big data storage systems (e.g., HDFS). The remainder",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "systems, and mod- ern big data storage systems (e.g., HDFS). The remainder of this paper is structured as follows. We describe PDGF brieﬂy in Section 2 and then show the core functionality of DBSynth in Section 3. After an evaluation in Section 4, we explain our demonstration in Section 5. In Section 6, we give an overview of related work, before concluding in Section 7. 1457 [Página 2] 2. PDGF The Parallel Data Generation Framework (PDGF) is a versatile, generic data generator [17]. PDGF has a novel computation-based generation strategy that enables a completely parallel generation of data. PDGF’s generation strategy is based on the exploitation of de- terminism in pseudo random number generators (PRNG). Random number sequences generated using PRNGs are repeatable, which means that the exact",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "number sequences generated using PRNGs are repeatable, which means that the exact same sequence can be generated over and over again. PDGF uses xorshift random number generators, which be- have like hash functions. Repeatable, parallel data generation is possible using an elaborate seeding strategy (depicted in Figure 1). In the example, starting with a project seed one seed per table is generated. This seed is used to create one seed per column, which again is used to generate one seed per abstract time unit and ﬁnally per ﬁeld. The ﬁeld seed is used to generate the random number sequence for the value generation. The values are generated using ﬁeld value generators. These can be simple generators, like number generators, generators based on dictionaries, or reference genera- tors, but",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "like number generators, generators based on dictionaries, or reference genera- tors, but also meta generators, which can concatenate results from other generators or execute different generators based on certain conditions [18]. The concept of meta generators enables a func- tional deﬁnition of complex values and dependencies using simple building blocks. Although the seeding hierarchy and meta genera- tor stacking seems expensive, most of the seeds can be cached and the cost for generating single values is very low. We will show an analysis of the exact costs in Section 4. Customer Row # / CustKey Name Address …Table RNG 1 2 3 seed t_id Column RNG seed c_id Update RNG seed u_id Generator(rn) rn 4 ID (Row) RNG seed id Figure 1: PDGF’s seeding strategy PDGF’s architecture is",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "(Row) RNG seed id Figure 1: PDGF’s seeding strategy PDGF’s architecture is shown in Figure 2. The user speciﬁes two XML conﬁguration ﬁles, one for the data model and one for the formatting instructions. These will be explained in more de- tail in Section 3. Additionally, all previously speciﬁed properties of a model and format (e.g., scale factors, table sizes, probabili- ties) can be changed in the command line interface. The controller then initializes the system. The meta scheduler manages multi- node scheduling, while the scheduler assigns work packages to the workers. A work package is a set of rows of a table that need to be generated. The workers then initialize the correct generators using the seeding system and the update black box. Whenever a work package",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "the seeding system and the update black box. Whenever a work package is generated, it is sent to the output system, where it can be formatted and sorted. PDGF XML XML Meta Scheduler Scheduler Worker Worker Worker Update Black Box Seeding System Worker Worker Generator Output System CSV DB RNG P Controller UI PPData Flow Control Flow Plugin Optional Figure 2: PDGF’s architecturePDGF has been successfully used to implement a variety of bench- marks, e.g., TPC-H [17], the Star Schema Benchmark [19], TPC- DI [6], and BigBench [7]. 3. DBSYNTH DBSynth is an extension to PDGF that automates the conﬁgu- ration and enables the extraction of data model information from an existing database. DBSynth’s abstract architecture and mode of operation can be seen in Figure 3. In DBSynth,",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "and mode of operation can be seen in Figure 3. In DBSynth, the user speciﬁes projects, which integrate workﬂows, such as data generation, data extraction, etc. The ﬁgure shows the complete automatic workﬂow, from model extraction to data generation. Not all steps are neces- sary for a given project. DBSynth connects to a source database via JDBC, using the model creation tool, schema information and a conﬁgurable level of additional information of the data model are extracted. Possible in- formation includes min/max constraints, histograms, NULL prob- abilities, as well as statistic information collected by the database system such as histograms. DBSynth also features a rule based system that searches for key words in the schema information and adds predeﬁned generation rules to the data model. For example, numeric",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "and adds predeﬁned generation rules to the data model. For example, numeric columns with name keyoridwill be generated with an ID generator. If sampling the database is permissible, the data extraction tool builds histograms and dictionaries of text-valued data and stores the according probabilities for values. Users can specify the amount of data sampled and the sampling strategy. In future versions, we will include a dynamic sampling, which adapts the sample size and sam- pling strategy according to the base data. If the text data contains multiple words, DBSynth uses a Markov chain generator, which an- alyzes the word combination frequencies and probabilities. These are stored and linked to the data model. Using the generated data model, PDGF can generate the data. The model is translated into a",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "model, PDGF can generate the data. The model is translated into a SQL schema, which is loaded into the target database using JDBC. The data can be loaded into the target database either using SQL statements generated by PDGF or a bulk load option, if featured by the target database. Listing 1 presents an excerpt of the automatically generated con- ﬁguration for a TPC-H data set [15]. The excerpt shows the gen- eral structure of the PDGF schema conﬁguration. It contains the project’s seed, changing the seed will modify every value of the generated data set, the random number generator, PdgfDefaultRa- ndom is a custom built, very fast xorshift PRNG, property deﬁni- tions, which can also be changed from the command line, and the schema information itself. A",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "be changed from the command line, and the schema information itself. A default property is the scaling factor SF, which is used to determine the size of the data set. DBSynth will generate a size property for each table and assign it the prod- uct of the scale factor and the original table size. This way other scaling dependencies can be easily speciﬁed in a centralized point in the model. Furthermore, all boundaries for numerical values and dates are stored in properties. The schema model is speciﬁed in form of table entries. Each ta- ble speciﬁes its size, which DBSynth sets to be linear with the scale factor as shown in the example. However, any formula can be used to calculate the size. Then the columns of the",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "can be used to calculate the size. Then the columns of the table are speciﬁed in form of ﬁeld entries. The ﬁrst ﬁeld is \"l_orderkey\", the name and size are extracted from the database’s schema information. The fact that it is a key is deduced from the column name, this is the reason why DBSynth chooses an ID generator for this column. The next ﬁeld is \"l_partkey\", which is a reference to the table \"partsupp\". This is speciﬁed in the schema, which is why DBSynth chooses a DefaultReferenceGenerator, which will generate consistent refer- ences to this table. The ﬁnal ﬁeld that is shown is \"l_comment\", 1458 [Página 3] Table TableMeta DataSource DatabaseDataSynth Model Creation PDGFData Model Markov Chains DictsData ExtractionJDBC JDBC Table TableTarget Database Meta DataSchema TranslatorFigure 3:",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "Markov Chains DictsData ExtractionJDBC JDBC Table TableTarget Database Meta DataSchema TranslatorFigure 3: Abstract architecture and data ﬂow in DBSynth a text ﬁeld containing free text. DBSynth chooses the Markov- Generator for this ﬁeld, thus it will sample the original database to build the Markov model. For a TPC-H data set the comment ﬁeld model contains 1500 words and 95 starting states, which can eas- ily be ﬁt in memory. The choice of the generator type used for a ﬁeld is based ﬁrst on referential integrity constraints, i.e., a refer- ence will always be generated by a reference generator independent of its type. Then the data type determines if a number generator, e.g., Long, Integer, Double, or a date generator, or a text gener- ator is used, DBSynth and",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "a date generator, or a text gener- ator is used, DBSynth and PDGF support all SQL 92 datatypes. If the database is not sampled, the column name is parsed to de- termine whether a matching high level generator construct exists, e.g., names, addresses, comment. In case nothing is found a ran- dom string is generated. The Markov generator builds dictionaries for single word text ﬁelds and Markov chains for free text, the pa- rameters for the Markov model are adjusted based on the original data. If the original data cannot be sampled or analyzed, DBSynth falls back to random values based on the database statistics as well as predeﬁned generators for URLs, addresses, etc. 4. EV ALUATION We evaluated the performance of PDGF and DBSynth on a 24",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "ALUATION We evaluated the performance of PDGF and DBSynth on a 24 node, dual socket, dual core cluster and on a single node with two sockets and eight cores per socket. Where possible, generated data was written to /dev/null to ensure the throughput was not I/O bound. In the experiments, we used either the BigBench data set or our custom implementation of the TPC-H data set. In the ﬁrst experiment, we evaluate the performance of PDGF by generating a BigBench data set of scale factor 5000, which results in a total data size of 4392 GB on the 24 node cluster. The results of this experiment can be seen in Figure 4. As is shown in the ﬁgure, PDGF has linear throughput scaling in the number of nodes.",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "the ﬁgure, PDGF has linear throughput scaling in the number of nodes. In the second experiment, we benchmark the scale out perfor- mance of PDGF by increasing the number of workers and thus threads used for the data generation. This experiment is conducted on the single node. The results can be seen in Figure 5. PDGF’s throughput increases linearly with the number of cores (16) and fur- ther increases with the number of hardware threads (32), but not as signiﬁcantly as for the number of cores. An interesting observation is that scheduling exactly the same number of workers as the num- ber of system cores or threads is not optimal due to the additional internal scheduling and I/O threads. In Figure 6, a comparison of the data generator",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "and I/O threads. In Figure 6, a comparison of the data generator DBGen and PDGF is shown. As can be seen, both tools achieve a similar perfor- mance. In parallel mode, it is not possible to write to /dev/null using DBGen, which is why in this experiment the throughput of both, DBGen and PDGF, was disk-bound. We also show PDGF’s CPU-<?xml version=\"1.0\" encoding=\"UTF-8\"?> <schema name=\"tpch\"> <seed>12456789</seed> <rng name=\"PdgfDefaultRandom\"></rng> <property name=\"SF\" type=\"double\">1</property> <property name=\"lineitem_size\" type=\"double\">6000000 * ${SF}</property> <table name=\"lineitem\"> <size>${lineitem_size}</size> <field name=\"l_orderkey\" size=\"19\" type=\"BIGINT\" primary=\"true\"> <gen_IdGenerator> </gen_IdGenerator> </field> <field name=\"l_partkey\" size=\"19\" type=\"BIGINT\" primary=\"false\"> <gen_DefaultReferenceGenerator> <reference table=\"partsupp\" field=\"ps_partkey\"></reference> </gen_DefaultReferenceGenerator> </field> [..] <field name=\"l_comment\" size=\"44\" type=\"VARCHAR\" primary=\"false\"> <gen_NullGenerator> probability=\".0000d\" <gen_MarkovChainGenerator> <min>1</min> <max>10</max> <file>markov\\l_comment_markovSamples.bin</file> </gen_MarkovChainGenerator> </gen_NullGenerator> </field> </table> [..] Listing 1: Excerpt of the schema deﬁnition for TPC-H bound performance, which is",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "1: Excerpt of the schema deﬁnition for TPC-H bound performance, which is 33% higher than its disk-bound per- formance. DBGen’s parallelization is non-transparent. This means that for each parallel stream a new instance is started, which writes its own ﬁles. As a result, DBGen’s parallel output will be split in as many ﬁles as instances were started, whereas PDGF writes sorted output into a single ﬁle. PDGF also supports the same par- allel generation strategy as DBGen does, which is starting multiple instances and generating a distinct range of the data set with each instance. With this approach it is possible to scale out the gener- ation to shared nothing systems with linear speedups [17]. When comparing the single process performance, i.e., starting only a sin- gle DBGen",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "comparing the single process performance, i.e., starting only a sin- gle DBGen instance and running PDGF with a single worker, DB- Gen achieves 48 MB/s and PDGF 30 MB/s. Thus, PDGF is has the 1459 [Página 4] 0200400600800100012001400 0 4 8 12 16 20Throughput MB/s Nodes 0100200300400500600700 -2 2 6 10 14 18 22Duration min NodesFigure 4: PDGF BigBench scale-out performance 0100200300400500 0 10 20 30 40 50Throughput MB/s ThreadsFigure 5: PDGF TPC-H scale-up per- formance 0510152025 1 10 30 100 300Duration s Scale FactorDBGen PDGF PDGF /dev/nullFigure 6: DBGen vs PDGF perfor- mance 050100150200250 Static Value (no Cache)Null Generator (100% NULL)Null Generator (0% NULL)ns Base Time Generator Base Time Sub Sub Generator 0100200300400500 0 10 20 30 40 50Throughput MB/s Threads Figure 7: Generation latency 050100150200250 Static",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "20 30 40 50Throughput MB/s Threads Figure 7: Generation latency 050100150200250 Static Value (no Cache)Null Generator (100% NULL)Null Generator (0% NULL)ns Base Time Generator Base Time Sub Sub Generator 0100200300400500600 DictList Long Double Date Stringns Generator 0200400600800100012001400160018002000 DictList Null (100%) Null (0%) Date (formatted)Sequential (2 double + long)Double (4 places)ns Generator Figure 8: Basic generator latency 050100150200250 Static Value (no Cache)Null Generator (100% NULL)Null Generator (0% NULL)ns Base Time Generator Base Time Sub Sub Generator 0100200300400500600 DictList Long Double Date Stringns Generator 0200400600800100012001400160018002000 DictList Null (100%) Null (0%) Date (formatted)Sequential (2 double + long)Double (4 places)ns Generator Figure 9: Complex generator latency same order of performance as DBGen, although being completely generic and adaptable. We conducted further experiments to determine the sources of la- tencies for the individual",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "further experiments to determine the sources of la- tencies for the individual value generation. The experiments were done in a single threaded setup, to get the per value overhead. These results show the pure computational requirements and do not dis- cuss latencies added by the I/O subsystem. In Figure 7, the latency of independent value generation is broken down into its subparts. For a static value, i.e., a column contains only one unique value that is never changed, the pure system overhead can be seen. It is in the order of 50 Nanoseconds (ns). If a NULL value generator is wrapped around a static value that is NULL with 100% probabil- ity, the overhead of the NULL generator is added to the generation of the (static) NULL value,",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "NULL generator is added to the generation of the (static) NULL value, this is again in the order of 50 ns. The deﬁnition of the NULL value generator can be seen in Listing 1. Fi- nally, if the NULL probability is 0% the inner static value generator has to be executed in all cases, this adds the base time for the sub- generator and the actual value generation, both of which are again ca 50 ns. Thus the total duration for each value is in the order of 200 ns. In Figure 8, it can be seen that this is a good ballpark number for simple values that are not formatted. Picking values from dictio- naries, computing random numbers, and generating random strings are all in the range",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "computing random numbers, and generating random strings are all in the range of 100 ns - 500 ns. String formatting is the most expensive operation in data generation in Java, this can seen in Figure 9. Formatting a date value (e.g., \"11/30/2014\") increases the generation cost to 1200 ns, which is similar to generating a value that consists of a formula that references 2 double values and concatenates it with a long. Although the formatting is expensive, its cost is ﬁxed since PDGF does lazy formatting, which means even very complex values will only be formatted once. This anal- ysis shows that using subgenerators incurs nearly negligible cost (ca. 100 ns) and it also shows that computing values rather than rereading them is much more efﬁcient. While generating",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "computing values rather than rereading them is much more efﬁcient. While generating complex values might cost up to 2000 ns, doing a single random read will cost ca. 10 ms on disk, which means the computational approach is 5000 times faster than an approach that reads previously gener- ated data to solve dependencies. Furthermore, the computational approach enables a completely parallel generation of data, which is Figure 10: Standard screen of DBSynth not possible using reading based approach without replicating data or extensive network communication. In our ﬁnal experiment, we tested the performance of the DB- Synth metadata extraction. Using a TPC-H database with scale factor 1 loaded in a PostgreSQL DBMS, it takes 600 ms to get the schema information, 1.3 s to get the table sizes,",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "to get the schema information, 1.3 s to get the table sizes, 600 ms to get NULL probabilities, 10 seconds to get all min and max constraints, and between 800 ms (0.001% samples) and 200 s (100% samples) to retrieve data for the Markov chains. These results indicate an in- teractive response time for data model generation. Using PDGF’s preview generation, which shows samples of the generated data in- stantaneously, data models can be built and improved very fast. 5. DEMONSTRATION To show the ease of use of DBSynth, we will demonstrate typ- ical work ﬂows for data generation. We will start by generating industry standard data sets such as TPC-H. The data will be gener- ated using PDGF, but this conﬁguration is compliant to the TPC-H data",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "ated using PDGF, but this conﬁguration is compliant to the TPC-H data set [15] and was developed in cooperation with the TPC-H subcommittee. This is a default project in DBSynth. The accord- ing selection screen can be seen in Figure 10. We will generate a 10 GB TPC-H data set. We will show how the data can be altered 1460 [Página 5] Figure 11: Mission Control interface for PDGF by changing the output format. To this end, the data will be writ- ten in CVS and XML format. The generation progress and system utilization will be monitored using Java Mission Control1. PDGF uses the Java Management Extensions internally for inter process communication and using these interfaces, the progress of single tables and the complete data set as well",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "the progress of single tables and the complete data set as well as general performance parameters can be visualized. This can be seen in Figure 11. After the data generation, we will demonstrate the conﬁgura- tion generation. To show a real use case, we will use the pub- licly available parts of the IMDb database2. The data set is hosted in a MySQL database, which was loaded using the imdbpy2sql.py script, which is part of the IMDbPY package3. We will ﬁrst use a basic schema extraction, where only the schema information is retrieved from the database and no tables are accessed. The gen- erated XML ﬁle will be explained, which contains the model for the data generation. Then we will do a second more elaborate schema extraction, where",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "generation. Then we will do a second more elaborate schema extraction, where min/max constraints, NULL values, and data samples for Markov chains will be read from the database. We will compare the newly extracted model with the ﬁrst one and then generate the data. The according screen can be seen in Figure 12. We will show excerpts of the generated data in comparison to the original data and verify the quality by running SQL queries on the original data and the generated data and compare the results. To this end, the generated data will be loaded to a database system. Finally, we will explain how the model can be changed or adapted. We will change the automatically generated conﬁguration by adding additional columns to the model and reﬁning",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "automatically generated conﬁguration by adding additional columns to the model and reﬁning correlations that could not automatically be detected. This will be done using an automat- ically generated version of the TPC-H conﬁguration. We will then show the differences between the original TPC-H conﬁguration and the newly generated conﬁguration and compare the generated data sets. 6. RELATED WORK There is a rich body of work on data generation for database benchmarking and testing. In the following, we will ﬁrst give an overview of related work on data generation in general and then show other approaches for synthesizing existing data sets. Even though there are many generic data generation tools, most data generators either produce very simple data or are non-reusable hard coded programs or collections of scripts. Examples",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "data or are non-reusable hard coded programs or collections of scripts. Examples for simple data generation are the data generator used by all variations of the 1http://www.oracle.com/technetwork/java/javaseproducts/mission- control/java-mission-control-1998576.html 2http://www.imdb.com/interfaces 3https://github.com/alberanid/imdbpy Figure 12: Data generation screen of DBSynth sorting benchmark (e.g., TeraSort4) and the Yahoo Cloud Serving Benchmark data generator [5]. Examples for hard coded generators are all TPC data generators, with the notable exceptions of TPC-DS [14] and TPC-DI [16]. While simple data is helpful for testing basic functionality it does not represent real world use cases. However, hard coded data generators cannot easily be adapted for changing requirements and for different systems. In many cases real data can be used, but due to the impossiblity of scaling, privacy constraints and cost of storage and transportation it is",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "of scaling, privacy constraints and cost of storage and transportation it is not feasible for bench- marking. An important characteristic for benchmarking data is repeatablity. Basis for repeatable parallel data generation is the work by Gray et al. on synthetic data generation [8]. This work describes how to generate non uniform data in parallel on shared nothing systems. These techniques along with parallel pseudo random number gen- erators for the basis of our Parallel Data Generation Framework. Modern generic data generators can be divided into three sub- sets according to their reference and correlation generation: (1) no reference generation; (2) reference tracking; and (3) reference computation. Many data generators do not generate references or correlations explicitly, but rely on users providing correct statistic distributions to generate correlating values.",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "but rely on users providing correct statistic distributions to generate correlating values. Generators that track references either compute all references at the same time that the original value is generated, or they track the original value when a references is generated. The former approach is frequently done us- ing graph models [10, 11] or declarative description [2, 24] which can lead to realistic data, however, typically is very slow and hard to parallelize. Tracking references is done by rereading the previ- ously generated data. This approach was for example presented by Bruno et al. [4], this approach is very ﬂexible but also very slow and does not scale well. A faster approach is generating all related data at the same time. Generic suites that use this approach include",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "data at the same time. Generic suites that use this approach include MUDD [22] and PSDG [9]. The fastest way of generating correct references in most cases is recomputing them. This approach was ﬁrst implemented in PDGF. A very similar data generator, Myriad, was built at the Technical University of Berlin [1]. It shares the same generation strategy with PDGF, however, does does not in- clude many of the features of PDGF, such as update generation and text generation. As part of the BigDataBench suite of test cases for big data systems, an early version of PDGF is used to generate structured data [13]. The suite comprises additional generators for graphs 4https://hadoop.apache.org/docs/current/ api/org/apache/hadoop/examples/terasort/ package-summary.html 1461 [Página 6] and a similar text generator as the current version of PDGF",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "6] and a similar text generator as the current version of PDGF fea- tures. Unlike PDGF’s text generator, the different generators are not connected and, therefore, cannot generate heterogeneous data sets with references in between different data sets, e.g., references from structured data into text. Although synthetic data is usually better suited for benchmark- ing purposes, synthetic data should reﬂect characteristics of real data. Therefore, typically real data sets are analyzed for modeling data sets for benchmarking. Although this step is typically manual, it can be automated. Like DBSynth other tools use the metadata stored in database systems to get information about the distribution and structure of the data. RSGen reads metadata and schema in- formation of existing data sets and generates similar data sets by using histograms",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "of existing data sets and generates similar data sets by using histograms of the original data [20]. Although similar to DB- Synth, the approach is limited to numerical data. Another tool that is able to scale existing data sets is UpSizeR [23]. It builds a graph of the original schema information and correlation information and generates data accordingly. However, the individual, non-key val- ues are deemed application speciﬁc and thus have to be speciﬁed by the user. DBSynth uses sampling of the data set to generate dictionaries and Markov chains for non-key, non-numerical values. Furthermore, DBSynth uses its built in dictionaries to increase the value domain in scale out scenarios. Myriad also comes with con- ﬁguration generation tool Oligos, which can analyze the schema and statistical information of",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "generation tool Oligos, which can analyze the schema and statistical information of a DB2 database [1]. However, it can- not not sample a database and also has no tools to analyze and synthesize values. A line of work that is orthogonal to our work is test data genera- tion for queries. QAGen analyzes queries to generate data that pro- duces desired results and intermediate results to cover all required test cases [3]. A similar tool is MyBenchmark [12]. Analyzing queries to ensure desired results is currently not a feature of our tool, but will be included in future versions. Given the determinis- tic approach of data generation, our tool will then also be able to directly execute the query without ever generating the data, which can be used",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "execute the query without ever generating the data, which can be used to verify results for correctness. Although many of the features of DBSynth are covered to some extent by other projects, none of these does include synthetic value generation. Values are always treated as atomic units. This is prob- lematic for big data sets, where values frequently are texts that have to be further analyzed using machine learning techniques. To gen- erate data sets, which satisfy requirements of big data use cases, DBSynth includes Markov chain generators and dictionaries that can generate realistic, synthetic values based on sampled data. 7. CONCLUSION We demonstrate DBSynth, an extension to the Parallel Data Gen- eration Framework, which enables a fully automatic conﬁguration of the data generator based on existing databases.",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "a fully automatic conﬁguration of the data generator based on existing databases. DBSynth can build realistic data models from a deployed database extracting schema information, sampling, and analyzing the database. It uses heuristics for data type determination and builds dictionaries and Markov models for data generation. The generated models and conﬁguration can be directly used by PDGF to generate data for a target database. A simple, intuitive graphical user interface ties all parts together and features wizards to guide users through every step of the process. In our demonstration, we showcase the ease of use, the high performance, and the ﬂexibility of the system. In future work, we will extend DBSynth to automate the com- plete benchmarking process. To this end, we will generate the queries consistently using",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "benchmarking process. To this end, we will generate the queries consistently using PDGF and build additional driver and analysis modules. Furthermore, we will include query analysis togenerate data sets with predeﬁned (intermediate) results and gener- ate veriﬁcation results for queries for given data models. 8. REFERENCES [1] A. Alexandrov, K. Tzoumas, and V . Markl. Myriad: Scalable and Expressive Data Generation. In VLDB, 2012. [2] A. Arasu, R. Kaushik, and J. Li. Data Generation Using Declarative Constraints. In SIGMOD, 2011. [3] C. Binnig, D. Kossmann, E. Lo, and M. T. Özsu. QAGen: Generating Query-aware Test Databases. In SIGMOD, 2007. [4] N. Bruno and S. Chaudhuri. Flexible Database Generators. InVLDB, pages 1097–1107, 2005. [5] B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears. Benchmarking Cloud Serving",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "A. Silberstein, E. Tam, R. Ramakrishnan, and R. Sears. Benchmarking Cloud Serving Systems with YCSB. InSoCC, pages 143–154, 2010. [6] M. Frank, M. Poess, and T. Rabl. Efﬁcient Update Data Generation for DBMS Benchmark. In ICPE, 2012. [7] A. Ghazal, T. Rabl, M. Hu, F. Raab, M. Poess, A. Crolotte, and H.-A. Jacobsen. BigBench: Towards an industry standard benchmark for big data analytics. In SIGMOD, 2013. [8] J. Gray, P. Sundaresan, S. Englert, K. Baclawski, and P. J. Weinberger. Quickly Generating Billion-Record Synthetic Databases. In SIGMOD, pages 243–252, 1994. [9] J. E. Hoag and C. W. Thompson. A Parallel General-Purpose Synthetic Data Generator. SIGMOD Record, 36(1):19–24, 2007. [10] K. Houkjær, K. Torp, and R. Wind. Simple and Realistic Data Generation. In VLDB, pages 1243–1246, 2006. [11] P. J.",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "and Realistic Data Generation. In VLDB, pages 1243–1246, 2006. [11] P. J. Lin, B. Samadi, A. Cipolone, D. R. Jeske, S. Cox, C. Rendón, D. Holt, and R. Xiao. Development of a Synthetic Data Set Generator for Building and Testing Information Discovery Systems. In ITNG, pages 707–712, Washington, DC, USA, 2006. IEEE Computer Society. [12] E. Lo, N. Cheng, and W.-K. Hon. Generating Databases for Query Workloads. PVLDB, 3(1-2):848–859, 2010. [13] Z. Ming, C. Luo, W. Gao, R. Han, Q. Yang, L. Wang, and J. Zhan. BDGS: A Scalable Big Data Generator Suite in Big Data Benchmarking. In WBDB, 2013. [14] M. Poess and C. Floyd. New TPC Benchmarks for Decision Support and Web Commerce. SIGMOD Record, 29(4):64–71, 2000. [15] M. Poess, T. Rabl, M. Frank, and M.",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "Record, 29(4):64–71, 2000. [15] M. Poess, T. Rabl, M. Frank, and M. Danisch. A PDGF Implementation for TPC-H. In TPCTC, 2011. [16] M. Poess, T. Rabl, H.-A. Jacobsen, and B. Cauﬁeld. TPC-DI: The First Industry Benchmark for Data Integration. PVLDB, 13(7):1367–1378, 2014. [17] T. Rabl, M. Frank, H. M. Sergieh, and H. Kosch. A Data Generator for Cloud-Scale Benchmarking. In TPCTC, pages 41–56, 2010. [18] T. Rabl, M. Poess, M. Danisch, and H.-A. Jacobsen. Rapid Development of Data Generators Using Meta Generators in PDGF. In DBTest, 2013. [19] T. Rabl, M. Poess, H.-A. Jacobsen, P. E. O’Neil, and E. O’Neil. Variations of the Star Schema Benchmark to Test Data Skew in Database Management Systems. In ICPE, 2013. [20] E. Shen and L. Antova. Reversing Statistics for Scalable Test",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "2013. [20] E. Shen and L. Antova. Reversing Statistics for Scalable Test Databases Generation. In DBTest, 2013. [21] V . Sikka. Does the World Need a New Benchmark? http://www.saphana.com/community/blogs/ blog/2013/09/16/ does-the-world-need-a-new-benchmark, 2013. [22] J. M. Stephens and M. Poess. MUDD: a multi-dimensional data generator. In WOSP, pages 104–109, 2004. [23] Y . Tay, B. T. Dai, D. T. Wang, E. Y . Sun, Y . Lin, and Y . Lin. UpSizeR: Synthetically Scaling an Empirical Relational Database. Information Systems, 38(8):1168–1183, 2013. [24] E. Torlak. Scalable Test Data Generation from Multidimensional Models. In FSE, 2012. 1462",
        "start_idx": 5568,
        "end_idx": 5663
      }
    ],
    "64561264-69f4-43da-aaab-595a6d3b8c8c": [
      {
        "text": "[Página 1] Full Terms & Conditions of access and use can be found at http://www.tandfonline.com/action/journalInformation?journalCode=teis20 Enterprise Information Systems ISSN: 1751-7575 (Print) 1751-7583 (Online) Journal homepage: http://www.tandfonline.com/loi/teis20 Schema on read modeling approach as a basis of big data analytics integration in EIS Slađana Jankovi ć, Sne žana Mladenovi ć, Dušan Mladenovi ć, Slavko Veskovi ć & Dra ženko Glavi ć To cite this article: Slađana Jankovi ć, Sne žana Mladenovi ć, Dušan Mladenovi ć, Slavko Veskovi ć & Dra ženko Glavi ć (2018): Schema on read modeling approach as a basis of big data analytics integration in EIS, Enterprise Information Systems, DOI: 10.1080/17517575.2018.1462404 To link to this article: https://doi.org/10.1080/17517575.2018.1462404 Published online: 18 Apr 2018. Submit your article to this journal View related articles View Crossmark data [Página 2] ARTICLE",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "to this journal View related articles View Crossmark data [Página 2] ARTICLE Schema on read modeling approach as a basis of big data analytics integration in EIS Slađana Jankovi ć, Sne žana Mladenovi ć,D ušan Mladenovi ć, Slavko Veskovi ć and Dra ženko Glavi ć Faculty of Transport and Tra ﬃc Engineering, University of Belgrade, Belgrade, Serbia ABSTRACT Big Data analysis is the process that can help organizations to make better business decisions. Organizations use data warehouses and busi- ness intelligence systems, i.e. enterprise information systems (EISs), to support and improve their decision-making processes. Since the ultimate goal of using EISs and Big Data analytics is the same, a logical task is to enable these systems to work together. In this paper we propose a framework of cooperation",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "to work together. In this paper we propose a framework of cooperation of these systems, based on the schema on read modeling approach and data virtualization. The goal of data virtua- lization process is to hide technical details related to data storage from applications and to display heterogeneous data sources as one inte- grated data source. We have tested the proposed model in a case study in the transportation domain. The study has shown that the proposed integration model responds ﬂexibly and e ﬃciently to the requirements related to adding new data sources, new data models and new data storage technologies.ARTICLE HISTORY Received 14 September 2017 Accepted 2 April 2018 KEYWORDS Big data analytics; data virtualization; schema on read; data warehouse; business intelligence system Introduction A large number",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "schema on read; data warehouse; business intelligence system Introduction A large number of new approaches and technological solutions in data modeling, storage, processing and analysis, grouped together under the common term ‘Big Data ’, have the task of keeping under control the massive in ﬂow of data and placing it in the service of organizations and individuals. The initial successful initiatives in the application of Big Data technologies soon gave rise to a problem known as Big Data integration. Big Data integration means any software integration involving the data characterized as Big Data, i.e. the data with at least one of the following features: volume, variety, velocity and veracity. According to Arputhamary and Arockiam (2015 ), there are two categories of Big Data integration, namely integration of several",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "there are two categories of Big Data integration, namely integration of several Big Data sources in Big Data environments and integration of the results of Big Data analysis with structured corporate data. This research is focused on addressing the second, above-mentioned category of the Big Data integration problem. An Enterprise Information System (EIS) is an integrated information system with the basic task of providing the management with the necessary information. This research addresses two major challenges encountered by modern EISs in the sphere of data management in order to be quali ﬁed as ‘integrated ’as per the above de ﬁnition. The promotion of business operation of organizations nearly always involves the introduction of new sources of corporate data. If new data sets fall into the category of Big",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "corporate data. If new data sets fall into the category of Big Data, they require the application of Big Data storage, processing and analysis CONTACT SlađanaJankovi ć s.jankovic@sf.bg.ac.rs Faculty of Transport and Tra ﬃc Engineering, University of Belgrade, Belgrade, SerbiaENTERPRISE INFORMATION SYSTEMS, 2018 https://doi.org/10.1080/17517575.2018.1462404 © 2018 Informa UK Limited, trading as Taylor & Francis Group [Página 3] methods. To use new corporate Big Data sets in a business context, they have to be integrated with the existing corporate data sets, after which the integrated data should be subjected to Big Data analysis. The integration of the existing and new corporate data sets to create the subject of the future Big Data analysis is the ﬁrst challenge to which this research will try to respond. The second challenge and",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "to which this research will try to respond. The second challenge and the subject of this research is the integration of the results of Big Data analysis with EIS. This task has to be solved regardless of whether corporate or external data are the subject of Big Data analysis. External data, such as social media and web data, are increasingly used as the subject of Big Data analyses in order to examine user satisfaction, habits and needs etc. Zdravkovi ćand Panetto ( 2017 ) highlighted that current challenges in EISs development are related to the growing need for ﬂexibility caused by cooperation with other EISs. EISs environment has become very dynamic and variable not only in terms of collaboration with other EISs, but also in terms of availability",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "terms of collaboration with other EISs, but also in terms of availability of data sources. The research aims to o ﬀer a solution that would e ﬃciently meet the following three key requirements: frequent appearance of new Big Data sources (either corporate or external), application of new data processing, analysis and visualization methods, and integration of structured (i.e. relational) and semi- and non-structured data sources. To solve the above problems, the schema alignment method of data integration has been selected. The traditional schema alignment method of data integration has been adapted to Big Data sources and methods of Big Data analysis by being based on the schema on read data modeling approach and data virtualization concepts. Schema on read means you create the schema only when reading the",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "Schema on read means you create the schema only when reading the data. Structure is applied to the data only when it ’s read, this allows unstructured data to be stored in the database. Since it ’s not necessary to de ﬁne the schema before storing the data it makes it easier to bring in new data sources on the ﬂy. Data virtualization is any approach to data management that allows an application to retrieve and manipulate data without requiring techni- cal details about the data, such as how it is formatted at source, or where it is physically located. The research also provides a technological framework for the implementation of the proposed integration model. It includes the following three technological environments: NoSQL databases, data virtualization servers and",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "includes the following three technological environments: NoSQL databases, data virtualization servers and data integration tools. The second section of this paper presents the reference literature review. In the third section, we propose and describe our Big Data analytics integration approach based on the ‘data integration on demand ’approach and the ‘schema on demand ’modeling approach. In order to evaluate our approach, we have implemented the proposed approach in a case study in the transportation domain. We have carried out the custom analysis of road tra ﬃc data on a Big Data platform and integrated it with the SQL Server database, Business Intelligence (BI) tool and tra ﬃc geo-applica- tion, according to the proposed integration approach. Finally, we will present our conclusions about the possibilities and constraints of our",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "we will present our conclusions about the possibilities and constraints of our integration approach. Literature review As pointed out in the introduction of the paper, this research does not deal with the integration of diﬀerent Big Data sources on Big Data platforms but with the integration of the results of Big Data analysis with structured corporate data. For this reason, the literature review includes the data integration approaches and solutions that can be applied to Big Data sources as well as the existing EIS architectures. For decades, there have been two main approaches to data integration, namely batch data integration and real-time data integration. Both approaches have secured a place for themselves in Big Data integration processes as well. From the data analytics perspective, Big Data systems support",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "processes as well. From the data analytics perspective, Big Data systems support the following classes of applications: batch-oriented processing, stream processing, OLTP (Online Transaction Processing) and interactive ad-hoc queries and analysis (Ribeiro, Silva, and da Silva 2015 ). The batch data integration approach is used in batch-oriented processing applications, whereas the real-time data integration approach is used in stream processing, OLTP and interactive ad-hoc queries and analysis applications. An overview of the most important approaches and2 S. JANKOVI ĆET AL. [Página 4] solutions in the ﬁeld of Big Data integration with EISs, both in the batch as well as the real-time mode, will be given in the text below. Batch data integration for big data When data exchange between two systems is performed through periodic big ﬁle transfers",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "data exchange between two systems is performed through periodic big ﬁle transfers on a daily, weekly or monthly basis, we call this batch data integration. In the era of the Internet of Things (IoT) and social media, i.e. the era of Big Data, this interval between two successive ﬁle transfers can be much shorter and measured in hours or even minutes. The transferred ﬁles include records with an unchangeable structure, which is adapted to the requirements of the system that receives them. This approach to integration is known as a ‘tightly coupled ’approach, because it implies that systems are compatible in terms of ﬁle and data format and that the format can only be changed if both systems simultaneously implement speci ﬁc changes (Reeve 2013 ). The standard",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "both systems simultaneously implement speci ﬁc changes (Reeve 2013 ). The standard batch data integration process includes the following operations: extract, transform, and load (ETL). Today, there is a large number of commercial and open-source ETL tools (Alooma 2018 ). The main purpose of these tools is to upgrade and facilitate the warehousing, archiving, and conversion of data. Big Data are most frequently raw data, which are ‘dirty ’and incomplete and therefore it is necessary to perform the operations of extracting, cleaning and data quality processing (Macura 2014 ; Chen and Zhang 2014 ) in order to work with them. In the Big Data context, ETL tools are used to extract, clean and transform raw data from Big Data platforms and NoSQL databases into a relational or another",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "from Big Data platforms and NoSQL databases into a relational or another required form, as well as to load the results of Big Data analytics into Enterprise Data Warehouses (EDWs) (Florea, Diaconita, and Bologa 2015 ). The task can only be performed by ETL tools enabling the creation of interfaces according to both traditional data sources (relational databases, ﬂatﬁles, XML ﬁles, etc.) as well as Big Data platforms (Hortonworks Data Platform, Cloudera Enterprise, SAP HANA Platform, etc.) and NoSQL databases (MongoDB, Cassandra, HBase, Neo4j, etc.). Such commercial ETL tools include Informatica, Oracle Data Integrator, Alooma, SAS ETL and Altova MapForce. The major open-source tools of this type include Apache NiFi, Talend and Pentaho Data Integration. Transformation as an operation can vary, ranging from an extremely simple operation to",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "as an operation can vary, ranging from an extremely simple operation to an inexecutable operation, and it may require the use of additional data collections. In the simplest case, it consists of the simple mapping of source ﬁelds to target ﬁelds, but most frequently it also includes operations such as aggregation, normalization and calculation. Some ETL tools, such as Altova MapForce, include a revolutionary interactive debugger to assist with the data mapping design. Apache Hadoop is an open-source distributed software platform for storing and processing data. Central to the scalability of Apache Hadoop is the distributed processing framework known as MapReduce (Sridhar and Dharmaji 2013 ). According to the research done by Russom ( 2013 ), the main reason to integrate Hadoop into Business Intelligence or Enterprise Data",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "the main reason to integrate Hadoop into Business Intelligence or Enterprise Data Warehouse is the expectation from Hadoop to enable Big Data analytics. The basic advantage of Hadoop is the possibility to use advanced non-OLAP (Online Analytic Processing) analytic methods, such as data mining, statistical analysis and complex SQL. However, in addition to the fact that it can be used as an analytical sandbox, Apache Hadoop includes many components useful for ETL. For example, Apache Sqoop is a tool for transferring data between Hadoop and relational databases. When data are located in the Hadoop File System, they can be e ﬃciently subjected to the ETL tasks of cleansing, normalizing, aligning, and aggregating for an EDW by employing the massive scalability of MapReduce (Intel Corporation 2013 ). In this",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "employing the massive scalability of MapReduce (Intel Corporation 2013 ). In this way, the Apache Hadoop platform represents a powerful ETL tool enabling the integration of the results of Big Data analysis of structured and non- structured data in an EDW. Research (Wang et al. 2016 ) has shown that the most important Big Data technologies that support batch data integration include the following: MapReduce, Hadoop (HDFS, Hive, HBase),ENTERPRISE INFORMATION SYSTEMS 3 [Página 5] Flume, Scribe, Dryad, Apache Mahout, Jaspersoft BI Suite, Pentaho, Skytree Server, Cascading, Spark, Tableau, Karmasphere, Pig and Sqoop. Real-time data integration for big data In many cases of data integration, the batch mode is unacceptable so that real-time or near real- time data integration has to be performed instead. Real-time data integration involves the",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "data integration has to be performed instead. Real-time data integration involves the transfer of much smaller quantities of data in one interaction, in the form known as a ‘message ’(Gokhe 2016 ). The quantity of data transferred in this way is limited and each interaction means ensuring security on all levels, the same as in batch data integration. Consequently, when it comes to larger quantities of data, real-time data movement is slower than batch data movement. The traditional ‘point-to-point ’interaction model means that there are direct ‘tightly coupled ’interfaces between each two systems which have to share data. The data from each data source have to be transformed as per the requirements of each target data format. If the number of systems which should be connected by an",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "format. If the number of systems which should be connected by an interface is n, the number of interfaces is (n * (n –1))/2. The most signiﬁcant and most important design pattern for architecting real-time data integration solutions is the ‘hub-and-spoke ’design for data interactions (Reeve 2013 ). The point of this interaction model is that data from all sources are transformed into a common, shared format, from which they are transformed into the target format. The number of interfaces for the connection of n systems is n in this case. From the technological point of view, the central segment of the real-time data integra- tion solution is the implementation of an enterprise service bus (ESB). An enterprise service bus is an application used to coordinate the movement",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "An enterprise service bus is an application used to coordinate the movement of data messages across di ﬀerent servers that may be running di ﬀerent technologies. XML (eXtensible Markup Language) has been a de facto standard for the exchange of information in the past two decades and, consequently, it also plays a major role in the ﬁeld of data integration. XML ﬁles are a typical example of semi-structured data (Gandomi and Haider 2015 ). Modern data integration software ena bles the transformation of data from XML ﬁles into other types of data warehouses (Big Data included) and vice versa. Other self- documenting data interchange formats that a re popular include JSON (Java Script Object Notation). Hadoop o ﬀers excellent performances in the processing of massive data sets, but",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "o ﬀers excellent performances in the processing of massive data sets, but query execu- tion on the Hadoop platform (e.g. Hive queries) is measured in minutes and hours. This constitutes a great challenge in the integration of Hadoop into a real-time analytics environment. Intel and SAP have joined forces to tackle this challenge (Intel Corporation 2014 ). The Intel® Distribution for Apache Hadoop (IDH) is highly optimized for performance on Intel® architecture. Intel and SAP have enabled the generation of queries that will be e ﬃciently executed on both platforms, SAP HANA as well as IDH. Research (Wang et al. 2016 ) has shown that the most important Big Data technologies that support stream processing and real-time integration include the following: Kafka, Flume, Kestrel, Storm, SQLstream, Splunk, SAP",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "real-time integration include the following: Kafka, Flume, Kestrel, Storm, SQLstream, Splunk, SAP Hana and Spark Streaming. Schema alignment in big data integration The main task of data integration, regardless of whether it is traditional or Big Data integration, batch or real-time data integration, is to download the required data from their current warehouse, to change their format in order to be compatible with the destination warehouse and to place them at the target location (Loshin 2013 ). It is the challenges which data integration has to address that have changed. The three main steps in data integration include schema alignment, record linkage and data fusion. Schema alignment should respond to the challenge of semantic ambi- guity, enabling the identi ﬁcation of attributes with the same meaning as well",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "enabling the identi ﬁcation of attributes with the same meaning as well as those without it. Record linkage should ﬁnd out which records refer to the same entity and which do not. Data4 S. JANKOVI ĆET AL. [Página 6] fusion should enable the identi ﬁcation of accurate data in an integrated data set in cases when diﬀerent sources o ﬀer con ﬂicting values. Dong and Srivastava ( 2015 ,3 5 )u n d e r l i n et h a t , ‘schema alignment is one of the major bottlenecks in building a data integration system ’. They believe that in the Big Data context, w h e r et h en u m b e ro fd a t as o u r c e si",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "e ro fd a t as o u r c e si sp e r m a n e n t l yo nt h er i s ea n dw h e r es o u r c es c h e m a sa r e expected to change all the time, no up-to-date schema mappings are possible. In contrast, Gal (2011 ) speaks of the important role schema matching plays in the data integration life cycle. He believes that the Big Data challenges of variet y and veracity can be dealt with by using schema matching, while the challenges of volume and velocity can be dealt with by using entity resolution (record linkage). Big data analytics integration framework This section of the paper presents the",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "Big data analytics integration framework This section of the paper presents the framework for the integration of Big Data sources with structured data sources, which still form the backbone of EISs. In the previous section, we have seen that both the batch data integration approach as well as the real-time data integration approach have their advantages as well as disadvantages and, consequently, our goal has been to propose a model capable of supporting both integration methods. In view of the fact that EISs are based on s tructured data (data warehouses, prede ﬁned business analytics and reports, etc.), we believ e that variety and veracity constitute the key challenges in the integration of Big Data analysis and EISs. The integration framework we propose is therefore based on the",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "and EISs. The integration framework we propose is therefore based on the upgrade of the model of application of the schema alignment (schema matching) method of data integration. The upgrade is expected to be the result of the application of the schema on read modeling approach and data virtualization concepts. In the text below, the two approaches will be ﬁrst brie ﬂy outlined and then the reason why they have been selected explained. Schema on read modeling approach in big data integration process Schema on write is a standard modeling approach, where we create a database schema and a database for a speci ﬁc purpose, and then we enter data into the database. This means that the data must be adequately prepared for the developed schema. The schema",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "the data must be adequately prepared for the developed schema. The schema on read approach involves storing raw data, and then, when we need it for a speci ﬁc purpose, we create a schema while reading data from a data storage ( Figure 1 ). Unlike schema on write, which requires you to expend time before loading the data, schema on read involves very little delay and you generally store the Figure 1. Schema on read modeling approach.ENTERPRISE INFORMATION SYSTEMS 5 [Página 7] data at a raw level. In data-intensive computation problems data is the driver, not analytical human or machines. When the schema on read modeling approach is used, these very large data sets can be used multiple times in di ﬀerent ways, for various types of",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "be used multiple times in di ﬀerent ways, for various types of analysis. However, we believe that the schema on read modeling approach has a big potential not only in the ﬁeld of Big Data analysis but also in the ﬁeld of Big Data integration. According to (EMC Education Services, ed 2015 ), the main phases of the data analytics life cycle include data discovery, data preparation, model planning, model building, communicate results and operationalize. However, in our experience, Big Data integration process, too, has to include almost all above phases, as shown in Figure 2 . Consequently, we shall speak of the roles the schema on read modeling approach plays in all mentioned activities, as the phases of Big Data integration process: ●Phase ‘discovery ’: at this",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "the phases of Big Data integration process: ●Phase ‘discovery ’: at this stage, the schema on read modeling approach plays an important role in getting to know the team with data and the selection of appropriate data preparation methods. ●Phase ‘data preparation ’: given that the possibilities of data transformation with ETL tools are nevertheless limited, the data in Big Data source systems have to be organized and formatted so as to be able to be transformed with ETL tools into the format required by EIS. The data in Big Data source systems can be prepared for ETL operations through adequate modeling. Data modeling when necessary, at the point of reading, is precisely what the schema on read modeling approach makes possible. In this way, ETL operations are",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "on read modeling approach makes possible. In this way, ETL operations are more e ﬀectively realized using the schema on read modeling approach. ●Phase ‘model planning ’: the schema on read modeling approach allows a deeper exploration of data and recognition of the relationships between individual variables. ●Phase ‘model building ’: at this stage, the schema on read modeling approach has the most signi ﬁcant role, because it allows ﬂexible creation, testing and changing of the models. In data integration process, the phases of ‘model planning ’and ‘model building ’ can occur several times. They will de ﬁnitely occur during the ETL operations and, if there is a data virtualization level, they will occur also during the creation of virtual tables. Due to the above roles the schema",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "the creation of virtual tables. Due to the above roles the schema on read modeling approach can play in Big Data integration process, we believe that thi s modeling approach is imperative for e ﬃcient Big Data integration. Figure 2. Schema on read modeling approach in Big Data integration lifecycle.6 S. JANKOVI ĆET AL. [Página 8] Data virtualization integration approach for big data analytics Big Data analytics is characterized by a permanent appearance of new data sources and new requirements regarding analytical models and methods, so that we have tried to adopt an integration approach likely to ensure a satisfactory degree of ﬂexibility. We have recognized the data virtualization concept as a suitable basis for ﬂexible ‘on-demand ’integration and multiple use of the same data, without copying. As",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "‘on-demand ’integration and multiple use of the same data, without copying. As van der Lans ( 2012 , 9) points out, ‘Data virtualization is the technology that o ﬀers data consumers a uni ﬁed, abstracted, and encapsulated view for querying and manipulating data stored in a heterogeneous set of data stores ’. Basically, when data virtualization is applied, the middle layer that hides from an application most of the technical aspects on where and when data are stored is provided. Besides that, all data sources are shown as one integrated data source. Data virtualization is available in various implementation processes. Some of them include the following: a server for data virtualization, Enterprise Service Bus (ESB) architecture, placing data warehouse on the cloud, a virtual in-memory database and object-relational",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "placing data warehouse on the cloud, a virtual in-memory database and object-relational mappers. We have concluded that all above phases of Big Data integration, which include data discovery, data preparation, mo del planning, model building, communicate results and oper- ationalize, can be performed on data virtualiz ation servers. This is not the case in other data virtualization implementation processes. Consequently, our approach to data virtualization implies the use of data virtualization servers. The main parts of a data virtualization server include source tables, mappings and virtual ta bles. Mappings represent the way to transform data from source tables to virtual tables. What ma kes virtualization servers powerful tools is the fact that source tables are not restrict ed to relational tables, but instead di ﬀerent data sources such",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "restrict ed to relational tables, but instead di ﬀerent data sources such as data generated by websites, the result of a web service call, a HTML page, a spreadsheet or a sequential ﬁl e ,c a nb eu s e d .U s e r sc a na c c e s sv i r t u a lt a b l e sb yu s i n gd i ﬀerent APIs (Application Programming Interface), such as the JDBC/SQL interface, MDX (MultiDimensional eXpressions) and the SOAP-b ased interface. That means that same tables would be seen di ﬀe r e n t l yb yd i ﬀerent users. A c c o r d i n gt o( v a nd e rL a n s 2012 ),",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "gt o( v a nd e rL a n s 2012 ), a data virtualization server consists of a design module and a runtime module. When data consumers acc ess the virtualization layer, they use the runtime module of a data virtualization serv er. The design module is an environment which data analysts and data model desi gners use to create concept de ﬁnitions, data models, and speciﬁcations for transformation, cleansing and integration. Some data virtualization servers enable the creation of unbound virtual tables. That means that it is possible to create data models using them, and to join them with the real data source afterwards. The runtime module of a data virtualization server represe nts a virtual sandbox for data scientists and enables managed self-service re porting",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "a virtual sandbox for data scientists and enables managed self-service re porting for business analysts. At a time when new data sources appear on a daily basis, in order to ensure the understanding and integrity of data, it is very important to manage metadata. Metadata must be a link between the existing and new data sources. As Zdravkovi ćet al. ( 2015 , 5) point out, ‘the capability to interoperate will be considered as the capability to “semantically interoperate ”.’It is very important that data virtualization servers allow the entering and using of data models, glossaries and taxonomies. The data virtualization integration approach can help in two ways in data integration processes enabling Big Data analytics. Firstly, data virtualization can help in the phases of data discovery and",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "Firstly, data virtualization can help in the phases of data discovery and data preparation according to the requirements of di ﬀerent analytical models. Big Data analyses can include only external data or only internal historical data stored in an EDW, but they often require the integration of external and corporate data. Considering that we are talking about analyzing a huge amount of external data coming at a high speed, it makes no sense to consider the physical integration of data based on their copying into a single central data warehouse. Instead of that, Big Data analysis is performed on Big Data platforms and in NoSQL databases withENTERPRISE INFORMATION SYSTEMS 7 [Página 9] appropriate storage and processing performances. In that case, the required corporate data can be ensured on",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "performances. In that case, the required corporate data can be ensured on the data virtualization layer, according to the requirements of a speci ﬁc Big Data analysis, and can then be exported to a Big Data platform, such as Hadoop. If data virtualization is conducted via a virtualization server, the required data are ensured by means of virtual tables. This means that no local copy of the selected data is made, but the data can instead be exported to diﬀerent warehouses, in the form de ﬁned by a given virtual table. Data virtualization servers have built-in functions for the export of data to di ﬀerent warehouses, Big Data platforms included. Secondly, the data virtualization integration approach can help in the phase of integration of the results of Big",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "can help in the phase of integration of the results of Big Data analytics and EIS. Aft er becoming familiar with the available data sources, the operations of model planning and model building can be performed on a data virtualization server, similarly as in any database management system. Data models are designed by creating unbound vir tual tables. Regrettably, at this point, not all data virtua- lization servers have this option. Once a virtu al table is created, it can be linked with some external or internal data source. The design of virtual tables depends on the form of analysis results which should be integrated and the data model into which they should be inte- grated. We propose that the designing of virt ual tables be based on",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "We propose that the designing of virt ual tables be based on the application of the schema alignment method and the available dat a virtualization concepts, such as nested virtual tables. Nested virtual tables are virtua l tables created on top o fo t h e rv i r t u a lt a b l e s . The schema alignment method and the way it is applied on a data virtualization server will be explained in detail in the next section. Schema alignment based on schema on read and data virtualization Schema alignment is used when one domain includes several di ﬀerent source schemas, which describe it in di ﬀerent ways. The results of schema alignment include the following: ●a mediated schema, which provides a uniform",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "schema alignment include the following: ●a mediated schema, which provides a uniform view over heterogeneous data sources, cover- ing the most important domain aspects; ●attribute matching, which matches attributes in all source schemas with the corresponding attributes in a mediated schema; ●schema mapping between each source schema and a mediated schema, specifying the semantic ties between the data described by source schemas and the data described by a mediated schema. There are two classes of schema mappings: Global-as-View (GAV) and Local-as-View (LAV). GAV deﬁnes a mediated schema as a set of views over source schemas. LAV expressions describe source schemas as views over a mediated schema. We shall ﬁrst de ﬁne GAV and LAV schema mappings and then, by using these two formalisms, we shall give an example",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "and then, by using these two formalisms, we shall give an example to show how the application of schema alignment method of data integration can be upgraded through the application of data virtualization concepts and the schema on read modeling approach. To demonstrate this, we have selected an example from the case study conducted to verify the proposed model. The case study is described in detail in the next section of the paper. This is followed by the de ﬁnitions of GAV and LAV schema mappings according to Doan, Halevy, and Ives ( 2012 ). Deﬁnition 1 (GAV Schema Mappings). Let G be a mediated schema, and let /C22S¼S1;... ;Sn fg be schemata of n data sources. A Global-as-View schema mapping /C22Mis a set of expressions of the",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "sources. A Global-as-View schema mapping /C22Mis a set of expressions of the form G iðXÞ/C19 QðSÞ, where ●Giis a relation in G, ●and appears in atmost one expression in M, and Q ðSÞis a query over the relations in S8 S. JANKOVI ĆET AL. [Página 10] Deﬁnition 2 (LAV Schema Mappings). Let G be a mediated schema, and let /C22S¼S1;...;Sn fg be schemata of n data sources. A Local-as-View schema mapping /C22Mis a set of expressions of the form SiðXÞ/C19 QiðGÞ, where ●Qiis a query over the mediated schema G, and ●Sia source relation and it appears in at most one expression in M. The example: The backbone of the EIS architecture consists of an enterprise data warehouse (EDW), a data virtualization server and a business intelligence (BI)",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "data warehouse (EDW), a data virtualization server and a business intelligence (BI) tool. This particular EIS is used by a road maintenance organization. We shall extract the relations modeling the road network, EDW. Road and EDW.Road_section, from the EDW schema. The problem in hand is to integrate new datasources, the Big Data analysis results and new reports to be created in the BI tool with the existing EIS. There are two new data sources: one stores the road traﬃ c data, the other stores the data on automatic traﬃ c counters monitoring traﬃ c. The new reports should enable the visualization of Big Data analysis results over integrated data. The traﬃ c data are stored in TXT ﬁles. In view of the fact that TXT ﬁles are semi-structured",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "TXT ﬁles. In view of the fact that TXT ﬁles are semi-structured and that they contain a large amount of data that is constantly growing, they are warehoused on the Big Data platform HDFS (Hadoop Distributed File System). The following three tasks have been identi ﬁed: ●Data on traﬃ c counters, which are small in volume and do not change often, should be integrated with EIS on the data warehouse level. ●Data on traﬃ cﬂow volume and structure, which will be the result of Big Data analysis, should be integrated with EIS on the corporate data model level. ●The new reports should be integrated with EIS on the corporate data model level. What we are interested in are the Road and RoadSection relations, which belong to the EDW:EDW.Road(RoadID,",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "in are the Road and RoadSection relations, which belong to the EDW:EDW.Road(RoadID, RoadName, RoadCategory), EDW.RoadSection(SectionID, SectionName, RoadID, SectionLength). Theﬁrst task will be solved by adding a new relation to the EDW system and by linking it to the RoadSection relation. The new relation is Counter: EDW.Counter(Location, Longitude, Latitude, SectionID, Type).The second task requires a far more complex solution. The integration of a new data source with EIS on the corporate data model level will be performed through the successive multiple application of the schema alignment method. The results of the application of this method will be implemented on a data virtualization server , by creating virtual tables and nested virtual tables. We have adopted a top-down approach to this problem. This means that we ﬁrst analyze the end",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "approach to this problem. This means that we ﬁrst analyze the end goal to be achieved through in tegration. The end goal is a data schema as required by new reports. Since this data schema should be a common, uniform view over theEDW and the Big Data source, it will be designed as a mediated schema by using GAV schema mappings. Its relations will be nested virtual tables (NVT_Counter and NVT_AADT), created as views over virtual tables (VT_Road , VT_Section, VT_Counter, VT_Tra ﬃc). The virtual tables VT_Road, VT_Section, VT_Counter and VT_Tra ﬃc will be created as unbound virtual tables. Their role is very important. At this point, they will enable the application of GAV schemamappings and the creation of a virtual mediated s chema. The following expressions describe",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "the creation of a virtual mediated s chema. The following expressions describe the above GAV schema mappings: Mediate.NVT_Counter(Location, Longitude, Latitude, RoadName, SectionName) ⊇ VT_Road(RoadID, RoadName),VT_Section(SectionID, SectionName, RoadID), VT_Counter(Location, Longitude, Latitude, SectionID). Mediate.NVT_AADT(Location, Year, AADT, AADT_D1, AADT_D2) ⊇ENTERPRISE INFORMATION SYSTEMS 9 [Página 11] VT_Tra ﬃc(Location, Year, AADT, AADT_D1, AADT_D2, AADT_A0, AADT_A1, AADT_A2, AADT_B1, AADT_B2, AADT_B3, AADT_B4, AADT_B5, AADT_C1, AADT_C2, AADT_X). The AADT ﬁeld represents Annual Average Daily Tra ﬃc, while AADT_D1 and AADT_D2 represent AADT by vehicle movement direction. The other ﬁelds represent AADT by vehicle categories. In the next phase, by using LAV schema mappings, the unbound virtual tables VT_Road, VT_Section and VT_Counter are linked with the corresponding EDW relations. The EDW schema represents a mediated schema in this case. The following expressions describe the above LAV schema",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "schema in this case. The following expressions describe the above LAV schema mappings: VT_Road(RoadID, RoadName) ⊆ EDW.Road(RoadID, RoadName, RoadCategory)VT_Section(SectionID, SectionName, RoadID) ⊆ EDW.RoadSection(SectionID, SectionName, RoadID, SectionLength) VT_Counter(Location, Longitude, Latitude, SectionID) ⊆ EDW.Counter(Location, Longitude, Latitude, SectionID, Type)Using LAV schema mappings, source schemas are created for the Big Data source (BD) based on VT_Tra ﬃc. The virtual table schema VT_Tra ﬃc represents a mediated schema in this case. The following expressions describe the above LAV schema mappings: BD.AADT(Location, Year, AADT) ⊆VT_Tra ﬃc(Location, Year, AADT, AADT_D1, AADT_D2, AADT_A0, AADT_A1, AADT_A2, AADT_B1, AADT_B2, AADT_B3, AADT_B4, AADT_B5, AADT_C1, AADT_C2, AADT_X) BD.AADTByDirections(Location, Year, AADT_D1, AADT_D2) ⊆VT_Tra ﬃc(Location, Year, AADT, AADT_D1, AADT_D2, AADT_A0, AADT_A1, AADT_A2, AADT_B1, AADT_B2, AADT_B3, AADT_B4,AADT_B5, AADT_C1, AADT_C2, AADT_X) BD.AADTByCategories(Location, Year, AADT_A0, AADT_A1, AADT_A2, AADT_B1, AADT_B2, AADT_B3, AADT_B4, AADT_B5, AADT_C1, AADT_C2, AADT_X)",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "Year, AADT_A0, AADT_A1, AADT_A2, AADT_B1, AADT_B2, AADT_B3, AADT_B4, AADT_B5, AADT_C1, AADT_C2, AADT_X) ⊆VT_Tra ﬃc(Location, Year, AADT, AADT_D1, AADT_D2, AADT_A0, AADT_A1, AADT_A2, AADT_B1, AADT_B2, AADT_B3, AADT_B4, AADT_B5, AADT_C1, AADT_C2, AADT_X) Once schemas for the Big Data sources BD.AADT, BD.AADTByDirections and BD. AADTByCategories are designed, the designing of Big Data analysis begins so as to get the resultsdescribed in the above schemas. This is when the schema on read modeling approach comes intoplay. It is applied to a Big Data source in situations when one knows what kind of data schema is required. In other words, the data on a Big Data platform are organized according to the schema derived through the successive application of GAV and LAV schema mappings. Once a Big Datasource is created according to the above",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "schema mappings. Once a Big Datasource is created according to the above schemas, it should be linked with a data virtualization server. After that, the unbound virtual table VT_Tra ﬃc is linked with the real Big Data source. This solves the task of integrating the results of Big Data analysis with EIS on the corporate data model level. The third task, integration of new reports with EIS on the corporate data model level, will be simply solved by linking the BI tool with the virtual schema Mediate on a data virtualization server. We can say now that the key factors of the proposed model of Big Data integration include in the following: ●a top-down approach to solving the integration problem, i.e. starting with reports and moving down to",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "solving the integration problem, i.e. starting with reports and moving down to data sources, ●application of GAV schema mappings in order to create a uniform view over the domain –a mediated schema, using the concept of unbound nested virtual tables on a data virtualizationserver, ●application of LAV schema mappings in order to create the required local and external data source schemas, using the concept of unbound nested virtual tables on a data virtualization server,10 S. JANKOVI ĆET AL. [Página 12] ●application of the schema on read modeling approach in creating data schemas for Big Data sources, derived by using the above combined GLAV (Global-as-Local-as-View) schema map- ping approach. Although some authors, such as Dong and Srivastava ( 2015 ), believe that schema alignment is not an appropriate Big",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "( 2015 ), believe that schema alignment is not an appropriate Big Data integration method, we have shown that it can be e ﬀectively implemented using unbound nested virtual tables and bound virtual tables on the data virtualiza- tion server. Big data analytics integration scenarios Between the enterprise information system and the Big Data analytic tool a two-way data exchange is necessary. In Big Data analysis for business purposes, apart from data originating from external sources, such as sensor data, data generated by various machines, social networking data etc., corporative data are used, too. Corporative data that are used in Big Data analysis or are crossed with Big Data analysis results frequently appear on their own as a result of some prede ﬁned analysis in a business intelligence",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "as a result of some prede ﬁned analysis in a business intelligence system. Thus, it is necessary to enable integration of corporative data and other data that are the object of Big Data analysis. One corporative data part, which is archived and traditionally used for business reporting, is structured. However, a signiﬁcant part of corporative data are semi-structured and unstructured data. On the other hand, external sources generate heterogeneous data that are stored in di ﬀerent types of data storages. The amount of external data that are of interest for corporative analysis as a rule increases. The results of Big Data analysis should become available to business analysts and other business users, and sometimes even end users, such as buyers, service users, etc. This can be achieved through",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "users, such as buyers, service users, etc. This can be achieved through data integration or through integration on the report level. Integration of corporate data, external data and Big Data is done in the phase of preparing input data for various advanced Big Data analysis techniques. After Big Data analysis is completed, it is necessary to integrate the results of the analysis with the corporate data. Big Data analysis scenarios can be diﬀerent. Only the data analyzed on a Big Data platform can be analyzed without the use of corporate data. In this case, the only remaining task is to integrate the Big Data analysis results with EIS. The Big Data analytics integration framework we suggest allows us to integrate Big Data analysis and EIS on three levels:",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "allows us to integrate Big Data analysis and EIS on three levels: data warehouse level, corporate data model level and report level (Figure 3 ). The example described in the previous section demonstrates all three levels of integra- tion, as shown in Figure 3 . It has been mentioned earlier that all data integration phases can be conducted on the data virtualization server. Consequently, as seen in Figure 3 , integration on the corporate data model level is performed directly between the Big Data platform and the data virtualization server, without the mediation of ETL tools. Integration on the data warehouse level means that the Big Data platform is used to design schema on read which is identical to the one segment of the data warehouse model. Data",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "is identical to the one segment of the data warehouse model. Data from the Big Data platform can be obtained, transformed and loaded into data warehouse tables by using some ETL tool. It has been mentioned earlier that, among other things, the goal of the schema on read approach to modeling Big Data is to prepare Big Data so that ETL operations could be more eﬃcient. As seen in Figure 3 , the ETL tool is linked with one of the ‘schemas on read ’on the Big Data platform. In the case of data warehouse level of integration, the ﬁrst four phases of the Big Data integration process from Figure 2 : discovery, data preparation, model planning and model building are executed on the Big Data platform, or",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "planning and model building are executed on the Big Data platform, or within ETL tools, and most often combined in both environments ( Figure 3 ). The last two phases from Figure 2 : communicate results and operationalize, are executed in the data warehouse ( Figure 3 ). This kind of integration is suitable for batch-oriented Big Data analysis which is repeated periodically (monthly, quarterly, yearly) or on demand ( Figure 4 ).ENTERPRISE INFORMATION SYSTEMS 11 [Página 13] Integration on the corporate data model level can be carried out in two ways. The ﬁrst method involves the prior preparation of the organization and storage of data on the Big Data platform and the creation of schemas on read according to the corporate data model. The di ﬀerence between",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "on read according to the corporate data model. The di ﬀerence between this method of integration and integration on the data warehouse level is that, in this way, the integration is done on the virtual level. The data virtualization server connects virtual tables derived from internal data sources and virtual tables generated from external –Big Data sources (schemas on read). In the case of integration on the corporate data model level, the ﬁrst two phases of the Big Data integration process from Figure 2 : discovery and data preparation are executed on the Big Data platform ( Figure 3 ). The remaining four phases from Figure 2 : model planning, model building, communicate results and operationalize, are realized on the data virtualization server (Figure 3 ). The second",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "are realized on the data virtualization server (Figure 3 ). The second method involves the implementation of the schema on read modeling approach only within the design module of the data virtualization server. This means that by designing unbound virtual tables, a data model is created, which is subsequently associated with real data sources. The key stages of the schema on read modeling approach are Explore Data and Develop Model (Figure 1 ). Both of these phases, according to our integration framework, can be performed on Big Data platforms over Big Data sources, but also on the data virtualization server, over integrated internal and external data sources. This is shown by schemas on reads in the form of a puzzle puzzle segment in Figure 3 . If we",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "form of a puzzle puzzle segment in Figure 3 . If we observe the three mentioned levels of integration, only integration on the corporate data model level enables all types of Big Data analysis applications: batch-oriented processing, stream processing, OLTP (Online Transaction Processing) and interactive ad-hoc queries and analysis (Figure 4 ). Integration on the report level means creating schemas on read on Big Data platforms. These schemas are created with the aim of representing the data sources for the prede ﬁned reports and are designed so as to suit the reports ’requirements. In the case of integration on the report level, theﬁrst four phases of the Big Data integration process from Figure 2 : discovery, data preparation, model planning and model building are executed on the Big",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "data preparation, model planning and model building are executed on the Big Data platform ( Figure 3 ). The remaining two phases from Figure 2 : communicate results and operationalize, are executed on the BI tool ( Figure 3 ). This kind of integration is used for the following Big Data analysis applica- tions: batch-oriented processing, stream processing and OLTP ( Figure 4 ). Figure 3. Proposed framework for Big Data analytics integration in EISs.12 S. JANKOVI ĆET AL. [Página 14] In existing batch data integration solutions, which are based only on the use of ETL tools, phases: discovery, data preparation, model planning and model building, include copying and temporary storage of large amounts of data in the data staging area. Our integration framework does not envision data",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "in the data staging area. Our integration framework does not envision data staging area, because these Big Data integration phases are performed either on the Big Data platform, or at the virtual level on the server for data virtualization. If these four phases are implemented on the Big Data platform, our integration framework does not exclude the use of some existing solutions, such as Apache Hadoop components useful for ETL. When it comes to real-time data integration scenarios, our integration framework does not exclude existing ESB-based solutions. On the contrary, our approach enables the development of a traditional ESB approach, by the implementation of the ‘hub-and-spoke ’design on the data virtualization server. As described in the previous section, data from all sources are transformed into a uni ﬁed",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "previous section, data from all sources are transformed into a uni ﬁed shared format called the mediated schema. If we do not want to store permanently the data in a data warehouse, integration on the data warehouse level can be replaced with integration on the corporate data model level. Additionally, integration on the report level can be replaced with integration on the corporate data model level. The prerequisite for that is to imply data virtualization as an integration approach. As the needs of business analysts and data analysts are becoming similar, the proposed approach enables the integration of reporting and analytical tools with enterprise data warehouse and external data sources. Depending on the categories of Big Data analytics use cases and the speciﬁc needs and skills of a",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "Data analytics use cases and the speciﬁc needs and skills of a particular user, the proposed framework enables the following integration scenarios: (1) integration on the data warehouse level, for data analysts and developers; (2) integration on the corporate data model level, for business analysts (self-service analysis), data analysts and developers; (3) integration on the report level, for end users, business analysts, data analysts and developers; (4) integration on the corporate data model level and data warehouse level, for data analysts and developers; Figure 4. Levels of integration and Big Data analysis applications.ENTERPRISE INFORMATION SYSTEMS 13 [Página 15] (5) integration on the corporate data model level and report level, for business analysts (self- service analysis), data analysts and developers; (6) integration on the data warehouse level and report",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "analysts and developers; (6) integration on the data warehouse level and report level, for data analysts and developers, and (7) integration on the corporate data model level, the data warehouse level and the report level, for data analysts and developers. The integration scenarios appropriate for particular user categories are presented in Figure 5 . Implementation of integration framework in transportation domain Traﬃc data are an excellent example of heterogeneous data that are continuously coming, making a demand for Big Data storage and analysis. Excellent tailor-made tra ﬃc data are the best basis for excellent transportation models (Jankovi ćet al. 2016a ). We want to provide the tra ﬃc engineers and authorities with pre-attributed maps tailored to their speci ﬁc needs. For the analysis of tra ﬃc ﬂow, the",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "their speci ﬁc needs. For the analysis of tra ﬃc ﬂow, the tra ﬃc engineers calculate the indicators on an annual basis. For example, Annual Average Daily Tra ﬃc (AADT), along with its main characteristics of composition and time distribution (minutes, hourly, daily, monthly, yearly), is the basic and key input to the tra ﬃc-technical dimen- sioning of road infrastructure and road facilities. This parameter is used in capacity analysis, level of service analysis, cost bene ﬁt analysis, safety analysis, environmental assessment impact analysis of noise emission and air pollution, analyses of pavement construction, as well as for the static calculation of road infrastructure objects, tra ﬃc forecasting, etc. To count the tra ﬃc at the speci ﬁed locations on the state roads in the Republic of Serbia,",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "speci ﬁed locations on the state roads in the Republic of Serbia, 391 inductive loop detectors were used (Lipovac et al. 2015 ). These detectors are QLTC-10C automatic traﬃc counters (ATC). The case study included the analysis of tra ﬃc data in ten locations on the state roads and streets in the city of Novi Sad, Serbia, which the tra ﬃc counters generated during 2015. In order to have sensor data, it is necessary to link them to the tra ﬃc infrastructure data. As two di ﬀerent data categories exist, namely one that is continually generated and the other that is changed rarely, we recognized the need to process them di ﬀerently. The tra ﬃc data that are continually generated in our case study are analyzed on the",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "that are continually generated in our case study are analyzed on the Big Data platform, while the data related to the tra ﬃc infrastructure are stored in the local relational database. Obviously, there is a need for their integration. In this study, we have integrated Big Data analytics with the existing EIS, ﬁrst traditionally, without a data virtualization layer, then by using a data virtualization server. Figure 5. Levels of integration from Big Data analytics use cases point of view.14 S. JANKOVI ĆET AL. [Página 16] Before developing the integration solution for our use case, we needed to go through the following phases: (1) A relational data model was developed and the SQL server database STATE ROADS created. These enable storing the data on the state road reference",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "ROADS created. These enable storing the data on the state road reference system in the Republic of Serbia and the data on the automatic tra ﬃc counters used on these roads. The most important entities of the relational model are the following: road, road section, intersection, automatic traﬃc counter, etc. (2) Each automatic tra ﬃc counter generated 365 text ﬁles in 2015. Each ﬁle contained about 10,000 records on average, so that the collected data amounted to 10 ⋅365⋅ 10,000 = 36,500,000 records. (3) For the storage and processing of tra ﬃc data, the Apache Hadoop platform was chosen. Using the Apache Ambari user interface, on the Hortonworks Sandbox –a single-node Hadoop cluster, with the help of Apache Hive data warehouse software and HiveQL query language, a Hive",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "of Apache Hive data warehouse software and HiveQL query language, a Hive database named TRAFFIC ANALYSIS was created. (4) An ETL application was designed to ‘clean up ’the text ﬁles of any invalid records generated by tra ﬃc counters. Also, for each counter, this application consolidated the content of all 365 .txt ﬁles into a single text ﬁle which generated ten large .txt ﬁles. After that, we uploaded each of the ten large .txt ﬁles into the HDFS (Hadoop Distributed File System). White ( 2015 ) did useful work on HDFS. Using HiveQL query language we ‘ﬁlled ’Hive database tables with the data from the .txt ﬁles that are stored on HDFS. Integration approach without data virtualization The traditional integration solution –without data virtualization –is presented in Figure",
        "start_idx": 8236,
        "end_idx": 8364
      },
      {
        "text": "virtualization The traditional integration solution –without data virtualization –is presented in Figure 6 . This integration solution was implemented in the following phases: (5) We carried out numerous HiveQL queries on the Hadoop TRAFFIC ANALYSIS database resulting in useful information on tra ﬃc volumes, tra ﬃc structure, vehicle speeds, etc. (Jankovi ćet al. 2016b ). HiveQL has a powerful technique known as Create Table As Select (CTAS). This type of HiveQL queries allow us to quickly derive Hive tables from other tables in order to build powerful schemas for Big Data analysis. This data modeling approach is known as schema on read. Schemas of Hive tables are designed so as to be joined to the relational model of the local SQL server database. This enables the integration of",
        "start_idx": 8352,
        "end_idx": 8480
      },
      {
        "text": "model of the local SQL server database. This enables the integration of Big Data analytics with EIS on the corporate data model level. The query results include tra ﬃc volume and traﬃc safety indicators for each counting place: AADT, AADT by directions and vehicle categories, Monthly Average Daily Tra ﬃc (MADT), average speed of vehicles, 85thpercentile of vehicle speed, percentage of vehicles that exceed the speed limit, average speeding, etc. (6) In the IDE Microsoft Visual Studio 2015, a Windows Forms geo-application called Tra ﬃc Counting was developed. It has the following features: ●An intuitive GUI that allows the tra ﬃc engineers to de ﬁne the query parameters and start executing the queries against Hive tables on the Hadoop database TRAFFIC ANALYSIS and tables from the local SQL",
        "start_idx": 8468,
        "end_idx": 8596
      },
      {
        "text": "on the Hadoop database TRAFFIC ANALYSIS and tables from the local SQL server database STATE ROADS. This enables the integration of Big Data analytics with EIS on the report level. Access to the Hadoop database TRAFFIC ANALYSIS from the Windows Forms geo-application Tra ﬃc Counting was enabled with the help of Hortonworks ODBC Driver for Apache Hive. ●A GUI for graphical and tabular visualization of query results and their geo-location. For the geo-location of query results in the Tra ﬃc Counting application, we used Bing Maps and OpenStreetMaps.ENTERPRISE INFORMATION SYSTEMS 15 [Página 17] (7) The results of the query of the Hadoop database TRAFFIC ANALYSIS were stored in the SQL Server database STATE ROADS with the help of Hortonworks ODBC Driver for Apache Hive and the Windows Forms",
        "start_idx": 8584,
        "end_idx": 8712
      },
      {
        "text": "help of Hortonworks ODBC Driver for Apache Hive and the Windows Forms geo-application Tra ﬃc Counting. This enabled the integration of Big Data analytics with EIS on the data warehouse level. Integration approach based on data virtualization The architecture of the integration solution based on data virtualization is presented in Figure 7 . This integration solution includes the ﬁrst phase of the traditional integration approach, while its second and third phase di ﬀer from the traditional approach: (2) A virtual data source was created on the Denodo Express 6.0 data virtualization platform, by virtualizing and integrating data from the local SQL Server database STATE ROADS and the Hadoop database TRAFFIC ANALYSIS ( Figure 8 ). In this way, the data on volume, structure and speed of tra ﬃcﬂow",
        "start_idx": 8700,
        "end_idx": 8828
      },
      {
        "text": "this way, the data on volume, structure and speed of tra ﬃcﬂow that are generated on the Big Data platform are connected with the locally stored data on state roads in the Republic of Serbia. This enables the integration of Big Figure 6. Big Data analytics integration solution without data virtualization.16 S. JANKOVI ĆET AL. [Página 18] Data analytics with EIS on the corporate data model level. In Figure 9 , a tree view and a relationship view of data schemas created by combining (merging) ﬁelds from the local and Big Data sources are shown. One should notice that the local data source is a relational table, and a non-relational table that does not even have the primary key. The query results based on which ﬁeld combining from",
        "start_idx": 8816,
        "end_idx": 8944
      },
      {
        "text": "the primary key. The query results based on which ﬁeld combining from the mentioned heterogeneous data sources is performed are presented in Figure 10 . (3) The Tra ﬃc Counting geo-application, which was developed during the sixth phase of the traditional approach to integration, was linked to the unique virtual data source on the Denodo Express 6.0 data virtualization platform. In this way, the Tra ﬃc Counting geo- application uses the results of Big Data analysis from the Hadoop database TRAFFIC ANALYSIS and data from the local SQL Server database STATE ROADS, integrated on the Figure 7. Architecture of the proposed Big Data analytics integration solution.ENTERPRISE INFORMATION SYSTEMS 17 [Página 19] data virtualization platform. Figure 11 shows one window from the Tra ﬃc Counting geo- application that displays",
        "start_idx": 8932,
        "end_idx": 9060
      },
      {
        "text": "shows one window from the Tra ﬃc Counting geo- application that displays average speeding for each counting place. As seen in Figure 11 , visualization is achieved on the tabular and graphical level and the maps. Conclusions The continuous emergence of new data sources, data models, database management systems and data integration platforms, coupled with the pronounced need for the self-service analytics used by business analysts, makes it increasingly necessary to integrate Big Data analytics with traditional EISs on demand. IFAC TC5.3 Technical Committee for Enterprise Integration and Networking of the International Federation for Automatic Control has recognized the most serious challenges that must be solved in the Next Generation Enterprise Information System (NG EIS) development. The following have been selected as its key required features: omnipresence, a",
        "start_idx": 9048,
        "end_idx": 9176
      },
      {
        "text": "The following have been selected as its key required features: omnipresence, a model-driven architec- ture and openness. ‘In the ideal scenario, NG EIS will become a software shell, a core execution Figure 8. Big Data analytics integration solution based on data virtualization.18 S. JANKOVI ĆET AL. [Página 20] environment with the integrated interoperability infrastructure. Such an environment is foreseen as a highly ﬂexible and scalable, deployable on any and every platform, using the external models and services infrastructure, exclusively or on a sharing basis. ’(Zdravkovi ćand Trajanovi ć2015 ). Our approach to integration of Big Data analytics with EISs is based on a ﬂexible appending of external models and their joining with the existing corporate data model on the virtual level. In this research, an approach that enables",
        "start_idx": 9164,
        "end_idx": 9292
      },
      {
        "text": "model on the virtual level. In this research, an approach that enables ﬂexible integration from heterogeneous external sources and Big Data analytics with EISs is developed. The key drivers of our integration approach include ﬂexibility, the reuse of raw/atomic data and querying multiple data stores and types at once. The proposed Big Data analytics integration framework enables seven integration scenarios, which can include integration on the corporate data model level, on the data warehouse level and on the report level. Only integration on the corporate data model level enables all kinds of Big Data analysis applications, which include batch-oriented processing, stream processing, OLTP and inter- active ad-hoc queries and analysis. Integration on the data warehouse level enables integration in Big Data analysis applications based on batch-oriented processing. Integration",
        "start_idx": 9280,
        "end_idx": 9408
      },
      {
        "text": "enables integration in Big Data analysis applications based on batch-oriented processing. Integration on the report level enables integration in Big Data analysis applications based on batch-oriented processing, stream processing and OLTP. All integration scenarios start with the designing of schemas for data analysis at the time of reading raw Big Data sources. Schemas on read are designed so as to be integrated into the existing relational corporate data models and/or the existing business reports, taking into account the structure of the source data ﬁles. From the point of view of Big Data analytics use cases, integration scenarios can be divided into three categories. For end users, integration scenarios that include integration on the report level Figure 9. Tree view and relationships view of data source on data virtualization",
        "start_idx": 9396,
        "end_idx": 9524
      },
      {
        "text": "9. Tree view and relationships view of data source on data virtualization platform.ENTERPRISE INFORMATION SYSTEMS 19 [Página 21] are appropriate. For business analysts and business reporting, integration scenarios that include integration on the corporate data model level and/or on the report level are appropriate. For data analysts, data discovery and developers, integration scenarios that include integration on all three levels, namely the corporate data model level, the data warehouse level and the report level, are appropriate. The case study conducted has con ﬁrmed that the use of a data virtualization layer o ﬀers numerous advantages. These can be classi ﬁed into three groups. The ﬁrst group of advantages comes into play if the user accesses only one data source and it consists of the following: a data virtualization",
        "start_idx": 9512,
        "end_idx": 9640
      },
      {
        "text": "one data source and it consists of the following: a data virtualization layer with the capability of language translation and API supported by a language data warehouse and API suitable for data users, independence from data source technologies (in the era of the IoT and Big Data, the possibility of exchanging a non-SQL data warehouse with a SQL warehouse is very important), and minimal negative user in ﬂuence of data warehouse perfor- mance. The second group of advantages is connected to metadata speci ﬁcation, such as: a table structure, cleansing and transformation operations, aggregation and similar. When data virtualiza- tion is used metadata speci ﬁcation is implemented only once and it is not necessary to copy it for several data users. In other words, data users share and",
        "start_idx": 9628,
        "end_idx": 9756
      },
      {
        "text": "it for several data users. In other words, data users share and use metadata speci ﬁcations on multiple occasions, with which they achieve more simple table structures, centralized data transformation, centralized data cleansing, simpli ﬁed application development, more consistent application beha- vior and more consistent results. The third group refers to data integration from multiple data sources and includes the following: a uni ﬁed approach to di ﬀerent types of data warehouses (SQL Server database, Excel worksheets, index sequential ﬁles, NoSQL databases, XML ﬁles, HTML web Figure 10. Query results view on data virtualization platform.20 S. JANKOVI ĆET AL. [Página 22] pages, etc.), centralized data integration and sharing of integration programming code, consistent report results and e ﬃcient distributed data access. In view of the positive experiences gained",
        "start_idx": 9744,
        "end_idx": 9872
      },
      {
        "text": "e ﬃcient distributed data access. In view of the positive experiences gained while using a data virtualization platform, the authors ’future research will focus on the use of the above platform in the integration of Big Data analytics with NoSQL databases, such as column and key-value databases. Disclosure statement No potential con ﬂict of interest was reported by the authors. Funding This paper has been partially supported by the Ministry of Education, Science and Technological Development of the Republic of Serbia project under No. 36012, and the project under the name “Novel Decision Support tool for Evaluating Strategic Big Data investments in Transport and Intelligent Mobility Services –NOESIS ”. NOESIS project has received funding from the European Union ’s Horizon 2020 research and innovation programme under grant agree-",
        "start_idx": 9860,
        "end_idx": 9988
      },
      {
        "text": "European Union ’s Horizon 2020 research and innovation programme under grant agree- ment No 769980. The data generated by automatic tra ﬃc counters have been provided by the company MHM - Project from Novi Sad. References Alooma. 2018 .“ETL Tools. ”January 4. https://www.etltools.net/ Arputhamary, B., and L. Arockiam. 2015 .“A Review on Big Data Integration. ”International Journal of Computer Applications Proceedings on International Conference on Advanced Computing and Communication Techniques for High Performance Applications 5: 21 –26. Chen, C. L. P., and C. Y. Zhang. 2014 .“Data-Intensive Applications, Challenges, Techniques and Technologies: A Survey on Big Data. ”Information Sciences 275: 314 –347. doi: 10.1016/j.ins.2014.01.015 . Doan, A., A. Halevy, and Z. Ives. 2012 .Principles of Data Integration . Waltham: Morgan Kaufmann. Figure 11. Traﬃc Counting geo-application –Average Speeding",
        "start_idx": 9976,
        "end_idx": 10104
      },
      {
        "text": "Integration . Waltham: Morgan Kaufmann. Figure 11. Traﬃc Counting geo-application –Average Speeding window.ENTERPRISE INFORMATION SYSTEMS 21 [Página 23] Dong, X. L., and D. Srivastava. 2015 .Big Data Integration (Synthesis Lectures on Data Management) . Williston: Morgan & Claypool Publishers. EMC Education Services, ed. 2015 .Data Science & Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data . Indianapolis: John Wiley & Sons. Florea, A. M. I., V. Diaconita, and R. Bologa. 2015 .“Data Integration Approaches Using ETL. ”Database Systems Journal (VI)3: 19 –27. Gal, A. 2011 .Uncertain Schema Matching . (Synthesis Lectures on Data Management). Williston: Morgan & Claypool Publishers Gandomi, A., and M. Haider. 2015 .“Beyond the Hype: Big Data Concepts, Methods, and Analytics. ”International Journal of Information Management 35: 137 –144. doi: 10.1016/j.ijinfomgt.2014.10.007 . Gokhe, P.",
        "start_idx": 10092,
        "end_idx": 10220
      },
      {
        "text": "Journal of Information Management 35: 137 –144. doi: 10.1016/j.ijinfomgt.2014.10.007 . Gokhe, P. 2016 .“Enterprise Real-Time Integration. ”E-book. http://www.enterpriserealtimeintegration.com/enterprise- real-time-integration/ Intel Corporation. 2013 .“Extract, Transform, and Load Big Data with Apache Hadoop. ”White Paper Big Data Analytics. https://software.intelcom/sites/default/ ﬁles/article/402274/etl-big-data-with-hadoop.pdf Intel Corporation. 2014 .“Real-Time Big Data Analytics for the Enterprise. ”White Paper Intel® Distribution for Apache Hadoop. https://www.intel.com/content/dam/www/public/us/en/documents/white-papers/big-data-hadoop-real- time-analytics-for-the-enterprise-paper.pdf Jankovi ć, S., D. Mladenovi ć, S. Mladenovi ć, S. Zdravkovi ć, and A. Uzelac. 2016a .“Big Data in Tra ﬃc.”InProceedings of the First International Conference Transport for Today ’s Society –TTS 2016 , edited by M. M. Todorova, 28 –37. Bitola, Macedonia: Faculty of Technical Science. Jankovi ć, S., S. Zdravkovi ć,S. Mladenovi ć, D. Mladenovi ć, and A. Uzelac. 2016b .“The Use of Big Data Technology in the Analysis",
        "start_idx": 10208,
        "end_idx": 10336
      },
      {
        "text": "A. Uzelac. 2016b .“The Use of Big Data Technology in the Analysis of Speed on Roads in the Republic of Serbia. ”InProceedings of the Third International Conference on Tra ﬃc and Transport Engineering - ICTTE Belgrade 2016 , edited by O. Čokorilo, 219 –226. Belgrade: City Net Scienti ﬁc Research Center. Lipovac, K., M. Vujani ć, T. Ivani šević, and M. Rosi ć.2015 .“Eﬀects of Application of Automatic Tra ﬃc Counters in Control of Exceeding Speed Limits on State Roads of Republic of Serbia. ”InProceedings of the 10th Road Safety in Local Community International Conference , edited by Pro ﬀ. K. Lipovac and M. Ne šić, 131 –140. Belgrade: Academy of Criminalistic and Police Studies. Loshin, D. 2013 .Big Data Analytics: From Strategic Planning to Enterprise Integration with",
        "start_idx": 10324,
        "end_idx": 10452
      },
      {
        "text": "D. 2013 .Big Data Analytics: From Strategic Planning to Enterprise Integration with Tools, Techniques, NoSQL, and Graph . Waltham: Elsevier. Macura, M. 2014 .“Integration of Data from Heterogeneous Sources Using ETL Technology. ”Computer Science 15 (2): 109 –132. doi: 10.7494/csci.2014.15.2.109 . Reeve, A. 2013 .Managing Data in Motion . Waltham: Elsevier. Ribeiro, A., A. Silva, and A. R. da Silva. 2015 .“Data Modeling and Data Analytics: A Survey from A Big Data Perspective. ” Journal of Software Engineering and Applications 8: 617 –634. doi: 10.4236/jsea.2015.812058 . Russom, P. 2013 .Integrating Hadoop into Business Intelligence and Data Warehousing . Renton, WA: Data Warehousing Institute. Sridhar, P., and N. Dharmaji. 2013 .“AComparative Study on How Big Data Is Scaling Business Intelligence and Analytics. ”International Journal of Enhanced Research in Science",
        "start_idx": 10440,
        "end_idx": 10568
      },
      {
        "text": "Scaling Business Intelligence and Analytics. ”International Journal of Enhanced Research in Science Technology & Engineering 2 (8): 87 –96. -izbaciti. van der Lans, R. F. 2012 .Data Virtualization for Business Intelligence Systems . Waltham: Elsevier. Wang, H., Z. Xu, H. Fujita, and S. Liu. 2016 .“Towards Felicitous Decision Making: An Overview on Challenges and Trends of Big Data Technologies. ”Information Sciences 367 –368: 747 –765. doi: 10.1016/j.ins.2016.07.007 . White, T. 2015 .Hadoop: The De ﬁnitive Guide . Sebastopol, CA: O ’Reilly Media. Zdravkovi ć, M., F. Luis-Ferreira, R. Jardim-Goncalves, and M. Trajanovi ć.2015 .“On the Formal De ﬁnition of the Systems ’Interoperability Capability: An Anthropomorphic Approach. ”Enterprise Information Systems 11 (3): 389 – 413. doi: 10.1080/17517575.2015.1057236 . Zdravkovi ć, M., and H. Panetto. 2017 .“The Challenges of Model-Based",
        "start_idx": 10556,
        "end_idx": 10684
      },
      {
        "text": ". Zdravkovi ć, M., and H. Panetto. 2017 .“The Challenges of Model-Based Systems Engineering for the Next Generation Enterprise Information Systems. ”Information Systems and e-Business Management 15 (2): 225 –227. doi: 10.1007/ s10257-017-0353-z . Zdravkovi ć, M., and M. Trajanovi ć. 2015. “On the Runtime Models for Complex, Distributed and Aware Systems ”In Proceedings of the 5th International Conference on Information Society and Technology –ICIST 2015 , edited by M. Zdravkovi ć, M. Trajanovi ć, and Z. Konjovi ć, 236 –240. Kopaonik, Serbia: Society for Information Systems and Computer Networks.22 S. JANKOVI ĆET AL.",
        "start_idx": 10672,
        "end_idx": 10766
      }
    ],
    "ffa90a09-f02d-4af5-baf2-e3c80f3aa6fb": [
      {
        "text": "[Página 1] 1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing 1 Common Pitfalls of Benchmarking Big Data Systems Gwen Shapira, Y anpei Chen. Cloudera Inc. fgshapira,yanpeig@cloudera.com Abstract —It is challenging to get reliable performance benchmarking results. Benchmarking matters because one of the deﬁning characteristics of big data systems is the ability to process large datasets faster. “How large” and “how fast” drive technology choices, purchasing decisions, and cluster operations. Even with the best intentions, performance benchmarking is fraught with pitfalls - easy to",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "the best intentions, performance benchmarking is fraught with pitfalls - easy to get numbers, hard to tell if they are sound. This paper discusses ﬁve common pitfalls drawn from engineering and customer experiences at Cloudera, a leading big data vendor. These pitfalls are: “Comparing Apples to Oranges”- when too many parameters are modiﬁed and comparison is impossible, “Not Testing at Scale” - trying to test a big data system by extrapolating from an under-sized test system, “Believing in Miracles” - failing to question suspicious results, “Using Unrealistic Benchmarks” - using workloads far removed from what will realistically be used by customers, and “Communicating Results Poorly” - neglecting to communicate sufﬁcient information for customers to understand and reproduce the results. These pitfalls offers a behind-the-scenes look at internal engineering",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "reproduce the results. These pitfalls offers a behind-the-scenes look at internal engineering and review processes that produces rigorous benchmark results. Readers working on big data in both the industry and in academia can draw lessons from our experience. Index Terms —Big data, performance, benchmarking, case studies. F 1 I NTRODUCTION Done poorly, performance benchmarking produces dis- astrous results. Here are two stories from the authors’ early careers. An engineer ran a benchmark on a proof-of-concept 5-node cluster. Extrapolating the results, the engineer as- sumed the system will scale linearly and plans for a 50-node cluster to support the required production workloads. The production cluster ran for 30 minues before latency became completely unacceptable. It hit network bottlenecks not revealed at the proof-of-concept scale. As a result, rollout of",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "bottlenecks not revealed at the proof-of-concept scale. As a result, rollout of the production system had to be delayed by a week as the scalability problems were being resolved. A graduate student ran a Hadoop benchmark without realizing that he accidentally mounted the Hadoop Dis- tributed File System (HDFS) on the departmental network ﬁler. The benchmark promptly took down the ﬁler for all professors, staff, and students at the department. The stu- dent received angry e-mails from the system administrators for days following the incident. These two particular stories reveal how difﬁcult it is to do performance benchmarking in a way that does not disrupt customer-facing, production systems, in a way that represent real-life workloads running there. Despite perfor- mance being an increasingly visible aspect of big data",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "there. Despite perfor- mance being an increasingly visible aspect of big data sys- tems, there has not yet been many case studies of common benchmarking pitfalls, nor ways to avoid them. In this in- dustry experience paper, we offer a collection of stories that illustrate important principles of conducting performance benchmarking and assessing others’ results: 1) Workload and hardware choices should be relevant to the expected use of the product.2) When modifying a standard benchmark, the modi- ﬁcation should be documented and justiﬁed. 3) Testing big data means testing the system along multiple dimensions of large scale: Large number of jobs, jobs with large number of tasks, large data size, large clusters, and large nodes. 4) Tests designed to compare systems across a single parameter, e.g., new version",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "Tests designed to compare systems across a single parameter, e.g., new version of platform, must make sure this parameter was the only change. Changing additional parameters invalidates the comparison. 5) Having a model of expected behavior of the system is mandatory. Otherwise it is impossible to reason about the results. 6) Benchmark results should include enough informa- tion to reproduce the result - hardware, conﬁgura- tion, and workload. 7) Make sure any results tables and charts are clear, meaningful, and not misleading. The stories in this paper come from internal engineering and customer experiences at Cloudera, a leading big data vendor. The pitfalls involve performance benchmarking of different components in the Hadoop ecosystem. This is not a comprehensive categorization of all possible mistakes, our goal is to give",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "a comprehensive categorization of all possible mistakes, our goal is to give readers in both the industry and in academia tools with which they can improve their own work. 2 C OMPARING APPLES TO ORANGES We often run two tests, expecting only one parameter to change, while in fact many parameters changed and a comparison is impossible - in other words, we compare apples to oranges. [Página 2] 1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing 2 Late 2013, the Hadoop community adopted",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "IEEE Transactions on Services Computing 2 Late 2013, the Hadoop community adopted MapRe- duce 2 (MR2) running on Yet Another Resource Negotiator (YARN) as the default MapReduce execution framework [1], [25]. This change offers functionality improvements over the original MapReduce, or MapReduce 1 (MR1) [12]. Many cluster operators did performance benchmarking on their own when they considered whether to upgrade. They ini- tially reported a performance regression from MR1 to MR2. What actually happened was that a straightforward com- parison ended up comparing two different things, in other words, “comparing apples to oranges”. Two issues led to this discrepancy. One issue was that TeraSort, a limited but frequently used benchmark, changed between MR1 and MR2 [24]. To reﬂect rule changes in the GraySort benchmark on which it is",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "To reﬂect rule changes in the GraySort benchmark on which it is based, the data generated by the TeraSort included with MR2 is less compressible. A valid comparison would use the same version of TeraSort for both releases, because map output compression is enabled by default as a performance optimization in Cloudera Distribution with Apache Hadoop (CDH). Otherwise, MR1 will have an unfair advantage by using more compressible data (Figure 1). Another issue was the replacement of “task slots” in MR1 with “containers” in MR2. YARN has several conﬁg- uration parameters that affected how many containers will be run on each node [5]. A valid comparison would set these conﬁgurations such that there is the same degree of parallel processing between MR1 and MR2. Otherwise, depending on whether",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "degree of parallel processing between MR1 and MR2. Otherwise, depending on whether hardware is over or under-committed, either MR1 or MR2 will have the advantage. We committed these pitfalls ourselves in the early days of ensuring MR1 and MR2 performance parity. We regularly compared MR1 and MR2 performance on our nightly CDH builds, and the “regression” was caught the very ﬁrst time we did this comparison. Our MapReduce and Performance Engineering teams collaborated to identify the code changes and understand what makes a valid performance compari- son. This effort culminated in MR2 shipped in CDH5.0.0 at performance parity with MR1. Here are some questions to ask regarding your own per- formance tests: If you are comparing hardware, are you run- ning identical workloads? If you are comparing software,",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "hardware, are you run- ning identical workloads? If you are comparing software, are you running your workload on identical hardware? Identical data, with identical formats and compression? Did the test procedure or test harnesses change? 3 N OTTESTING AT SCALE Big data is called big for a reason. Testing small workloads on small clusters and expecting the results to extrapolate to large scale systems simply does not work. ”Scale” for big data systems can mean data scale, con- currency scale (number of jobs and number of tasks per job), cluster scale (number of nodes/racks), or node scale (per node hardware size). Failing to test “at scale” for any of these dimensions can lead to surprising behavior for your production clusters. It is illustrative to look at another aspect",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "for your production clusters. It is illustrative to look at another aspect of our efforts to drive MR2 to performance parity with MR1. We wanted to verify that MR2 and MR1 perform at parity when a large number of jobs are running. We ran SWIM [6], which Fig. 1. Terasort performance when the data generation in MR1 and MR2 use different algorithms (left) or the same algorithm (right). submits many jobs concurrently over hours or even days, simulating the workload logged on actual production clus- ters. The ﬁrst runs of SWIM on MR2 revealed a live-lock issue [3] where the jobs would appear as submitted, but none of them would make any progress. Figure 2 shows a web user-interface (UI) screenshot of a YARN Resource Manager that is",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "a web user-interface (UI) screenshot of a YARN Resource Manager that is experiencing live-lock. The cause of the live-lock is not straightfoward. Each MR2 job has an Application Master, which is a book-keeping type task that tracks the progress of the entire job. The Application Master still requires a YARN container to run. Without additional conﬁgurations, YARN would give all available resources to the Application Masters, leaving no room for the actual tasks. The tasks are behaving normally, but making no progress, i.e., live-lock. This issue escaped detection in our other scale tests that covered a range of data, cluster, and node scales. The live- lock occurs only when all the containers in a cluster are taken up by Application Masters. On a cluster of non-trivial size, this",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "taken up by Application Masters. On a cluster of non-trivial size, this means hundreds or even thousands of concurrent jobs. SWIM is speciﬁcally designed to reveal such issues by replaying production workloads with their original level of concurrency and load variation over time. In this case, we found a critical issue. 4 B ELIEVING IN MIRACLES If something is too good to be true, it is probably not true. We should always have a model of expected system behav- ior and bottlenecks. This way, we can tell if a performance improvement is reasonable, or too good to be true. Here are some recent “miracles” we debunked. 4.1 Miracle 1: 1000x SQL speedup A customer reported that Impala [11], a SQL-on-Hadoop system, performs more than 1000x better than their",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "Impala [11], a SQL-on-Hadoop system, performs more than 1000x better than their existing relational database manage system (RDBMS). The customer wanted us to help them set up a new cluster to handle their growing production workload. The 1000x difference is orders of magnitude larger than our own measurements [14], and immediately made us skeptical. Following much discussion, we realized that the customer was comparing very simple queries running on [Página 3] 1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing 3 Fig. 2. YARN",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing 3 Fig. 2. YARN Resource Manager screenshot of live-lock symptoms. a proof-of-concept Impala cluster versus complex queries running on a heavily-loaded production RDBMS system. We helped the customer do an apple-to-apple compar- isons, and turns out Impala still has an advantage (average 2x faster and up to 4.5x faster, from [14]). We left the customer with realistic plans for how to grow their data management systems. 4.2 Miracle 2: Indirect writes faster than direct writes A customer asked us to run several conﬁgurations of Sqoop [2], a Hadoop-to-RDBMS connector used to bulk transfer data between the two types of systems. The intent was to ﬁnd the conﬁguration leading to the best perfor- mance of exporting data from Hadoop to RDBMS.",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "to the best perfor- mance of exporting data from Hadoop to RDBMS. Among other tests, we compared the performance of loading data to new partitions through Oracle’s direct path writes, to loading the same data through normal inserts. We expect direct path writes to be signiﬁcantly faster, since they bypass the busy buffer-cache and redo log sub- systems, writing data blocks directly to Oracle’s data ﬁles. In this test, the normal inserts exercising an indirect write path were 3 times faster than the direct path writes. This suspicious result called for additional investigation. The investigation revealed that Sqoop was exporting around 50GB of data to an otherwise idle Oracle cluster with over 300GB of memory dedicated to the buffer cache. Loading data into memory in a server with",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "to the buffer cache. Loading data into memory in a server with no contention is obviously faster than writing the same data to disk. We explained the results to the customer and recommended repeating the tests on a cluster with realistic workloads. 4.3 Miracle 3: 100x Hadoop sort speedup A customer asked us for comment on a Hadoop sort bench- mark result in the trade press. The result was more than100x faster than what we found internally. It turns out that the data size being tested was consid- erably smaller than the available memory in the cluster. In other words, a knowledgeable operator would be able to conﬁgure Hadoop in a way that the sort takes place completely in memory. This departed from the common practice of conﬁguring",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "place completely in memory. This departed from the common practice of conﬁguring sort with data size much greater than total cluster memory. The more-than-100x gap came from the inherent hardware difference between memory and disk IO, rather than a difference between two software systems. The ability to identify miracles requires us having mod- els of expected performance beyond just a “gut-feeling”. These models can come from prior results, or an under- standing of where the system bottlenecks should be. Bench- marking without such models would give you a lot of numbers but not a lot of meaning. 5 U SING UNREALISTIC BENCHMARKS Unrealistic benchmarks are benchmarks where the work- load, hardware, or presentation is chosen without regard of real-life requirements. Rather, these choices intend to inﬂate the capabilities",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "regard of real-life requirements. Rather, these choices intend to inﬂate the capabilities of benchmarked system under test. Here are some warning signs of a biased benchmark: 5.1 Misleading workloads Examples of misleading workloads include when some- one ran benchmarks on 100GB of data when the system is intended for 100TB data sets, or when a transactional workload is used to test a system with mostly analyti- cal use-cases. Terasort, a very popular benchmark for big [Página 4] 1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "change prior to final publication. Citation information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing 4 Fig. 3. Different dimensions of a big data workload. data systems, is also potentially misleading. Terasort has very speciﬁc characteristics that stress very speciﬁc subsets of the processing subsystem. It is not necessarily a good benchmark to evaluate how the system will scale for all workloads, even though it is a useful ﬁrst step in comparing different hardware conﬁgurations. An example of how we avoid it at Cloudera: Terasort is only one job in our MapReduce performance benchmarking suite. We run a set of stand-alone, artiﬁcial jobs designed to stress in isolation different components of the MapReduce IO and compute pipeline; this suite includes open source jobs such as Terasort, and some jobs",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "this suite includes open source jobs such as Terasort, and some jobs written in-house that we consider proprietary assets. We also use an open source tool [6] to replay full customer workloads with a large range of job sizes, types, and arrival patterns. We run both the stand-alone jobs and multi-job workloads under different dimensions of scale beyond just data size (See Section 3). 5.1.1 What makes a representative workload? Cluster operators often ﬁnd it challenging to reason about their own workload. If someone has no idea what their production workload looks like, they will have no idea whether the workload captured in a benchmarking study will match their own use case. Figure 3 is a diagram to help readers characterize their workload. In broad strokes, there are",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "diagram to help readers characterize their workload. In broad strokes, there are three dimensions - the data characteristics, the compute characteristics, and the load-over-time characteristics [7]. Readers should ask themselves what is the following for their workload: Data: \u000fHow large is the data? \u000fWhat is the data schema, i.e., how do different parts of the data relate to each other? \u000fIs there any data skew, i.e., whether some data is accessed more frequently than others? \u000fHow is the data represented and stored, i.e, what is the data format or data type? Compute: \u000fWhat is the hardware bottleneck for the computation done? CPU, memory, disk, or network? \u000fIf the workload is a SQL workload, whether the queries involve joins, scans, ﬁlters, group-by’s?\u000fIf the workload is MapReduce, whether the jobs",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "involve joins, scans, ﬁlters, group-by’s?\u000fIf the workload is MapReduce, whether the jobs need to do a lot of shufﬂe, sort, combiner operations, are they map-heavy or reduce heavy? \u000fIf the workload is something else, characterize it in terms of the semantics of that processing paradigm. Load: \u000fWhat is the load average? \u000fHow long and how high are bursts in load? \u000fHow do the mix of jobs or queries change over time? \u000fAre there diurnal patterns? These questions should get readers started on charac- terizing their own workload. Answering these questions direct the discussion to other, more complicated, case-by- case characteristics that are also important to capture. In a real-world example, we start by identifying the primary components of a production workload. If, say, MapReduce, HBase, and Impala are",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "components of a production workload. If, say, MapReduce, HBase, and Impala are all involved, we need to make sure the test workload combines all of those. Drilling farther in, we may see that most of the MapReduce workload is map-only, with very little data being shufﬂed or reduced. We may also see that the HBase workload is 75% put and 20% get and 5% scans, and the Impala workload consists of star-schema joins that include one large table and many smaller tables, the results of which will be aggregated by day and month. We make sure our benchmark workload includes this level of details. The next step is to note the data sizes, and either copy sufﬁcient data from production, or write a small script that will generate",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "sufﬁcient data from production, or write a small script that will generate synthetic data for the benchmark. It is recommended to note speciﬁc data patterns that should be part of the test - for example, if the workload involves sales data, it is likely that some regions and dates have signiﬁcantly more records than others. This type of skew can impact performance and therefore benchmark results. The last step is to check characteristics of the load pat- terns. Start with ﬁnding out how many concurrent jobs and queries typically run in production. Then decide whether to test with average load, peak load, expected future peak load, or perhaps the test should increase the load to the point the system breaks in order to ﬁnd theoretical limits (test to",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "point the system breaks in order to ﬁnd theoretical limits (test to destruction). Since multiple workloads are involved (MapReduce, Impala and HBase), we need to know if those workloads are typically executed together, or if they run during different times. For example, if we run Impala queries mostly during business hours and MapReduce dur- ing the night, the test should combine light Impala load with heavy MapReduce load and vice-versa, to simulate expected production conditions. This type of planning leads to more meaningful results and is well worth the extra effort. 5.2 Premium hardware Benchmark reports often contain results that come from hardware not typically used in real-life - solid state drives (SSDs) in environments that commonly use hard disk drives (HDDs), or premium SSDs not available in",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "commonly use hard disk drives (HDDs), or premium SSDs not available in the general market. The Transaction Processing Council - C (TPC-C) [30] benchmark allows the use of hardware that is not available provided that availability dates are published. It is wise to [Página 5] 1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing 5 check if the hardware choices make results irrelevant for guiding purchase decisions. An example of how we avoid it at Cloudera: We have explored MapReduce performance for SSDs [20].",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "avoid it at Cloudera: We have explored MapReduce performance for SSDs [20]. We were very conscious of SSD’s prevalence in the market compared with HDDs. This prompted us to suggest to our hardware partners to track SSD performance-per-cost, which shows SSDs approaching parity with HDDs, even though the gap in capacity-per-cost remains large. 5.3 Cherry picking queries or jobs Some reports pick very speciﬁc queries out of a standard benchmark, but cannot explain the choice with objective criteria that is relevant to the real-life use cases (or worse, does not disclose that a choice was made). An example of how we avoid it at Cloudera: Our past Impala performance results [13], [14] used 20 queries derived from the TPC - Decision Support (TPC-DS) [32] benchmark. These queries were",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "from the TPC - Decision Support (TPC-DS) [32] benchmark. These queries were chosen based on what our customers observed for business intelligence (BI) use cases. They cover interactive, reporting, and deep analytic use cases. At the time, it was a major improvement over a frequently cited set of ﬁve queries [21] that were constructed without empirical backing from actual customer use cases. The 20 queries also represent a step forward from our own early efforts [9] using queries derived from TPC-H [31]. TPC-H is a less demanding benchmark with fewer and less complex queries than TPC-DS, while both are backed by customer surveys from vendors in the TPC Consortium. We have kept the set of 20 queries derived from TPC-DS to help ourselves compare against our own prior",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "queries derived from TPC-DS to help ourselves compare against our own prior results, and we are well aware they are less than the full set of 99 queries in the ofﬁcial TPC-DS. Look for our future reports in this space. 5.4 Questions to ask all benchmark reports To an extent all commercial and even research benchmarks are suspect of bias, since they are performed by a spe- ciﬁc vendor or research group to promote their products or search project. Cluster operators can hold benchmark reports accountable by understanding their own workload and have a conversation about whether a product or re- search project addresses their speciﬁc use case. The follow- ing is a list of questions to ask. \u000fWhat hardware did you use? \u000fHow was it conﬁgured? \u000fIs",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "to ask. \u000fWhat hardware did you use? \u000fHow was it conﬁgured? \u000fIs it similar to the hardware you are selling? \u000fWhich jobs or queries did you run? \u000fWhy do you think they mimic my workload? \u000fWere they modiﬁed from a well-known spec? \u000fHow did you choose these speciﬁc jobs or queries? \u000fWhat if the jobs or queries are different? With these questions, cluster operators force benchmark reports to discuss the limits of their own work. 6 (M IS)COMMUNICATING RESULTS Poor communication detracts from otherwise good perfor- mance results. Here at Cloudera, we check all external- facing benchmarking communications for the following:We select a benchmark that \u000fIs unbiased (see Section 5), \u000fExercise workloads relevant to actual customers, and \u000fScales across data size, concurrency level, cluster size, and node size.",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "and \u000fScales across data size, concurrency level, cluster size, and node size. We report sufﬁcient information for industry peers to assess the signiﬁcance of the result, and to reproduce the tests if needed. This requires reporting \u000fThe benchmark used and why we chose it, \u000fThe metrics used and how we measured them, \u000fThe hardware used and the software tuning applied. These simple guidelines are often neglected in results coming from both industry and academia. One more aspect of a good benchmarking report is whether the results have been independently veriﬁed or audited. The purpose of an independent audit is to have the above checks done by someone other the organization that produced study. Results that passed independent audit are more likely to be communicated clearly and completely. There",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "independent audit are more likely to be communicated clearly and completely. There are several gold-standards for audit and veriﬁca- tion practices established before the rise of big data: Dedicated auditors The Transaction Processing Council (TPC) [28] uses ded- icated auditors. Each auditor is certiﬁed to audit a particular benchmark only after passing a test designed by the work- ing group who initially speciﬁed that benchmark [29]. Validation kit and fair-use rules The Standard Performance Evaluation Corporation (SPEC) [27] uses validation checks built into benchmark- ing kits, fair-use rules governing how the results should be reported, and review by the SPEC organization, which encompasses many industry peers of the test sponsor. Peer review The ofﬁcial Sort Benchmark [26] has new submissions reviewed by past winners. The winners would “hand",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "[26] has new submissions reviewed by past winners. The winners would “hand over the torch” only if new entries are sufﬁciently rigorous. There are not yet any widely accepted audit and veri- ﬁcation processes for big data. The need for complete and neutral benchmarking results sometimes gets diluted by the need to stand out in the trade press. However, the past year has seen a phenomenal growth in the level of per- formance knowledge in the broader technical community. Every benchmark report is now scrutinized by industry and academia peers. This increases the need to be rigourous and open about performance benchmarking results. 6.1 A picture in need of 1000 words Performance reports often use graphs to summarize re- sults. Poor graphs can unintentionally or deliberately mis- lead",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "to summarize re- sults. Poor graphs can unintentionally or deliberately mis- lead readers. We include here an example of a poorly- communicated graph and a better-communicated graph. Figure 4 comes from one of the author’s early work mea- suring the performance of distributed databases. None of the axes were labeled, the performance metrics are unclear, [Página 6] 1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing 6 Fig. 4. An example of a poorly-communicated graph. Fig. 5. An example of a better-communicated graph. It",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "a poorly-communicated graph. Fig. 5. An example of a better-communicated graph. It still needs a lot of surrounding text for a full explanation. and the test scenario and test settings are unclear. Even the graph’s creator cannot recollect what was being displayed. Here is what the authors together deciphered. The graph is showing database throughput measured in transactions per minute (TPM), query latency (response time), and CPU utilization of the system. The horizontal axis is likely show- ing the number of concurrent user or a similar sense of “load”. CPU utilization increases under higher load, and the right vertical axis is of the correct numerical range for CPU utilization in percentages. The left vertical axes could be either TPM in number of queries, or response time in milliseconds.",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "be either TPM in number of queries, or response time in milliseconds. There is no way to tell without additional information. Without proper labeling and documentation, every well-done performance benchmarking studies lose their meaning over time. Figure 5 appears in a recent Cloudera blog [9]. It is a better communicated graph. Without further text, here is what the ﬁgure communicates: The graph shows Impala multi-tenant performance, with the metric being a nor- malized, unitless metric of multi-tenant performance as a fraction of stand-alone performance. This metric has the property that “higher is better”. The graph comes from ﬁve tests, with Impala receiving an increasing fraction of system resources ranging from 25% to 75%. There is large performance variation as shown by the error bars. There isalso a model",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "performance variation as shown by the error bars. There isalso a model of desired system behavior, one that suggests Impala should show fraction xof stand-alone performance when given fraction xof system resources. There is still a lot of information missing from the graph: What was the workload being tested? It was Impala running concurrently with MapReduce on the same cluster, speciﬁ- cally one MapReduce job concurrent with one Impala query at a time. The cluster is conﬁgured to give fraction xof the resources to Impala, with MapReduc receiving the remain- ing fraction 1\u0000x. What metric is being normalized? Impala query duration when the cluster is executing only the Impala query vs. when the cluster is executing an Impala query with a MapReduce job. What do the error bars",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "an Impala query with a MapReduce job. What do the error bars show and why are they so large? Each data point is the arithmetic average of 56 MapReduce job and Impala query combinations. The 56 job-query combi- nations cover a large range of MapReduce job types and Impala query types, hence the large variation. The error bars themselves represent 25th to 75th percentile range across the job-query combinations. What fractions of resources were assigned to Impala for the 2nd and 4th markers? It is not immediately clear from the ticker mark intervals on the horizontal axes, but the 2nd and 4th markers represent 40% and 60% of the cluster resourcess assigned to Impala. What about MapReduce multi-tenant performance? The com- panion graph for MapReduce multi-tenant performance is",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "MapReduce multi-tenant performance? The com- panion graph for MapReduce multi-tenant performance is Figure 6. The graph guides the discussion to more interesting topics, such as why should the performance model be as it is, whether the test workload is realistic and useful, and whether the performance is actually good. The following is a list we use to check our own graphs. \u000fDoes the graph need a title, or is one unnecessary based on surrounding text? \u000fIf the graph shows multiple data series, is a legend displayed or included in the graph caption? \u000fAre the graph axes labeled? Do the labels include appropriate units? \u000fIs there one or several performance metrics being graphed? \u000fIf there is a single performance metric graphed, is it on the vertical axes? \u000fAs big",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "single performance metric graphed, is it on the vertical axes? \u000fAs big data performance is variable from measure- ment to measurement, are error bars necessary? \u000fIf a line or curve is drawn connecting two markers, is it reasonable to extrapolate across a range of un- measured settings? \u000fIf there is a model of desirable behavior, is the model also shown on the graph? Big data systems have evolved to the point where the meaning of performance can be complex, and the number of relevant metrics can be large. This is especially true when we consider different big data processing engines not as stand- alone components, but as concurrently active frameworks sharing resources on the same cluster. Thus, we should make every effort to ensure clear communication. [Página 7]",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "Thus, we should make every effort to ensure clear communication. [Página 7] 1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing 7 Fig. 6. Companion graph to Figure 5, showing MapReduce multi-tenant performance. 6.2 Following our own advice - Miracle checking Earlier we highlighted the need to check any miracle re- sults for their validity. In Figure 5, the fact that multi- tenant performance turned out better than modeled is an immediate warning sign for a possible “miracle” result. Since Impala and MapReduce were",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "warning sign for a possible “miracle” result. Since Impala and MapReduce were concurrently active for the multi-tenant scenario, the results would be reasonable if MapReduce multi-tenant performance suffered and was lower than modeled. The opposite happened, and the com- panion MapReduce multi-tenant performance also exceeded our model (Figure 6). This was indeed a “miracle” result worth understanding. Two factors caused this result. First, our test scenarios run through 56 pairs of concurrent MapReduce jobs and Impala queries, one pair at a time. For any given pair, either the MapReduce job or Impala query would complete ﬁrst. Thereafter, the remaining MapReduce job or Impala query would receive the entire cluster’s resources. In other words, our test procedure systematically skewed the results in favor of being better than the model.",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "systematically skewed the results in favor of being better than the model. Another reason is the statistical multiplexing of hard- ware resource demands. This is a subtle effect of multi- tenant processing. For our tests, a MapReduce job and an Impala query need different hardware resources at different times. The resource demands are frequently not overlap- ping, i.e., statistically multiplexed. This multiplexing hap- pens due to the range of processing covered in the 56 job- query pairs and the different design of the MapReduce and Impala processing engines. In other words, the cluster hardware is better utilized when there are different kinds of processing present on the system. Understanding the cause of this “miracle” result helped us improve our test scenario. Our latest multi-tenant work- loads run many",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "us improve our test scenario. Our latest multi-tenant work- loads run many concurrent Impala queries and MapReduce jobs, so that the system resources are fully utilized regard- less of statistical multiplexing. Also, we run continuous streams of MapReduce jobs and Impala queries, such that for the duration of measurement, there will always be two different frameworks competing for resources. 7 P RACTITIONER USE OF BENCHMARKS There are few cases when a big data practitioner would need to run a benchmark:\u000fValidating an existing system following a system upgrade or migration \u000fCompare between technologies for a new system \u000fAssessing the impact of workload changes In our experience, benchmarks are used in different ways in each scenario. When upgrading or migrating an existing system, bench- marks validate whether the new infrastructure",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "or migrating an existing system, bench- marks validate whether the new infrastructure delivers expected performance. It is key to ensure apples-to-apples comparisons between different setups. The new infrastructure should be validated with the existing workload. If the workload includes batch jobs, simply replicating data to the new system and running the batch jobs is all that is required. If the workload is more interactive, then a load-generation harness such as HP LoadRunner [18] or Apache JMeter [4] is often used. In some cases, the speciﬁc production workload cannot be replicated in the new environment. In those cases, it is very common to choose an industry standard benchmark to try to emulate the production workload. When trying to compare technologies for a newly de- signed system, insist on full",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "to compare technologies for a newly de- signed system, insist on full disclosure, and make sure the benchmarks used are a good substitute for the workload planned for the cluster. Speciﬁcally, ensure the benchmark report makes apples-to-apples comparisons against compet- ing technologies. Some common benchmarks used include: Terasort and SWIM [6] for MapReduce, TPC-DS [32] and TPC-H [31] for SQL-on-Hadoop, and YCSB [33] for NoSQL key-value stores. Depends on the workload planned for the cluster, they may or may not be appropriate. The gold standard for validating results is indepen- dent audit. Some commericial vendors who use industry standard benchmark show such results. An alternative to independent audit is to try to reproduce the reported results on a pre-production environment. We have seen cases where a published performance",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "on a pre-production environment. We have seen cases where a published performance result cannot be reproduced on identical trial systems provided by cluster operators. When running a home-grown benchmark kit based on real workloads, independent audit is nearly impossible and reproducing the result may simply reproduce built-in errors. There, a good practice is to compare the measured perfor- mance to published results of similar systems, review the differences, and see whether the performance differences can be explained with a reasonable model. We discussed some examples of reviewing differences in Section 2 and the importance of performance models in Section 4. 8 R ELATED WORK 8.1 Qualities of a good benchmark The criteria for a good performance benchmark have been the topic of decades of publications [15], [16], [19].",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "benchmark have been the topic of decades of publications [15], [16], [19]. Prior work has identiﬁed the following essential properties: \u000fRepresentative: The benchmark should measure per- formance under real life environments and use met- rics that are relevant to real life applications. \u000fPortable: The benchmark should be fair and portable to competing solutions that target the needs of the same applications. [Página 8] 1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing 8 Benchmark Publications SPECjbb (2000 - 2005) 1,050 TPC-C 760 SPEC SFS",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "8 Benchmark Publications SPECjbb (2000 - 2005) 1,050 TPC-C 760 SPEC SFS 730 SPECweb (96 - 2009) 700 TPC-D/H 650 TABLE 1 Benchmark Result Publications \u000fScalable: The benchmark should measure the perfor- mance of systems within a wide range of scale. As technology progresses, systems increase in scale and performance capabilities. The benchmark should be able to accommodate for that increase. \u000fVeriﬁable: The benchmark should prescribe repeat- able measurements that produce the same results and can be independently veriﬁed. \u000fSimple: The conceptual elements of the benchmark should be reduced to a minimum and made easily understandable. The benchmark should also abstract away details that represent case-by-case conﬁgura- tions or system administration choices that do not affect performance. Selecting a benchmark with the above qualities is a ﬁrst step",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "performance. Selecting a benchmark with the above qualities is a ﬁrst step towards addressing many of the pitfalls identi- ﬁed: non-representative benchmarks lead to the unrealis- tic benchmarks pitfall, non-portable benchmarks make it easier to commit the comparing apples-to-oranges pitfall, non-scalable benchmarks lead to the not testing at scale pitfall, non-veriﬁable benchmarks make it easier to believe in miracles, non-simple benchmarks make it easier to mis- communicate results. Unfortunately, the crowded ﬁeld of emerging big data benchmarks often fall short on the “representative” charac- teristic. The two most critical shortcomings we see are (1) failing to capture a multi-job, multi-query workload and (2) failing to provide empirical evidence to justify the choice of jobs, queries, and data that are included in the benchmark. Our prior work [8] contains",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "data that are included in the benchmark. Our prior work [8] contains a critique of several recent big data benchmarks. 8.2 Successful benchmarks and their making A few benchmarks have reached the level of active indus- try standards. When it comes to benchmarks measuring complete or end-to-end systems, two organizations have dominated: SPEC and TPC. Each organization has published a number of bench- marks with various degrees of success. One criteria for success is the level at which the benchmark is being used by various organizations. While internal use is difﬁcult to quantify, external publication of benchmark results is easy to tally and represents a clear success criteria. Table 1 shows the most published benchmarks from TPC [28] and SPEC [27]. Of these benchmarks, TPC-C and TPC-D/H followed",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "TPC [28] and SPEC [27]. Of these benchmarks, TPC-C and TPC-D/H followed a similar process of ﬁnding representative customer work- loads that provide insight regarding how to create a big data benchmark. Little has been written about the insider’s views of the benchmarks deﬁnition process. The making of TPC-C is published only recently [10].The key ideas from this process are: \u000fGround the benchmark based on empirical survey of customer use cases, in TPC-Cs case a survey of hundreds of customers across multiple countries. \u000fDevelop abstract functions, datasets, and execution scheduling models that cover common characteris- tics across use cases without being burdened by the speciﬁc quirks of any single use case. \u000fSpecify the benchmark in a technology agnostic fash- ion to ensure the benchmark is portable. \u000fSpecify the",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "technology agnostic fash- ion to ensure the benchmark is portable. \u000fSpecify the benchmark with special attention to how should the benchmark scale the functions, datasets, and execution scheduling. \u000fBuild the benchmark execution harness with spe- cial attention to how the harness can scale without adding overhead. \u000fEnsure the benchmark behaves deterministically, or at least within statistical bounds, so that the bench- mark can be rigorously audited. The authors are involved in ongoing efforts to develop the TPC Decision Support (TPC-DS) benchmark for big data. These considerations present some of the hardest technical challenges, especially because the benchmark has to ensure the results have technical merit despite competing commeri- cial interests from different test sponsors. 8.3 Parametric vs empirical models A more theoretical consideration brought about by big data",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "vs empirical models A more theoretical consideration brought about by big data concerns what kind of models benchmarks should employ to generate the load and the data. The traditional approach is to use analytical models with a small number of parameters. For example, a common parametric model for arrival patterns is the Poisson or memoryless arrival model, used previously to generate network trafﬁc [22]. A common parametric model for data patterns is the Zipf or long-tail frequency model, used for populating synthetic databases [17]. This approach works less well for big data, because the complex, diverse, non-stationary nature of the customer workloads make it hard to capture representative behav- ior using simple statistical processes with a small number of parameters. An alternative is to use empirical models, where",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "small number of parameters. An alternative is to use empirical models, where the workload traces arethe model. One can think of empirical models as models with an inﬁnite number of parameters. Recent work has started shifting towards empirical mod- els, for example, showing that TELNET and FTP session ar- rivals approximate Poisson processes whose average arrival rates are empirical constants that change at the hourly or ﬁner granularity [22]. A recent and successful MapReduce benchmark uses a fully empirical model, with the bench- mark test workload being a statistical sample of the original historical workload trace [6]. The shift is an interesting one, because it illustrates that big data benchmarks sometimes need to compromise simplicity (favoring analytic models) to achieve represen- tativeness (favoring empirical models). Furthermore, both kinds",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "analytic models) to achieve represen- tativeness (favoring empirical models). Furthermore, both kinds of models will fail to completely capture complex, non-stationary behavior [23]. [Página 9] 1939-1374 (c) 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI 10.1109/TSC.2015.2494595, IEEE Transactions on Services Computing 9 Once people reading benchmark results and people producing benchmark results get past the basic pitfalls discussed earlier, they would confront deeper technical challenges such as the choice of benchmark models, and whether that helps or hinders understanding system behav- ior in real-life. 9 C ONCLUSION Performance benchmarking is a",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "system behav- ior in real-life. 9 C ONCLUSION Performance benchmarking is a challenging task. When done well, benchmarks can guide ourselves as well as the community. Cloudera is a leading vendor in big data, and we make special effort to ensure our performance studies are fair, rigorous, and thus useful to ourselves and our customers. The stories here show that even with good intent and best practices, performance benchmarking is fraught with challenges. Anyone can make benchmarking errors, everyone can learn from them, and everyone can beneﬁt from reviewing their own work. ACKNOWLEDGMENTS We would like to thank our colleagues at Cloudera who helped with various studies cited in this paper, especially Sandy Ryza, Karthik Kambatla, Jeff Bean, Justin Erickson, David Rorke, Dileep Kumar, and Arun Singla. REFERENCES",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "Jeff Bean, Justin Erickson, David Rorke, Dileep Kumar, and Arun Singla. REFERENCES [1] Apache Software Foundation, “Apache Hadoop NextGen MapReduce (YARN),” http://hadoop.apache.org/docs/current/ hadoop-yarn/hadoop-yarn-site/YARN.html. [2] Apache Software Foundation, “Apache Sqoop,” http://sqoop. apache.org. [3] Apache Software Foundation, “With Fair Scheduler, cluster can logjam when all resources are consumed by AMs,” Apache Jira YARN-1913, https://issues.apache.org/jira/browse/YARN-1913. [4] Apache Software Foundation, “Apache JMeter,” http://jmeter. apache.org/. [5] J. Bean, “Apache Hadoop YARN: Avoiding 6 Time-Consuming ‘Gotchas’,” Cloudera Developer Blog, 2014, http://blog.cloudera.com/blog/2014/04/ apache-hadoop-yarn-avoiding-6-time-consuming-gotchas/. [6] Y. Chen, S. Alspaugh, A. Ganapathi, R. Grifﬁth, and R. Katz, “Sta- tistical workload injector for mapreduce,” https://github.com/ SWIMProjectUCB/SWIM/wiki. [7] Y. Chen, S. Alspaugh, and R. Katz, “Interactive Query Processing in Big Data Systems: A Cross-Industry Study of MapReduce Workloads,” in VLDB 2012 . [8] Y. Chen, A. Ganapathi, R. Grifﬁth,",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "Workloads,” in VLDB 2012 . [8] Y. Chen, A. Ganapathi, R. Grifﬁth, and R. Katz, “The Case for Evaluating MapReduce Performance Using Workload Suites,” in MASCOTS 2011 . [9] Y. Chen, P . Gokhale, and A. Singla, “Conﬁguring Impala and MapReduce for Multi-tenant Performance,” Cloudera Developer Blog, 2013, http://blog.cloudera.com/blog/2013/06/ conﬁguring-impala-and-mapreduce-for-multi-tenant-performance/. [10] Y. Chen, F. Raab, and R. Katz, “From tpc-c to big data benchmarks: A functional workload model,” Lecture Notes on Computer Science , vol. 8163, 2014. [11] Cloudera Inc., “Cloudera Impala,” http://www.cloudera.com/ content/cloudera/en/products-and-services/cdh/impala.html. [12] J. Dean and S. Ghemawat, “Mapreduce: Simpliﬁed data processing on large clusters,” in OSDI 2004 . [13] J. Erickson, M. Kornacker, and D. Kumar, “New SQL Choices in the Apache Hadoop Ecosystem: Why Impala Continues to Lead,” Cloudera Developer Blog, 2014, http://blog.cloudera.com/blog/2014/05/new-sql-choices-in- the-apache-hadoop-ecosystem-why-impala-continues-to-lead.[14]",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "Ecosystem: Why Impala Continues to Lead,” Cloudera Developer Blog, 2014, http://blog.cloudera.com/blog/2014/05/new-sql-choices-in- the-apache-hadoop-ecosystem-why-impala-continues-to-lead.[14] J. Erickson, G. Rahn, M. Kornacker, and Y. Chen, “Impala Per- formance Update: Now Reaching DBMS-Class Speed,” Cloudera Developer Blog, 2014, http://blog.cloudera.com/blog/2014/01/ impala-performance-dbms-class-speed/. [15] D. Ferrari, Computer systems performance evaluation . Prentice-Hall, 1978. [16] J. Gray, “The Benchmark Handbook For Database and Transaction Processing Systems - Introduction,” in The Benchmark Handbook For Database and Transaction Processing Systems , J. Gray, Ed. Morgan Kaufmann Publishers, 1993. [17] J. Gray, P . Sundaresan, S. Englert, K. Baclawski, and P . J. Wein- berger, “Quickly generating billion-record synthetic databases,” in SIGMOD 1994 . [18] Hewlett-Packard, “HP LoadRunner,” http://www8.hp.com/us/ en/software-solutions/loadrunner-load-testing/. [19] K. Huppler, “The art of building a good benchmark,” in TPC Technical Conference 2009 . [20] K. Kambatla and",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "good benchmark,” in TPC Technical Conference 2009 . [20] K. Kambatla and Y. Chen, “The Truth About MapReduce Perfor- mance on SSDs,” in LISA 2014 . [21] A. Pavlo, E. Paulson, A. Rasin, D. J. Abadi, D. J. DeWitt, S. Mad- den, and M. Stonebraker, “A Comparison of Approaches to Large- scale Data Analysis,” in SIGMOD 2009 . [22] V . Paxson and S. Floyd, “Wide area trafﬁc: the failure of poisson modeling,” Networking, IEEE/ACM Transactions on , vol. 3, no. 3, Jun 1995. [23] V . Paxson, “Empirically derived analytic models of wide-area tcp connections,” IEEE/ACM Trans. Netw. , vol. 2, no. 4, Aug. 1994. [24] S. Ryza, “Getting MapReduce 2 Up to Speed,” Cloudera Developer Blog, 2014, http://blog.cloudera.com/blog/2014/02/ getting-mapreduce-2-up-to-speed/. [25] The Apache Software Foundation Blog, “The",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "Developer Blog, 2014, http://blog.cloudera.com/blog/2014/02/ getting-mapreduce-2-up-to-speed/. [25] The Apache Software Foundation Blog, “The Apache Software Foundation Announces Apache Hadoop 2,” https://blogs.apache.org/foundation/entry/the apache software foundation announces48. [26] The Sort Benchmark, http://sortbenchmark.org/. [27] The Standard Performance Evaluation Corporation, http://www. spec.org/. [28] Transaction Processing Council, www.tpc.org. [29] Transaction Processing Council Auditors, http://www.tpc.org/ information/who/whoweare.asp#auditors. [30] Transaction Processing Council, “TPC-C Benchmark,” http:// www.tpc.org/tpcc/. [31] Transaction Processing Council, “TPC-H Benchmark,” http:// www.tpc.org/tpch/. [32] Transaction Processing Council, “TPC-DS Benchmark,” http:// www.tpc.org/tpcds/. [33] YCSB Community, “Yahoo! Cloud Serving Benchmark,” https:// github.com/brianfrankcooper/YCSB/. Gwen Shapira is a Software Engineer on the Platform Engineering team at Cloudera, working on data ingest products. She has 15 years of experience working with customers to design scalable data architectures. She specializes in migrating data warehouses to Hadoop, integrat- ing Hadoop with relational databases,",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "in migrating data warehouses to Hadoop, integrat- ing Hadoop with relational databases, building scalable data processing pipelines, and scaling complex data analysis algorithms. Yanpei Chen is a software engineer on the Performance Engineering team at Cloudera. He works on multiple Hadoop ecosystem com- ponents including MapReduce, Impala, Solr, HBase, and Hive, because someone has to make sure the entire Hadoop ecosystem per- forms well together. His work involves internal engineering optimizations and external competi- tive benchmarking and customer support.",
        "start_idx": 7424,
        "end_idx": 7503
      }
    ],
    "cf15c644-78c4-4c4e-ade1-dbd066633f5f": [
      {
        "text": "[Página 1] A Big Data Solution for Troubleshooting Mobile Network Performance Problems K. Skračić, I. Bodrušić Ericsson Nikola Tesla, Zagreb, Croatia E-mail: kristian.skracic@ericsson.com Abstract - Big Data has become a major competitive advantage for many org anizations. The analytical capabilities made possible by Big Data analytics platforms are a key stepping stone for advancing the business of every organization. This paper illustrates the development of a big data analytics system for mobile telecommunication systems. The authors developed a solution for analyzing data produced by mobile network nodes which contain data relevant for predictive maintenance and troubleshooting purposes. The solution is built around the problem of working with small files in t he Hadoop environment. The logs collected from mobile ne twork nodes are small binary files between 5",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "collected from mobile ne twork nodes are small binary files between 5 and 15MB in size. These binary log files need to be decoded to a readable format, and then analyzed to extract useful information. In this paper, the authors provided a benchmark of various scenarios for collecting and decoding the binary log files in a Hadoop cluster. As a result, the scenario with the highest performance has been used in the implementation of our solution. The developed solution has been built and tested on a live Hadoop cluster using real-world data obtained from several telecom operators around the world. I. I NTRODUCTION The telecommunications industry is one of the largest and fastest growing industries in the world. Over the past few decades we have witnessed the change",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "the world. Over the past few decades we have witnessed the change from 2G, 3G, 4G and now 5G in the near future [1]. With the rising demand for constant connectivity, the availability of telecommunication systems and their various componentshas never been more important. This trend is particularly apparent in the case of mobile networks. As shown in [2], during 2016 there were 7.5 billion mobile subscriptions in the world. The report in [2] estimates that in 2022 there will be 8.9 billion mobile subscriptions, 8 billion mobile broadband subscriptions and 6.1 billion unique mobile subscribers in the world. Mobile networks allow a subscriber to consume various telecommunication services via mobile phone from any place of coverage. Thus, they have become one of the most important resources today.",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "coverage. Thus, they have become one of the most important resources today. This paper proposes a so lution for analyzing data produced by mobile network nodes which containinformation relevant for pr edictive maintenance and troubleshooting purposes. As telecommunication systems are becoming larger and more advanced, it is necessary to constantly monitor and measure the performance of their various subsystems. However, larger networks and higher Internet access speeds carry with them the need to analyze larger amounts of data in a short period of time, in order to prevent network outages as soon as possible. Such requirements can be addressed by leveraging the analytical capabilities made possible by Big Data analytics platforms [3]. Apart from improved performance, Big Data can enable deeper analytics by providing access to historical data.",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "Big Data can enable deeper analytics by providing access to historical data. For example, by storing network performance data it becomespossible to compare the current results with those obtained in past measurements. By storing the insight obtainedthrough troubleshooting, it becomes possible to predict and prevent the same failures from happening again, or at least to shorten the response time when the same or similar failures present themselves again [4]. In this paper, the authors developed a Big Data solution for analyzing data produced by mobile network nodes, which contain important information used for troubleshooting purposes. The solution ingests large amounts of logs which contain network event information. The logs are gathered through an event-based monitoring (EBM) system, which is an embedded recording tool in the Ericsson EPG, SGSN",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "system, which is an embedded recording tool in the Ericsson EPG, SGSN and MME nodes [5]. This paper is organized as follows. Section II provides an overview of existing research on small files in the Hadoop environment, as well as the research on the applicability of Big Data solutions in the telecommunications industry. Section III shows the architecture of the developed solution and how the small files problem in Hadoop impacts it. Section IV shows the benchmark results for the scenarios laid out in Section III. II. R ELATED WORK This section provides an overview of the results of existing research in the field of applying Big Data in the telecommunications industry, as well as previous work done on the analysis of small files in Hadoop. A. Big",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "work done on the analysis of small files in Hadoop. A. Big Data Analytics Platforms in the Telecommunications Industry Big Data solutions have become an important part of today’s industry for all types of businesses, such as finance [6], law enforcement [7], education [8] and others.To show the applicability of Big Data solutions in the telecommunications industry, we briefly summarizedsome existing use cases and their impact on the industry. Telecom operators have access to large amounts of valuable data that can be used for various analytical usecases. The research in [9] shows a way to leverage Big 472MIPRO 2017, May 22- 26, 2017, Opatija, Croatia [Página 2] Data analytics for classifying subscribers based on their movement. Reusing existing data for new use cases such as this is the",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "Reusing existing data for new use cases such as this is the first step in data monetization, which is expected to become a major source of income for all types of businesses in the near future [10]. The work in [11] is able to predict customer churn for telecom operators,which has a direct impact on the operator’s profitability. Similarly, the work in [12] predicts customer experience when using over-the-top (OTT) applications such as WhatsApp or Viber. As the Internet of Things (IoT) is evolving, it is expected to have an impact in the way telecommunications providers analyze the large amounts of sensor data such systems bring with them [13]. We would also like to note that the move to Big Data has a big impact on network infrastructure",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "the move to Big Data has a big impact on network infrastructure evolution , as such systems require higher link speeds to transfer the data form one node to another [14]. The solution developed in this paper is focused on mobile network performance troubleshooting. Thus, it is used to calculate various key performance indicators (KPIs) relevant for this domain. KPI measurement is frequently used by mobile operators and mobile network infrastructure vendors as a means to systematically search and identify network bottlenecks and anomalies [15]. B. Analyzing Small Files in the Hadoop Environment Hadoop is an open-source software framework used for distributed storage and pr ocessing of very large data sets [16]. The Hadoop distributed file system (HDFS) has been widely adopted as the standard for storing data",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "system (HDFS) has been widely adopted as the standard for storing data in Hadoop based clusters [17]. In the Hadoop ecosystem, access to stored data is handled by a system called Namenode, which manages the file system namespace and regulates client access. First the client asks the Namenode for instructions on where to find the files it needs to read, as well as the location of a free block it can write to [18].Figure 1 illustrates this process. DataNodes provide block storage and serve I/O requests from clients. Namenode Client Datanode Datanode Datanode1: Get block locations 2: read/write operations 2: read/write operations Figure 1. HDFS system overview A major drawback of HDFS is its poor performance with large numbers of small files, which has attracted significant attention [19].",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "with large numbers of small files, which has attracted significant attention [19]. According to the research in[19], the main reasons for such lower performance are: /g120large numbers of small files impose a heavy burden on NameNode memory;/g120correlations between small files are not considered for data placement; /g120no optimization mechanism, such as prefetching, is provided to improve I/O performance We would like to note that when small files are stored on HDFS, disk utilization is not a bottleneck. The research in [20] shows tha t a small file stored on HDFS does not take up any more disk space than is required to store its contents. More precisely, a 6 MB file stored with an HDFS block size of 128 MB uses 6 MB of disk space, not 128",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "size of 128 MB uses 6 MB of disk space, not 128 MB. HDFS is designed to read/write large files, and provides no optimization for handling small files. In cases where large amounts of small files are accessed directly in HDFS, a mismatch of accessing patterns will emerge [21].HDFS will ignore the optimization offered by the native storage resource, which will lead to local disk access becoming a bottleneck [22]. Additionally, in such a scenario data prefetching is not employed to improve access performance for HDFS [22]. The research in [21] considers all files smaller than 16MB as small files, although no justification or proof were provided as to why this size was chosen as the cut-off point between large and small files in the context of HDFS",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "cut-off point between large and small files in the context of HDFS .The research in [19] has quantified this cut-off point through experimentation. The study indicates that access efficiency starts to drop significantly with files smaller than 4.35 MB. The small file processing problem in Hadoop has seen many different solutions with various levels of success,depending on the nature of the data. One of these is the merging of multiple small files into a single bigger file, which has shown some significant performance improvements [21], [23]. This paper explores different scenarios in which this solution can be applied to mobile network data. The scenarios are explained in more detail in the following sections. III. S YSTEM OVERVIEW This section provides an overview of the developed Big Data solution",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "OVERVIEW This section provides an overview of the developed Big Data solution for mobile network performance troubleshooting . A. Data Collection Event data has been used for various troubleshooting purposes [5]. The event data is collected from EPG, SGSN and MME nodes within the core network. The environment is based on the Evolved Packet Core (EPC) [24], as sown on Figure 2. This study uses only event data generated by these nodes. The authors used an event - based monitoring (EBM) system, which is an embeddedrecording tool in the Ericsson EPG, SGSN and MME. We collected events on 2G, 3G and 4G networks. The event data is collected in small log files which are between 5 and 15 MB in size, depending on the configuration. The overall size",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "and 15 MB in size, depending on the configuration. The overall size and and velocity of the logs depends on the size of the network (e.g. number of base stations/eNodeB, number of EPG, SGSN and MME 473 [Página 3] nodes, overall network throughput). An average operator will generate around 200 GB of logs per day. The log files are stored in a binary format which needs to be decoded to text (usually CSV) in order to be processed. After the decoding process, the files are up to 10 times larger than in their binary format. EBM logs contain information that documents successful and unsuccessful events for comp leted mobility and session management procedures. As shown in the following section, the developed solution was tested on several small and",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "the following section, the developed solution was tested on several small and large networks around the world. GSM Network eNodeBBSC RNC SGW PGWMMESGSNGGSN WCDMA Network LTE Network Trusted Non-3GPP Network Untrusted Non-3GPP NetworkRBSRBS PDN PDNGPRS Network EPC NetworkRadio Networks External Networks Figure 2 - Evolved Packet Core (EPC) schema B. Architecture Figure 3 shows the architecture of the developed solution. Apache Flume is used to transfer the data from a network location to HDFS. Flume was chosen because it is a widely used distributed, reliable and available service for efficiently collecting, aggregating and moving large amounts of streaming event data [25]. The binary logs are usually dumped to a server, which is usually somewhere in the operator’s network. Using Flume, the binary logs are transferred to the cluster",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "operator’s network. Using Flume, the binary logs are transferred to the cluster that hosts the proposed solution. Although the proposed solution can be deployed within the operator’s networ k, we argue that a centralized off-site deployment is more appropriate. A centralized approach enables data aggregation from various networks into a single cluster, and thus enriches the data with variety that comes from different network configurations and environments. HDFS is used to store the raw binary log files until they are decoded. A MapReduce job is used to decode the binary files into CSV. The decoding process is explained in more detail in the following section. The decoded CSV files are imported into an Apache Hive database [26]. Hive offers an SQL-like query language which enables data access. The",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "[26]. Hive offers an SQL-like query language which enables data access. The developed solution has a number of Hive queries that calculate various KPI’s, which provide insight about mobile network bottlenecks. This is a quick method of finding out which parts of the network are worth looking into during troubleshooting. The results, or KPI’s calculated from such queries, are stored in a separate relation database, which is based on PostgreSQL. External applications can also connect to the Hive database and query the data to calculate KPI’s relevant for mobile network troubleshooting. One of the goals for this solution is to enable data mining. Mobile network experts can connect to the Hive database either though the Hive shell or by using a visualization tool like Tableau. Using the original",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "shell or by using a visualization tool like Tableau. Using the original measurement data stored in Hive, mobile network experts can extract new insight and get to the root cause of a problem. This is often not possible with aggregated KPI data because it hides much of the information it is derived from. Hive provides several mechanisms for optimizing the storage of the data and query performance. The developed solution makes use of partitioning and bucketing functionalities offered by Hive. Partitioning in Hive is the process of horizontally dividing the data into a number of smaller and more manageable slices. Every partition is stored as a directory within a data warehouse table in Hive. The developed solution partitions the decoded CSV data based on the event identifier (or",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "solution partitions the decoded CSV data based on the event identifier (or event name) attribute. Binary log files locationFlume HDFS HIVE SCRIPTS for KPI calculation Visualization platformMapReduce decoder Hive databaseDecoded CSV log file PostgreSQL database Figure 3 – Architecture of the developed solution Bucketing is another technique of decomposing data into more manageable parts. This optimization method distributes the data evenly across multiple files. It is used to distribute and organize the table or partition data into multiple files so that similar records are present in the same file. The value of this column will be hashed into buckets by a user-defined number. Bucketing has many performance benefits, most notably faster Map side joins, more efficient grouping using “Group By” statements and more efficient sampling. As the developed",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "grouping using “Group By” statements and more efficient sampling. As the developed solution is used by mobile network experts, such statements are used very often when accessing the Hive database directly. 474 [Página 4] C. Log Decoder and the Im pact of Small Files in Hadoop The developed solution implements a MapReduce job to decode the binary files into the CSV format. The MapReduce job first reads the binary files in memory and then decodes them in parallel. The number of parallel decoding jobs is de fined by the number of input splits in Hadoop. This is where the small files problem influences the developed solution. If each raw log file is decoded separately, it will have a negative performance impact. In the developed solution, we tried to",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "have a negative performance impact. In the developed solution, we tried to influence the number of input splits by combining multiple raw log files into one larger file. More precisely, we tested the performance impact of using small files with the following scenarios: /g120Scenario 1: the raw logs are stored as small files in HDFS and they are directly used as input splits in the MapReduce job. A custom Hadoop input reader is used to read th e binary log files, which is based on the native RecordReader class in Hadoop (org.apache.hadoop.mapreduce.RecordReader ) /g120Scenario 2: the raw logs are combined into larger files, stored in HDFS and then decoded using MapReduce jobs. Each line in the combined file is a hexadecimal representation of the binary log file. In",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "combined file is a hexadecimal representation of the binary log file. In this scenario, Flume is used to combine the raw log files. Thus, a native Hadoop input reader is used (located in org.apache.hadoop.mapreduce.lib.input.TextInput Format ) /g120Scenario 3: the raw logs are combined into larger files, stored in HDFS and decoded using MapReduce jobs with only mappers and no reducers. Like in Scenario 2, each line in the input file contains a single file, and Flume is used to combine the raw log files IV. R ESULTS For the purposes of this study, a Hadoop -based cluster was used to evaluate the developed solution. The cluster is based on the Hortonworks Data Platform (HDP) [27] and is composed of 10 servers (2 masters and 8 slaves). The master",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "is composed of 10 servers (2 masters and 8 slaves). The master nodes have 2 model E5-2630 CPU-s, 128GB of RAM and 6 hard disk drives (HDD), each with 3TB of space. The slave nodes have 1 model E5- 2623v3 CPU, 64GB of RAM, and 8 HDDs, each with 2TB of space. Each node runs on top of CentOS v7, which is installed ona separate SSD disk which is not part of the HDFS. The input data was collected from several small and large networks around the world, including operators from Europe, North and South America, and Southeast Asia. A. Small Files D ecoding Benchmark The combined files in Scenario 2 and 3 were grouped into larger files. Several performance tests were carried out on batches of 2,",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "larger files. Several performance tests were carried out on batches of 2, 9 and 33 GB of raw log files. Table 1 shows how the decoder performs in Scenario 1, when the small log files are used directly. It can be seen that the MapReduce decoder is having difficulties processing even the smaller batches of 2 and 9 GB of raw logs. As stated in previous sections, the reason for this is the large number of small files that is imposing a heavy burden on NameNode memory. In contrast, Scenario 2 (Table 2) shows a significant performance improvement of the MapReduce decoder. Also, we used 18 reducers in Scenario 2, one for each event type available through the logging system. Undoubtedly, the best performance was achieved in Scenario",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "through the logging system. Undoubtedly, the best performance was achieved in Scenario 3 by combining the input files into larger files and using map-only jobs (Table 3). The reason for such an improvement is that both the shuffle-sort and the reduce phases of the MapReduce job are skipped, thus drastically reducing the amount of processing power and memory needed to decode a large input file. For the largest batch of 33 GB, we can see that there is a 37% improvement compared to Scenario 2. We would like to note that this improvement increases with batch size (Figure 4). Figure 5 shows the performance gain for each scenario. The performance improvements rise with the batch size, which is traditionally not the case for solutions that are not based",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "which is traditionally not the case for solutions that are not based on Big Data. The drawback of using the approach in Scenario 3 is that the output of the decoder is split into several smaller files which need to be imported into Hive. This is due to the way MapReduce jobs work. The intermediate results of the mappers are shuffled and sorted before being delivered to the reducers. By having only map jobs in our decoder, the unsorted intermediate results become the output. In contrast, when the reducers are used we can influence the number of files that will be generated. For example, each event type could be stored into a separate file. However, the files can easily be merged within Hive,as the output is textual (CSV).",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "files can easily be merged within Hive,as the output is textual (CSV). Also, the partitioning and bucketing in Hive restructures the physical layout of the data, so that the output of the map-only decoder does not influence the performance of the Hive queries. TABLE 1. D ECODER BENCHMARK FOR SCENARIO 1–USING SMALL LOG FILES AS INPUT Raw log size (GB) Seconds Minutes 2.00 843 14.05 9.00 3206.00 53.43 33.00 13740 229.00 TABLE 2. D ECODER BENCHMARK FOR SCENARIO 2–USING 18 REDUCERS AND SEVERAL LARGER COMBINED INPUT FILES Raw log size (GB) Seconds Minutes 2.00 428.00 7.13 9.00 1405.00 23.41 33.00 7282.00 121.36 TABLE 3. D ECODER BENCHMARK FOR SCENARIO 3–USING ONLY MAPPERS AND SEVERAL LARGER COMBINED INPUT FILES Raw log size (GB) Seconds Minutes 2.00 348.00 5.80 9.00",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "INPUT FILES Raw log size (GB) Seconds Minutes 2.00 348.00 5.80 9.00 883.00 14.71 33.00 3194.00 53.23 475 [Página 5] B. Comparison with existing solutions In order to clearly show the performance benefits when using big data solutions and technologies in this scenario, the study also shows the benchmark for decoding EBM logs without the proposed solution. Table 4 shows the performance benchmark on a single server, with the same hardware as the master node in the HDP cluster. As shown in table Table 4, the proposed solution is up to 6 times faster than existing solutions. The largest set of logs was unable to be measured since it was not possible with the existing solutions. We note that the performance gains represent only one benefit of the",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "We note that the performance gains represent only one benefit of the proposed solution. The main advantage of the proposed solution is the ability to process a muchlarger set of logs than was possible with legacy solutions. Also, the proposed solution provides a way to continuously gather and store logs for deeper analytics, which was no the case with legacy solutions. TABLE 4EXISTING SOLUTIONS BENCHMARK Raw log size (GB) Seconds Minutes 2.00 1264.00 21.07 9.00 5538.00 92.30 33.00 N/A N/AV. C ONCLUSION Big Data analytics can provide insight into the data available within the telecommunications industry.This paper demonstrates one usecase in which analytics can be leveraged to improve the efficiency and value of troubleshooting in mobile networks. The main advantage of the developed solution is its ability to",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "networks. The main advantage of the developed solution is its ability to adapt to any new analytical requests, as well as the ability to adapt to changing input file sizes. More precisely, the results of this study show that the developed solution is capable of processing small files in an efficient manner within the Hadoop environment, which was not built for processing large amounts of small files. Additionally, the study shows that by skipping the reduce phase we can decrease the execution time of the MapReduce job used for decoding. This was shown to be the most time-consumingprocess. The developed solution was tested using log data collected in various small and large networks from around the world, which demonstrates its applicability for mobile network troubleshooting. The developed solution",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "world, which demonstrates its applicability for mobile network troubleshooting. The developed solution has proven that Big Data platforms are suitable for processing large batches of mobile network data, and that they bring significant performance and scalability improvements co mpared to traditional solutions. Future research may include the use of other Big Data tool that run on the Hadoop platform. Most notably, the use of Apache Cassandra instead of the Hive, and the use of Apache Spark as a replacement to the MapReduce decoder job. A CKNOWLEDGMENT This study was fully funded by Ericsson Nikola Tesla. The authors thank their leadership team for providing an environment in which it was possible to combine research and industry into one. R EFERENCES [1] G. Fettweis and S. Alamouti, “5G: Personal mobile",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "one. R EFERENCES [1] G. Fettweis and S. Alamouti, “5G: Personal mobile internet beyond what cellular did to telephony,” IEEE Communications Magazine , vol. 52, no. 2, pp. 140–145, Feb. 2014. [2] C. Patrik, L. Anette, and J. Peter, “Ericsson Mobility Report,” EAB-16:018498 Uen, Revision A, Nov. 2016. [3] D. Šipuš, “Big data analytics for communication service providers,” in 2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO) , 2016, pp. 513–517. Figure 4 – Batch processing benchmark for 3 considered Scenarios Figure 5 – Scenario to scenario processing time reduction comparison 476 [Página 6] [4] L. Yang, G. Kang, W. Cai, and Q. Zhou, “An Effective Process Mining Approach against Diverse Logs Based on Case Classification,” in 2015 IEEE International Congress on Big",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "Logs Based on Case Classification,” in 2015 IEEE International Congress on Big Data , 2015, pp. 351–358. [5] I. da Silva, Y. Wang, F. Mismar, and W. Su, “Event-based performance monitoring for inter-system cell reselection: A SON enabler,” in 2012 International Symposium on Wireless Communication Systems (ISWCS) , 2012, pp. 6–10. [6] A. Munar, E. Chiner, and I. Sales, “A Big Data Financial Information Management Architecture for Global Ban king,” in 2014 International Conference on Future Internet of Things and Cloud , 2014, pp. 385–388. [7] A. Jain and V. Bhatnagar, “Crime Data Analysis Using Pig with Hadoop,” Procedia Computer Science , vol. 78, pp. 571–578, Jan. 2016. [8] F. Xhafa, D. Garcia, D. Ramirez, and S. Caballé, “Performance Evaluation of a MapReduce Hadoop-Based Implementation for Processing Large",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "S. Caballé, “Performance Evaluation of a MapReduce Hadoop-Based Implementation for Processing Large Virtual Campus Log Files,” in 2015 10th International Conference on P2P, Parallel, Grid, Cloud and Internet Computing (3PGCIC) , 2015, pp. 200–206. [9] B. Furletti, L. Gabrielli, C. Renso, and S. Rinzivillo, “Analysis of GSM calls data for understanding user mobility behavior,” in 2013 IEEE International Conference on Big Data , 2013, pp. 550– 555. [10] H. Cao et al. , “SoLoMo analytics for telco Big Data monetization,” IBM Journal of Research and Development , vol. 58, no. 5/6, p. 9:1-9:13, Sep. 2014. [11] H. Li, D. Yang, L. Yang, YaoLu, and X. Lin, “Supervised Massive Data Analysis for Telecommunication Customer Churn Prediction,” in 2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "2016 IEEE International Conferences on Big Data and Cloud Computing (BDCloud), Social Computing and Networking (SocialCom), Sustainable Computing and Communications (SustainCom) (BDCloud-SocialCom-SustainCom) , 2016, pp. 163–169. [12] E. Diaz-Aviles et al. , “Towards real-time customer experience prediction for telecommunication operators,” in 2015 IEEE International Conference on Big Data (Big Data) , 2015, pp. 1063–1072. [13] S. Din, H. Ghayvat, A. Paul, A. Ahmad, M. M. Rathore, and I. Shafi, “An architecture to analyze big data in the Internet of Things,” in 2015 9th International Conference on Sensing Technology (ICST) , 2015, pp. 677–682. [14] I. Tomkos, C. Kachris, P. S. Khodashenas, and J. K. Soldatos, “Optical networ king solutions and technologies in the big data era,” in 2015 17th International Conference on Transparent Optical Networks (ICTON) , 2015,",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "in 2015 17th International Conference on Transparent Optical Networks (ICTON) , 2015, pp. 1– 1.[15] S. Singh, Y. Liu, W. Ding, and Z. Li, “Evaluation of Data Mining Tools for Telecommunication Monitoring Data Using Design of Experiment,” in 2016 IEEE International Congress on Big Data (BigData Congress) , 2016, pp. 283–290. [16] K. Shvach ko, H. Kuang, S. Radia, and R. Chansler, “The Hadoop Distributed File System,” 2010 IEEE 26th Symposium on Mass Storage Systems and Technologies (MSST) , pp. 1– 10, May 2010. [17] W. Tantisiriroj, S. Patil, and G. Gibson, “Data-intensive File Systems for Internet Services: A Rose by Any Other Name... (CMU-PDL-08-114),” Parallel Data Laboratory , Oct. 2008. [18] S. Bende and R. Shedge, “Dealing with Small Files Problem in Hadoop Distributed File System,” Procedia",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "Shedge, “Dealing with Small Files Problem in Hadoop Distributed File System,” Procedia Computer Science , vol. 79, pp. 1001–1012, Jan. 2016. [19] B. Dong, Q. Zheng, F. Tian, K.-M. Chao, R. Ma, and R. Anane, “An optimized approach for storing and accessing small files on cloud storage,” Journal of Network and Computer Applications , vol. 35, no. 6, pp. 1847–1862, Nov. 2012. [20] T. White, Hadoop: The Definitive Guide . O’Reilly Media, Inc., 2009. [21] X. Liu, J. Han, Y. Zhong, C. Han, and X. He, “Implementing WebGIS on Hadoop: A case study of improving small file I/O performance on HDFS,” in 2009 IEEE International Conference on Cluster Computing and Workshops , 2009, pp. 1–8. [22] J. Shafer, S. Rixner, and A. L. Cox, “The Hadoop distributed filesystem:",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "J. Shafer, S. Rixner, and A. L. Cox, “The Hadoop distributed filesystem: Balancing portability and performance,” in 2010 IEEE International Symposium on Performance Analysis of Systems Software (ISPASS) , 2010, pp. 122–133. [23] P. Gohil, B. Panchal, and J. S. Dhobi, “A novel approach to improve the performance of Hadoop in handling of small files,” in 2015 IEEE International Conference on Electrical, Computer and Communication Technologies (ICECCT) , 2015, pp. 1–5. [24] G. Kuhn, J. Eisl, and H. Bec ker, “Co-operative handover in 3G System Architecture Evolution,” in 32nd IEEE Conference on Local Computer Networks (LCN 2007) , 2007, pp. 643–650. [25] P. B. Makeshwar, A. Kalra, N. S. Rajput, and K. P. Singh, “Computational scalability with Apache Flume and Mahout for large scale round the clock analysis",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "with Apache Flume and Mahout for large scale round the clock analysis of sensor networ k data,” in 2015 National Conference on Recent Advances in Electronics Computer Engineering (RAECE) , 2015, pp. 306–311. [26] G. P. Haryono and Y. Zhou, “Profiling apache HIVE query from run time logs,” in 2016 International Conference on Big Data and Smart Computing (BigComp) , 2016, pp. 61–68. [27] K. K. Gadiraju, M. Verma, K. C. Davis, and P. G. Talaga, “Benchmar king performance for migrating a relational application to a parallel implementation,” Future Generation Computer Systems , vol. 63, pp. 148–156, Oct. 2016. 477",
        "start_idx": 4524,
        "end_idx": 4624
      }
    ],
    "d914a09f-7c3b-40ea-b1b3-9430ba82db59": [
      {
        "text": "[Página 1] Exploring the Specificities and Challenges of Testing Big Data Systems Daniel Staegemann MRCC VLBA Otto-von-Guericke University Magdeburg, Germany daniel.staegemann@ovgu.de Matthias Volk MRCC VLBA Otto-von-Guericke University Magdeburg, Germany matthias.volk@ovgu.de Abdulrahman Nahhas MRCC VLBA Otto-von-Guericke University Magdeburg, Germany abdulrahman.nahhas@ovgu.de Abstract — Today, the amount and complexity of data that is globally produced increases continuously, surpassing the abilities of traditional approaches. Therefore, to capture and analyze those data, new concepts and techniques are utilized to engineer powerful big data systems. However, despite the existence of sophisticated approaches for the engineering of those systems, the testing is not sufficiently researched. Hen ce, in this contribution, a comparison of traditional software testing, as a common procedure, and the requirements of big data testing is drawn. The determined specificities in the big",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "of big data testing is drawn. The determined specificities in the big data domain are mapped to their implications on the implementation and the consequent challenges. Furthermore, those findings are transferred into six guidelines for the test ing of big data systems. In the end, limitations and future prospec ts are highlighted. Keywords— Big Data, System, Engineering, Verification, Validation, Testing, Benchmarking, Technologies, Guidelines I. INTRODUCTION Big data and its accompanying technologies dramatically changed many aspects of today’s world by allowing the purposeful analysis of information that would have been forfeited just a few years ago [1]. The derived opportunities of those sources of knowledge affect a plethora of application areas like healthcare [2–4], civil protection [5, 6], business [7–10], opinion mining [11] and transportation [12]. Apart from the",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "6], business [7–10], opinion mining [11] and transportation [12]. Apart from the wide applicability, the ever increasing rate of data generation exemplifies the importance of technologies to create value from this fairly new resource that has been described as digital oil [13]. While the amount of data created by modern industry in the year 2015 is stated with about 1000 exabytes, it is predicted to increase 20-fold by 2025 [14]. Due to its ubiquitousness, versatility, and impact, this topic unites researchers and practitioners in their desire to push the boundaries and facilitate more and more advanced solutions in numerous areas [15–17]. This interest shows in a multitude of different facts and metric. From a scientific perspective, the number of publications, whose title incorporates “big data”, in the scientific",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "the number of publications, whose title incorporates “big data”, in the scientific literature meta- database Scopus has grown immensely. While 3.056 of those publications are published between 2011 to 2014, the number between 2015 and 2018 adds up to 15.128 entries. Additionally, as Fig. 1 Fig. 1 depicts, there is a continuing and significant annual growth in numbers in this timespan. Fig. 1. \"Big data\" publications per year in Scopus. However, not only the scientific figures thrive, but also the economical point of view suggests an outstanding significance of the topic. The market intelligence provider International Data Cooperation (IDC), for example, predicts that the worldwide market for big data and business analytics solution will reach $189.1 billion in 2019 and increase to $274.3 billion by 2022 [18]. Accompanying",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "billion in 2019 and increase to $274.3 billion by 2022 [18]. Accompanying this growth in financial volume, the number of big data companies has reached a four-digit number [19]. Furthermore, [20] have shown the potential for increasing a company’s productivity through the usage of big data analytics, therefore proving the factual economical value. While the opportunities and attention are immense, the same applies to the challenges accompanying them [21]. One often overlooked challenge is the testing of the created big data systems landscape [22]. Even though the single components might have been tested extensively on their own, a s i t i s o f t e n t h e c a s e w i t h p o p u l a r a p",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "i t h p o p u l a r a p p l i c a t i o n s , i t i s s t i l l required to ensure the correctness of their interaction. Thus, to assure the validity of the obtained findings. In this regard , the resulting task resembles traditional software testing. While the process of testing in the domain of conventional software has already been extensively researched [23], the same does not hold true for big data systems [24]. Therefore, to support the big data engineering procedure [25], the following research question will be answered in the course of this work: Mohammad Abdallah Department of Software Engineering Al-Zaytoonah University of Jordan Amman, Jordan m.abdallah@zuj.edu.jo Klaus Turowski MRCC VLBA Otto-von-Guericke",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "Al-Zaytoonah University of Jordan Amman, Jordan m.abdallah@zuj.edu.jo Klaus Turowski MRCC VLBA Otto-von-Guericke University Magdeburg, Germany klaus.turowski@ovgu.de 2892019 15th International Conference on Signal-Image Technology & Internet-Based Systems (SITIS) 978-1-7281-5686-6/19/$31.00 ©2019 IEEE DOI 10.1109/SITIS.2019.00055 Authorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore. Restrictions apply. [Página 2] RQ1: What are the specificities of testing big data applications compared to common software testing? RQ2: How can the identified differences be taken into account in the creation of test scenarios? To answer the research questions, the publication is structured as follows. After introducing and motivating the topic in the first section, the second section discusses the fundamentals of big data and software testing. Subsequently, big data systems and software systems are compared, the",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "software testing. Subsequently, big data systems and software systems are compared, the challenges and requirements of big data testing are explored and guidelines for the testing are presented and discussed. The work ends with a conclusion that also includes limitations and future prospects. II. FUNDAMENTALS In the following, the domains of big data and software testing, constituting the fundamentals of the publication at hand, are introduced and the most important concepts explained. A. Big Data To answer the research questions, at first it is necessary to clarify the meaning of big data. While the term itself has no universally applied definition, there are several explanations, which are all describing the same phenomenon, but often slightly differ in detail. One of the most popular definitions is provided by the",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "in detail. One of the most popular definitions is provided by the National Institute of Standards and Technology (NIST) and states, that big data “consists of extensive datasets primarily in the characteristics of volume, velocity, variety, and/or variability that require a scalable architecture for efficient storage, manipulation, and analysis” [26]. Volume indicates the amount of data that has to be processed by the system to fulfill a given task. This can, on the one hand, refer to the number of records that have to be managed and, on the other hand, to the size of the handled data [27]. Either way, a tremendous increase of the volume is noticeable. Sometimes the relevant metrics are changed from gigabytes/terabytes to petabytes or even zettabytes [28]. In addition, a growing number",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "gigabytes/terabytes to petabytes or even zettabytes [28]. In addition, a growing number of data points is generated and registered for the various subjects of interest [29]. Velocity also refers to two, not necessarily identical challenges. It ca n denominate the pace of incoming data that have to be handled by the system, but also the required speed when fulfilling a processing request [30]. Variety describes the multitude of different sources, data types, structures (structured/ semi- structured/ unstructured) and notational conventions that can be present in a single data analytics application [31]. While gathering that diverse information can yield significant benefits in terms of the gained insights, their integration can pose a major challenge. Variability represents the changes regarding the other dimensions. Since the real world is in a",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "changes regarding the other dimensions. Since the real world is in a constant state of change, the data that are deemed relevant, as well as their amount are also continuously evolving. This dynamic must be taken into account during the design stage of big data systems to ensure a higher level of flexibility and scalability. Furthermore, distinct events can cause a short- term alteration of the composition of the received data, therefore posing additional challenges in terms of the system’s flexibility [32]. Another important characteristic, despite not being mentioned in the initial definition, is the veracity. It “refers to the accuracy of the data” [26] and describes the trustworthiness of different data sources. This in turn affects the effort that has to be put into the preprocessing of",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "affects the effort that has to be put into the preprocessing of the data before the actual analysis can take place. Furthermore, the validity signifies the temporal component. While data might have a high veracity, it is possible, that they are too old for the contained information to be still useful for the purpose of certain analysis, possibly even leading to wrong results [26, 28]. Although the given definitions are providing some orientation, there is, despite attempts for clarification [33, 34], no universal definition at which point the characteristics , depicted in Table 1 apply. TABLE 1. BIG DATA CHARACTERISTICS CHARACTERISTIC DESCRIPTION Volume Volume represents the (high) amount of data the system is confronted with. Velocity Velocity refers to the speed at which the data have to be",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "Velocity refers to the speed at which the data have to be handled. Variety Variety corresponds to the heterogeneity of the data and its sources. Variability Variability denominates the variation relating to t he other characteristics. Veracity Veracity stands for the accuracy and therefore the trustworthiness of the data. Validity Validity signifies the task related assessment of the data’s actuality. Because the manifestation of those characteristics can immensely vary, depending on the wanted results and the prevailing conditions, the required solutions and therefore the developed systems are highly individual [34, 35]. B. Software Testing Since the research questions are targeted on the potential distinctions between common software testing and the testing of big data applications, it is self-evident, that an understanding of both is required. Hence, to emphasize",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "is self-evident, that an understanding of both is required. Hence, to emphasize the practice of software testing, the activity itself, the motivations, and approaches are described in more detail. Software testing can be defined as “a process, or a series of processes, designed to make sure computer code does what it was designed to do and, conversely, that it does not do anything unintended” [36]. Although the testing itself is no productive endeavor, it constitutes a crucial, auxiliary activi ty to ensure the quality of the created software. The oversight of existing issues or the deliberate decision to ignore them can cause severe consequences [37]. As a result, companies worldwide spend about one-quarter of their total IT budget on quality assurance and testing, exemplifying its importance [38]. Generally",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "IT budget on quality assurance and testing, exemplifying its importance [38]. Generally speaking, software testing comprises the design, the execution and analysis of test cases, and is possibly followed by the reporting of the results [39]. There are several different styles (black box, white box, gray box) that reflect the tester’s amount of insights into the system [40]. Comprehensive testing usually occurs on different levels of growing scale (unit, integration, system) [41]. Furthermore, the testing can be conducted with varying intentions and reasons. Examples of those are the initial testing of new contents, ensuring, that the software runs at al l (smoke test), making sure, that changes did not negatively affect already functioning components (regression test) and assuring the principal’s satisfaction with the finished product (acceptance test) [42].",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "and assuring the principal’s satisfaction with the finished product (acceptance test) [42]. An overview of those described concepts, that in conjunction allow to categorize a testing endeavor, is given in Fig. 2. 290 Authorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore. Restrictions apply. [Página 3] Fig. 2. Concepts in software testing. III. COMPARISON OF BIG DATA AND SOFTWARE SYSTEMS Even though big data applications heavily rely on software to fulfill their designated tasks, it would not be sufficient to observe them as common software, including the accompanying testing regimes. Instead, big data testing is a category of its own, which is concerned with hardware as well as software aspects and additional challenges [22, 43]. A. Testing of the",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "as software aspects and additional challenges [22, 43]. A. Testing of the Systems Since the characteristics, depicted in Table 1, cannot be handled by traditional approaches, they require the creation of suitably adapted systems. A certain magnitude of volume, for instance, exceeds the possibilities of a system’s vertical growth. This results in the necessity to expand horizontally, leading to a widespread network of servers to collectively handle the given tasks [26]. Additionally, it is prevalent to use commodity hardware for the sake of cost reduction, which in turn increases the system’s heterogeneity [44]. This results in additional risks and potential incompatibilities, which have to be covered in the testing process. The distributed nature also introduces reliance on communication between the components for the sake of coordination and task",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "on communication between the components for the sake of coordination and task fulfillment. Accordingly, an outage of components is an omnipresent threat that is amplified by the reduced reliability of the used commodity hardware in comparison to highly specialized business solutions [45]. Hence, in the domain of big data, it is necessary to incorporate the examination of the resiliency of a system. While those decentralized structures also allow handling the characteristic of velocity by distributing workloads and tasks across numerous heterogeneous servers, the specifications and therefore performance might vastly differ. Since the usefulness of big data applications often heavily relies on non-functional properties, those have to be tested under consideration of the aforementioned constraints [46]. Another consequence of a huge supply of data lies in the challenge of",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "consequence of a huge supply of data lies in the challenge of determining which are to be used and how to do so. While typical software commonly has a well-defined and v e r i f i a b l e b e h a v i o r , b i g d a t a a p p l i c a t i o n s a r e c o m m o n l y intended to generate new information out of existing data. For this reason, there are additional aspects of the data sourcing and the reasoning of the underlying computations that have to be assessed. Hence, while in traditional software, the input s and desired outputs or reactions are usually known",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "software, the input s and desired outputs or reactions are usually known and the congruency is verified, this approach does often not apply for big data applications. In consequence of their nature and purpose, those systems often constitute black boxes without a corresponding test oracle, which exacerbates the detection of logical flaws and therefore poses additional challenges [47]. Besides that task, the consequential variety of data requires pre-processing to harmonize the different formats, structures, and conventions as well as measures to incorporate inputs from a multitude of different sources, which might use completely different interfaces. Furthermore, it is common for data to be incorrect or incomplete [48], necessitati ng procedures fo r improving data qu al ity . Th us , to en su re th e qu",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "ity . Th us , to en su re th e qu ality of t he obt ain ed res ult s. Those steps of integrating sources and preparing their data have to be checked as well, which results in adding another layer of complexity to the testing process. This leads to new challenges, since there might be a big number of rules and dependencies, whose adherence has to be verified, because errors in this stage can have a huge impact on the quality of the later analysis [49]. Another major difference in comparison to traditional software testing can be caused by the variability in regards to the other characteristics. While the possible valid inputs of s o f t w a r e a r e u",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "s o f t w a r e a r e u s u a l l y k n o w n a n d m o d i f i c a t i o n s a r e p r e - planned, the input-characteristics of big data applications can change over time or caused by events, therefore necessitating a modification of the application itself [26, 32]. Those modifications, in turn, add uncertainty, which has to be reflected by the testing. An additional factor of uncertainty occurs, when (a high number of) external sources are used. Since there is no option to directly control, how the data are provided, manipulations by the source or through malicious attacks on the source are possible. Those encounters",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "source or through malicious attacks on the source are possible. Those encounters might either compromise the data quality (cp. veracity) or even try to invoke damage to the system itself. In general, while software testing is limited to testing software, testing big data systems has to deal with socio- technical systems, which adds additional aspects and challenges that have to be factored in [22]. Therefore, even though traditional software testing is already a demanding task, testing in big data is even more challenging, because its characteristics and stipulations cause increased complexity and uncertainty. This circumstance is depicted in Fig. 3, representing a summary of the made investigations and therefore an answer on the previously formulated RQ1. This results in additional challenges that have to be overcome to realize",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "This results in additional challenges that have to be overcome to realize an effective quality assurance. While most of those challenges originally stem from the aforementioned characteristics, the area of application also often plays a crucial role in complicating the task by adding a certain degree of complexity to the issue. These dimensions, influencing the baseline situation of big data endeavors, lead to specific conditions for the implementation of the aspired solutions. As a result, the realization of the according system s i s a c c o m p a n i e d b y a d d i t i o n a l c h a l l e n g e s , w h i c h a r e n o",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "s , w h i c h a r e n o t a s prevalent in traditional software engineering, but have to be considered and therefore also covered in big data engineering and the according testing. 291 Authorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore. Restrictions apply. [Página 4] Fig. 3. Mapping of the data characteristics, implementation det ails, and occurring challenges. B. Challenges and Requirements of Big Data Testing Due to the fact that big data applications highly rely on software, many aspects and procedures of traditional software testing also persist in the big data domain. Therefore, the approaches depicted in Fig. 2 are still relevant and can in principle also be extended to the hardware",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "still relevant and can in principle also be extended to the hardware components and the composition of software and hardware. However, derived from the differences that have been highlighted, additional tasks emerge, that have to be performed, to provide comprehensive quality assurance of big data systems. With regards to the content, it might, depending on the use case, be necessary to constantly review the undertaken calculations and the conducted usage of data sources for this purpose. This necessity arises due to the agility of the examined subjects and circumstances, degradation of data sources, changes in the explored questions, as well as the constantly emerging new technologies and insights in the field [26, 50]. Regarding technical aspects, the distributed and heterogeneous nature of the big data landscapes requires extensive",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "the distributed and heterogeneous nature of the big data landscapes requires extensive tests of the used components, since they are a part of the system under test and might possibly induce errors due to faultiness [37]. Furthermore, the communication between the components, including possible data transformations, has to be tested and the system’s reaction to the outage of nodes has to be assessed. Another consequence of the multitude of nodes and especially sources, which might not be under direct control, is the plethora of potential weak points for possible attackers . This might concern attacks on the system itself, but also attempts to manipulate the data and therefore the results of the analysis. For this reason, the security of the system and, depending on the use case, also",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "the security of the system and, depending on the use case, also the ability to detect manipulations have to be tested regarding those circumstances. In general, in the area of big data, it is an ambitious task to create or find ways to validate data. This also applies to the determination or creation of a test oracle. Because of the explorative nature of the applications, the desired outcome is often not known, impeding the according testing. Since non-functional aspects, like response times, often play a major role, it is also necessary to extensively benchmark the application to assure conformity with those requirements. Especially the timely detection of the need to scale and the ability to do so have to be taken into account, including the needed time and",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "so have to be taken into account, including the needed time and possible capacity limits. When an applications sole purpose is a timely evaluation of data, fo r example in High-Frequency Trading, a delay of several seconds is not only a nuisance but might effectively render the whole application useless, stressing the importance of this aspect [51]. Another demand, derived from the dynamic and constant change in the application areas is the convertibility and extensibility of the tests to allow the adjustment to changes o f t h e s y s t e m u n d e r t e s t . S i n c e c h a n g e s i n t h e u s e d algorithms, technologies",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "s i n t h e u s e d algorithms, technologies and data sources are to be expected, the testing solutions should provide an according amount of flexibility. A possible solution for this task might be a highl y modular structure, which allows swapping elements according to the prevailing needs [47]. Therefore, when facing big data applications, testers should follow the six guidelines depicted in Fig. 4. Those constitute the additionally needed steps on top of the common software testing procedures to take account of the specificities of those systems and represent an answer on the formulated RQ2. Fig. 4. Developed guidelines for testing big data applications. 292 Authorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore. Restrictions",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore. Restrictions apply. [Página 5] It should be noted, however, that the respective relevancy of each of those guidelines depends on the concrete use case. Depending on the circumstances, some of those might not, or only to a lesser extent, be applicable to a testing scenario. F or example, an application that has no direct or indirect external connections and only receives internal sensor data by production facilities is not prone to external attacks or manipulation. Therefore, testing the resiliency versus malicious attacks is unessential. On the other hand, when using cloud services, the appropriateness of the hardware can rarely by verified, due to factors like a lack of direct access and knowledge of the used components. Hence,",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "a lack of direct access and knowledge of the used components. Hence, while the guidelines help in understanding the specificities of big data testing, their concrete application and implementation still remains in the big data engineers or testers responsibility. Furthermore, finding an appropriate test oracle in the big data domain remains a challenging and highly individual task that is facilitated neither by common software testing practices nor by the application of the guidelines. C. Discussion Because big data systems are heavily relying on software, the concepts of software testing that are depicted in Fig. 2 ar e also applicable to the testing of big data systems. Nevertheless, while the major intentions for a test scenario are the same, the three possible styles persist and the general operations remain",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "the same, the three possible styles persist and the general operations remain unchanged, the levels slightly differ. A unit, for instance, might comprise a combination of software and hardware, instead of just a piece of software. An example could be a server, which can be considered a unit in big data testing, even though it is not only relying on software or hardware, but on the conjunction of both. Though, since the application areas and characteristics of big data create additional challenges, it is necessary, to consider them in the creation of test scenarios. By applying the guidelines presented in Fig. 4, most of them can be tackled, increasing the value of quality assurance. However, even considering all of those stipulations, depicted in Fig. 3, and finding an",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "considering all of those stipulations, depicted in Fig. 3, and finding an applicable test oracle, does not assure a highly productive and effective big data application. Since it is also necessary to incorporate further aspects, besides the system itself, implementing a holistic quality assurance process, accommodating the socio- technical nature and the complexity of the endeavour is required [22]. Consequently, future research should focus on examining the identified challenges one by one in detail, allowing for deeper insights and possibly new solution approaches. Considering both of the mentioned domains, this can be approached from a software as well as a systems engineering perspective. Hence, in the future the testing of big data systems would not only be observed in terms of the deployed software and the correct way",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "be observed in terms of the deployed software and the correct way of functioning but also the underlying architecture and its connections to the environment. IV. CONCLUSION Due to the characteristics that define big data, properly handling them and effectively generating value is a demanding task. The paper at hand showed that the systems that are used for this purpose exceed the scope of common software since they combine sophisticated software with complex hardware formations. For this reason, traditional software testing is not sufficient and further measures are required. Those are determined based on the additional challenges depicted in Fig. 3 and constitute the six guidelines for taking account of the specificities when testing big data applications in comparison to common software. The conjunction of the specificities with",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "applications in comparison to common software. The conjunction of the specificities with the guidelines also constitutes the answer to the second research question. The increased clarity concerning the domain and the guidelines will support scientists as well as practitioners in their endeavors to understand, apply and advance the subject of big data, therefore contributing to the formation of tomorrow’s system creation and management. The following step could be the accumulation of best practices and use case independent techniques for each of those guidelines to further support the dissemination of big data analytics. Furthermore, reliable techniques for the definition of test oracles are still to be developed, leaving this as one of the most significant future challenges in the regarded domain. REFERENCES [1] N. Khan et al., “Big data:",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "in the regarded domain. REFERENCES [1] N. Khan et al., “Big data: survey, technologies, opportunities, and challenges,” The Scientific World Journal , vol. 2014, pp. 1–18, 2014. [2] Y. Wang, L. Kung, W. Y. C. Wang, and C. Cegielski, “Developing a Big Data-Enabled Transformation Model in Healthcare: A Practice Based View,” in Proceedings of Thirty Fifth International Conference on Information Systems , 2014. [3] J. Kallinikos and N. Tempini, “Patient Data as Medical Facts: Social Media Practices as a Foundation for Medical Knowledge Creation,” Information Systems Research , vol. 25, no. 4, pp. 817–833, 2014. [4] A. Farseev and T.-S. Chua, “Tweet Can Be Fit,” ACM Transactions on Information Systems , vol. 35, no. 4, pp. 1–34, 2017. [5] K. Domdouzis, B. Akhgar, S. Andrews, H. Gibson, and",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "1–34, 2017. [5] K. Domdouzis, B. Akhgar, S. Andrews, H. Gibson, and L. Hirsch, “A social media and crowdsourcing data mining system for crime prevention during and post- crisis situations,” Journal of Systems and Information Technology , vol. 18, no. 4, pp. 364–382, 2016. [6] D. Wu and Y. Cui, “Disaster early warning and damage assessment analysis using social media data and geo-location information,” Decision Support Systems , vol. 111, pp. 48–59, 2018. [7] D. Staegemann, M. Volk, and K. Turowski, “Mobile Procurement Management,” in Springer Reference Wirtschaft, Handbuch Digitale Wirtschaft , T. Kollmann, Ed., Wiesbaden: Springer Fachmedien Wiesbaden, 2019, pp. 1–15. [8] T. Nguyen, L. Zhou, V. Spiegler, P. Ieromonachou, and Y. Lin, “Big data analytics in supply chain management: A state-of-the-art literature review,” Computers & Operations",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "analytics in supply chain management: A state-of-the-art literature review,” Computers & Operations Research , vol. 98, pp. 254– 264, 2018. [9] K. Nagorny, P. Lima-Monteiro, J. Barata, and A. W. Colombo, “Big Data Analysis in Smart Manufacturing: A Review,” International Journal of Communications, Network and System Sciences , vol. 10, no. 03, pp. 31– 58, 2017. [10] Y. Tang, J. J. Xiong, Y. Luo, and Y.-C. Zhang, “How Do the Global Stock Markets Influence One Another? Evidence from Finance Big Data and Granger Causality Directed Network,” International Journal of Electronic Commerce , vol. 23, no. 1, pp. 85–109, 2019. 293 Authorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore. Restrictions apply. [Página 6] [11] X. Wu, X. Zhu, G.-Q.",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "IEEE Xplore. Restrictions apply. [Página 6] [11] X. Wu, X. Zhu, G.-Q. Wu, and W. Ding, “Data mining with big data,” IEEE Transactions on Knowledge and Data Engineering , vol. 26, no. 1, pp. 97–107, 2014. [12] H. Lee, N. Aydin, Y. Choi, S. Lekhavat, and Z. Irani, “A decision support system for vessel speed decision in maritime logistics using weather archive big data,” Computers & Operations Research , vol. 98, pp. 330– 342, 2018. [13] X. Yi, F. Liu, J. Liu, and H. Jin, “Building a network highway for big data: architecture and challenges,” IEEE Network , vol. 28, no. 4, pp. 5–13, 2014. [14] S. Yin and O. Kaynak, “Big Data for Modern Industry: Challenges and Trends [Point of View],” Proceedings of the IEEE , vol.",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "Challenges and Trends [Point of View],” Proceedings of the IEEE , vol. 103, n o. 2, pp. 143–146, 2 015. [15] P. Mikalef, I. O. Pappas, J. Krogstie, and M. Giannakos, “Big data analytics capabilities: a systematic literature review and research agenda,” Inf Syst E-Bus Manage , vol. 16, no. 3, pp. 547–578, 2018. [16] Z. A. Al-Sai, R. Abdullah, and M. h. husin, “Big Data Impacts and Challenges: A Review,” in 2019 IEEE Jordan International Joint Conference on Electrical Engineering and Information Technology (JEEIT) , Amman, Jordan, A pr. 2019 - Apr. 20 19, pp. 15 0–155. [17] A. Oussous, F.-Z. Benjelloun, A. Ait Lahcen, and S. Belfkih, “Big Data technologies: A survey,” Journal of King Saud University - Computer and Information Sciences , vol. 30, no.",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "King Saud University - Computer and Information Sciences , vol. 30, no. 4, pp. 431–448, 2018. [18] International Data Cooperation, IDC Forecasts Revenues for Big Data and Business Analytics Solutions Will Reach $189.1 Billion This Year with Double-Digit Annual Growth Through 2022. [Online] Available: https://www.idc.com/getdoc.jsp?containerId=prUS449 98419. Accessed on: May 21 2019. [19] M. Turck and D. Obayomi, The Big Data Landscape. [Online] Available: http://dfkoz.com/big-data- landscape/. Accessed on: May 21 2019. [20] O. Müller, M. Fay, and J. Vom Brocke, “The Effect of Big Data and Analytics on Firm Performance: An Econometric Analysis Considering Industry Characteristics,” Journal of Management Information Systems , vol. 35, no. 2, pp. 488–509, 2018. [21] O. Hummel, H. Eichelberger, A. Giloj, D. Werle, and K. Schmid, “A Collection of Software Engineering Challenges for Big",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "Werle, and K. Schmid, “A Collection of Software Engineering Challenges for Big Data System Development,” in 44th Euromicro Conference on Software Engineering and Advanced Applications (SEAA) , Prague, 2018, pp. 362– 369. [22] D. Staegemann, M. Volk, N. Jamous, and K. Turowski, “Understanding Issues in Big Data Applications - A Multidimensional Endeavor,” in Twenty-fifth Americas Conference on Information Systems , Cancun, 2019. [23] V. Garousi and M. V. Mäntylä, “A systematic literature review of literature reviews in software testing,” Information and Software Technology , vol. 80, pp. 195–216, 2016. [24] C. Tao and J. Gao, “Quality Assurance for Big Data Application – Issuses, Challenges, and Needs,” in The 28th International Conference on Software Engineering and Knowledge Engineering , 2016, pp. 375–381. [25] M. Volk, D. Staegemann, M. Pohl,",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "Engineering , 2016, pp. 375–381. [25] M. Volk, D. Staegemann, M. Pohl, and K. Turowski, “Challenging Big Data Engineering: Positioning of Current and Future Development,” in Proceedings of the 4th International Conference on Internet of Things, Big Data and Security , Heraklion, Crete, Greece, 2019, pp. 351–358. [26] NIST, NIST Big Data Interoperability Framework: volume 1, definitions, version 2. [Online] Available: https://bigdatawg.nist.gov/_uploadfiles/NIST.SP.1500- 1r1.pdf. Accessed on: Jan. 31 2019. [27] P. Russom, Big Data Analytics: TDWI Best Practices Report Fourth Quarter 2011. [Online] Available: https://vivomente.com/wp- content/uploads/2016/04/big-data-analytics-white- paper.pdf. Accessed on: May 22 2019. [28] L. Cai and Y. Zhu, “The Challenges of Data Quality and Data Quality Assessment in the Big Data Era,” CODATA , vol. 14, no. 2, pp. 1–10, 2015. [29] S. Sagiroglu and D. Sinanc, “Big data:",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "2, pp. 1–10, 2015. [29] S. Sagiroglu and D. Sinanc, “Big data: A review,” in 2013 International Conference on Collaboration Technologies and Systems (CTS) , 2013, pp. 42–47. [30] A. Gandomi and M. Haider, “Beyond the hype: Big data concepts, methods, and analytics,” International Journal of Information Management , vol. 35, no. 2, pp. 137–144, 2015. [31] A. Gani, A. Siddiqa, S. Shamshirband, and F. Hanum, “A survey on indexing techniques for big data: taxonomy and performance evaluation,” Knowledge and Information Systems , vol. 46, no. 2, pp. 241–284, 2016. [32] A. Katal, M. Wazid, and R. H. Goudar, “Big data: Issues, challenges, tools and Good practices,” in Sixth International Conference on Contemporary Computing , M. Parashar et al., Eds., 2013, pp. 404– 409. [33] D. Laney, “Information",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "Parashar et al., Eds., 2013, pp. 404– 409. [33] D. Laney, “Information Economics, Big Data and the Art of the Possible with Analytics,” 2012. [34] M. Volk, S. W. Hart, S. Bosse, and K. Turowski, “How much is Big Data? A Classification Framework for IT Projects and Technologies,” in Twenty-second Americas Conference on Information Systems , San Diego, 2016. [35] A. Amado, P. Cortez, P. Rita, and S. Moro, “Research trends on Big Data in Marketing: A text mining and topic modeling based literature analysis,” European Research on Management and Business Economics , vol. 24, no. 1, pp. 1–7, 2018. [36] G. J. Myers, T. Badgett, and C. Sandler, The art of software testing, 3rd ed. Hoboken, N.J: J. Wiley & Sons, 2011. [37] R. Patton, Software testing",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "Hoboken, N.J: J. Wiley & Sons, 2011. [37] R. Patton, Software testing . Indianapolis: SAMS, 2001. [38] Capgemini, Sogeti, HPE, and Micro Focus, Proportion of budget allocated to quality assurance and testing as a percentage of IT spend from 2012 to 2018. [Online] Available: https://www.statista.com/statistics/500641/worldwide- qa-budget-allocation-as-percent-it-spend/. Accessed on: May 23 2019. [39] P. Ammann and J. Offutt, Introduction to software testing . Cambridge: Cambridge University Press, 2008. [40] M. Kaur and R. Singh, “A Review of Software Testing Techniques,” International Journal of Electronic and Electrical Engineering , vol. 7, no. 5, pp . 463– 474, 2014. [41] L. Copeland, A Practicioner's Guide to Software Test Desing, 11th ed. Boston: Artech House Publihsers, 2010. 294 Authorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore. Restrictions apply. [Página 7] [42] R. Binder, Testing object-oriented systems: Models, patterns and tools, 7th ed. Boston, Madrid: Addison- Wesley, 2006. [43] M. Abdallah, “Big Data Quality Challenges,” in 2019 International Conference on Big Data and Computational Intelligence (ICBDCI) , Pointe aux Piments, Mauritius, 2019, pp. 1–3. [44] A. McAfee and E. Brynjolfsson, “Big Data: The Management Revolution: Exploiting vast new flows of information can radically improve your company’s performance. But first you’ll have to change your decision-making culture.,” Harvard Business Review , vol. 91, no. 5, pp. 1–9, 2012. [45] G. Wang, L. Zhang, and W. Xu, “What Can We Learn from Four Years of Data Center Hardware Failures?,” in 47th",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "We Learn from Four Years of Data Center Hardware Failures?,” in 47th Annual IEEE/IFIP International Conference on Dependable Systems and Networks: 26-29 June 2017, Denver, Colorado : proceedings , Denver, CO, USA, 2017, pp. 25–36. [46] M. Gudipati, S. Rao, N. Mohan, and N. K. Gajja, “Big Data : Testing Approach to Overcome Quality Challenges,” vol. 11, pp. 65–73, 2013. [47] D. Staegemann, J. Hintsch, and K. Turowski, “Testing in Big Data: An Architecture Pattern for a Development Environment for Innovative, Integrated and Robust Applications,” in Proceedings of the WI2019 , 2019, pp. 279–284. [48] I. Taleb, M. A. Serhani, and R. Dssouli, “Big Data Quality: A Survey,” in 2018 IEEE International Congress on Big Data , 2018, pp. 166–173. [49] W. Verbeke, C. Bravo, and B. Baesens,",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": ", 2018, pp. 166–173. [49] W. Verbeke, C. Bravo, and B. Baesens, Profit driven business analytics: A practitioner's guide to transforming big data into added value . Hoboken, New Jersey: John Wiley & Sons, Inc, 2017. [50] D. Lazer, R. Kennedy, G. King, and A. Vespignani, “Big data. The parable of Google Flu: traps in big data analysis,” (eng), Science (New York, N.Y.) , vol. 343, no. 6176, pp. 1203–1205, 2014. [51] B. Fang and P. Zhang, “Big Data in Finance,” in Big Data Concepts, Theories, and Applications , S. Yu and S. Guo, Eds., Cham: Springer International Publishing, 2016, pp. 391–412. 295 Authorized licensed use limited to: University of Exeter. Downloaded on May 07,2020 at 10:35:46 UTC from IEEE Xplore. Restrictions apply.",
        "start_idx": 5684,
        "end_idx": 5807
      },
      {
        "text": "10:35:46 UTC from IEEE Xplore. Restrictions apply.",
        "start_idx": 5800,
        "end_idx": 5807
      }
    ],
    "04bb09c3-b758-4880-9d1e-517db37ff1a7": [
      {
        "text": "[Página 1] White-Box Testing of Big Data Analytics with Complex User-Defined Functions Muhammad Ali Gulzar Shaghayegh Mardani University of California, Los Angeles USAMadanlal Musuvathi Microsoft Research, USAMiryung Kim University of California, Los Angeles USA ABSTRACT Data-intensive scalable computing (DISC) systems such as Google’s MapReduce, Apache Hadoop, and Apache Spark are being lever- aged to process massive quantities of data in the cloud. Modern DISC applications pose new challenges in exhaustive, automatic testing because they consist of dataflow operators, and complex user-defined functions (UDF) are prevalent unlike SQL queries. We design a new white-box testing approach, called BigTest to reason about the internal semantics of UDFs in tandem with the equiva- lence classes created by each dataflow and relational operator. Our evaluation shows that, despite ultra-large scale input data",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "and relational operator. Our evaluation shows that, despite ultra-large scale input data size, real world DISC applications are often significantly skewed and inadequate in terms of test coverage, leaving 34% of Joint Dataflow and UDF (JDU) paths untested. BigTest shows the potential to min- imize data size for local testing by 105to 108orders of magnitude while revealing 2X more manually-injected faults than the previous approach. Our experiment shows that only few of the data records (order of tens) are actually required to achieve the same JDU cover- age as the entire production data. The reduction in test data also provides CPU time saving of 194X on average, demonstrating that interactive andfastlocal testing is feasible for big data analytics, obviating the need to test applications on huge production data.",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "data analytics, obviating the need to test applications on huge production data. CCS CONCEPTS •Software and its engineering →Cloud computing ;Soft- ware testing and debugging ;•Information systems →MapR- educe-based systems . KEYWORDS symbolic execution, dataflow programs, data intensive scalable computing, map reduce, test generation ACM Reference Format: Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miry- ung Kim. 2019. White-Box Testing of Big Data Analytics with Complex User-Defined Functions. In Proceedings of the 27th ACM Joint European Soft- ware Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE ’19), August 26–30, 2019, Tallinn, Estonia. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/3338906.3338953 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia ©2019 Association for Computing Machinery. ACM ISBN 978-1-4503-5572-8/19/08. . . $15.00 https://doi.org/10.1145/3338906.33389531 INTRODUCTION Data-intensive scalable computing (DISC) systems such as Mapre- duce [ 20], Apache Hadoop [ 1], Apache Spark [ 48] are commonly used today to process terabytes",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "1], Apache Spark [ 48] are commonly used today to process terabytes and petabytes of data. At this scale, rare and buggy corner cases frequently show up in production [ 49]. Thus, it is common for these applications to either crash after run- ning for days or worse, silently produce corrupted output. Unfortu- nately, the common industry practice for testing these applications remains running them locally on randomly sampled inputs, which obviously does not flush out bugs hiding in corner cases. This paper presents a systematic input generation tool, called BigTest , that embodies a new white-box testing technique for DISC applications. BigTest is motivated by the recent successes of sys- tematic test generation tools [ 22,24,39]. However, the nature of DISC applications requires extending these in important",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "22,24,39]. However, the nature of DISC applications requires extending these in important ways to be effective. Unlike general-purpose programs addressed by existing testing tools, DISC applications use a combination of relational op- erators, such as join andgroup-by , and dataflow operators, such asmap,flatmap , along with user-defined functions (UDFs) written in general purpose languages such as C/C++, Java, or Scala. In order to comprehensively test DISC applications, BigTest rea- sons about the combined behavior of UDFs with relational and dataflow operations. A trivial way is to replace these operations with their implementations and symbolically execute the resulting program. However, existing tools are unlikely to scale to such large programs, because dataflow implementation consists of almost 700 KLOC in Apache Spark. Instead, BigTest includes a logical abstrac- tion for",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "KLOC in Apache Spark. Instead, BigTest includes a logical abstrac- tion for dataflow and relational operators when symbolically exe- cuting UDFs in the DISC application. The set of combined path con- straints are transformed into SMT (satisfiability modulo theories) queries and solved by leveraging an off-the-shelf theorem prover, Z3 or CVC4, to produce a set of concrete input records [ 11,19]. By using such a combined approach, BigTest is more effective than prior DISC testing techniques [ 31,34] that either do not reason about UDFs or treat them as uninterpreted functions. To realize this approach, BigTest tackles three important chal- lenges that our evaluation shows are crucial for the effectiveness of the tool. First, BigTest models terminating cases in addition to the usual non-terminating cases for each dataflow",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "terminating cases in addition to the usual non-terminating cases for each dataflow operator. For exam- ple, the output of a join of two tables only includes rows with keys that match both the input tables. To handle corner cases, BigTest carefully considers terminating cases where a key is only present in the left table, the right table, and neither. Doing so is crucial, as based on the actual semantics of the join operator, the output can contain rows with null entries, which are an important source of bugs. Second, BigTest models collections explicitly, which are created by flatmap and used by reduce . Prior approaches [ 31,34] 290 [Página 2] ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim 1val",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim 1val x,y,z; 2if(x<y) 3 z = y/x; //PC1: x < y = true, Effect: z=y/x 4else 5 z = x/y; //PC2: x >= y = true, Effect: z=x/y Figure 1: Symbolic PathFinder produces a set of path con- straints and their corresponding effects do not support such operators, and thus are unable to detect bugs if code accesses an arbitrary element in a collection of objects or if the aggregation result is used within the control predicate of the subsequent UDF. Third, BigTest analyzes string constraints because string manipulation is common in DISC applications and frequent errors are ArrayIndexOutOfBoundException and String- IndexOutOfBoundsException during segmentation and parsing. To evaluate BigTest , we use a benchmark set of 7",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "parsing. To evaluate BigTest , we use a benchmark set of 7 real-world Apache Spark applications selected from previous work such as PigMix[ 35],Titian [28], and BigSift [25]. While these programs are representative of DISC applications, they do not adequately rep- resent failures that happen in this domain. To rectify this problem, we perform a survey of DISC application bugs reported in Stack Overflow and mailing lists and identify seven categories of bugs. We extend the existing benchmarks by manually introducing these categories of faults into a total of 31 faulty DISC applications. To the best of our knowledge, this is the first set of DISC application bench- marks with representative real-world faults. Such benchmarks are crucial for further research in this area. We assess JDU (Joint Dataflow",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "crucial for further research in this area. We assess JDU (Joint Dataflow and UDF) path coverage, sym- bolic execution performance, and SMT query time. Our evaluation shows that real world datasets are often significantly skewed and inadequate in terms of test coverage of DISC applications, still leav- ing 34% of JDU paths untested. Compared to Sedge [31],BigTest significantly enhances its capability to model DISC applications—In 5 out of 7 applications, Sedge is unable to handle these applications at all, due to limited dataflow operator support and in the rest 2 applications, Sedge covers only 23% of paths modeled by BigTest . We show that JDU path coverage is directly related to improve- ment in fault detection— BigTest reveals 2X more manually injected faults than Sedge on average. BigTest",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "BigTest reveals 2X more manually injected faults than Sedge on average. BigTest can minimize data size for local testing by 105to 108orders of magnitude, achieving the CPU time savings of 194X on average, compared to testing code on the entire production data. BigTest synthesizes concrete input records in 19 seconds on average for all remaining untested paths. Below, we highlight the summary of contributions. •BigTest is the first piece of DISC white-box testing that comprehensively models dataflow operators and the internal paths of user-defined functions in tandem. •BigTest makes three important enhancements to improve fault detection capability for DISC applications—(1) It con- siders both terminating andnon-terminating cases of each dataflow operator; (2) It explicitly models collections created byflatmap and translates aggregation logic into an iterative aggregator; and (3)",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "created byflatmap and translates aggregation logic into an iterative aggregator; and (3) It models string constraints explicitly. •It puts forward a benchmark of manually injected DISC application faults along with generated test data, inspired by the characteristics of real world DISC application faults evidenced by Stack Overflow and mailing lists.1val trips = sc.textFile(\"trips_table.csv\") 2 .map{s => 3 val cols = s.split(\",\") 4 (cols(1),cols(3).toInt/cols(4).toInt) } 5 //Returns location and speed 6val zip = sc.textFile(\"zipcode_table.csv\") 7 .map{s => 8 val cols = s.split(\",\") 9 (cols(1),cols(0) } 10 // Returns location and its name 11 .filter { 12 s => s._ 2 == \"Palms\" } 13 val joined = trips. join (zip) 14 joined 15 .map{s => 16 if (s._2._1 > 40) (\"car\",1) 17 else if (s._2._1 > 15) (\"bus\",1) 18",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "(s._2._1 > 40) (\"car\",1) 17 else if (s._2._1 > 15) (\"bus\",1) 18 else (\"walk\",1) 19 } 20 .reduceByKey (_+_ ) 21 .saveAsTextFile(\"hdfs://...\")➊ ➋ ➌ ➍ ➎ Figure 2: Alice’s program estimates the total number of trips originated from “Palms.” •BigTest finds 2X more faults than Sedge , minimizes test data by orders of magnitude, and is fast and interactive. Our results demonstrate that interactive local testing of big data analytics is feasible, and that developers should not need to test their program on the entire production data. For example, a user may monitor path coverage with respect to the equivalent classes of paths generated from BigTest and skip records if they belong to the already covered path, constructing a minimized sample of the production data for local development",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "path, constructing a minimized sample of the production data for local development and testing. The rest of the paper is organized as follows. Section 2 provides a brief introduction to Apache Spark and symbolic execution. Section 3 describes a motivating example. Section 4 describes the design of BigTest . Section 5 describes evaluation settings and results. Section 6 discusses related work. Section 7 concludes the paper. 2 BACKGROUND Apache Spark. BigTest targets Apache Spark, a widely used data intensive scalable computing system. Spark extends the MapRe- duce programming model with direct support for dataflow and traditional relational algebra operators ( e.g.,group-by ,join , and filter ). Datasets can be loaded in Spark runtime using several APIs that create Resilient Distributed Datasets (RDDs), an abstrac- tion of distributed collection",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "that create Resilient Distributed Datasets (RDDs), an abstrac- tion of distributed collection [ 47]. RDDs can be transformed by invoking dataflow operations on them ( e.g., val filterRdd = rdd.filter(_ >5) ). Dataflow operators such as map,reduce , and flatmap are implemented as higher-order functions that take a user defined function (UDF) as an input parameter. The actual evalua- tion of an RDD occurs when an action such as count orcollect is called. Internally, Spark translates a series of RDD transformations into a Directed Acyclic Graph (DAG) where each vertex represents a transformation applied to the incoming RDD. The Spark scheduler executes each stage in a topological order. Symbolic Execution using Java Path Finder. BigTest builds on Symbolic Java PathFinder (SPF) [ 36]. Internally, SPF relies on the",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "on Symbolic Java PathFinder (SPF) [ 36]. Internally, SPF relies on the 291 [Página 3] White-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Trips Zipcode Map: 𝑓map1 Map: 𝑓map2 Filter: 𝑓filter Join: ⨝ Map: 𝑓map3 ReduceByKey: 𝑓Agg ~𝑓filter(K2, V2) T1 T4FalseTrue 𝑓filter(K2, V2) ⋀K1= K2 (K1, V1)(K2, V2)(K1, (V1, V2))(S ,1)(S ,N) 𝑓filter(K2, V2) ⋀K1∉Zipcode K1∉ZipcodeK2∉Trips 𝑓filter(K2, V2) ⋀K2∉Trips T2 T3TZ (a) Dataflow operators’ paths by BigTest Map: 𝑓map1String : T T.split(“,”).length >= 5 ⋀isInt(T.split(“,”)(3)) ⋀isInt(T.split(“,”)(4)) ⋀T.split(“,”)(4).toInt!= 0 K1=T.split(“,”)(1) V1=T.split(“,”)(3).toInt/T.split(“,”)(4).toInt =>String : Z Z.split(“,”).length >= 2=> Map: 𝑓map2K2=Z.split(“,”)(1)V2=Z.split(“,”)(0) String : K2, String : V2 V2 == “Palms”True Filter: 𝑓filterString : K1, Int: V1, String : V2 V1>40=>S=“car” Map: 𝑓map315<V1≤40 V1<15 =>S=“public”=>S=“walk”String : S , Int[K] : [a1,a2,a3,...,aK] ReduceByKey: 𝑓AggK==1=>N=a1 K==2",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "𝑓map315<V1≤40 V1<15 =>S=“public”=>S=“walk”String : S , Int[K] : [a1,a2,a3,...,aK] ReduceByKey: 𝑓AggK==1=>N=a1 K==2 K==n =>N=𝑓Agg([a1 , a2]) =>N=𝑓Agg(a1,𝑓Agg(a2,…,𝑓Agg(an-1,an)…) . . . =>(b) Non-terminating path conditions of individual UDFs Map: 𝑓map1String : T T.split(“,”).length < 5T.split(“,”).length >= 5 ⋀NotInt(T.split(“,”)(3))T.split(“,”).length >= 5 ⋀isInt(T.split(“,”)(3)) ⋀NotInt(T.split(“,”)(4))T.split(“,”).length >= 5 ⋀isInt(T.split(“,”)(3)) ⋀isInt(T.split(“,”)(4)) ⋀T.split(“,”)(4).toInt== 0 Z.split(“,”).length < 2 =>X=>X=>X=>X Map: 𝑓map2String : Z =>X (c) Path constraints for terminating paths in UDFs➊➋ ➌ ➍ ➎➊ ➋ ➌ ➍ ➎ ➊ ➋ Figure 3: Solid and dotted boxes represent transformations and path constraints, respectively. BigTest identifies path con- straints for both non-terminating and terminating program paths while symbolically executing the program. analysis engine of Java PathFinder (JPF) model checking [ 43]. It interprets Java bytecode on symbolic inputs and produces a set of symbolic constraints. Each constraint",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "on symbolic inputs and produces a set of symbolic constraints. Each constraint represents a unique path in the program, and can be ingested by a theorem solver to generate test inputs. Figure 1 illustrates an example symbolic execution result. By attaching listeners to SPF, the path conditions and the effects of each path can be captured. For this program, SPF produces two path conditions: (1) the first path produces the effect of z=y/x , when the path condition x<yholds true and (2) the second path produces z=x/y as an effect, when the path condition x≥yis satisfied. 3 MOTIVATING EXAMPLE This section presents a running example to motivate BigTest . Sup- pose that Alice writes a DISC application in Spark to analyze the Los Angeles commuting dataset. She wants",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "application in Spark to analyze the Los Angeles commuting dataset. She wants to find the total number of trips originating from the “Palms” neighborhood using: (1) a pub- lic transport whose speed is assumed to be faster than 15 but slower than 40 mph, (2) a personal vehicle which is estimated to be faster than 40 mph, and (3) on foot which is estimated as slower than 15 mph. Each row in the Trips dataset represents a unique identifier for the trip, the start and end location in terms of a zip code, the trip distance in miles, and the trip duration in hours, for example, 1,90034,90024, 10, 1 . To map an area zip code to its correspond- ing area name, Alice uses another dataset that assigns",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "to its correspond- ing area name, Alice uses another dataset that assigns a name to each zip code in the following manner: 90034,Culver City To perform this analysis, Alice writes a Spark application in Figure 2. She loads both datasets (lines 1 and 6), parses each dataset, selects the start location of a trip as a key, and computes the average speed as a value by dividing the distance by duration (lines 2-4). Alice outputs a zip code as a key and an area name as a value (lines 7-9) and filters the area name with “Palms\" at line 12. She joins the two data sets (line 13). In the subsequent mapoperation (line 15-18), she categorizes the trips based on the average speed into three categories. She finally",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "the trips based on the average speed into three categories. She finally counts the frequency of each trip kindand stores them (lines 20 and 21). Though this program is only 21 lines long, it poses several challenges for modeling test paths. Equivalence Classes of Dataflow Operators. Consider filter ➌at line 11. To exhaustively test this operator, we must consider two equivalence classes: the first where a data record satisfies the filter and moves onto the next operator and the second where the filter does not satisfy and its data flow terminates. If we only model non-terminating case then test data would contain passing data records only and hence, would not detect a fault in which filter is removed from the DISC application. To model join at line 13,",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "is removed from the DISC application. To model join at line 13, we must have three equivalence classes—two terminating cases and one non-terminating case: (1) an input record in the left table (“Trip”) does not have a matching key on the right table (“ZipCode”), terminating its data flow, (2) an input in the right table does not have a matching key on the left, terminating its data flow, and (3) there exists a key that appears in both tables, passing the joined result to the next operator. Modeling such terminating cases is crucial otherwise test data generated produce the same output for both join andleftOuterJoin and do not reveal faults that are based on incorrect join type usage. UDF Paths. Consider the map➍at lines 15-18. There are three",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "type usage. UDF Paths. Consider the map➍at lines 15-18. There are three internal path conditions: (1) speed >40 mph, (2) 15 mph <speed ≤40 mph, and (3) speed ≤15 mph. The sub figure ➍in Figure 3b shows corresponding path conditions and effects. String Constraints. To analyze the second map➋at lines 7 to 9, we must reason about the entailed string constraints. Given a string Zin sub figure ➋in Figure 3b and 3a, to split the data into two columns, it must satisfy a string constraint Z.split(\",\").length ≥2to produce the effect where the key K2is(Z.split(\",\") (1)and the value V2is(Z.split(\",\")(0)) . String manipulation is critical to many DISC applications. In the above example, at least one test must contain a string zwithout delimiter \",\", so thatz.split(\",\")(1) leads to ArrayIndexOutOfBoundsException which",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "contain a string zwithout delimiter \",\", so thatz.split(\",\")(1) leads to ArrayIndexOutOfBoundsException which will then expose the inability of the UDF to handle exceptions. 292 [Página 4] ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim Table 1: Generated input data where each row represents a unique path. Variables T,Z,V, and Kare defined in Figure 3a. # Constraint Trips Zipcode C1 T.split(\",\").length < 5 \"\" _ _ C2 T.split(\",\").length ≥5∧NotInt(T.split(\",\")(3)) _, _, _, \"\", _ _, _ C3 T.split(\",\").length ≥5∧isInt(T.split(\",\")(3)) ∧NotInt(T.split(\",\")(4)) _, _, _, \"-2\", \"\" _, _ C4 T.split(\",\").length ≥5∧isInt(T.split(\",\")(3)) ∧isInt(T.split(\",\")(4)) ∧T.split(\",\")(4).toInt = 0 _, _, _, \"-2\", \"0\" _, _ C5 Z.split(\",\").length < 2 _ \"\" C6 Z.split(\",\").length ≥2∧V2!= \"Palms\" _ _, \"\\x00\" C7T.split(\",\").length ≥5∧isInt(T.split(\",\")(3)) ∧isInt(T.split(\",\")(4)) ∧T.split(\",\")(4).toInt != 0",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "Z.split(\",\").length ≥2∧V2!= \"Palms\" _ _, \"\\x00\" C7T.split(\",\").length ≥5∧isInt(T.split(\",\")(3)) ∧isInt(T.split(\",\")(4)) ∧T.split(\",\")(4).toInt != 0 ∧Z.split(\",\").length ≥2∧V2= \"Palms\"∧K1<Zipcode_, \"!0!\", _, _, _ \"\\x00\", \"Palms\" C8 . . .∧V2= \"Palms\"∧K2<Trips _, \"\"!0!\", _, _, _ \"\\x00\", \"Palms\" C9 . . .∧V2= \"Palms\"∧K1= K 2∧V1> 40 _, \"\\x00\", _, \"41\", \"1\" \"\\x00\", \"Palms\" C10 . . .∧V2= \"Palms\"∧K1= K 2∧15< V 1< 40 _, \"\\x00\", _, \"16\", \"1\" \"\\x00\", \"Palms\" C11 . . .∧V2= \"Palms\"∧K1= K 2∧V1< 15 _, \"\\x00\", _, \"0\", \"1\" \"\\x00\", \"Palms\" Otherwise, this application may crash in production, when the input record does not have an expected delimiter. Arrays. To analyze reduceByKey ➎at line 20 (also in Figure 3b), we must model how the UDF operates on the input array of size K, [a1,a2,. . .,aK]and produces the corresponding",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "the input array of size K, [a1,a2,. . .,aK]and produces the corresponding output faдд(a1 ,faдд(a2. . .faдд(aK−1,aK). . .)). For example, the UDF (_+_) returns the sum of two input arguments. When the array size Kis given by a user, the final output Nisa1+(a 2+. . .(aK−1+aK)). Summary. Due to the internal path conditions entailed by indi- vidual UDFs, instead of four high-level dataflow paths shown in Figure 3a, Alice must consider eleven paths in total, which are enu- merated in Table 1. Figure 3 shows the symbolic execution tree at the level of dataflow operators on the left and the internal symbolic execution trees for individual UDFs on the right. Lastly, example data generated by BigTest for each JDU path using Z3 is shown in Table 1.",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "BigTest for each JDU path using Z3 is shown in Table 1. While these example data records may not look realis- tic, such data is necessary to exercise the downstream UDFs that are otherwise unreachable with the original dataset. For instance, filtering a dataset without any passing data record will result in an empty set and consequently, the UDFs after the filter will never get tested with the original data. Therefore, synthetic data is necessary and crucial to expose downstream program behavior. 4 APPROACH BigTest takes in an Apache Spark application in Scala as an input and generates test inputs to cover all paths of the program up to a given bound by leveraging theorem provers Z3 [ 19] and CVC4 [ 11]. 4.1 Dataflow Program Decomposition A",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "Z3 [ 19] and CVC4 [ 11]. 4.1 Dataflow Program Decomposition A DISC application is comprised of a direct acyclic graph where each node represents a dataflow operator such as reduce and cor- responding UDFs. As the implementation of dataflow operators in Apache Spark spans several hundred thousand lines of code, it is not feasible to perform symbolic execution of a DISC application along with the Spark framework code. Instead, we abstract the internal implementation of a dataflow operator in terms of logical specifi- cations. We decompose a DISC application into a dataflow graph where a node calls each UDF and combine the symbolic execution of the UDFs using the logical specification of dataflow operators. UDF Extraction. BigTest compiles the DISC application into Java bytecode and traverses each",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "Extraction. BigTest compiles the DISC application into Java bytecode and traverses each Abstract Syntax Tree (AST) to search for a method invocation corresponding to each dataflow operator. The input parameters of such method invocation are UDFs repre- sented as anonymous functions as illustrated in Figure 4b. BigTest stores the UDF as a separate Java class shown in Figure 4c and1sc.textFile(\"zipcode.csv\"). map{...} 2 .filter {_._2 == \"Palms\"} (a) DISC Application MethodDeclaration Name : “main” Body MethodInvocation Name : “fitler” parameter ClassInstanceCreation UDFfilter AnonymousClassDeclaration . . . . . . . . . . . . . . . . . . . . . . . . (b) Generated AST1class filter { 2 static void main(String args[]){ 3 apply(null); 4 } 5 static boolean apply(Tuple2 s){ 6 return s._2().equals(\"Palms\")",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "3 apply(null); 4 } 5 static boolean apply(Tuple2 s){ 6 return s._2().equals(\"Palms\") 7 } 8} (c) Extracted Filter UDF Figure 4: BigTest extracts UDFs corresponding to dataflow op- erators through AST traversal. generates a configuration file required by JPF for symbolic execu- tion. BigTest also performs dependency analysis to include external classes and methods referenced in the UDF. 1def f(a:Int,b:Int){ 2 return a+b; 3} 4//Usage in reduce 5...reduce {f} (a)1def f_reduce(arr:Array[Int]){ 2 var sum = 0; 3 for(a <- 1 to K)//K is bound 4 sum = udf(sum,arr(a)); 5 return sum; } (b) Figure 5: (a) a normal invocation of reduce with a corre- sponding UDF. (b) an equivalent iterative version with a bound K Handling Aggregator Logic. For aggregation operators, the at- tached UDF must be transformed.",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "Aggregator Logic. For aggregation operators, the at- tached UDF must be transformed. For example, the UDF for reduce is an associative binary function, which performs incremental ag- gregation over a collection shown in Figure 5a. We translate it into an iterative version with a loop shown in Figure 5b. To bound the search space of constraints, we bound the number of iterations to a user provided bound K(default is 2). 4.2 Logical Specifications of Dataflow Operators This section describes the equivalence classes generated by each dataflow operator’s semantics. We use CIto represent a set of path constraints on the input data, I, for a particular operator. A single element cinCIcontains path constraints that must be satisfied to exercise a corresponding unique path. We define fas the set 293",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "to exercise a corresponding unique path. We define fas the set 293 [Página 5] White-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Table 2: CIrepresents a set of incoming constraints from the input table I, where each constraint c∈CIrepresents a non- terminating path. c(t)represents that record t∈Imust satisfy constraint c.fdefines the set of path constraints generated by symbolically executing ud fand f(t)represents the path constraint of a unique path exercised by input tuple t. Operator Inputs Logical Specification filter( ud f)I: Input Table ud f :t→BoolNon-Terminating ❶∃t:t∈I∧c∈CI∧c(t)∧f(t) Terminating ❷∃t:t∈I∧c∈CI∧c(t)∧¬ f(t) map( ud f)I : Input Table, O : Output Table ud f :t→t′where t′∈ONon-Terminating ❸∃t:t∈I∧c∈CI∧c(t)∧f(t) flatmap( ud f)I : Input Table, O: Output Table ud f :t→Collection of t′where t′∈ONon-Terminating",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": ": Input Table, O: Output Table ud f :t→Collection of t′where t′∈ONon-Terminating ❹∃t:tI∈I∧c∈CI∧c(t)∧f(t) joinR : Right Table, tR∈R L : Left Table, tL∈LNon-Terminating ❺∃tR,tL:cR∈CR∧cL∈CL∧cR(tR)∧tR,key=tL,key∧cL(tL) Terminating ❻∃key,tR:cR∈CR∧cL∈CL∧cR(tR)∧tR,key=key∧(∀tL∈L:cL(tL)∧tL,key,key) Terminating ❼∃key,tL:cR∈CR∧cL∈CL∧cL(tL)∧tL,key=key∧(∀tR∈R:cR(tR)∧tR,key,key) groupByKeyI : Input Table t∈Iandt=(tkey,tvalue)Non-Terminating ❽∃t:t∈I∧c∈CI∧c(t)∧|{ X|x∈I∧xkey=tkey}|>0 reduce( ud f) reduceByKey( ud f)I : Input Table, O: Output ud f :(t,t)→t′where t′∈ONon-Terminatingud f′is an iterative version of the original UDF ud f, given as an input to reduce/reduceByKey. f′represents the set of path constraints generated from symbolic execution of ud f′. ❾∃t1,t2,t3, . . . , tn∈I:c1,c2, . . . , cn∈CI∧c1(t1)∧c2(t2)∧. . . ..∧cn(tn)∧f′(I) of symbolic path constraints of a UDF where f(t)represents con- straints of a unique path exercised by input t. By abstracting the implementation of dataflow operators into logical specifications, BigTest does not have to symbolically",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "of dataflow operators into logical specifications, BigTest does not have to symbolically execute the Spark framework code (about 700KLOC), as it focuses on application level faults only as opposed to framework implementation faults which is out of the scope of this paper. BigTest supports all popular dataflow operators with the exception of deprecated operators such as co-join . Filter. Filter takes a boolean function ud f deciding if an input record should be passed to downstream operators or not. Therefore, we model two equivalence classes: (1) there exists a record tthat satisfies ud f and one of the incoming constraints CIfrom input table I(i.e.,the table produced by its upstream satisfying operator), shown in ❶Table 2; (2) there exists a record tthat satisfies one of the incoming constraints but",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "there exists a record tthat satisfies one of the incoming constraints but not ud f, shown in ❷Table 2. Map and Flatmap. Maptakes a UDF ud f as an input and applies it to each input record to produce an output record. It has one equivalence class, where there exists tuple tfrom the input table Isatisfying one of the incoming constraints, c∈CIand also one of the path constraints in fi.e.,path constraints generated by symbolically executing ud f, shown in ❸Table 2. Mapis supported by the previous work Sedge butSedge considers the UDF ud f as a black box, uninterpreted function. Flatmap splits an input record using a ud f to generate a set of records, and thus the equivalence class of flatmap is similar to that of map,",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "thus the equivalence class of flatmap is similar to that of map, as shown in ❹.BigTest handles flatmap by explicitly modeling a collection, described in Section 4.3. Join. Join performs an inner-join of two tables tron the right and table tlon the left based on the equality of keys, assuming that records from both tables are of the type Tuple (key, value). We model the output records of join into three equivalence classes: (1) the key of tuple tRin the right table matches with a key of tuple tL on the left; (2) the key of tuple tRin the right table does not match with any key of tuple tLon the left; and (3) the key of tuple tLin the left table does not match with any key",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "of tuple tLin the left table does not match with any key of tuple tRon the right. ❺,❻, and❼in Table 2 represent the three equivalence classes. Reduce and ReduceByKey. reduce takes a ud f and a collection as inputs and outputs an aggregated value, while reduceByKey performs a similar operation per key. As discussed in Section 4.1, BigTest generates an equivalent iterative version of the ud f with a loop. By this refactoring of ud f toud f′, the equivalence classes could be modeled similar to that of map, where there exist inputrecords t1,t2, . . . , tn∈Ion which each of the corresponding non- terminating constraint ( c1,c2, . . . , cn)∈CIfrom the input table I holds true. In addition, each record must satisfy the constraints",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "table I holds true. In addition, each record must satisfy the constraints of ud f′, satisfying f′([t1,t2, . . . , tn]), as shown in ❾of Table 2. 4.3 Path Constraint Generation This section describes several enhancements in Symbolic Path Finder (SPF) to tailor symbolic execution for DISC applications. DISC applications extensively use string manipulation operations and rely on a Tuple data structure to enable key-value based op- erations. Using an off-the-shelf SPF naïvely on a UDF would not produce meaningful path conditions, thus, overlooking faults dur- ing testing. 1def parse(s:String){ 2 val cols = s.split(\",\") 3 (cols(0) , cols(1)) } Figure 6: A UDF with string manipulation Strings. Operations such as split for converting an input record into a key-value pair are common in DISC applications but",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "input record into a key-value pair are common in DISC applications but are not supported by SPF. BigTest extends SPF by capturing calls to split , recording the delimiter, and returning an array of symbolic strings. When an n-th element of this symbolic array is requested, SPF returns a symbolic string encoded as splitn with a corresponding index. By representing the effect of Figure 6 as (splitn(\",\",s,0), splitn(\",\",s,1)) ,BigTest generates one terminating constraint, where scan only split into fewer than two segments, and one non- terminating constraint where scan split into at least two segments. Due to no split support, naïve SPF generates a string without any delimiter as a test input e.g.,\"\\x00\" instead of \"\\x00,\\x00\". This input would lead to ArrayIndexOutOfBoundsException while accessing a string using split(\",\")(1)",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "This input would lead to ArrayIndexOutOfBoundsException while accessing a string using split(\",\")(1) . 1def agg(arr:Array[Int]){ 2 val sum = arr(0); // Bound K=3 3 for(a <- 1 to min(arr.size,2)) sum += arr(a)<0 ? 0 : arr(a); 4 sum } Figure 7: An iterative version of aggregator UDF Collections. Constructing and processing collections through op- erators such as flatmap are essential in DISC applications. There- fore, BigTest explicitly models the effect of applying a UDF on a collection. In Figure 7, an iterative version of aggregator logic 294 [Página 6] ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim produced by BigTest takes a collection as input and sums up each element, if the element is greater than or equal to",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "up each element, if the element is greater than or equal to zero. Given a user-provided bound K=3 BigTest unrolls the loop three time and generates four pairs of a path condition (P) and the corresponding effect (E): (1)P:a(1)<0∧a(2)<0 ,E:a(0) (2)P:a(1)≥0∧a(2)<0 ,E:a(0)+a(1) (3)P:a(1)<0∧a(2)≥0,E:a(0)+a(2) (4)P:a(1)≥0∧a(2)≥0,E:a(0)+a(1)+a(2) A naïve SPF does not handle collections well and thus may generate an array of length 1 only, not exercising line 3 in Figure 7. For example, agg({3}) outputs the same sum of 3, when arr(a)<0 is mutated to arr(a)>0 ), because the loop starts from 1 instead of 0, andsumis initialize to the first element of the array. Thus, it is not possible to defect the fault using an array of length 1. Exceptions. BigTest extends SPF to explicitly model exceptions. For example,",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "length 1. Exceptions. BigTest extends SPF to explicitly model exceptions. For example, when an expression involves a division operator, divi- sion by zero is possible, which can lead to program termination. In Figure 1, BigTest creates two additional terminating path conditions, due to division by zero (i.e., x<y∧x==0 andy≤x∧y==0 ). Combining UDF symbolic execution with equivalence classes. BigTest combines the path conditions of each UDF with the incom- ing constraints from its upstream operator. For example, the UDF offilter (➌) in Section 3 produces a path condition of s._2 == \"Palms\" . Suppose that the upstream operator mapproduces one non-terminating path condition s.split(\",\").length ≥2with the effect s._2 =splitn(s,\",\",1) . Inside the equivalence classes offilter —rows ❶and❷in Table 2, BigTest plugs in the incoming path conditions (/effects) of an",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "Table 2, BigTest plugs in the incoming path conditions (/effects) of an upstream operator maptoCIand the path conditions (/effects) of the filter ’s UDF to f, producing the following path conditions. •c(t)∧f(t):s.split(\",\").length ≥2∧ splitn (s,\",\",1) == \"Palms\" •c(t)∧¬ f(t):s.split(\",\").length ≥2∧ ¬(splitn (s,\",\",1) == \"Palms\") Joint Dataflow and UDF Path. BigTest defines the final set of paths of a DISC application as Joint Dataflow and UDF (JDU) paths. We define a JDU path as follows: let G=(D,E)represent a directed acyclic graph of a DISC application where Dis a set of vertices representing dataflow operators and Erepresents directed edges connecting dataflow operators. Imagine a DISC application con- structed with a mapfollowed by filter andreduce . We represent this dataflow graph as G=(D,E)such that D={d1,d2,d3,t1} andE={(d1,d2),(d2,d3),(d2,t1)}where d1,d2, and d3aremap, filter ,",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "dataflow graph as G=(D,E)such that D={d1,d2,d3,t1} andE={(d1,d2),(d2,d3),(d2,t1)}where d1,d2, and d3aremap, filter , and reduce respectively. filter introduces a terminating edge(d2,t1)where a terminating vertex is t1. Since each dataflow operator takes a user-defined function f, for a vertex di, we define a subgraph Gi=(Vi,Ei)which represents the control flow graph of f. In this subgraph, a vertex v∈Vi represents a program point and an edge (va,vb)∈Eirepresents the flow of control from vatovb.Gihasv1=start andvn=stop corresponding to the first and last statements. Then from each dataflow operator node di, we add a call edge from dito the start node of Giand from the stop node of Gito the di+1. Since some UDFs include a loop and thus have a cycle in the control flow graph,1(assert (= line2 (str.++ (str.++ line20 \",\") line21)))",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "in the control flow graph,1(assert (= line2 (str.++ (str.++ line20 \",\") line21))) 2(assert 3 (= line1 4 (str.++ (str.++ \" \" \",\") 5 (str.++ (str.++ line11 \",\") 6 (str.++ (str.++ \" \" \",\") (str.++ (str.++ line13 \",\") line14)))))) 7(assert 8 (and (not (= (str.to.int line14) 0)) 9 (and (isinteger line14) 10 (and (isinteger line13) 11 (and (= \"Palms\" line21) 12 (and (= x11 line20) 13 (and (<= s21 15) 14 (and (<= s21 40) (and (= s21 x621) (and (= s1 x61) (= s22 x622))))))))))))))) 15 (assert 16 (and (= x11 line11) 17 (and (= x12 (/ (str.to.int line13) (str.to.int line14))) 18 (and (= x61 x11) 19 (and (= x621 x12) (and (= x622 x42) (and (= x71 \"walk\") (= x72 1)))))))))))) Figure 8: Output SMT query constructed",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "(= x71 \"walk\") (= x72 1)))))))))))) Figure 8: Output SMT query constructed by BigTest to reflect JDU path constraint C11of Table 1 from motivating example. we finitize the loop using a user provided bound Kand unroll the loop Ktimes. We enumerate a set of all unique paths PKfor the graph Gwith expanded subgraphs and call each unique path a Joint Dataflow and UDF (JDU) path . For an arbitrary test suite T, the JDU path coverage is measured as a set of covered paths, PK(T)={p|p∈PK,∃t∈ T and t|=Cp}where a test input tsatisfies the path condition Cp of path p. Given a user-provided bound Kfor unrolling a loop, JDU path coverage is|PK(T)| |PK|. 4.4 Test Data Generation BigTest rewrites path constraints into an SMT query. For constraints on integer",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "BigTest rewrites path constraints into an SMT query. For constraints on integer variables, BigTest uses analogous arithmetic and logical operators available in SMT. For string constraints, BigTest uses operations such as str.++ ,str.to.int , and str.at .BigTest in- troduces a new splitn symbolic operation. If a path constraint con- tains a clause v = splitn(\",\" s,1) ,BigTest generates (assert (= s (str.++ \" \" (str.++ \",\" v)))) that is equivalent to s = \" ,v\"where vis a symbolic string. The path conditions produced byBigTest do not contain arrays and instead model individual elements of an array up to a given bound K. BigTest generates interpreted functions for Java native methods not supported by Z3. For example, BigTest replaces isInteger with an analogous Z3 function. BigTest executes each SMT",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "BigTest replaces isInteger with an analogous Z3 function. BigTest executes each SMT query sepa- rately and finds satisfying assignments (i.e., test inputs) to exercise a particular path. While executing each SMT query independently may lead to redundant solving of overlapping constraints, in our experiments, we do not find it as a performance bottleneck. Theoret- ically, the number of path constraints increases exponentially due to branches and loops; however, empirically, our approach scales well to DISC applications, because UDFs tend to be much smaller (in order of hundred lines) than DISC frameworks and we abstract the framework implementation using logical specifications. Figure 8 shows an SMT query produced by BigTest for Figure 2. Lines 1 to 6 constrict the first table to have four segments and the second table",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "constrict the first table to have four segments and the second table to have two segments separated by a comma. Lines 7 to 10 restrict a string to be a valid integer. To enforce such constraint 295 [Página 7] White-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Table 3: Subject Programs Subject # of Program Characteristics JDU Paths#ProgramOutputOperatorsOperatorsString Parsing # Branches # UDFs (K=2) P1 IncomeAggregate Total income of individuals earning ≤$300 weekly 3 map, filter, reduce ✓ 2 4 6 P2 MovieRatings Total number of movies with rating ≥4 4 map, filter, reduceByKey ✓ 1 4 5 P3 AirportLayover Total layer time of passengers per airport 3 map, filter, reduceByKey ✓ 2 4 14 P4 CommuteTypeTotal number of people",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "map, filter, reduceByKey ✓ 2 4 14 P4 CommuteTypeTotal number of people using each form of transport for daily commute6map, fitler, join, reduceByKey✓ 3 5 11 P5 PigMix-L2 PigMix performance benchmark 5 map, join ✓ 2 6 4 P6 Grade Analysis List of classes with more than 5 failing students 5flatmap, filter, reduceByKey, map✓ 2 3 30 P7 WordCount Finds the frequency of words 3 flatmap, map, reduceByKey ✓ 1 3 4 P1 P2 P3 P4 P5 P6 P7020406080100 100 100 100 100 100 100 10016.7 40 14.3 18.2 25 13.3 2566.7 60 28.6 54.5 75 76.7 100JDU Path Co veraдe (Normalized ,%)BigTest Sedge Original Figure 9: JDU path coverage of BigTest ,Sedge , and the original input dataset that crosses the boundary of strings and integers, BigTest",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "original input dataset that crosses the boundary of strings and integers, BigTest uses a custom function isinteger and Z3 function str.to.int . Lines 11 to 14 enforce a record to contain “Palms” and the speed to be less than or equal to 15. Lines 15 to 19 join these constraints generated from a UDF to the subsequent dataflow operator. 5 EVALUATION We evaluate the effectiveness and efficiency of BigTest using a diverse set of benchmark DISC applications. We compare BigTest against Sedge in terms of path coverage, fault detection capability, and testing time. We compare test adequacy, input data size, and potential time saving against three alternative testing methods: (1) random sampling of k% records, and (2) using a subset of the first k% records, and (3) testing",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "(2) using a subset of the first k% records, and (3) testing on the entire original data. •To what extent BigTest is applicable to DISC applications? •How much test coverage improvement can BigTest achieve? •How many faults can BigTest detect? •How much test data reduction does BigTest provide? •How long does BigTest take to generate test data? Subject Programs. In terms of benchmark programs, we use seven subject programs from earlier works on testing [ 31] and debugging DISC applications [ 25,28], listed in Table 3. The PigMix bench- mark package contains a data generator script that generates large scale datasets. We utilize mapandflatmap with UDFs in Apache Spark to translate unsupported Pig operators like load As and split . Three programs MovieRating (P2), AirportLayover (P3), andWordCount (P7)",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "As and split . Three programs MovieRating (P2), AirportLayover (P3), andWordCount (P7) are adapted from BigSift [25]. Each program is paired with a large scale dataset. The rest are self-created cus- tom Apache Spark applications to add heterogeneity in dataflow operators and UDFs. Table 3 shows detailed descriptions of subject programs. All applications (1) involve complex string operations including split ,substring , and toInt , (2) perform complex arith- metics, (3) use type Tuple for key-value pairs, and (4) generate and process a collection with custom logic using flatmap . Experimental Environment. We run all large-scale data process- ing on a 16-node cluster. Each node is running at 3.40GHz and equipped with 4 cores, 32GB of RAM, and 1TB of storage allowingP1 P2 P3 P4 P5 P6 P7020406080100",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "RAM, and 1TB of storage allowingP1 P2 P3 P4 P5 P6 P7020406080100 100 100 100 100 100 100 10066.7 60 28.6 18.2 75 76.7 10066.7 60 28.6 18.2 75 76.7 100JDU Path Co veraдe (Normalized ,%)BigTest Random Sample first 1% Data Figure 10: JDU path coverage of BigTest in comparison to al- ternative sampling methods us to run up to 120 tasks simultaneously. For storage, we use HDFS version 1.0.4 with a replication factor of 3. Due to a very small size of test data generated by BigTest , we leverage Apache Spark’s local running mode to perform experiments on a single machine. 5.1 Dataflow Program Support BigTest supports a variety of dataflow operators prevalent in DISC applications. For instance, Apache Spark provides flatmap and reduceByKey for constructing",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "DISC applications. For instance, Apache Spark provides flatmap and reduceByKey for constructing and processing collections. The pre- vious approach Sedge is designed for PIG Latin with only a limited set of operators support [ 31].Sedge is neither open-source nor have any implementation available for Apache Spark for direct comparison. Therefore, we faithfully implement Sedge precisely based on the technical details provided elsewhere [ 31]. We manu- ally downgrade BigTest by removing symbolic execution for UDFs and equivalence classes for certain operators to emulate Sedge . The implementations of both Sedge and BigTest are publicly available1. Out of seven benchmark applications written in Apache Spark, five applications contain flatmap andreduceByKey , therefore, Sedge is not able to generate testing data for these 5 applications. 5.2 Joint Dataflow and UDF",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "generate testing data for these 5 applications. 5.2 Joint Dataflow and UDF Path Coverage We evaluate code coverage of BigTest ,Sedge , and the original input dataset based on JDU path coverage defined in Section 4.3. JDU Path Coverage Evaluation. We compare BigTest with three alternative sampling techniques: (1) random sampling of k% of the original dataset, (2) selection of the first k% of the original dataset, as developers often test DISC applications using head -n , and (3) a prior approach Sedge . To keep consistency in our experiment setting, we enumerate JDU paths for a given user-provided bound K and measure how many of these paths are covered by each approach. Figure 9 compares the test coverage from BigTest ,Sedge , and the original dataset. Y",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "the test coverage from BigTest ,Sedge , and the original dataset. Y axis represents the normalized JDU path coverage ranging from 0% to 100%. Across seven subject programs, we ob- serve that Sedge covers significantly fewer JDU paths (22% of what 1https://github.com/maligulzar/BigTest 296 [Página 8] ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim 10−110010110220304050 Loдscale k(%)JDU Path Co veraдe (Normalized ,%)k%Random Sample First k %of Data (a) JDU Path Coverage10−110010110205101520 Loдscale k(%)Test runnin дtime(s) k%Random Sample First k %of Data (b) Test Execution time Figure 11: The number of JDU paths covered and the test exe- cution time when k%of the data is randomly selected and the f irst k %of data is selected for subject program CommuteType .",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "f irst k %of data is selected for subject program CommuteType . is covered by BigTest ). By not modelling the internal paths of UDFs, Sedge fails to explore many JDU paths. Even when the complete dataset is used, the JDU path coverage reaches only 66% of what BigTest could achieve. The entire dataset achieves better coverage than Sedge but it still lacks coverage compared to BigTest . In other words, using the entire bigdata for testing does not necessarily provide high test adequacy. In Figure 10, both random 1% sample andfirst 1% sample provide 59% of what is covered by BigTest . We perform another experiment to measure the impact of different sample sizes on JDU path cover- age and test execution time. Figure 11a and Figure",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "JDU path cover- age and test execution time. Figure 11a and Figure 11b present the results on CommuteType . InCommuteType , the covered JDU paths increases from two to six when the percentage of the selected data increases from 0.1% to 50%. For those small samples, input tables do not have matching keys to exercise downstream operators and the time and distance columns may not have specific values to exercise all internal paths of the UDF. In terms of running time, as the sample size ( k) increases, the test execution time also increases linearly (see Figure 11b in which x-axis is in log scale). 5.3 Fault Detection Capability We evaluate BigTest ’s ability to detect faults by manually injecting commonly occurring faults. Because DISC applications are rarely",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "faults by manually injecting commonly occurring faults. Because DISC applications are rarely open-sourced for data privacy reasons and there is no existing benchmark of faulty DISC applications, we create a set of faulty DISC applications by studying the characteristics of real world DISC application bugs and injecting faults based on this study. We carefully investigate Stack Overflow and Apache Spark Mail- ing lists with keywords; Apache Spark exceptions, task errors, failures , and wrong outputs and inspect top 50 posts. Many errors are re- lated to performance and configuration errors; thus, we filter out those and analyze 23 posts related to coding errors. For each post, we investigate the type of fault by reading the question, posted code, error logs, answers, and accepted solutions. We categorize our findings",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "posted code, error logs, answers, and accepted solutions. We categorize our findings into seven common fault types: (1)incorrect string offset: e.g., a user uses 1 instead of 0 as the start- ing index in method substring and encounters StringIndex- OutOfBoundsException [7]. (2)incorrect column selection: e.g., a user accesses a wrong col- umn in a csv file and thus receives ArrayIndexOutOfBound- sException [5]. (3)use of wrong delimiters:e.g., while splitting a string a user uses \"[ ]\" instead of \"\\[\\]\", leading to a wrong output [8].Table 4: Fault detection capabilities of BigTest and Sedge Subject program P1 P2 P3 P4 P5 P6 P7 Seeded Faults 3 6 6 6 4 4 2 Detected by BigTest 3 6 6 6 4 4 2 Detected by Sedge 1 6 4 4 2",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "6 4 4 2 Detected by Sedge 1 6 4 4 2 3 0 (4)incorrect branch conditions: e.g., a user places a wrong order of control predicates, executing only one branch’s side [4]. (5)wrong join types: e.g., a user uses a wrong relational operator such as cartesian join instead of inner join [3]. (6)swapping a key with a value: e.g., a user tries to join two tables while the keys and values are interleaved [6]. (7)other common mutations such as incorrect arithmetic or Boolean operator in UDFs. When applicable, we inject one of each fault type in every ap- plication. For example, fault types 1 and 3 could only be inserted when substr orsplit method is used. When a fault type is appli- cable to multiple locations, we",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "used. When a fault type is appli- cable to multiple locations, we select a location which is inspired by and similar to the fault location in the corresponding StackOver- flow/Mailing List post. For instance, for fault type (2) above, we manually modify code to extract the first column instead of the second as a key in line 4 of Figure 2. Similarly, for fault type (3), we introduce fault by replacing the delimiter \",\" with \":\". In total, our benchmark comprises of 31 faulty DISC applications. While Sedge is not designed to handle string constraints, the main goal of this exercise is to justify the need to model UDFs and string constraints. Sedge represents the internal UDFs as uninterpreted functions and, therefore, is unable to model all internal",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "UDFs as uninterpreted functions and, therefore, is unable to model all internal UDF paths. Conversely, BigTest treats UDFs as interpreted functions by representing them symbolically and models all internal UDF paths (up to bound k) which is crucial for high coverage testing of UDF’s internal. Table 4 shows a comparison of fault detection by BigTest and Sedge .BigTest detects 2X more injected faults than Sedge . For in- stance, in application P4, BigTest detects 6 faults, whereas Sedge detects 4 faults. Sedge uses concrete execution to model the UDF exercising line 16 of Figure 2 only. Therefore, it is unable to find an input for detecting fault at line 17 when the binary operator \">\" is replaced with \"<\"(i.e.,s._2._1>15 tos._2._1<15 ). Similarly, when join in line 13 is",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "replaced with \"<\"(i.e.,s._2._1>15 tos._2._1<15 ). Similarly, when join in line 13 is changed to rightOuterJoin ,Sedge cannot detect any difference in the output because the equivalence classes do not model the terminating cases of join. Table 5: Modelling terminating and non-terminating cases Output from programApproach Test Input DataOriginal Faulty BigTestTerminating CS100:41,01l Non-terminating CS200:0,0,0,0,0,0CS200CS100 CS200 Alternative Non-terminating CS200:0,0,0,0,0,0 CS200 CS200 As another example, application P6 identifies courses with more than 5 failing students. A faulty version of P6 replaces the filter predicate count>5 tocount>0 to output courses with at least one failing student. The original version of P6 uses mapandfilter to parse each row and identify failing students, reduceByKey to count the number of failing students, and uses filter to find courses with more than 5 failing students. BigTest",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "uses filter to find courses with more than 5 failing students. BigTest generates at least two records to exercise both terminating and non-terminating cases of the last filter ; thus, the original and faulty versions produce different 297 [Página 9] White-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia P1 P2 P3 P4 P5 P6 P71011071013 6 514 1143064·109 5.21·1054.48·1083.2·1082.4·108 4·1071.11·108#of input records(loдscale)Minimal input data selected for maximal JDU coverage Entire data Figure 12: Reduction in the size of the testing data by BigTest P1 P2 P3 P4 P5 P6 P7020406080100 15.5 13.759.9 29.724.252.6 2.2 2.9 4.2 6.4 4.2 5.31.8Test Runnin дTime(s) Entire data Test data generated by BigTest Figure 13: Test running time of entire data on large-scale cluster vs.",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "Figure 13: Test running time of entire data on large-scale cluster vs. testing on local machine with BigTest outcomes on this data. On the other hand, a record is generated to exercise a non-terminating case only. Such data would produce the same outcome for both the original and the faulty versions, unable to detect the injected fault, as shown in Table 5. 5.4 Testing Data Reduction Testing DISC applications on the entire dataset is expensive and time-consuming. BigTest minimizes the size of the dataset, while maintaining the same test coverage. It generates only a few data records (in order of tens) to achieve the same JDU path coverage as the entire production data. Four out of seven benchmarks have an accompanied dataset, whereas the rest relies on a",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "seven benchmarks have an accompanied dataset, whereas the rest relies on a synthetic dataset of around 20GB each. Figure 12 shows the comparison result. In application P6, BigTest generates 30 rows of data to achieve 33% more JDU path coverage than the entire dataset of 40 million records. In other words, BigTest produces testing data 106times smaller than the original dataset. Across all benchmark applications, BigTest generates data ranging from 5 to 30 rows. This is 105to 108 times smaller than the original dataset, showing the potential to significantly reduce dataset size for local testing. 5.5 Time and Resource Saving By minimizing test data without compromising JDU path coverage, BigTest consequently reduces the test running time. The benefit of a smaller test data is twofolds: (1) the amount",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "The benefit of a smaller test data is twofolds: (1) the amount of time required to run a test case decreases, and (2) the amount of resources (worker nodes, memory, disk space, etc.) for running tests also decreases. We measure, on a single machine, the total running time by BigTest and compare it with the testing time on a 16-node cluster with the entire input dataset. We present a breakdown of the total running time into test data generation vs. executing an application on the generated data. Figure 13 represents the evaluation results. In application P6, it takes 5.3 seconds on a single machine to test with data from BigTest otherwise testing takes 387.2 CPU seconds (24.2 seconds x 16 machines) on the entire dataset, which still lacks",
        "start_idx": 8004,
        "end_idx": 8132
      },
      {
        "text": "(24.2 seconds x 16 machines) on the entire dataset, which still lacks complete JDU path coverage. Across the seven subject programs,P1 P2 P3 P4 P5 P6 P7020406080 10.67.374.2 23 817.6 4.78.44.470 16.6 4.112.3 2.93.7 3.8 3.5 3.9 3.8 3.8 2.6Runnin дTime(s) Constraint Generation Constraint Solver Test Execution Figure 14: Breakdown of BigTest ’s running time 1 2 3 4 5100102104 Deдree of bound(K)#ofJDU PathsW ord Count Grades Anal ysis Income A ддreдate 1 2 3 4 5101102103 Deдree of bound(K)Test Generation Time (s) Figure 15: BigTest ’s performance when the degree of upper bound (K) on loop iteration and collection size changes BigTest improves the testing time by 194X, on average, compared to testing with the entire dataset. Figure 14 reports the complete breakdown of the total running",
        "start_idx": 8120,
        "end_idx": 8248
      },
      {
        "text": "entire dataset. Figure 14 reports the complete breakdown of the total running time of BigTest . The maximum test generation time observed is 70 seconds for Airport Layover (P3) in which 66 seconds are consumed by constraint solving. This is because the resulting JDU paths include integer arithmetics and complex string constraints together. Solving such constraints that cross the boundaries of different dimensions (integer arithmetics vs. string constraints) is time consuming even after BigTest ’s optimizations. If we combine both the test running time and test generation time and compare BigTest with the testing time with the entire dataset, BigTest still outperforms. In fact, BigTest still is 59X faster than testing on the entire dataset. 5.6 Bounded Depth Exploration BigTest takes a user-provided bound Kto bound the number",
        "start_idx": 8236,
        "end_idx": 8364
      },
      {
        "text": "Bounded Depth Exploration BigTest takes a user-provided bound Kto bound the number of times a loop is unrolled. We assess the impact of varying Kfrom 1 to 5 and present the results in Figure 15. At K=2, the number of JDU paths for GradeAnalysis is 36. When Kis 3, BigTest generates 438 JDU paths. An exponential-like increase in the test generation time can be seen across the subject program, as we increase K. When K=2 in GradeAnalysis ,BigTest takes 12 seconds and with K=3,BigTest takes 204 seconds. We empirically find K=2 to be a reasonable upper bound for loop iteration to avoid path explosion. 5.7 Threats to Validity As we manually seed faults in the benchmark applications, the loca- tion of faults may introduce a bias in fault",
        "start_idx": 8352,
        "end_idx": 8480
      },
      {
        "text": "applications, the loca- tion of faults may introduce a bias in fault detection rate of BigTest posing a threat to internal validity. However, as mentioned before, most type of faults are only applicable to a single code location. If a fault type is applicable to multiple locations, we then select the fault location inspired by the corresponding StackOverflow/Mailing List post. In case of external validity, our classification of DISC faults may not be representative of all possible DISC application faults out there, as the survey is based on 50 StackOverflow/mailing lists 298 [Página 10] ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim posts. Additionally, the selection of fault types in our evaluation may be unfair to prior approaches. We",
        "start_idx": 8468,
        "end_idx": 8596
      },
      {
        "text": "fault types in our evaluation may be unfair to prior approaches. We attempt to mitigate this bias by restricting the evaluation to top seven most commonly occurring faults in DISC applications. To eliminate this threat in the future, we plan to perform a large scale study on DISC application faults. 6 RELATED WORK Testing Map-Reduce Programs. Csallner et al. propose the idea of testing commutative and associative properties of Map-Reduce programs by generating symbolic constraints [ 18]. Their goal is to identify non-determinism in a Map-Reduce program arising from a non-associative or non-commutative user-defined function in thereduce operator. They produce counter examples as evidence by running a constraint solver over symbolic path constraints. Xu et al. add few more Map-Reduce program properties such as (1) operator selectivity ,",
        "start_idx": 8584,
        "end_idx": 8712
      },
      {
        "text": "add few more Map-Reduce program properties such as (1) operator selectivity , (2)operator statefulness , and (3) partition interfer- ence [45] . Both of these techniques test only high-level properties of individual dataflow operators and they do not model the inter- nal program paths of user-defined functions. Olsten et al. generate data for Pig Latin programs [ 34]. Their approach considers each operator in isolation and does not model internal program paths of UDFs—treated as black-box. Furthermore, Olsten et al. require knowing the inverse function of a UDF given to transform . Li et al. ( Sedge ) [31] is the most relevant approach to BigTest . Sedge has three main limitations. First, its symbolic execution does not analyze the internal paths of individual UDFs. It considers UDFs",
        "start_idx": 8700,
        "end_idx": 8828
      },
      {
        "text": "does not analyze the internal paths of individual UDFs. It considers UDFs as black box procedures and encodes them into uninterpreted functions . Second, it does not support operators such as flatmap , reduce , and reducebyKey , which are essential for constructing a collection and aggregating results from a collection in big data analytics. Third, the equivalence class modeling for each dataflow operator is not comprehensive, as it does not consider early termi- nating cases for some operators, where a data record does not flow to the next dataflow operator. Our empirical evaluation in Section 5 finds that these limitations lead to low defect detection in Sedge . Table 6 compares dataflow operator support for related approaches and shows that BigTest has the most comprehensive and advanced",
        "start_idx": 8816,
        "end_idx": 8944
      },
      {
        "text": "related approaches and shows that BigTest has the most comprehensive and advanced support for modern DISC applications. Test Generation in Databases. JDBC [ 41] or ODBC [ 2] enable software developers to write applications that construct and exe- cute database queries at runtime. Testing such programs requires test inputs and database states from a user. Emmi et al. perform con- colic execution of a program embedded with an SQL query [ 21] by symbolically executing the program till the point where a query is executed. Their approach is only applicable to basic SQL operations such as projection, selection, etc. ( e.g.,SELECT, WHERE ). Braberman et al. select input data to test the logic of computing additional fields from existing columns in the database [ 13]. They do not",
        "start_idx": 8932,
        "end_idx": 9060
      },
      {
        "text": "fields from existing columns in the database [ 13]. They do not handle arbitrary UDFs which are prevalent in DISC applications. Symbolic Execution. Symbolic execution is a widely used tech- nique in software engineering [ 12,27,37] and is used to generate test data using constraint solvers [ 14–16,23,32,33,40]. For ex- ample, Visser et al. use JPF (Java PathFinder [ 29]) to generate test input data [ 44]. However, the same approach cannot be applied to DISC applications directly because it would symbolically execute the application as well as the underlying DISC framework. Such practice will produce an unnecessarily large number of complexTable 6: Support of dataflow operators in related work Dataflow Operators Olston et al . Li et al . Emmi et al .Pan et al .BigTest Load",
        "start_idx": 9048,
        "end_idx": 9176
      },
      {
        "text": "Li et al . Emmi et al .Pan et al .BigTest Load ✓ ✓ ✓ ✓ ✓ Map (Select) ✓ ✓ ✓ ✓ ✓ Map (Transform) Incomplete Incomplete ✗ ✗ ✓ Filter (Where) ✓ ✓ ✓ ✓ ✓ Group ✓ ✓ ✗ ✗ ✓ Join Incomplete Incomplete ✗ Incomplete ✓ Union ✓ ✓ ✗ ✗ ✓ Flatmap (Split) ✗ Incomplete ✗ ✗ ✓ Intersection ✗ ✗ ✗ ✗ ✓ Reduce ✗ ✗ ✗ ✗ ✓ path constraints, facing scalability issues. This justifies and moti- vates our approach that abstracts dataflow operators as a logical specifications while performing symbolic execution for the UDFs. Rosette is a framework for designing a solver-aided language [ 42] to ease the process of translating each language construct into sym- bolic constraints. BigTest and",
        "start_idx": 9164,
        "end_idx": 9292
      },
      {
        "text": "process of translating each language construct into sym- bolic constraints. BigTest and Rosette both translate higher-order types such as arrays into lower-level constraints. Bang et al. address the problem of solving constraints crossing boundaries between different theories (numerics, integer, and string constraints) [ 10]. Such cross-theory constraints are known to be difficult to solve with Z3 or CVC4. They extend SPF by modeling strings into bit vectors and by integrating numeric model counting in ABC [ 9] which could be used for BigTest in the future. Regression Testing. Regression testing has been extensively stud- ied in software testing. Safe regression testing selects only those test cases that exercise the updated regions of a program [ 26]. Rothermel et al. summarize several regression testing techniques and evaluate them under",
        "start_idx": 9280,
        "end_idx": 9408
      },
      {
        "text": "Rothermel et al. summarize several regression testing techniques and evaluate them under a controlled environment [ 38]. Test augmentation tech- niques help developers generate new test data to cover code not ex- ercised by the available test cases using symbolic execution [ 17,30]. Xu et al. evaluate concolic and genetic test generation approaches and report trade-offs [ 46]. The aforementioned approaches are not directly applicable to DISC applications, as they do not explicitly model the combined behavior of dataflow (/relational) operators and the internal semantics of UDFs. 7 CONCLUSION Big data analytics are now prevalent in many domains. However, software engineering methods for DISC applications are relatively under-developed. To enable efficient and effective testing of big data analytics in real world settings, we present a novel white-box testing",
        "start_idx": 9396,
        "end_idx": 9524
      },
      {
        "text": "data analytics in real world settings, we present a novel white-box testing technique that systematically explores the combined behav- ior of dataflow operators and corresponding UDFs. This technique generates joint dataflow and UDF path constraints and leverages theorem solvers to generate concrete test inputs. BigTest can de- tect 2X more faults than the previous approach and can consume 194X less CPU time, on average than using the entire dataset. With BigTest ,fastlocal testing is feasible and testing DISC applications on the entire dataset may not be necessary. ACKNOWLEDGMENTS We thank the anonymous reviewers for their comments. The par- ticipants of this research are in part supported by Google PhD Fellowship, NSF grants CCF-1764077, CCF-1527923, CCF-1460325, CCF-1723773, ONR grant N00014-18-1-2037, Intel CAPA grant, and Samsung grant. We would also",
        "start_idx": 9512,
        "end_idx": 9640
      },
      {
        "text": "ONR grant N00014-18-1-2037, Intel CAPA grant, and Samsung grant. We would also like to thank Emina Torlak and Koushik Sen for their insightful discussions. 299 [Página 11] White-Box Testing of Big Data Analytics with Complex UDFs ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia REFERENCES [1] [n.d.]. Hadoop. http://hadoop.apache.org/. [2][n.d.]. Microsoft Open Database Connectivity (ODBC). https://msdn.microsoft. com/en-us/library/ms710252(v=vs.85).aspx. [3] 2015. . https://stackoverflow.com/questions/32190828. [4] 2016. . https://stackoverflow.com/questions/40494999. [5] 2017. . https://stackoverflow.com/questions/48021303. [6] 2017. . https://stackoverflow.com/questions/42459749. [7] 2018. . https://stackoverflow.com/questions/49505241. [8] 2018. . https://stackoverflow.com/questions/52083828. [9]Abdulbaki Aydin, Lucas Bang, and Tevfik Bultan. 2015. Automata-Based Model Counting for String Constraints. In Computer Aided Verification , Daniel Kroening and Corina S. Păsăreanu (Eds.). Springer International Publishing, Cham, 255– 272. [10] Lucas Bang, Abdulbaki Aydin, Quoc-Sang Phan, Corina S. Păsăreanu, and Tevfik Bultan. 2016. String Analysis",
        "start_idx": 9628,
        "end_idx": 9756
      },
      {
        "text": "Aydin, Quoc-Sang Phan, Corina S. Păsăreanu, and Tevfik Bultan. 2016. String Analysis for Side Channels with Segmented Oracles. In Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foun- dations of Software Engineering (FSE 2016) . ACM, New York, NY, USA, 193–204. https://doi.org/10.1145/2950290.2950362 [11] Clark Barrett, Christopher L. Conway, Morgan Deters, Liana Hadarean, Dejan Jovanovi’c, Tim King, Andrew Reynolds, and Cesare Tinelli. 2011. CVC4. In Proceedings of the 23rd International Conference on Computer Aided Verification (CAV ’11) (Lecture Notes in Computer Science) , Ganesh Gopalakrishnan and Shaz Qadeer (Eds.), Vol. 6806. Springer, 171–177. http://www.cs.stanford.edu/~barrett/ pubs/BCD+11.pdf Snowbird, Utah. [12] Robert S. Boyer, Bernard Elspas, and Karl N. Levitt. 1975. SELECT&Mdash;a Formal System for Testing and Debugging Programs by Symbolic Execution. In Proceedings of the International Conference on Reliable",
        "start_idx": 9744,
        "end_idx": 9872
      },
      {
        "text": "Programs by Symbolic Execution. In Proceedings of the International Conference on Reliable Software . ACM, New York, NY, USA, 234–245. https://doi.org/10.1145/800027.808445 [13] Víctor Braberman, Diego Garbervetsky, Javier Godoy, Sebastian Uchitel, Guido de Caso, Ignacio Perez, and Santiago Perez. 2018. Testing and Validating End User Programmed Calculated Fields. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2018) . ACM, New York, NY, USA, 827–832. https://doi.org/10.1145/3236024.3275531 [14] J. Burnim and K. Sen. 2008. Heuristics for Scalable Dynamic Test Generation. In Proceedings of the 2008 23rd IEEE/ACM International Conference on Automated Software Engineering (ASE ’08) . IEEE Computer Society, Washington, DC, USA, 443–446. https://doi.org/10.1109/ASE.2008.69 [15] Cristian Cadar and Dawson Engler. 2005. Execution Generated Test Cases: How",
        "start_idx": 9860,
        "end_idx": 9988
      },
      {
        "text": "[15] Cristian Cadar and Dawson Engler. 2005. Execution Generated Test Cases: How to Make Systems Code Crash Itself. In Proceedings of the 12th International Con- ference on Model Checking Software (SPIN’05) . Springer-Verlag, Berlin, Heidelberg, 2–23. https://doi.org/10.1007/11537328_2 [16] Cristian Cadar, Vijay Ganesh, Peter M. Pawlowski, David L. Dill, and Dawson R. Engler. 2006. EXE: Automatically Generating Inputs of Death. In Proceedings of the 13th ACM Conference on Computer and Communications Security (CCS ’06) . ACM, New York, NY, USA, 322–335. https://doi.org/10.1145/1180405.1180445 [17] Cristian Cadar, Patrice Godefroid, Sarfraz Khurshid, Corina S. Păsăreanu, Koushik Sen, Nikolai Tillmann, and Willem Visser. 2011. Symbolic Execution for Software Testing in Practice: Preliminary Assessment. In Proceedings of the 33rd Interna- tional Conference on Software Engineering (ICSE ’11) . ACM, New York, NY, USA, 1066–1071.",
        "start_idx": 9976,
        "end_idx": 10104
      },
      {
        "text": "on Software Engineering (ICSE ’11) . ACM, New York, NY, USA, 1066–1071. https://doi.org/10.1145/1985793.1985995 [18] Christoph Csallner, Leonidas Fegaras, and Chengkai Li. 2011. New Ideas Track: Testing Mapreduce-style Programs. In Proceedings of the 19th ACM SIG- SOFT Symposium and the 13th European Conference on Foundations of Soft- ware Engineering (ESEC/FSE ’11) . ACM, New York, NY, USA, 504–507. https: //doi.org/10.1145/2025113.2025204 [19] Leonardo De Moura and Nikolaj Bjørner. 2008. Z3: An efficient SMT solver. In International conference on Tools and Algorithms for the Construction and Analysis of Systems . Springer, 337–340. [20] Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: simplified data processing on large clusters. Commun. ACM 51, 1 (2008), 107–113. [21] Michael Emmi, Rupak Majumdar, and Koushik Sen. 2007. Dynamic Test Input Generation for Database Applications. In Proceedings of",
        "start_idx": 10092,
        "end_idx": 10220
      },
      {
        "text": "Sen. 2007. Dynamic Test Input Generation for Database Applications. In Proceedings of the 2007 International Symposium on Software Testing and Analysis (ISSTA ’07) . ACM, New York, NY, USA, 151–162. https://doi.org/10.1145/1273463.1273484 [22] Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto- mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI ’05) . ACM, New York, NY, USA, 213–223. https://doi.org/10.1145/1065010.1065036 [23] Patrice Godefroid, Nils Klarlund, and Koushik Sen. 2005. DART: Directed Auto- mated Random Testing. In Proceedings of the 2005 ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI ’05) . ACM, New York, NY, USA, 213–223. https://doi.org/10.1145/1065010.1065036 [24] Patrice Godefroid, Michael Y. Levin, and David A Molnar. 2008. Automated White- box Fuzz Testing. In Network Distributed",
        "start_idx": 10208,
        "end_idx": 10336
      },
      {
        "text": "David A Molnar. 2008. Automated White- box Fuzz Testing. In Network Distributed Security Symposium (NDSS) . Internet Society. http://www.truststc.org/pubs/499.html[25] Muhammad Ali Gulzar, Matteo Interlandi, Xueyuan Han, Mingda Li, Tyson Condie, and Miryung Kim. 2017. Automated Debugging in Data-intensive Scal- able Computing. In Proceedings of the 2017 Symposium on Cloud Computing (SoCC ’17). ACM, New York, NY, USA, 520–534. https://doi.org/10.1145/3127479.3131624 [26] Mary Jean Harrold, James A. Jones, Tongyu Li, Donglin Liang, Alessandro Orso, Maikel Pennings, Saurabh Sinha, S. Alexander Spoon, and Ashish Gujarathi. 2001. Regression Test Selection for Java Software. In Proceedings of the 16th ACM SIGPLAN Conference on Object-oriented Programming, Systems, Languages, and Applications (OOPSLA ’01) . ACM, New York, NY, USA, 312–326. https: //doi.org/10.1145/504282.504305 [27] W. E. Howden. 1977. Symbolic Testing and the DISSECT Symbolic Evaluation System.",
        "start_idx": 10324,
        "end_idx": 10452
      },
      {
        "text": "W. E. Howden. 1977. Symbolic Testing and the DISSECT Symbolic Evaluation System. IEEE Trans. Softw. Eng. 3, 4 (July 1977), 266–278. https://doi.org/10.1109/ TSE.1977.231144 [28] Matteo Interlandi, Ari Ekmekji, Kshitij Shah, Muhammad Ali Gulzar, Sai Deep Tetali, Miryung Kim, Todd Millstein, and Tyson Condie. 2018. Adding data provenance support to Apache Spark. The VLDB Journal 27, 5 (01 Oct 2018), 595–615. https://doi.org/10.1007/s00778-017-0474-5 [29] Sarfraz Khurshid, Corina S. Păsăreanu, and Willem Visser. 2003. Generalized Symbolic Execution for Model Checking and Testing. In Proceedings of the 9th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (TACAS’03) . Springer-Verlag, Berlin, Heidelberg, 553–568. http://dl. acm.org/citation.cfm?id=1765871.1765924 [30] James C. King. 1976. Symbolic Execution and Program Testing. Commun. ACM 19, 7 (July 1976), 385–394. https://doi.org/10.1145/360248.360252 [31] Kaituo Li, Christoph Reichenbach,",
        "start_idx": 10440,
        "end_idx": 10568
      },
      {
        "text": "ACM 19, 7 (July 1976), 385–394. https://doi.org/10.1145/360248.360252 [31] Kaituo Li, Christoph Reichenbach, Yannis Smaragdakis, Yanlei Diao, and Christoph Csallner. 2013. SEDGE: Symbolic example data generation for dataflow programs. In Automated Software Engineering (ASE), 2013 IEEE/ACM 28th Inter- national Conference on . IEEE, 235–245. [32] Rupak Majumdar and Koushik Sen. 2007. Hybrid Concolic Testing. In Proceedings of the 29th International Conference on Software Engineering (ICSE ’07) . IEEE Computer Society, Washington, DC, USA, 416–426. https://doi.org/10.1109/ICSE. 2007.41 [33] David Molnar, Xue Cong Li, and David A. Wagner. 2009. Dynamic Test Gener- ation to Find Integer Bugs in x86 Binary Linux Programs. In Proceedings of the 18th Conference on USENIX Security Symposium (SSYM’09) . USENIX Association, Berkeley, CA, USA, 67–82. http://dl.acm.org/citation.cfm?id=1855768.1855773 [34] Christopher Olston, Shubham Chopra, and Utkarsh Srivastava. 2009. Generating",
        "start_idx": 10556,
        "end_idx": 10684
      },
      {
        "text": "67–82. http://dl.acm.org/citation.cfm?id=1855768.1855773 [34] Christopher Olston, Shubham Chopra, and Utkarsh Srivastava. 2009. Generating Example Data for Dataflow Programs. In Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data (SIGMOD ’09) . ACM, New York, NY, USA, 245–256. https://doi.org/10.1145/1559845.1559873 [35] K. Ouaknine, M. Carey, and S. Kirkpatrick. 2015. The PigMix Benchmark on Pig, MapReduce, and HPCC Systems. In 2015 IEEE International Congress on Big Data . 643–648. https://doi.org/10.1109/BigDataCongress.2015.99 [36] Corina S. P ˇasˇareanu, Peter C. Mehlitz, David H. Bushnell, Karen Gundy-Burlet, Michael Lowry, Suzette Person, and Mark Pape. 2008. Combining Unit-level Symbolic Execution and System-level Concrete Execution for Testing Nasa Soft- ware. In Proceedings of the 2008 International Symposium on Software Testing and Analysis (ISSTA ’08) . ACM, New York, NY, USA, 15–26. https://doi.org/10.1145/ 1390630.1390635 [37] C.",
        "start_idx": 10672,
        "end_idx": 10800
      },
      {
        "text": "’08) . ACM, New York, NY, USA, 15–26. https://doi.org/10.1145/ 1390630.1390635 [37] C. V. Ramamoorthy, S. B. F. Ho, and W. T. Chen. 1976. On the Automated Generation of Program Test Data. IEEE Trans. Softw. Eng. 2, 4 (July 1976), 293–300. https://doi.org/10.1109/TSE.1976.233835 [38] Gregg Rothermel and Mary Jean Harrold. 1996. Analyzing Regression Test Selection Techniques. IEEE Trans. Softw. Eng. 22, 8 (Aug. 1996), 529–551. https: //doi.org/10.1109/32.536955 [39] Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A Concolic Unit Testing Engine for C. In Proceedings of the 10th European Software Engineering Conference Held Jointly with 13th ACM SIGSOFT International Symposium on Foundations of Software Engineering (ESEC/FSE-13) . ACM, New York, NY, USA, 263–272. https://doi.org/10.1145/1081706.1081750 [40] Matt Staats and Corina P ˇasˇareanu. 2010. Parallel Symbolic Execution for Struc- tural Test",
        "start_idx": 10788,
        "end_idx": 10916
      },
      {
        "text": "and Corina P ˇasˇareanu. 2010. Parallel Symbolic Execution for Struc- tural Test Generation. In Proceedings of the 19th International Symposium on Software Testing and Analysis (ISSTA ’10) . ACM, New York, NY, USA, 183–194. https://doi.org/10.1145/1831708.1831732 [41] Art Taylor. 2002. Jdbc: Database Programming with J2Ee with Cdrom . Prentice Hall Professional Technical Reference. [42] Emina Torlak and Rastislav Bodik. 2014. A Lightweight Symbolic Virtual Machine for Solver-aided Host Languages. In Proceedings of the 35th ACM SIGPLAN Con- ference on Programming Language Design and Implementation (PLDI ’14) . ACM, New York, NY, USA, 530–541. https://doi.org/10.1145/2594291.2594340 [43] Willem Visser, Klaus Havelund, Guillaume Brat, Seungjoon Park, and Flavio Lerda. 2003. Model Checking Programs. Automated Software Engg. 10, 2 (April 2003), 203–232. https://doi.org/10.1023/A:1022920129859 [44] Willem Visser, Corina S. P ˇasˇareanu, and Sarfraz Khurshid.",
        "start_idx": 10904,
        "end_idx": 11032
      },
      {
        "text": "203–232. https://doi.org/10.1023/A:1022920129859 [44] Willem Visser, Corina S. P ˇasˇareanu, and Sarfraz Khurshid. 2004. Test Input Gener- ation with Java PathFinder. In Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA ’04) . ACM, New York, NY, USA, 97–107. https://doi.org/10.1145/1007512.1007526 300 [Página 12] ESEC/FSE ’19, August 26–30, 2019, Tallinn, Estonia Muhammad Ali Gulzar, Shaghayegh Mardani, Madanlal Musuvathi, and Miryung Kim [45] Z. Xu, M. Hirzel, G. Rothermel, and K. L. Wu. 2013. Testing properties of dataflow program operators. In 2013 28th IEEE/ACM International Conference on Automated Software Engineering (ASE) . 103–113. https://doi.org/10.1109/ASE.2013.6693071 [46] Zhihong Xu, Yunho Kim, Moonzoo Kim, Gregg Rothermel, and Myra B. Co- hen. 2010. Directed Test Suite Augmentation: Techniques and Tradeoffs. In Proceedings of the Eighteenth ACM SIGSOFT International Symposium on",
        "start_idx": 11020,
        "end_idx": 11148
      },
      {
        "text": "and Tradeoffs. In Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foun- dations of Software Engineering (FSE ’10) . ACM, New York, NY, USA, 257–266. https://doi.org/10.1145/1882291.1882330 [47] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2012. Re- silient Distributed Datasets: A Fault-tolerant Abstraction for In-memory Cluster Computing. In Proceedings of the 9th USENIX Conference on Networked SystemsDesign and Implementation (NSDI’12) . USENIX Association, Berkeley, CA, USA, 2–2. http://dl.acm.org/citation.cfm?id=2228298.2228301 [48] Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, and Ion Stoica. 2010. Spark: Cluster Computing with Working Sets. In Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing (HotCloud’10) . USENIX Association, Berkeley, CA, USA, 10–10. http://dl.acm.org/citation.cfm? id=1863103.1863113 [49] Hucheng Zhou, Jian-Guang",
        "start_idx": 11136,
        "end_idx": 11264
      },
      {
        "text": "USENIX Association, Berkeley, CA, USA, 10–10. http://dl.acm.org/citation.cfm? id=1863103.1863113 [49] Hucheng Zhou, Jian-Guang Lou, Hongyu Zhang, Haibo Lin, Haoxiang Lin, and Tingting Qin. 2015. An Empirical Study on Quality Issues of Production Big Data Platform. In Proceedings of the 37th International Conference on Software Engineering - Volume 2 (ICSE ’15) . IEEE Press, Piscataway, NJ, USA, 17–26. http: //dl.acm.org/citation.cfm?id=2819009.2819014 301",
        "start_idx": 11252,
        "end_idx": 11311
      }
    ],
    "e4385e1a-3342-4d4e-aaf7-50e16ca2859f": [
      {
        "text": "[Página 1] QoS-Aware Proactive Data Replication for Big Data Analytics in Edge Clouds Qiufen Xia qiufenxia@dlut.edu.cn Dalian University of Technology Dalian, Liaoning, ChinaLuyao Bai bailuyao1997@outlook.com Dalian University of Technology Dalian , Liaoning, ChinaWeifa Liang wliang@cs.anu.edu.au Australian National University Canberra, ACT, Australia Zichuan Xu z.xu@dlut.edu.cn Dalian University of Technology Dalian, Liaoning, ChinaLin Yao yaolin@dlut.edu.cn Dalian University of Technology Dalian, Liaoning, ChinaLei Wang lei.wang@dlut.edu.cn Dalian University of Technology Dalian, Liaoning, China ABSTRACT We are in the era of big data and cloud computing, large quantity of computing resource is desperately needed to detect invaluable information hidden in the coarse big data through query evaluation. Users demand big data analytic services with various Quality of Service (QoS) requirements. However, cloud computing is facing new challenges in meeting stringent QoS requirements of",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "cloud computing is facing new challenges in meeting stringent QoS requirements of users due to the remoteness from its users. Edge computing has emerged as a new paradigm to address such shortcomings by bringing cloud services to the edge of the operation network in proximity of users for performance improvement. To satisfy the QoS requirements of users for big data analytics in edge computing, the data replication and placement problem must be properly dealt with such that user requests can be efficiently and promptly responded. In this paper, we consider data replication and placement for big data analytic query evaluation. We first cast a novel proactive data replication and placement problem of big data analytics in a two-tier edge cloud environment, we then devise an approximation algorithm with",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "a two-tier edge cloud environment, we then devise an approximation algorithm with an approximation ratio for it, we finally evaluate the proposed algorithm against existing benchmarks, using both simulation and experiment in a testbed based on real datasets, the evaluation results show that the proposed algorithm is promising. KEYWORDS Data replication and placement; big data analytics; edge clouds; query evaluation ACM Reference Format: Qiufen Xia, Luyao Bai, Weifa Liang, Zichuan Xu, Lin Yao, and Lei Wang. 2019. QoS-Aware Proactive Data Replication for Big Data Analytics in Edge Clouds. In 48th International Conference on Parallel Processing: Workshops (ICPP 2019), August 5–8, 2019, Kyoto, Japan. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3339186.3339207 Permission to make digital or hard copies of all or part of this work for personal or",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICPP 2019, August 5–8, 2019, Kyoto, Japan ©2019 Association for Computing Machinery. ACM ISBN 978-1-4503-7196-4/19/08. . . $15.00 https://doi.org/10.1145/3339186.33392071 INTRODUCTION Cloud platforms have been receiving ever-growing attentions in recent years to provide services in a wide range of information",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "in recent years to provide services in a wide range of information tech- nology (IT) domains, and offer on-demand processing, storage and bandwidth resources. Many services have been deployed on clouds and generate big data there, the big data are analyzed to obtain hidden valuable information for business advantages and decision- makings. However, cloud computing is facing new challenges in meeting the quality of service (QoS) requirements of emerging ap- plications, such as augmented reality, autonomous vehicles, timely query evaluation for big data analytics, to name a few. We argue that the most pressing requirement of those emerging applications is response latency, which is the time duration from submitting a request to the cloud to receiving the query result by the request user. The remote cloud data centers",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "the query result by the request user. The remote cloud data centers are not appropriate for achieving small response latencies, as it could suffer from limitations due to high transmission latency and risk of heavy workload as well as network bottlenecks. One promising solution to tackle the mentioned challenges is Edge Computing, which can exploit processing and storage capa- bilities at the edge of the network as near as possible to end-users. In this regard, the deployment of edge cloudlets in network access points can achieve remarkable benefits in terms of low-latency interactions and economic computing resource. Query evaluation for big data analytics demands large quantity of computing re- source and low response latency, by leveraging edge computing technologies, the response time to big data analytics queries can",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "edge computing technologies, the response time to big data analytics queries can be significantly reduced. To this end, an important approach is to proactively replicate a large dataset to multiple data centers or cloudlets so that query users can obtain their desired query results within their specified time duration. Although data replication and placement can improve system performance, it does not necessarily imply that more replicas will lead to better system performance, due to the fact that the mainte- nance of data consistency between the original dataset and its slave replicas in the network does incur cost. To maximize the benefit of query processing and dataset replications, strategic replicating and placing replicas of each dataset in a two-tier edge cloud is crucial. One fundamental problem thus is how",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "a two-tier edge cloud is crucial. One fundamental problem thus is how to place the replicas of datasets to different data centers or cloudlets in the two-tier edge cloud so that big data analytics queries can be evaluated, the [Página 2] ICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and Bai, et al. volume of datasets demanded by admitted queries is maximized, without violating the resource capacity constraints and delay re- quirements of users. Notice that, one main reason that we aim to maximize the volume of datasets demanded by admitted queries is as follows. Cloud service providers such as Amazon offer users a pay-as-you-go approach for pricing [ 3], maximizing the volume of datasets demanded by admitted queries means that users pay more for evaluating queries to",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "by admitted queries means that users pay more for evaluating queries to the cloud service providers who can thus obtain maximum income. Several studies on data replication and placement have been conducted in the past [ 1,6,26]. However, most these studies consid- ered neither data replications of the generated big data [ 1,26] nor QoS requirements of users [ 1,6,26]. In addition, there are several investigations on query evaluation and data placement [ 17,20]. Although some of them considered the data transmission cost, they did not incorporate the QoS requirements of users [ 17], or data replications and placements [ 20]. In this paper, we study proactive data replication and placement of query evaluation for big data analytics in a two-tier edge cloud with the aim to maximize",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "data analytics in a two-tier edge cloud with the aim to maximize the volume of datasets demanded by admitted queries while meeting users’ QoS requirements, subject to various resource capacities on an edge cloud network. The main contributions of this paper are as follows. •We first formulate a novel proactive QoS-aware data repli- cation and placement problem for big data analytic query evaluation in a two-tier edge cloud environment. We aim to maximize the volume of datasets demanded by admitted queries while meeting users’ end-to-end delay requirements. •We then propose an efficient approximation algorithm with provable approximation ratio for the problem through a primal-dual dynamic update technique. •We finally evaluate the performance of the proposed algo- rithm through experimental simulations and in a testbed using real datasets. The",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "rithm through experimental simulations and in a testbed using real datasets. The simulation results show that the per- formance of the proposed algorithm is promising, placing significantly higher volume of datasets demanded by queries admitted compared to some existing work. •To the best of our knowledge, this is the first time that the proactive QoS-aware data replication and placement prob- lem for big data analytics query evaluation in two-tier edge clouds is considered, and an efficient approximation algo- rithm is devised. The remainder of this paper is organized as follows. Section 2 in- troduces the system model and problem definition, followed by an approximation algorithm for the problem in Section 3. The per- formance evaluation of the proposed algorithm is conducted in Section 4. The related work is",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "the proposed algorithm is conducted in Section 4. The related work is presented in Section 5, and conclu- sions are given in Section 6. 2 PRELIMINARIES In this section, we first introduce the system model. We then give notations on big data analytics evaluation in the two-tier edge cloud under QoS requirements of users. We finally define the problem precisely.2.1 System model We consider a two-tier edge cloud G=(BS∪SW∪CL∪DC,E), which consists of a set BSof base stations through which users connect to edge cloudlets, a set SWof switches in a Wireless Met- ropolitan Area Network (WMAN), a set CLof edge cloudlets co- located with some switches in SW, and a setDCof data centers located at different geographical locations that are connected to the WMAN via the Internet to/from",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "geographical locations that are connected to the WMAN via the Internet to/from gateway nodes in SW. These edge cloudlets, switches (or access points), and data centers are inter-connected by a set Eof communication links, and e∈Eis a link between two cloudlets, two switches, a cloudlet and a switch, or a gateway node and a data center. LetCLibe an edge cloudlet in CL, and DCjbe a data center inDC. The computing resource of each edge cloudlet CLiand each data center DCjcan be used for processing data to evaluate queries, while their storage resource is used to store the query results and data replicas. The quantity of available computing re- source of each data center or edge cloudlet is limited, especially for cloudlets which usually consist of several servers to",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "is limited, especially for cloudlets which usually consist of several servers to fit into small machine rooms located in metropolitan areas. Denote by B(DCj)andB(CLi)the computing capacities of data center DCj and cloudlet CLi, respectively. Denote by A(CLi)andA(DCj)the available computing resources of edge cloudlet CLiand data center DCjat the moment. Evaluating queries of big data analytics in edge cloudlets and data centers consumes their computing resources. Letrmbe the amount of computing resource allocated to process a unit data. We do not restrict the capacity of storage resource of cloudlets and data centers, as the storage resource usually is abun- dant and inexpensive, compared with the expensive computing resource [26]. The processing and transmission of data in Gconsume comput- ing and bandwidth resources of edge clouds and thus incur process-",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "comput- ing and bandwidth resources of edge clouds and thus incur process- ing and transmission delays. Let d(CLi)andd(DCj)be the delays incurred by processing a unit data per unit computing resource in cloudlet CLiand data center DCjanddt(e)the transmission delay on link e∈Efor transferring a unit data. For simplicity, let V={CL∪DC}, and each node vl∈V represents either an edge cloudlet or a data center. An example of a two-tiered edge cloud Gis illustrated in Fig. 1. 2.2 Big data processing in the edge cloud With the wide adoption of cloud services, enterprise users usually have large scale of legacy services being outsourced to remote data centers, and these services generate large volume of data from their outsourced services, such as web logs, click streams, sensory data. Meanwhile, with the support",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "such as web logs, click streams, sensory data. Meanwhile, with the support of network service providers, more and more cloud services are deployed in edge cloudlets within the proximity of users to reduce the response time. To obtain valuable information and interesting patterns from such big data generated by services deployed at data centers and cloudlets, users may con- duct analysis on big data that are stored in remote data centers and edge cloudlets by issuing queries. Performing big data analytics in remote data centers causes very high latency, because large volume of intermediate results generated by processing the big data need to be transferred to edge cloudlets and join with the intermediate results there, the [Página 3] QoS-Aware Proactive Data Replication for Big Data Analytics ICPP 2019,",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "[Página 3] QoS-Aware Proactive Data Replication for Big Data Analytics ICPP 2019, August 5–8, 2019, Kyoto, Japan Figure 1: An example of a two-tier edge cloud G. delay requirements required by users may be violated ultimately. Therefore, proactively replicating big data from the remote cloud to edge cloudlets is an effective way to reduce data transmission delay and guarantee the timeliness of big data analytics. Meanwhile, the computing capacity of an edge cloudlet is very limited, it takes long time to evaluate queries, sometimes the computing resource of an edge-cloudlet even cannot satisfy the resource demands of big data query evaluation, so the big data generated in edge cloudlets can be proactively placed to the remote data centers and processed there, thereby reducing the processing delay to guarantee",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "data centers and processed there, thereby reducing the processing delay to guarantee the timeline of queries and satisfying the computing resource requirements of queries. We thus assume that the big data and their replicas can be replicated to the edge cloudlets or remote data centers in advance, such that the delay incurred by the joint analysis of datasets or transmission of intermediate results is no greater than the delay requirements of queries. LetSbe the collection of datasets generated by all services in remote data centers, denote by Sna dataset inS, where 1≤n≤|S| with|S|representing the number of datasets in S. Denote by qm a query for big data analytics. Each query qmusually requires to be evaluated based on a collection of datasets. Let S(qm)be the collection of datasets required",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "on a collection of datasets. Let S(qm)be the collection of datasets required by query qm. Evaluating a query qmis to abstract the intermediate results from its requested datasets that possibly are in different data centers or cloudlets, and aggregate the intermediate results at the home location of the query. Let hmbe the home location of query qm, which can be a data center or a cloudlet. Without loss of generality, we assume that the size of an intermediate result on each dataset Sn evaluated by query qmis a fraction size αnmofSn, i.e.,αnm·|Sn|, whereαnmis with 0<αnm≤1[21] and|Sn|is the volume of dataset Sn. 2.3 User QoS requirements As we consider query evaluation for big data analytics within strin- gent delay requirements, we refer to the delay requirement of a query",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "gent delay requirements, we refer to the delay requirement of a query as its quality of service (QoS) requirement , where the delay experienced by the query is defined as the duration from the query is issued to the evaluation result is received. Since the size of aquery is usually small, the transfer delay of the query from a user location to the edge cloud network is negligible. Each query may require multiple datasets or datasets’ replicas placed at different locations, the processing datasets and trans- mitting intermediate results can be performed in parallel among different datasets, therefore the delay experienced by qmdemand- ing multiple datasets is the maximum sum of the delays incurred in processing a dataset and transmitting the intermediate results of the dataset inS(qm)accessed by",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "a dataset and transmitting the intermediate results of the dataset inS(qm)accessed by qm, i.e., arдmax{(d(vl)·|Sn|+ dt(pvl,hm)·|Sn|·αnm)}. Denote by dqmthe maximum tolerable delay of query qm, that is to say, dqmis the QoS in terms of delay requirement of query qm. To make datasets in the two-tier edge cloud highly available, reliable and scalable, the datasets usually have several replicas, while in order to reduce the cost for data consistency, we thus assume that each dataset Snhas at most K replicas in the system with K∈Z+, the replication of datasets to data centers or cloudlets are conducted in advance before the evaluation of queries, and the delay incurred by dataset replications is not accounted into the QoS requirement of queries. 2.4 Problem definition Given a collection Sof datasets, a",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "requirement of queries. 2.4 Problem definition Given a collection Sof datasets, a set of queries Q={qm|1≤ m≤M}for big data analytics, and a two-tier edge cloud network G=(BS∪SW∪V,E), where V=CL∪DC, the computing resource of each node vl∈Vis capacitated. Different queries have different QoS requirements. The proactive data replication and placement problem for query evaluation of big data analytics in the two-tiered edge cloud network Gis to place at most Kreplicas for each dataset Sn∈Sto cloudlets or data centers in advance such that the volume of placed datasets demanded by admitted queries is maximized while meeting the delay requirements of all admitted queries, subject to the computing resource capacities on edge cloudlets and data centers, where Kis a given small integer with K≥1. Here, a query is admitted if the",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "given small integer with K≥1. Here, a query is admitted if the QoS requirement of the query can be satisfied and the computing capacity of each cloudlet and data center is not violated, the admitted queries will be evaluated by the cloudlets or data centers. Notice that, we here only consider the proactive replication and placement for static data, as for the dynamic aspect of data, we set a threshold, which is a ratio of the volume of new generated data to the volume of original data at a time point. When the ratio of the volume of new generated data achieves the threshold, an update operation is made between the original data and its replicas to keep data consistent in the whole network. 3 AN APPROXIMATION ALGORITHM",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "to keep data consistent in the whole network. 3 AN APPROXIMATION ALGORITHM FOR PROACTIVE DATA REPLICATION AND PLACEMENT In this section, we first give an overview of the proposed algorithm, we then formulate an Integer Linear Programming (ILP) solution to the proactive data replication and placement problem for query eval- uation of big data analytics, and devise an approximation algorithm with an approximation ratio by the primal-dual dynamic-update technique, we finally analyze the correctness and time complexity of the approximation algorithm. [Página 4] ICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and Bai, et al. 3.1 Algorithm overview In the proactive data replication and placement, each query can demand several datasets each time, and a dataset can be demanded by multiple different queries at each time. It is",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "can be demanded by multiple different queries at each time. It is NP-hard [ 5] to find an optimal solution for the problem. However, for a special case where each query demands only one dataset, there is an approximation algorithm based on the primal-dual dynamic-update technique. Therefore for a general case where each query demands multiple datasets, we can also get an approximation algorithm by invoking the proposed approximation algorithm in the special case. 3.2 Integer linear programming We formulate the problem as an integer linear programming (ILP). We first define a set of decision variables. Recall that, in the problem, there are a set Qof queries and a collection Sof datasets, these queries demand datasets for evaluation with different delay require- ments, some replicas of the datasets",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "for evaluation with different delay require- ments, some replicas of the datasets should be created and placed at appropriate locations in G, such that the volume of dataset replicas demanded by admitted queries is maximized while satisfying the de- lay requirements of the queries, subject to the capacity constraints on data centers and edge cloudlets. As maintaining data consistency between an original dataset Sn∈Sand its replicas incurs cost, we assume that each dataset has at most Kreplicas in the edge cloud. Therefore, the proactive data replication and placement problem is equivalent to determining where the replicas of each dataset should be proactively placed, and which queries should be assigned to which data centers or edge cloudlets for evaluation. Recall that V=CL∪DCis the set of edge cloudlets and data",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "for evaluation. Recall that V=CL∪DCis the set of edge cloudlets and data centers in G, each location node vl∈Vis either an edge cloudlet or a data center, 1≤l≤|C L∪DC|. We thus use a binary decision variable xnl indicating whether a replica of dataset Snis placed at a location nodevlinG. Similarly, we use a binary variable πmlto indicate whether a query qmis assigned to a location node vlto access the replica of dataset Sn∈S( qm). Once a query qmis assigned to a location node vlwhere the replicas demanded by qmare placed, the processed intermediate results will be transferred to the home location node hmofqm, via a shortest path whose transmission delay is the minimum one. We then formulate the objective of the proactive data replica- tion and placement problem,",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "formulate the objective of the proactive data replica- tion and placement problem, which is to maximize the volume of datasets demanded by admitted queries that can be expressed by maximizeÕ qm∈QÕ vl∈V|Sqm|·πml (1) subject to the following constraints, Õ qm∈Q|Sqm|·rm·πml≤A(vl),∀vl∈V (2) πml−xqml≤0,∀qm∈Qand∀vl∈V (3) |Sqm|·[d(vl)+dt(pvl,hm)·αqm]·πml≤dqm, ∀qm∈Q,∀vl∈V (4)Õ vl∈Vxnl≤K,∀Sqm∈S (5) πml∈{0,1}, (6) xnl∈{0,1}, (7)where Constraint (2) ensures that the computing resource of node vlallocated to evaluate queries that demand dataset Sqmis no greater than the available computing resource of vl. Constraint (3) ensures that only when the dataset Snrequired by query qmis placed at node vl, query qmcan then be assigned to vl. Con- straint (4) guarantees that the delay requirement dqmof each query qmis met. Constraint (5) ensures that each dataset has at most K replicas in G. 3.3",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "ensures that each dataset has at most K replicas in G. 3.3 An approximation algorithm We consider the above ILP for the proactive data replication and placement problem as the Primal problem. We first calculate the Dual of the Primal, we then devise an approximation algorithm for the Dual problem. To be specific, we define four dual variables θl,yml,ηmlandµqm, then the dual of the Primal problem can be formulated as minÕ vl∈VA(vl)·θl+Õ qm∈QÕ vl∈Vdqm·ηml+Õ qm∈QK·µqm(8) subject to the following constraints, |Sqm|·rm·θl+yml+|Sqm|·[d(vl)+dt(pvl,hm)·αqm] ·ηnl≥|Sqm|,∀qm∈Qand∀vl∈V (9)Õ Sqm∈Sµqm−Õ qm∈Qyml≥0,∀vl∈V (10) θl≥0, (11) yml≥0, (12) ηml≥0, (13) µqm≥0. (14) The primal complementary slackness conditions are as follows: •For each query qm∈Qand each node vl∈V, ifπml>0 then |Sqm|·rm·θl+yml+|Sqm|·[d(vl)+ dt(pvl,hm)·αqm]·ηnl=|Sqm| (15) •For each node vl, ifxqml>0then Õ Sqm∈Sµqm−Õ qm∈Qyml=0 (16) We apply the complementary slackness",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "node vl, ifxqml>0then Õ Sqm∈Sµqm−Õ qm∈Qyml=0 (16) We apply the complementary slackness approach to the approx- imation algorithm by defining relaxed complementary slack- ness . The relaxed primal complementary slackness conditions are as follows: •For each query qm∈Qand each node vl∈V, ifπml>0 then |Sqm|≤|Sqm|·rm·θl+yml+|Sqm|·[d(vl)+ dt(pvl,hm)·αqm]·ηnl≤β·|Sqm|(17) •For each node vl, ifxqml>0then Õ Sqm∈Sµqm−Õ qm∈Qyml=0 (18) [Página 5] QoS-Aware Proactive Data Replication for Big Data Analytics ICPP 2019, August 5–8, 2019, Kyoto, Japan Algorithm 1 An approximation algorithm Appro-S for the proactive data placement problem where a query demands only one single dataset each time . Input: The set Qof queries, the set Sof datasets, the set Vof nodes in the two-tier edge cloud. Output: The maximum volume of datasets demanded by admitted queries. 1:Q′←∅ // set of admitted queries",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "of datasets demanded by admitted queries. 1:Q′←∅ // set of admitted queries ; 2:V′←∅ //set of nodes where the replicas of the datasets are placed; 3:S′←∅ // set of placed replicas; 4:θ←0,y←0,η←0,µ←0; 5:N←0//the volume of datasets demanded by admitted queries; 6:while eachµqm≤KorQ′,Qdo 7: Uniformly increase µqmby 1 in a unit time, that is we create one replica of dataset Sqmdemanded by qm; 8: Increaseymluniformly, i.e., increase ymlby 1 in a unit time. After a while, Eq. 10 becomes tight, that isÍ Sqm∈Sµqm−Í qm∈Qyml=0, i.e.,µqm−yml=0as each query qmonly demands one single dataset Sqmeach time; 9: Increase uniformly all θlandηnlsimultaneously, that is in a unit timeθlandηnlincrease 1. After a while Eq. 9 becomes tight, it means we have yml=|Sqm|−|Sqm|·rm·θl−|Sqm|· [d(vl)+dt(pvl,hm)·αqm]·ηnl; 10: AddqmintoQ′, remove qmfrom Q, that is Q′←Q′∪{qm}, andQ←Q\\{qm}; 11:",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "yml=|Sqm|−|Sqm|·rm·θl−|Sqm|· [d(vl)+dt(pvl,hm)·αqm]·ηnl; 10: AddqmintoQ′, remove qmfrom Q, that is Q′←Q′∪{qm}, andQ←Q\\{qm}; 11: Add SqmintoS′, i.e.,S′←S′∪{Sqm}; 12: AddvlintoV′, i.e., V′←V′∪{vl}; 13: Declare that query qmandSqmare assigned to node vl; 14: N←N+|Sqm|; 15:Return N. Observations: From the dual we can observe the meanings of dual variables: θlmeans the computing cost for evaluating qm on nodevl;ymlrepresents the cost by assigning qmtovl;ηml is the cost for satisfying the delay requirement of qm, ifqmis assigned to node vl;µqmis the cost for creating a replica of a dataset demanded by query qm. Based on the observations, for a special case where a query demands only one single dataset, we can devise an approximation algorithm that calculates the placed datasets S′ and admitted queries Q′, and satisfies the delay requirements of queries. For simplicity, we",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "queries Q′, and satisfies the delay requirements of queries. For simplicity, we refer algorithm 1 as Appro-S . Notice that approximation algorithm Appro-S works in a special case where each query demands only one single dataset. In contrast, for a general case where each query demands multiple datasets each time, we can still use algorithm Appro-S to derive another approximation algorithm, that is once a query demands a dataset we invoke algorithm Appro-S . The specific algorithm is detailed in algorithm 2, which is referred as Appro-G . Theorem 1. The approximation algorithm Appro-S gives an ap- proximation ratio arдmax(|Q|,|V|/K), the approximation algorithm Appro-G gives an approximation ratio arдmax(|Q|·|S| ,|V|·|S|/ K), where|Q|is the number of queries in the system, |V|is the numberAlgorithm 2 An approximation algorithm Appro-G for",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "in the system, |V|is the numberAlgorithm 2 An approximation algorithm Appro-G for the proactive data placement problem where a query demands multiple datasets each time . Input: The set Qof queries, the set Sof datasets, the set Vof nodes in the two-tier edge cloud. Output: The maximum volume of datasets demanded by admitted queries. 1:N′←0// the total volume of datasets demanded by admitted queries; 2:N←0// the volume of datasets demanded by admitted queries inAppro-S ; 3:foreach qm∈Qand each dataset Sn∈S(qm)do 4: Invoke algorithm 1 ; 5: N′←N′+N; 6:Return N′. of cloudlets and data centers, |S|is the number of datasets, and Kis the maximum number of replicas of each dataset. Proof. Letθ,y,ηandµbe the returned dual-feasible solution. To prove the approximation ratio, we need to compare the max- imum volume",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "prove the approximation ratio, we need to compare the max- imum volume of datasets demanded by admitted queries of the approximated solution, which isÍ qm∈QÍ vl∈V|Sqm|, to the cost of the dual feasible solution ( θ,y,ηandµ), which isÍ vl∈VA(vl)· θl+Í qm∈QÍ vl∈Vdqm·ηml+Í qm∈QK·µqm. As described in algorithm Appro-S , after some recurrence, we have qmandvlwhich make µqm−yml=0, based on formula (10) we have |Sqm|≤\u0000|Sqm|·rm·θl+µqm+ |Sqm|·[d(vl)+dt(pvl,hm)·αqm]·ηnl\u0001, (19) then we have Õ qm∈QÕ vl∈V|Sqm|≤ Õ qm∈QÕ vl∈V|Sqm|·rm·θl+Õ qm∈QÕ vl∈Vµqm +Õ qm∈QÕ vl∈V|Sqm|·[d(vl)+dt(pvl,hm)·αqm]·ηnl =Õ vl∈VA(vl)·θl·Í qm∈Q|Sqm|·rm A(vl)+Õ qm∈Q Õ vl∈Vdqm·ηnl·|Sqm|·[d(vl)+dt(pvl,hm)·αqm] dqm +Õ qm∈QK·µqm·|V| K ≤Õ vl∈VA(vl)·|Q|+Õ qm∈QÕ vl∈Vdqm·ηnl+ Õ qm∈QK·µqm·|V| K(20) Therefore, the approximation ratio of algorithm Appro-S ismax{|Q|,|V| K}. As each query can maximumly demand |S|datasets at each time, [Página 6] ICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "time, [Página 6] ICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and Bai, et al. so the approximation ratio of algorithm Appro-G isarдmax(|Q|· |S|,|V|·|S|/ K). □ 4 PERFORMANCE EVALUATION In this section, we evaluate the performance of the proposed algo- rithms Appro-S andAppro-G , and investigate the impact of impor- tant parameters on the algorithmic performance, by both simula- tions and a proof-of-concept in a real test-bed using real datasets. 4.1 Experimental environment For simulation, we consider a two-tier edge cloud consisting of 6 data centers, 24cloudlets and 2switches, there is a link between each pair of nodes (data centers, cloudlet, and switches) with a probability of 0.2, generated by the GT-ITM tool [ 8]. The delay The computing capacities of each data center and cloudlet are randomly",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "delay The computing capacities of each data center and cloudlet are randomly drawn from a value interval [ 200,700] and [ 8,16] units (GHz) [ 26] respectively. Each user produces several Gigabytes of data, we thus emulate the volume of the dataset generated by each user is in the range of [1, 6] GB [ 26], and the amount of computing resource assigned to the processing of 1GB data is a value in the range of [0.75, 1.25] GHz [ 2,4]. The numbers of datasets and queries in the system are randomly drawn in the range of [5, 20] and [10, 100], respectively. The number of datasets required by the query is randomly drawn from interval [1, 7]. Taking the transfer delay in real cables into consideration, the",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "[1, 7]. Taking the transfer delay in real cables into consideration, the QoS in terms of delay requirement of each query depends on the size of dataset demanded by the query, the reason is to avoid some users who demand more dataset require the same delay as users who demand few dataset. Unless otherwise specified, we will adopt the default settings in our experiments. Each value in the figures is the mean of the results by applying each mentioned algorithm on 15different topologies of the two-tier edge cloud. We evaluate the performance of the proposed algorithms against two benchmarks. The first benchmark adopts a greedy strategy, it selects a data center or cloudlet with largest available computing resource to place a replica of a dataset. If the delay",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "computing resource to place a replica of a dataset. If the delay requirement cannot be satisfied, it then selects a data center or a cloudlet with the second largest available computing resource to place the replica. This procedure continues until the query is admitted or there are already Kreplicas of the dataset in the system. Another benchmark is from an existing work [ 10] that places Kreplicas for each dataset at data centers or cloudlets, if the delay requirement of the query can be satisfied by evaluating the replica at the data center or the cloudlet. This procedure continues until the query is admitted or there are already Kreplicas of the dataset in the system. It then makes a graph partitioning with maximum volume of datasets demanded by",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "then makes a graph partitioning with maximum volume of datasets demanded by admitted queries. For simplicity, we refer to the two benchmarks as Greedy-S andGraph-S for the special case where each query only demands a single dataset, while for the general case where each query demands multiple datasets we refer to them asGreedy-G andGraph-G , respectively. In addition, we also evaluate the proposed algorithms in a real testbed. For which, we leased a number of virtual machines from a cloud service provider DigitalOcean [ 9]. These virtual machines are located at geo-distributed locations. A two-tier edge cloud is deployed by making use of both the leased virtual machines andlocal servers, based on which we evaluate the proposed algorithms against an existing work [ 13]. The benchmark work first",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "proposed algorithms against an existing work [ 13]. The benchmark work first calculates the popularity of a node (cloudlet and data center) according to the ratio of the number of dataset replicas on the node to the to- tal number of dataset replicas of all nodes. It then selects a node with the highest popularity for each dataset, and places a replica of the dataset if the delay requirement of a query can be satisfied; otherwise, if then selects another node with the second highest popularity to place the replica; this procedure continues until the query is admitted or there are already Kreplicas of the dataset. As we consider the special case where each query only demands one single dataset and a general case where each query demands",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "demands one single dataset and a general case where each query demands multiple datasets, we thus refer to the benchmark as algorithm Popularity-S for the special case and Popularity-G for the gen- eral case for simplicity, respectively. 4.2 Performance evaluation of different algorithms by simulations (a) The volume of datasets demanded by admitted queries. (b) The system throughput. Figure 2: The performance of different algorithms Appro-S , Greedy-S and Graph-S in terms of the volume of datasets demanded by admitted queries and the system throughput, where each query demands a single dataset each time. We first evaluate the proposed algorithm Appro-S against al- gorithms Greedy-S andGraph-S by varying the network size for the special case where each query demands a single dataset each time, in terms of the",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "each query demands a single dataset each time, in terms of the volume of datasets demanded by admitted queries and the system throughput which is a ratio of the number of admitted queries to the total number of queries in the system. It can be seen from Fig. 2(a) and Fig. 2(b) that the volume of datasets [Página 7] QoS-Aware Proactive Data Replication for Big Data Analytics ICPP 2019, August 5–8, 2019, Kyoto, Japan demanded by admitted queries is over 4times than that by algo- rithm Greedy-S and 2 times than that by algorithm Graph-S , the system throughput by Appro-S is15% higher than that by algo- rithm Greedy-S and10%higher than that by Graph-S , respectively. The rationale behind is that Appro-S places the replicas of datasets from",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "The rationale behind is that Appro-S places the replicas of datasets from an overall perspective, it jointly considers data replication and query assignment by smartly finding appropriate number and placement locations of replicas for each dataset, it also fully utilizes the available computing resource and the delay requirements of queries when placing replicas. Whereas Greedy-S intends to place a replica at a location with largest available computing resource while pays less attention to the delay requirement when choosing locations to place replicas; similarly Graph-S places replicas at lo- cations under the constraints of location (data center or cloudlets) capacities and delay requirements of queries, it then use graph partitioning with maximum volume of datasets demanded by ad- mitted queries, it thus can better user the resources of locations",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "ad- mitted queries, it thus can better user the resources of locations compared with Greedy-S but not fully make use of the resources and admit as many queries as possible compared with Appro−S. Notice that when the network size is too high, e.g., 200, the system throughput and volume of datasets demanded by admitted queries slightly decrease, this is because when the network size is too large, transmission delay of some paths from evaluation locations to home locations of queries has a higher probability to increase which may violate the delay requirements of some queries, thereby reducing the system throughput and the volume of datasets demanded by admitted queries. (a) The impact on the volume of datasets de- manded by admitted queries. (b) The impact on the system",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "datasets de- manded by admitted queries. (b) The impact on the system throughput. Figure 3: Impacts of the maximum number of datasets de- manded by each query on the performance by Appro-G , Greedy-G and Graph-G .We then evaluate the proposed algorithm Appro-G against al- gorithms Greedy-G andGraph-G by varying the network size for the general case where each query demands multiple datasets each time, in terms of the volume of datasets demanded by admitted queries and the system throughput. It can be seen from Fig. 3(a) and Fig. 3(b) that the volume of datasets demanded by admitted queries is 5and1.7times than those by algorithms Greedy-G and Graph-G , respectively. The system throughput by Appro-G is2.1 and 1.5times than those by algorithms Greedy-G andGraph-G , respectively. The arguments",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "and 1.5times than those by algorithms Greedy-G andGraph-G , respectively. The arguments are the same as that in Fig. 2, we do not repeat here. (a) The volume of datasets demanded by admitted queries. (b) The system throughput. Figure 4: The performance of different algorithms Appro-G , Greedy-G and Graph-G in terms of the volume of datasets demanded by admitted queries and the system throughput, where each query demands multiple datasets each time. Impact of the maximum number of datasets demanded by each query on the algorithmic performance: We now eval- uate the impact of the maximum number of datasets demanded by each query by varying the number from 1 to 6 for the general case where each query demands multiple datasets each time, on the performance of",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "where each query demands multiple datasets each time, on the performance of algorithms Appro-G ,Greedy-G andGraph-G , in terms of the volume of datasets demanded by admitted queries and the system throughput. Notice that we did not evaluate the im- pact of the maximum number of datasets demanded by each query on the algorithmic performance for the special case, as a query only demands a single dataset each time for the special case. For simplicity, we refer to the maximum number of datasets demanded by each query as F. From Fig. 4(a) we can see that the system throughput of three algorithms decreases with the growth of F, [Página 8] ICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and Bai, et al. the rationale is that a query",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "Japan Xia and Bai, et al. the rationale is that a query could be admitted by the system, only when the delay for evaluating all datasets demanded by the query is no greater than the delay requirement of the query, that is to say, the more number of datasets is demanded by a query, the harder the QoS requirements of queries would be satisfied, so the harder the query would be admitted. Although the system throughput decreases with the growth of F, the volume of datasets demanded by admitted queries firstly increases with the growth of Ffrom 1 to 5, and then slightly decreases after F=5. The reason is that before F=5, the total number of datasets demanded by admitted queries increases as queries demand more datasets, however",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "datasets demanded by admitted queries increases as queries demand more datasets, however when Fis 6, so many queries are rejected by the system due to the violated delay requirements, the volume of datasets demanded by admitted queries thus decreases. It can be clearly seen that the volume of datasets demanded by admitted queries and system throughput by algorithm Appro-G is higher than those by algorithms Greedy-G andGraph-G , the reasons are similar those of Figs. 2(a) and 2(b). (a) The impact of Kon the volume of datasets demanded by admitted queries. (b) The impact of Kon the system throughput. Figure 5: Impacts of the maximum number Kof replicas of each dataset on the performance by Appro-G ,Greedy-G , and Graph-G in terms of the volume of datasets demanded",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": ",Greedy-G , and Graph-G in terms of the volume of datasets demanded by ad- mitted queries and the system throughput. Impacts of the maximum number Kof replicas on the algo- rithmic performance: We then evaluate the impact of the maxi- mum number Kof replicas of a dataset by varying Kfrom 1 to 7 for the general case where each query demands multiple datasets each time, on the performance of Appro-G ,Greedy-G andGraph-G in terms of the volume of datasets demanded by admitted queries and the system throughput. From Fig. 5(a) and Fig. 5(b) we can see that the volume of datasets demanded by admitted queries and systemthroughput are increasing with the growth of the value of K, the rationale is that as more replicas of each dataset are",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "K, the rationale is that as more replicas of each dataset are placed in the system, the delay requirements of queries are easier to be satis- fied, thus the system throughput and volume of datasets demanded by admitted queries increase. Obviously, the volume of datasets demanded by admitted queries and system throughput achieved byAppro-G are significantly higher than those by Greedy-G and Graph-G . The reason is that Appro-G places the replicas of datasets from the perspective of all the system to optimize the use of system resources, it jointly considers data replication and query assignment by smartly finding appropriate number and placement locations of replicas for all datasets, it also fully utilizes the available comput- ing resource and the delay requirements of queries when placing replicas. 4.3",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "ing resource and the delay requirements of queries when placing replicas. 4.3 Performance evaluation in a real test-bed We now evaluate the performance of the proposed algorithms in a real testbed that is composed of virtual machines in different geo-locations that are provided by a cloud service provider, and a controller that executes the proposed algorithms. Testbed settings: We lease 20 virtual machines (VMs) from a cloud service provider DigitalOcean [ 9], these VMs are located at loca- tions San Francisco, New York, Toronto, and Singapore. It must be mentioned that since we focus on the replica placement in a two-tier edge cloud, we use 4 VMs to represent data centers, and 16 VMs to represent cloudlets in the edge cloud network G, we also use a local",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "cloudlets in the edge cloud network G, we also use a local server as a controller to control the running of algorithms and 2 switches. Although the scale of each node representing a data center in this testbed may not be comparable to a large-scale data center, the implementation can be easily extended to a test-bed with large-scale data centers. An illustration of the testbed is in Fig. 6. Figure 6: The topology of the testbed with leased VMs. Datasets: The datasets used in the experiment are mobile appli- cation usage information from 3 million anonymous mobile users for a period of three months. We divide the data into a number of datasets according to the data creation time, and randomly distrib- ute the datasets into the data",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "data creation time, and randomly distrib- ute the datasets into the data centers and cloudlets of the testbed. Big data analytic queries are issued to find some evaluation results: [Página 9] QoS-Aware Proactive Data Replication for Big Data Analytics ICPP 2019, August 5–8, 2019, Kyoto, Japan such as the most popular applications, at what time the found ap- plications would be used, and the usage pattern of some mobile applications, etc. Results: We first evaluate the performance of the proposed algo- rithm Appro-S against a benchmark Popularity-S for the special case where a query demands one single dataset each time by vary- ing the maximum number of datasets demanded by each query. Due to page limits, we here put only a set of figures about the impact of",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "we here put only a set of figures about the impact of the maximum number Fof datasets demanded by each query on the performance of algorithm Appro-S against benchmark Popularity-S illustrated in Fig. 7. From Figs. 7(a) and 7(b) we can see that, algorithm Appro-S outperforms algorithm Popularity-S by delivering a higher volume of datasets demanded by admitted queries and system throughput. We can see that the volume of datasets demanded by admitted queries increases with the growth ofFfrom Fig. 7(a), and the system throughput decreases as the value of Fincreases from Fig. 7(b), the arguments are similar with those in Figs. 4(a) and 4(b). (a) The volume of datasets demanded by admitted queries by Appro-S and Popularity-S on the real testbed. (b) The system throughput by Appro-S",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "and Popularity-S on the real testbed. (b) The system throughput by Appro-S and Popularity-S on the real testbed. Figure 7: The performance evaluation of the proposed algo- rithm Appro-S against benchmark Popularity-S on the real testbed for the special case. We then investigate the performance of the proposed algorithm Appro-G against benchmark Popularity-G for a general case where each query demands multiple datasets each time, by varying the number Kof dataset replicas. Comparably, because of page limits and pattern similarity, we here put only one set of figures about the impact of the maximum number Kof replicas of each dataset on the algorithmic performance. It can be seen from Figs. 8(a) and8(b) that Appro-G achieves a higher volume of datasets demanded by admitted queries and a higher system",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "higher volume of datasets demanded by admitted queries and a higher system throughput than those byPopularity-G . The rationale behind is that Appro-G places the replicas of datasets from the perspective of the whole system by smartly finding appropriate number and placement locations of replicas for all datasets, it also fully utilizes the available computing resource and the delay requirements of queries when placing repli- cas. The volume of datasets demanded by admitted queries and the system throughput increase with the growth of K. This is because as more replicas of each dataset are placed in the system, the delay requirements of queries are easier to be satisfied, thus the volume of datasets demanded by admitted queries and system throughput increase. (a) The volume of datasets demanded by",
        "start_idx": 6612,
        "end_idx": 6740
      },
      {
        "text": "queries and system throughput increase. (a) The volume of datasets demanded by admitted queries by Appro-G and Popularity-G on the real testbed. (b) The system throughput by Appro-G and Popularity-G on the real testbed. Figure 8: The performance evaluation of the proposed algo- rithm Appro-G against benchmark Popularity-G on the real testbed for the general case. 5 RELATED WORK Several studies on data placement and query evaluation have been conducted in the past [ 1,6,7,17,18,20,22–26], and the others focused on multi-layered network architecture and edge clouds for dealing with big data [ 11,14–16,27]. Most of these studies either did not consider data replications of generated big data [ 1,11,14– 16,20,26,27] or ignored the QoS requirement of users [ 1,6,17,23, 26], or some of them only considered traffic cost",
        "start_idx": 6728,
        "end_idx": 6856
      },
      {
        "text": "users [ 1,6,17,23, 26], or some of them only considered traffic cost while neglecting other costs [17]. For example, Baev et. al. [6] considered a problem of placing replicated data in arbitrary networks to minimize the total storage [Página 10] ICPP 2019, August 5–8, 2019, Kyoto, Japan Xia and Bai, et al. and access cost. Golab et al. [10] studied a data placement problem to determine where to store the data and where to evaluate data- intensive tasks with a goal to minimize the data traffic cost. Kayyoor et. al [17] addressed a problem of minimizing average query span, which is the number of servers involved in answering a query. They ignored other costs and QoS requirements of users [ 6,17], and did not consider data replications [",
        "start_idx": 6844,
        "end_idx": 6972
      },
      {
        "text": "requirements of users [ 6,17], and did not consider data replications [ 10]. Agarwal et al. [1] proposed a data placement mechanism Volley for geo-distributed cloud services to minimize the user-perceived latency. Xia et. al [26] considered a big data management problem in distributed cloud environments to maximize the system throughput while minimizing the operational cost of service providers. No data replications and QoS require- ments of users are discussed in the two works [ 1,26]. Pu et al. [20] presented a system for low latency geo-distributed analytics, which used an heuristic to redistribute datasets among the data centers prior to queries’ arrivals, and placed the queries to reduce network bottlenecks during the query’s execution. Heintz et al. [12] studied the tradeoff between the delay and errors of",
        "start_idx": 6960,
        "end_idx": 7088
      },
      {
        "text": "et al. [12] studied the tradeoff between the delay and errors of obtained results in streaming analytics in an architecture consisting of a single center and multiple edge servers. In the study [ 20], authors did not con- sider data replications of datasets. The work in [ 16] considered a layered architecture for the satellite-based data center infrastruc- ture, and big data storage by leveraging such data centers. The authors [ 14,15] studied a service provisioning problem in the edge cloud network, with an objective to maximize the profit of network operators. No data replication is considered in these works [ 14–16]. In contrast, we studied the proactive QoS-aware data replication and placement problem for query evaluation of big data analytics in a two-tier edge cloud environment, where",
        "start_idx": 7076,
        "end_idx": 7204
      },
      {
        "text": "evaluation of big data analytics in a two-tier edge cloud environment, where the number of replicas of big datasets should be appropriately determined and the loca- tions to place the replicas should be strategically selected, with an objective to maximize the volume of datasets demanded by admit- ted queries such that the service providers can obtain maximum benefits by offering a pay-as-you-go pricing approach to process the datasets, while meeting the QoS requirements of queries and resource capacity constraints. 6 CONCLUSIONS In this paper, we studied query evaluation of big data analytics in a two-tier edge cloud network through efficient and effective data replication and placement with the aim to maximize the volume of datasets demanded by admitted queries, subject to computing resource capacities on data centers and",
        "start_idx": 7192,
        "end_idx": 7320
      },
      {
        "text": "by admitted queries, subject to computing resource capacities on data centers and edge cloudlets, while meet- ing various delay requirements of user queries. To this end, we first formulated a novel QoS-aware data replication and placement prob- lem of query evaluation for big data analytics. We then proposed an efficient approximation algorithm with provable approximation ratio for the problem. We finally evaluated the performance of the proposed algorithm through experimental simulations in a real testbed based on real datasets. Simulation results demonstrate that the proposed algorithm achieves several times higher volume of datasets demanded by admitted queries and system throughput than existing works.ACKNOWLEDGEMENT The work of Qiufen Xia and Zichuan Xu is partially supported by the National Natural Science Foundation of China (Grant No. 61802047, 61802048, 61772113, 61872053),",
        "start_idx": 7308,
        "end_idx": 7436
      },
      {
        "text": "National Natural Science Foundation of China (Grant No. 61802047, 61802048, 61772113, 61872053), the fundamental research funds for the central universities in China (Grant No. DUT19RC(4)035, DUT19RC(5)001, DUT19GJ204), and the “Xinghai Scholar” Program at Dalian University of Technology, China. REFERENCES [1]S. Agarwal, J. Dunagan, N. Jain, S. Saroiu, A. Wolman, and H. Bhogan. Volley: au- tomated data placement for geo-distributed cloud services. Proc. of NSDI , USENIX, 2010. [2] https://aws.amazon.com/ec2/ , accessed in Jan. 2019. [3] https://aws.amazon.com/pricing/?nc1=h_ls , accessed in Jan. 2019. [4] https://aws.amazon.com/s3/ , accessed in Jan. 2019. [5]H. An, M. Singh, and O. Svensson. LP-based algorithms for capacitated facility location. Proc. of FOCS’14 , IEEE, 2014. [6]I, Baev, R. Rajaraman, and C. Swamy. Approximation algorithms for data placement problems. SIAM J. on Computing , Vol.38, No.4, pp.1411-1429,",
        "start_idx": 7424,
        "end_idx": 7552
      },
      {
        "text": "for data placement problems. SIAM J. on Computing , Vol.38, No.4, pp.1411-1429, 2008. [7]M. W. Convolbo, J. Chou, and S. Lu. DRASH: A data replication-aware scheduler in geo-distributed data centers. Proc. of CloudCom , IEEE, 2016. [8] K. Calvert, and E. Zegura. Gt-itm: georgia tech internetwork topology models. [9] Digital Ocean. https://www.digitalocean.com , accessed in Jan. 2019. [10] L. Golab, M. Hadjieleftheriou, H. Karloff, and B. Saha. Distributed data placement to minimize communication costs via graph partitioning. Proc. of SSDBM , ACM, 2014. [11] S. Guo,D. Zeng, L. Gu, and J. Luo. When green energy meets cloud radio access network: joint optimization towards brown energy minimization. Mobile Networks and Applications , Springer, pp.1-9, 2018. [12] B. Heintz, A. Chandra, and R. K. Sitaraman. Trading timeliness and accuracy in",
        "start_idx": 7540,
        "end_idx": 7668
      },
      {
        "text": "Heintz, A. Chandra, and R. K. Sitaraman. Trading timeliness and accuracy in geo-distributed streaming analytics Proc. of SoCC , ACM, 2016. [13] T. Hou, G. Feng, S. Qin, and W. Jiang. Proactive content caching by exploiting transfer learning for mobile edge computing. International Journal of Communica- tion Systems , Vol, 31, No. 2, 2017. [14] H. Huang, and S. Guo Adaptive service provisioning for mobile edge cloud. ZTE Communications , Vol. 15, No. 2, pp.1-9, 2017. [15] H. Huang, and S. Guo Service provisioning update scheme for mobile application users in a cloudlet network Proc. of ICC , IEEE, 2017. [16] H. Huang, S. Guo, and K. Wang. Envisioned wireless big data storage for low- earth-orbit satellite-based cloud. IEEE Wireless Communications , Vol.25, No.1, pp.26-31, 2018. [17] A.",
        "start_idx": 7656,
        "end_idx": 7784
      },
      {
        "text": "satellite-based cloud. IEEE Wireless Communications , Vol.25, No.1, pp.26-31, 2018. [17] A. K. Kayyoor, A. Deshpande, and S. Khuller. Data placement and replica selection for improving co-location in distributed environments. Computing Research Repository (CoRR), arXiv:1302.4168, 2012. [18] P. Li , S. Guo, T, Miyazaki, X. Liao, H. Jin, A. Y. Zomaya, and K. Wang. Traffic- aware geo-distributed big data analytics with predictable job completion time. IEEE Trans. on Parallel and Distributed Systems , Vol.28, No.6, pp.1785-1796, 2017. [19] H. Li, H. Xu, and S. Nutanong. Bohr: similarity aware geo-distributed data analytics. Open Access Media , USENIX, 2017. [20] Q. Pu, G. Ananthanarayanan, P. Bodik, S. Kandula, A. Akella, P. Bahl, and I. Stoica. Low latency analytics of geo-distributed data in the wide area. Proc. of SIGCOMM , ACM,",
        "start_idx": 7772,
        "end_idx": 7900
      },
      {
        "text": "of geo-distributed data in the wide area. Proc. of SIGCOMM , ACM, 2015. [21] S. Rao, R. Ramakrishnan, A. Silberstein, M. Ovsiannikov, and D. Reeves. Sailfish: a framework for large scale data processing. Proc. of SoCC , ACM, 2012. [22] W. Xiao, W. Bao, X. Zhu, and L. Liu. Cost-aware big data processing across geo- distributed data centers. IEEE Trans. on Parallel and Distributed Systems , Vol.28, No.11, pp.3114-3127, 2017. [23] Q. Xia, W. Liang, and Z. Xu. The operational cost minimization in distributed clouds via community-aware user data placements of social networks. Computer Networks , Vol.112, pp.263-278, 2017. [24] Z. Xu and W. Liang. Operational cost minimization for distributed data centers through exploring electricity price diversity. Computer Networks , Vol. 83, pp.59-75, Elsevier, 2015. [25] Z. Xu,",
        "start_idx": 7888,
        "end_idx": 8016
      },
      {
        "text": "diversity. Computer Networks , Vol. 83, pp.59-75, Elsevier, 2015. [25] Z. Xu, W. Liang, and Q. Xia. Electricity cost minimization in distributed clouds by exploring heterogeneities of cloud resources and user demands. Proc. of ICPADS’15 , IEEE, 2015. [26] Q. Xia, Z. Xu, W. Liang, and A. Zomaya. Collaboration- and fairness-aware big data management in distributed clouds. IEEE Trans. on Parallel and Distributed Systems , Vol.27, No.7, pp.1941-1953, 2016. [27] S. Yu, M. Liu, W. Dou, X. Liu, and S. Zhou. Networking for big data: A survey. IEEE Communications Surveys & Tutorials , Vol. 19, No.1, pp. 531-549, 2017.",
        "start_idx": 8004,
        "end_idx": 8103
      }
    ],
    "2530e8bd-807c-4462-90f4-b2bd1edb9db4": [
      {
        "text": "[Página 1] A survey on quality assurance techniques for big data applications Pengcheng Zhang1, Xuewu Zhou1, Wenrui Li2, Jerry Gao3,4 1College of Computer and Information, Hohai University, Nanjing, China 2School of Mathematics & Information Technology, Nanjing Xiaozhuang University, Nanjing, P.R. China 3San Jose State University, San Jose, CA, \u0017Taiyuan University of Technology, China Email Address: {pchzhang@hhu.edu.cn; jerry.gao@sjsu.edu} AbstractüüWith the rapid \u0003advance of big data and cloud computing, building high quality big data systems in different application fields has gradually became a popular research topic in academia and industry as well as government agencies. However, more quality problems lead to application errors. Although the current research work has discussed how to ensure the quality of big data applications from several aspects, there is no systematic discussion on how to",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "applications from several aspects, there is no systematic discussion on how to ensure the quality of large data applications. Therefore, a systematic study on big data application quality assurance is very necessary and critical. This paper focuses on the survey of quality assurance techni ques of big data applications, and it introduces big data properties and quality attributes. It mainly discusses the key approaches to ensure the quality of big data applications and they are testing, model-driven architecture (MDA), monitoring, fault tolerance, verification and also prediction techniques. In addition, this paper also discusses the impact of big data characteristics on big data applications. Index Terms üQuality Assurance, Big data, Big data application, MDA, Testing, Verification, Fault tolerance, Monitoring, Prediction I. INTRODUCTION According to IDC report, the Big Data",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "tolerance, Monitoring, Prediction I. INTRODUCTION According to IDC report, the Big Data technology market will grow at \"a 27% compound annual growth rate (CAGR) to $32.4 billion through 2017” [1]. It shows that large-scale data computing and big data application services become more and more popular and have more influences on people's daily lives. Big data applications are now widely used in many aspects, such as monitoring systems , forecasting , and statistical reporting applications. However, big data applications pose new challenges for Quality Assurance (QA) engineers due to the large big data characteristics (e.g., velocity of arriving data, volume of data) [2], [3]. For examples, because of the volume and timeliness of the data, verification the accuracy of big data prediction systems is a difficult task, and",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "the accuracy of big data prediction systems is a difficult task, and it is a hard job to validate the correctness of a big data prediction system due to the large scale data size and the feature of timeliness. Therefore, quality assurance techniques for big data applications become a key concern and research topic. Although there are many published papers addressing data quality assurance in the past, a few of them focused on the systematic study on the quality assurance techniques for big data applications. Towards this research direction, the main purpose of this paper is to investigate literature relevant for the quality assurance techniques for big data applications so that it can provide a comprehensive reference to the challenges of quality assurance approaches for big data applications.Unlike",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "reference to the challenges of quality assurance approaches for big data applications.Unlike existing work, this paper provides the contributions in the following aspects: x It discusses quality assurance approaches for big data applications, mainly from the six aspects: testing, model-driven architecture (MDA), monitoring, fault tolerance, verification and prediction for big data applications. x It also combines quality assurance techniques with big data characteristics while it considers the quality assurance of big data applications, and it explores the big data 4V properties of existing quality assurance techniques for big data application. The rest of the paper is organized as follows. Section II reviews related work. Section III introduces the different types of big data applications, and the quality assurance approaches. Section IV provides an overview and comparison of the",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "quality assurance approaches. Section IV provides an overview and comparison of the existing approaches for quality assurance of big data applications, specifically in testing, model-driven architecture (MDA), monitoring, fault tolerance, verification and prediction. Section V discussed big data 4V properties and the quality assurance of big data applications. Section VI concludes the paper. II. RELATED SURVEY Many scholars have investigated the analysis of big data quality assurance. Let us cons ider the most interesting approaches from our point of view results obtained by them. Because of the widespread use of big data applications, big data quality assurance research has been tried by scholars. However, due to the huge volume of generated data, the fast velocity of arriving data, and the large variety of heterogeneous data, the quality of",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "arriving data, and the large variety of heterogeneous data, the quality of data is far from perfect [4] . Therefore, big data quality assurance in big data service applications and academic research has become an important and critical issue due to 4V in big data applications. In general, big data quality assurance refers to the study and application of various assurance processes, methods, standards, criteria, and systems to ensure the quality of big data in terms of a set of quality parameters. Gao et al. [2] provide informative discussions for big data validation and quality assurance, including the essential concepts, focuses, and validation process. Moreover, they present a comparison among big data validation tools and several major players in industry are discussed. Also, they discuss the big data",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "major players in industry are discussed. Also, they discuss the big data quality assurance issues, challenges and needs. Furthermore, these discussions may bring great benefits to the future of large data quality assurance. We have collected some data quality parameters from the published papers, and we have presented in Table I. It includes quality parameter and the corresponding attribute meaning. 2017 IEEE Third International Conference on Big Data Computing Service and Applications 978-1-5090-6318-5/17 $31.00 © 2017 IEEE DOI 10.1109/BigDataService.2017.42313 [Página 2] Table I. Quality Parameters for Big Data Quality ParametersAttribute Meaning Data accuracy It refers to the degree of closeness between the observed result and the true value or value that is accepted as true. Therefore, we can know this quality parameter is typically used to measure the",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "we can know this quality parameter is typically used to measure the collected sensor data by comparing the multiple sources. Data correctnessThis data quality parameter is much helpful to evaluate the correctness of big data sets in term of data types, formats, and so on. Data consistencyData consistency refers to data collection methods, schedules, and locations. It is much helpful to evaluate the consistency of the big data sets in abundant and different angles. Data security This quality parameter could be helpful to evaluate the security of the given big data sets in different perspectives. They also discussed big data quality verification tools and players. They compare with tools in terms of operating environment, supported data sources, data validation, and current successful applications. Now, when big data quality",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "sources, data validation, and current successful applications. Now, when big data quality a ssurance is discussed, the quality of big data applications is also concerned. Of course, the quality factors of big data applications have gradually opened the mystery. Conventional quality factor such as performance ,robustness ,security , etc., can be applicable onto big data applications. From the published papers in [5], Tao et al. focus on big data system validation and quality assurance, and the paper includes informative discussions about essential quality parameters ,primary focuses , and validation process . Compared with traditional software testing, they discussed the big data application specific test process. The test procedure comprises the following steps [6] . Step 1: System function testing, including rich oracles, intelligent algorithms, learning capability, as well",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "System function testing, including rich oracles, intelligent algorithms, learning capability, as well as domain-specific functions; Step 2: System non-function testing, including system consistency, security, robustness, and QoS (Quality of Service); Step 3: System feature testing, checks usability, system evolution, visualization, and so on; Step 4: System timeliness testing, targets time related feature testing, including co ntinuous testing, real-time testing, life-time testing, and others. In addition, they also discuss the quality factors of different systems, including prediction systems, recommendation systems and so on. Based on those, we can draw out the quality factors of big data applications, and presented below: x Performance: This factor indicates the performance of the big data applications, such as availability, response time, etc. x Reliability: This factor helps to evaluate the durability of the",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "etc. x Reliability: This factor helps to evaluate the durability of the big data applications when the required function is performed within a specified time period under specified conditions. x Correctness: This is a quality factor used to assess the correctness of big data applications. x Scalability: This quality factor means that big data application should be able to support large data sets now and in the future, and all components of big data application can be extended to address the growing complexity of complex data sets. x Security: This factor helps to evaluate security of the big data application in various perspectives at the different levels. Our brief survey of the litera ture has demonstrated that although big data quality assurance has been studied, quality assurance techniques",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "that although big data quality assurance has been studied, quality assurance techniques for big data applications has also been studied. However, th ere has been little scientific research aimed at understanding, defining, classifying and communicating quality assurance techniques of big data applications. Consequently, there is no clear way to deal with quality assurance of big data applications. Therefore, discussing quality assurance techniques of big data applications is very necessary. III. The SURVEY FRAMEWORK In this section, we brie fly summarize the articles which we researched, and we describe the articles in several sections. We have studied new research results in the last five years, discussed the application domain of big data applications, and show whether the quality assurance technique is applied at design-time or run-time. In addition, we",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "quality assurance technique is applied at design-time or run-time. In addition, we also discussed the big data applications functional properties or non-functional properties (e.g., performance, reliability, availability, etc.), which are very important. We all know that big data has its own properties, such as Volume ,Velocity ,Variety andVeracity. Volume means the sheer size of the databases. Variety means the different types of data which can be stored within a single data container, and everything from discrete numeric and string values to texts and images and to video films and audio recordings. All of this can be stored and retrieved in various sequences or combinations [7] . Velocity means the speed with which the objects can be retrieved and put together. The search algorithms are constructed in such a",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "retrieved and put together. The search algorithms are constructed in such a way that many multiple search paths are executed parallel to one another. In the end the results of the different searches are joined together to form a consistent whole. We discussed the quality assurance of big data application, so big data itself unique 4V properties (i.e., volume, velocity, variety, and veracity) are also focused. For quality assurance, quality assurance techniques are particularly important. Therefore, we are mainly from these aspects to analyze the article. In the Table II, we have a simple induction for the articles which we have researched. Through the analysis of Table II, and related articles, large data applications are widely applicable to many areas, especially in recent years. Quality assurance tec hnology",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "applicable to many areas, especially in recent years. Quality assurance tec hnology of big data applications are rapid developed. Consequently, we can conclude that there are six main ways, including testing, model-driven arch itecture (MDA), monitoring, fault tolerance, verification and prediction to typically ensure the quality of big data applications. In the next part, 314 [Página 3] we will conduct a detailed description and analysis of the approaches of thesesix aspects. Table II. Comparison of Quality Assura nce Approaches of Big Data Application Year Reference Application Domain Technique Design-time or Run-timeFunctional or Non-functional PropertiesProperties 2014 [8] Application Testing Testing Design-time Performance 4V characteristics of big data 2015 [7] Big data bases Testing Design-time Validity ConsistencyVolume, Variety 2015 [9] Big Data and Cloud Computing to process large data.Testing Design-time",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "2015 [9] Big Data and Cloud Computing to process large data.Testing Design-time NULL Volume, Variety, Velocity 2015 [10] Data-intensive software systemsMDA Design-time Reliability Safety EfficiencyVolume, Velocity 2015 [11] Not Mentioned Specific Application DomainMDA Design-time Performance Not Mentioned 2012 [12] Enterprise Application Performance ManagementMonitoring Not Mentioned Performance Velocity 2016 [22] Not Mentioned Specific Application DomainMonitoring Design-time Reliability Volume, Velocity 2015 [13] Distributed storage systemsFault tolerance Design-time Performance Velocity 2012 [14] Modern cloud computing systems and so onPrediction Run-time Reliability Volume 2015 [15] MapReduce 9HUL¿FDWLRQ Design-time Integrity, PerformanceNot Mentioned IV. THE SURVEY APPROACHES Research shows that quality assurance techniques of big data application are mainly these aspects – MDA, Testing, Verification, Fault tolerance, Monitoring, and Prediction. A. Model-Driven Architecture (MDA) MDA derives from the well-known idea of separating the specification",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "Architecture (MDA) MDA derives from the well-known idea of separating the specification of system operations from the system. MDA provides a way (through related tools) to standardize a platform-independent application, selects a specific implementation platform for the application, and then transforms application specifications to a specific implementation platform. The three main goals of MDA are: to achieve portability, interoperability, and reusability through architectural separation [16], [17] . The model driven approach is a well-known one and has been widely exploited in many areas of software engineering. The goal of the MDA is to design applications in a model-driven approach which is more abstract than the implementation of the techniques. For example, Alodib et al. [11] propose an extension to automate the integration of the Hadoop platform. This is",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "an extension to automate the integration of the Hadoop platform. This is intended to break up each problem into multiple su b-tasks using a simple programming model (MapReduce). After the analysis is calculated, the results are submitted to the Score table linked to the protocol service. The approach harnesses the capability of Model-Driven Architecture (MDA) to automate the creation, and integration of the architecture. Largely, due to existing models and QA techniques ignore properties of data such as volumes, velocity and so on. Casale et al. [10] present the research agenda of DICE. It is a quality-aware MDE technology for big data cloud applications. And its goal is to developing a quality engineering tool chain offering simulation, verification, and architectural optimization for Big Data applications. They also present",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "simulation, verification, and architectural optimization for Big Data applications. They also present the main challenge in this approach. These challenges are due to the fact that data operations and data characteristics cannot be fully described. Etani [18] describes database application model and its service for drug discovery introducing their proposed software development process in MDA into their research process. The issue of veracity can be solved when pinpoint data are selected from drug properties in big data analytics with domain model. Our approach of software development process in MDA will be useful for developing a big data application and a new service by “veracity” of big data. All in all, MDA provides a complete solution for integration of big data applications at different lifecycle stages. It advocates the",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "integration of big data applications at different lifecycle stages. It advocates the use of formalized system models as the core of application integration. Consequently, we can know MDA is an important method for quality assurance of big data application. B. Testing Application testing is a test of the entire product to verify whether the application meets the requirements specification definition, and to identify inconsistent with the requirements specification or contradictory places, so as to propose a more complete solution. 315 [Página 4] The volume and variety of big data presents a particular challenge to the testing of the big data application. Therefore, Sneed et al. [7] consider that there is no other way but to automate the test process to test the applications. Due to the volume and",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "the test process to test the applications. Due to the volume and variety of big data, they think it is impossible to test big data application manually, and testing big data need new processes and higher degree of automation. People need automated tools to scan through the big data and check the validity and consistency of the content. The performance of big data applications is particularly important. Performance testing is a test method, which belongs to a typically non-functional testing. During performance testing, the system tests by simulating various normal and abnormal peak load conditions to reduce operational, upgrade, or patch deployment risk through performance testing (such as information systems) to achieve a user response time load. But the existing performance testing techniques are not suitable for the",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "load. But the existing performance testing techniques are not suitable for the big data application. Liu [8] proposes test technique for performance testing. The technique provided testing goal analysis, testing design, load design for big data applications. The characters for different big data applications could be supported to consider specific multiple test data design method under this framework. This performance technique is used to test some applications and demonstrated its effectiveness. Jesús Morán et al. [9] propose a testing technique named MRFlow, which is based on data flow test criteria and oriented to transformations analysis between the input and the output, and it can test defects in MapReduce programs. MapReduce is a programming model for parallel computing of large-scale data sets. MapReduce achieves reliability by distributing the large-scale",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "computing of large-scale data sets. MapReduce achieves reliability by distributing the large-scale operations on the data set to each node on the network. Moreover, they tested the technology, and the testing results are better. In summary, the testing for big data quality assurance is very important, especially because of the big data properties, testing is a very good quality assurance method. And we can learn that the testing will work in many cases which we meet. C. Verification Applications based on big data are now widely used, such as recommendation, prediction and decision systems. Research shows that current research rarely explores how to effectively verify big data applications to ensure the quality of big data applications. Big data properties have taken many challenges for big data applications. For",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "Big data properties have taken many challenges for big data applications. For example, because of the volume of data and the timeliness of data, it is a very difficult task to verify the correctness of big data applications. Gao et al. [5] have discussed the validation methods for big data application. They discussed and reviewed existing research results in software testing methods that have been used to validate various types of big data applications, including data mining programs, bioinformatics programs, and learning-based applications. And those methods include program-based software testing, classification-based testing, metamorphic testing (MT), learning-based testing, crowd-sourced testing, data model- based testing, rule-based software testing and so on. Result integrity is one of the most important security issues in cloud-based big data computing scenarios. Wang et al.",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "important security issues in cloud-based big data computing scenarios. Wang et al. [15] present MtMR, a Merkle tree-based verification method to ensure the high integrity of the MapReduce tasks. MtMR covers MapReduce in a hybrid cloud environment and performs two rounds of Merkle tree-based verification for the pre-reduction and restoration phases. In each round of verification, MtMR samples a small portion of the reduced task input/output records on the private cloud, and then performs Merkle tree-based verification of all task input/output records. After analysis, they believe that MTMR can significantly improve the comprehensive, while can produce moderate performance overhead. Traditional software verification models and standards have been unable to meet the quality requirements of big data applications (because of the existence of big data properties)[19] . Although many",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "applications (because of the existence of big data properties)[19] . Although many scholars have studied the quality verification problem of big data applications, but not enough, the quality verification and assurance of big data application challenges remain. D. Fault tolerance The so-called fault tolerance refers to the existence of the fault in the case of the system does not fail, still is able to work properly. Fault tolerance is rather a fault, not an error. The use of fault toleran ce to ensure the quality of big data applications can usually measure in terms of application reliability, availability, and testability. Due to the trends towards Big Data, people want to provide large storage systems, and those are accessible by many servers. The shared storage has been the performance",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "are accessible by many servers. The shared storage has been the performance bottleneck and a single-point of failure. Lundberg et al. [13] suggest that we introduce a cac he in the distributed storage system. The cache system must be fault tolerant so that no data is lost when the hardwa re failure happened. According to the study, we know that the cache system is a way to improve the performance of most systems. As we all known, NoSQL databases are critical for supporting big data applications, because they can handle a large number (i.e., volume) of highly variable (i.e., variety) user-generated content while guaranteeing fault tolerance, availability, and scalability. However, all NoSQLs are somewhat different from each other, even if they are considered to belong to the same",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "each other, even if they are considered to belong to the same database family. Scavuzzo et al. [20] pose an efficient and fault tolerance data migration method. In general, data migration should be able to tolerate faults or inte rruptions by recovering to the last correct state, since NoSQL typically stores large amounts of data, which means long-running migration tasks, but on the contrary, higher risk of faults will happen. However, their approach tolerates a sudden fault of any component involved in the data migration process without any data loss. Experiments show that the method used to perform the data migration is efficient, fault tolerance, and really can improve the NoSQL technology interoperability. Likewise, there is an in creasing interest in the reliability and availability of big data",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "an in creasing interest in the reliability and availability of big data cloud applications. And fault tolerance is a very effective means to solve the problem of reliability and usability. Jhawar et al. [21] focus on describing repetitive faults in typical cloud computing applications, analyzing the im pact of faults on user applications, and investigatin g fault tolerance solutions corresponding to each type of failure. And they also talk 316 [Página 5] about providing fault tolerance as a service to user applications as an effective means of addressing reliability and availability issues. From those researches, we can know that the fault tolerance is helpful to quality assurance of big data applications. E. Monitoring In recent years, a large number of structured, semi- structured and unstructured data is generated.",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "a large number of structured, semi- structured and unstructured data is generated. These data are huge, complex, and rapidly changing. If the data cannot be filtered, the real-time monitoring of information cannot be achieved. Therefore, one of the biggest challenges with big data applications is how to analyze and process huge amounts of data in real time. And real-time monitoring is an effective way to ensure the quality of large data applications. Therefore, improving the real-time performance of large data monitoring is very necessary. In order to improve the real-time performance of big data monitoring, Shi et al. [22] dish a dual cloud architecture to take full advantage of cloud resources and network bandwidth. They also propose a real-time monitoring algorithm based on user evaluation in Hadoop platform,",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "propose a real-time monitoring algorithm based on user evaluation in Hadoop platform, which uses a combination of computing nodes. The monitoring algorithm can eliminate nonsense data such as spam, malice evaluation, brush score, brush reputation and brush list by establishing user evaluation system. As a result, it can significantly reduce the amount of data, but also can greatly improve the operational efficiency. Thus, it can ensure real-time monitoring information, reliability and accuracy. Distributed systems are typically big data applications. State monitoring has been widely used to detect critical events and anomalies in distributed systems. Unfortunately, existing distributed state monitoring methods are usually designed based on the condition that we assume always- online distributed monitoring nodes and reliable inter-node communicate. Therefore, based on these methods, it often produces misleading",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "reliable inter-node communicate. Therefore, based on these methods, it often produces misleading results, which leads to various problems being introduced to rely on state monitoring results to perform automatic management tasks of the user. Meng et al. [23] introduced a new state monitoring approach, and this method exposed and handled communication dynamics such as message delay and loss in Cloud monitoring environments. Firstly, by quantitatively estimating the accuracy of monitoring results, it can capture uncertainties which are introduced by messaging dynamics. This characteristic is useful to distinguish trustworthy monito ring results from one heavily deviated from the truth. Secondly, they can configure the monitoring algorithm, which minimizes monitoring errors. And there are also other methods related to monitoring, which we can find in paper [24], [25] . Therefore,",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "to monitoring, which we can find in paper [24], [25] . Therefore, we can know that big data brings some trouble to big data applications, and using special monitoring approaches can improve the quality assurance of big data applications and improve reliability, performance and other non-functional properties. F. Prediction Big data applications will have a variety of failures. If we can predict the upcoming failure; it will greatly improve the quality of large data applications. Therefore, the prediction technique for big data quality assurance is an effective way. Yang et al. [26] design a general framework named Hdoctor for hard drive failure prediction. Hdoctor demonstrated a number of innovations, and building time- dependent features to characterize Self-monitoring, Analysis and Reporting Technology (SMART) value transitions during disk failures is",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "Self-monitoring, Analysis and Reporting Technology (SMART) value transitions during disk failures is the important one. Meanwhile, Hdoctor automatically collects/labels samples and updates model, and works well for all kinds of disk failure prediction in their intelligent data center. Existing production applicatio ns are short of real-time performance status of production process active perception, resulting in the production abnormal conditions processed lag, leading to the frequency problems of deviations in production tasks execution and planning. To address this problem, Zhang et al. [27] advance they should extend an advanced identification technology to the manufacturing field to acquire the real-time performance data. Based on the sensed real-time manufacturing data, they present a prediction method which applies the Dynamic Bayesian Networks (DBN) theory and methods. Achieving the prediction of the performance",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "Bayesian Networks (DBN) theory and methods. Achieving the prediction of the performance status of production system and potential anomalies is the goal of the method, and it can provide the important and abundant prediction information. All in all, Dynamic Bayesian Networks theory and method is used to make the mathematical modeling of performance prediction for production system based on manufacturing big data. In modern cloud computing systems, thousands of cloud servers are interconnected through multiple layers of networks. Faults are common in such large and complex systems. In order to predict the failure, we should monitor the system implementation process, and collect health- related runtime performance data. Guan et al. [14] present an unsupervised failure detection method based on an ensemble of Bayesian models. It characterizes the normal",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "method based on an ensemble of Bayesian models. It characterizes the normal system execution state and detects anomalous behavior. The tagged data is available after the system administrator verifies the exception. Then, supervised learning based on decision tree classifier is used to predict future failures. There are other predictive methods in paper [28], [29] and other papers which we do not know. Dealing with faults which have been happened may be very difficult, and fault prediction is particularly important. Therefore, it is necessary to discuss the fault prediction method of big data applications. Therefore, I think prediction will play an important role in quality assurance of big data applications. V Discussion By reading a lot of literature, we can summarize a number of approaches to ensure the quality",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "literature, we can summarize a number of approaches to ensure the quality of big data applications, including MDA, Testing, Verification, Fault tolerance, Monitoring, and Prediction. In TABLE III, we further summarize functional or non-functional properties involved in these six aspects, as well as the big data properties. As we can see from the TABLE III , in the process of considering big data application quality assurance, 317 [Página 6] performance of this non-functional property is basically the main consideration. However, the big data properties have a great impact on quality assurance of big data application. As shown in TABLE III, we know one of big data properties which are common for mostly approaches is variety. However, the variety often is solved by NOSQL which can handle structured data,",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "the variety often is solved by NOSQL which can handle structured data, semi-structured data, and unstructured data of big data. NoSQL databases are key to supporting Big Data applications, since they enable handling large quantities (i.e., volume) of highly-variable (i.e., variety), user-generated contents while guaranteeing fault tolerance, availability (i.e., velocity) and scalability [20] . TABLE III. Approaches, Functional or Non-functional Properties, 4V properties of big data application Approaches Functional or Non-functional Properties4V properties Model-Driven Architecture (MDA)Performance, ScalabilityVeracity, Volume, Variety Testing Availability, PerformanceVariety, Velocity Verification Performance, ReliabilityVolume, Variety Fault tolerance Performance, ScalabilityVariety, Volume Monitoring Performance, real- timeVariety, Velocity Prediction Performance, DependabilityVariety, Veracity Not only that, according to the research, we can get big data properties of the challenges, and how to use the novel technique to solve the problems.",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "challenges, and how to use the novel technique to solve the problems. Consequently, we can summarize the big data properties, challenges, as well as the techniques for those challenges in following form. TABLE IV. Properties, Challenges and Techniques Properties Challenge Novel Technique Volume Storage/Scale Distributed File Systems Velocity Fast Processing Parallel Programming Variety Heterogeneity NOSQL Databases When we consider the big data properties and quality requirements, it is anticipated to aid requirement analysts in the specification of quality requirements while keeping big data properties in mind. There are some major issues and challenges in big data application quality assurance. Here are typical ones. Issue #1 - Lack of awareness and good understanding of quality assurance techniques for big data applications. With the fast development of big data technologies",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "for big data applications. With the fast development of big data technologies and analytics approaches, more big data applications and service systems are developed to be used in many areas of our daily life. Consequently the increasing deployment of big data applications and services dishes quality assurance concerns. Then, most people will find ways to solve a specific problem until the big data application problems happened. Hence, according to real world practitioners, there is a clear demand on understanding the quality assurance of big data application. This brings the first demand of big data application quality assurance. Need #1 - Full understanding the quality assurance techniques to solve the special functions and needs of big data applications and services. Issue #2 - Lack of approaches to solve quality",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "applications and services. Issue #2 - Lack of approaches to solve quality assurance issues in different big data applications. For specific big data applications, there are specific ways to solve the quality assurance pr oblem. However, there is currently no strictly defined approach to solve the problem. Therefore, it brings the second demand of big data application quality assurance. For example, testing oracle may be a big issue for big data applications due to the 4V properties. Need #2 - Define and develop well-defined big data application quality assurance standards, and define some approaches to solve quality assurance issues. Those approaches can be extracted from the six aspects of this paper. Issue #3 – Lack of solutions to coordinate big data properties with quality assurance techniques. Today, big",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "solutions to coordinate big data properties with quality assurance techniques. Today, big data applications, such as social media, generate more data in a short period of time than was previously available requiring new techniques for quality assurance. Existing techniques have no adequate scalability and facing challenges because of big data properties such as Volume, Velocity, Variety and Veracity. Therefore, it brings the last demand of big data application quality assurance. Need #3 - Consider functional or non-functional properties with big data properties together to ensure the quality of big data applications. In addition, we have general approaches to deal with big data properties: - Distributed File Systems for Volume; - Parallel Programming for Velocity; - NOSQL Databases for Variety (structured, semi- structured and unstructured data). VI. Conclusion and",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "Databases for Variety (structured, semi- structured and unstructured data). VI. Conclusion and Future Work This paper focuses on the quality assurance of big data application. It mainly discusses the state-of-art approaches to ensure the quality of big data applications. The surveyed approaches are mainly testing, model-driven architecture (MDA), monitoring, fault tolerance, verification and prediction. In addition, this paper discusses the impact of big data characteristics on big data applications. Although researchers have proposed some quality assurance techniques for big data applications, the challenge of big data applicati ons still exists. Consequently, how to effectively ensure the quality of big data applications is still a hot res earch issue. In the follow-up study, we should conduct more research based on big data 4V properties. We can try to deal",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "research based on big data 4V properties. We can try to deal with big data 4V problems and we can also consider functional or non- functional properties of big data applications with big data properties together to ensure the quality of big data applications. 318 [Página 7] Acknowledgement This work is supported by the National Natural Science Foundation of China (No. 61572171) and the Fundamental Research Funds for the Central Universities (No. B15020191). REFERENCES [1] Big Data Technology and Services at $32.4 Billion in 2017 - IDC[J]. San/lan, 2013. [2] Gao J, Xie C, Tao C. Big Data Validation and Quality Assurance -- Issuses, Challenges, and Needs[C]// IEEE, IEEE International Symposium on Service- Oriented System Engineering. IEEE, 2016: 433-441. [ 3 ] G a r g N ,",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "IEEE, 2016: 433-441. [ 3 ] G a r g N , S i n g l a S , J a n g r a S . C h a l l e n g e s a n d Techniques for Testing of Big Data[J]. Procedia Computer Science, 2016, 85: 940-948. [4] Yesudas M, Menon S G, Nair S K. High-Volume Performance Test Framework using Big Data[C]// International Workshop on Large-Scale Testing. ACM, 2015: 13-16. [5] Tao C, Gao J. Quality Assurance for Big Data Applications– Issues, Challenges, and Needs[C]// The Twenty-Eighth International Conference on Software Engineering and Knowledge Engineering. 2016. [6] Guerriero M, Tajfar S, Tamburri D A, et al. Towards a model-driven design tool for big data architectures[C]// The, International Workshop. 2016: 37-43. [7]",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "design tool for big data architectures[C]// The, International Workshop. 2016: 37-43. [7] Sneed H M, Erdoes K. Testing big data (Assuring the quality of large databases) [C]// IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops. IEEE, 2015: 1-6. [8] Liu Z. Research of performance test technology for big data applications[C]// IEEE International Conference on Information and Automation. IEEE, 2014: 53-58. [9] Jesús Morán, Riva C D L, Tuya J. Testing data transformations in MapReduce programs[C]// The, International Workshop. 2015: 20-25. [10] Casale G, Ardagna D, Artac M, et al. DICE: Quality- Driven Development of Data-Intensive Cloud Applications[C]// IEEE/ACM, International Work- shop on Modeling in Software Engineering. ACM, 2015: 78-83. [11] Alodib M, Malik Z. A Big Data approach to enhance the integration of Access Control",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "Z. A Big Data approach to enhance the integration of Access Control Policies for Web services[C]// IEEE/ACIS, International Conference on Computer and Information Science. IEEE, 2015: 41-46. [12] Rabl T, Mez-Villamor S, Sadoghi M, et al. Solving big data challenges for enterprise application performance management[J]. Proceedings of the Vldb Endowment, 2012, 5(12): 1724-1735. [13] Lundberg L, Grahn H, Ilie D, et al. Cache Support in a High Performance Fault-Tolerant Distributed Storage System for Cloud and Big Data[C]// Parallel and Distributed Processing Symposium Workshop. IEEE, 2015: 537-546. [ 1 4 ] G u a n Q , Z h a n g Z , F u S . E n s e m b l e o f B a y e s i a n Predictors and Decision",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "f B a y e s i a n Predictors and Decision Trees for Proactive Failure Management in Cloud Computing Systems[J]. Journal of Communications, 2012, 7(1): 52-61. [15] Wang Y, Shen Y, Wang H, et al. MtMR: Ensuring MapReduce Computation Integrity with Merkle Tree- based Verifications[J]. 2016: 1-1. [16] Xuan P, Zheng Y, Sarupria S, et al. SciFlow: A Dataflow-Driven Model Architecture for Scientific Computing using Hadoop[C]// IEEE Big Data 2013 Workshops: Big Data and Science - Infrastructure and Services. IEEE, 2013: 36-44. [17] Klein J, Buglak R, Blockow D, et al. A reference architecture for big data systems in the national security domain[C]// International Workshop on Big Data Software Engineering. 2016: 51-57. [18] Etani N. Database application model and its service for drug discovery in Model-driven architecture[J].",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "Database application model and its service for drug discovery in Model-driven architecture[J]. Journal of Big Data, 2015, 2(1): 1-17. [19] Hussain M, Almourad M B, Mathew S S. Collect, Scope, and Verify Big Data -- A Framework for Institution Accreditation[C]// International Conferen- ce on Advanced Information NETWORKING and Applications Workshops. IEEE, 2016: 187-192. [20] Scavuzzo M, Tamburri D A, Nitto E D. Providing big data applications with fault-tolerant data migration across heterogeneous NoSQL databases[C]// International Workshop on Big Data Software Engineering. 2016: 26-32. [21] Jhawar R, Piuri V. Chapter 7 - Fault Tolerance and Resilience in Cloud Computing Environments[M]// Computer and Information Security Handbook. Elsevier Inc. 2013: 125-141. [22] Shi G, Wang H. Research on Big Data Real-Time Public Opinion Monitoring under the Double Cloud Architecture[C]// IEEE Second",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "Data Real-Time Public Opinion Monitoring under the Double Cloud Architecture[C]// IEEE Second International Conference on Multimedia Big Data. IEEE Computer Society, 2016: 416-419. [23] Meng S, Iyengar A K, Rouvellou I M, et al. Reliable State Monitoring in Cloud Datacenters[C]// IEEE, International Conference on Cloud Computing. IEEE, 2012: 951-958. [24] Iuhasz G, Dragan I. An Overview of Monitoring Tools for Big Data and Cloud Applications[C]// International Symposium on Symbolic and Numeric Algorithms for Scientific Computing. 2015: 363-366. [25] Zareian S, Fokaefs M, Khazaei H, et al. A big data framework for cloud monitoring[C]// The, International Workshop. 2016: 58-64. [26] Yang W, Hu D, Liu Y, et al. Hard Drive Failure Prediction Using Big Data[C]// Reliable Distributed Systems Workshop. IEEE, 2015: 13-18. [27] Zhang Y, Liu S, Si S,",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "Systems Workshop. IEEE, 2015: 13-18. [27] Zhang Y, Liu S, Si S, et al. Production system performance prediction model based on manufactu- ring big data[C]// IEEE, International Conference on Networking, Sensing and Control. IEEE, 2015. [28] Xu J, Li H. The Failure Prediction of Cluster Systems Based on System Logs[M]// Knowledge Science, Engineering and Management. Springer Berlin Heidelberg, 2013:526-537. [29] D ai D, Chen Y, Kimpe D, et al. Provenance-based object storage prediction scheme for scientific big data applications[C]// IEEE International Conference on Big Data. IEEE, 2014: 271-280. 319",
        "start_idx": 5916,
        "end_idx": 6005
      }
    ],
    "5255389e-58cb-4ab8-ae65-8c6c390357b3": [
      {
        "text": "[Página 1] QuantCloud: A Software with Automated Parallel Python for Quantitative Finance Applications Peng Zhang Applied Mathematics Department Stony Brook University NY 11794, United States Peng.Zhang@Stonybrook.eduYuxiang Gao Midea Emerging Technology Center CA 95134 , United States Yuxiang1.Gao@Midea.comXiang Shi Advanced Risk & Portfolio Management (ARPM) , NY 10023 United States Xiang.Shi@arpm.co Abstract —Quantitative Finance is a field that replies on data analysis and big data enabling software to discover market signals. In this, a decisive factor is the speed that concerns execution speed and software development speed. So, an efficient software plays a key role in helping trading firms. Inspired by this, we present a novel software: QuantCloud to integrate a parallel Python system with a C++-coded Big Data system. C++ is used to implement this big data system",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "Big Data system. C++ is used to implement this big data system and Python is used to code the user methods. The automated parallel execution of Python codes is built upon a coprocess-based parallel strategy. We test our software using two popular algorithms: moving -window and autoregressive moving- average (ARMA). We conduct an extensive comparative study between Intel Xeon E5 and Xeon Phi processors. The results show that our method achieved a nearly linear speedup for executing Python codes in parallel , prefect for today’s multicore processor s. Keywords —Quantitative Finance Software, Parallel Python, Big Data, Cloud computing. I. I NTRODUCTION Quantitative Finance is a field that extends mathematical models to the finance problem thus it is also known as computational finance. In this field, the revolution of",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "is also known as computational finance. In this field, the revolution of computational technologies has be en shaping the best practice and future of quantitative finance. In the high-frequency trading age, a program trading system is developed to use powerful computers to transact a large number of orders as quickly as possible. The whole order and withdraw process may happen in a microsecond level or even less [1] . However, as the age of big data arrives, the science, social and economic including quantitative finance have been undergoing a fierce yet great revolution. In the past, the high-frequency traders had been pursuing a high speed between exchanges for facilitating the buying and selling of shares, curr encies and other assets [2, 3]. At present, the finance firms want",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "encies and other assets [2, 3]. At present, the finance firms want to compete on strategies as the race for transacting speed among high-frequency traders hit peak [4, 5]. Traders are building more complicated data analysis models to derive deeper profitable signals out of the big finance datasets [6]. Thus, there is a need to construct a novel software that allows fast-developing and fast-testing strategies. This need inspires this work. Speed is always a decisive factor in maintaining a finance firm’s competitive advantage but its meaning is extend ing. To have a transaction speed without prediction is of no practical value. Currently, the speed concerns with not only the executionspeed of a big data analysis model but also the software development speed of a complicated mathematical model. In",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "but also the software development speed of a complicated mathematical model. In the practice of this field, Python is the most preferred high-level programming language as it requires fewer lines of code and also has wide availability of statistics libraries in timeseries analysis. On the other hand, C++ is the most ideal language to implement the big data infrastructure system that is able to handle massive amounts of market data as it provides high speed of execution. Considering these facts, we develop an integration system that combines a C++-based big data infrastructure and an automated parallel Python system. As being applied to quantitative finance, this system handles the timeseries market information by this big data infrastructure and meanwhile performs timeseries analysis models that are coded in Python. Revolutionary",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "and meanwhile performs timeseries analysis models that are coded in Python. Revolutionary technological advances have been stimulating evolutionary industrial adaptation. In this trend, the quantitative finance is a grand pioneer for adapting advanced technologies such as novel multicore processor architectures. Of these, Intel Xeon Phi processor, codenamed as Knights Landing (KNL), is a representative of modern multi-core processors. Different from its former processor families, Intel’s KNL has a higher density of processor cores and thus it is optimized more for highly parallel workloads. However, fully exploiting the power of such kind of high-density multi-core processor is by no means a trivial challenge. This needs an effective yet agile way to bring a degree of parallelism to the execution of programs. To this end, we develop a coprocess-based parallel",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "the execution of programs. To this end, we develop a coprocess-based parallel execution for Python. Contribution synopsis: the main contribution of this work is the design of a software suite, QuantCloud that combines a big data infrastructure with an automated parallel Python system. We also conducted an extensive application-level comparative study using commodity hardware. The results show the efficacy of this software and characterize all essential aspects of performance such as the wallclock time, the speedup and parallel efficiency for the codes in Python, the tick-level latency for commonly-used QF applications on real-world market data. II. B ACKGROUND AND MOTIVATION A. Big Data in Quantittative Finance Big data is becoming a critical issue in finance, particularly the quantitative finance with multiple applications, wider usage, given advances in enabling",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "the quantitative finance with multiple applications, wider usage, given advances in enabling technologies [7]. Big data in finance has covered all principle interests of Big Data such as the data 3882018 IEEE International Conference on Software Quality, Reliability and Security 978-1-5386-7757-5/18/$31.00 ©2018 IEEE DOI 10.1109/QRS.2018.00052 [Página 2] volume, velocity and variety [8-10]. Data volume of market information has been ever increasing at a tremendous rate. For instance, the total shares changed hand is tenfold of 20 years ago and the total number of transactio ns is increased by 50 times, with this number being more than 120 times during the financial crisis [8]. The prevalence of high-frequency trades (HFTs) has spurred up growth of high-speed data in trading activities. For example, about 70% of the U.S. equity trades",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "in trading activities. For example, about 70% of the U.S. equity trades are computer driven [10]. B. Python for Quantitative Finance In daily practice of most trading systems, Python is the most preferred language. For example, Quartz is Bank of America Merrill Lynch’s integrated trading, position management, pricing and risk management platform and its entire tech stack uses Python. Athena is J.P. Morgan’s next -generation pricing, risk management, analysis and tr ade management platform, and is a Python-based rapid develo pment environment. Meanwhile, a compiled language C++ is used for the high-performance core of this system, while Python is used for building logic and apps. Python is becoming more and more popular for being easier to use and faster to program than traditional languages including the C++ programming",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "use and faster to program than traditional languages including the C++ programming language. So far, there’s been a huge spike in demand for Python in the investment banks including Bank of America and J.P. Morgan that are using Python to replace historic legacy systems built in Java/C++. From a practical perspective in quantitative finance, we choose Python as a language to program timeseries analysis algorithms and models on the finance big data. C. Python Limitation CPython is the default and most widely-used interpreter for the Python programming language. It is written in C and offers rich extensions with several languages including C. In CPython, global interpreter lock, or GIL, is a mutex lock that prevents concurrent executions of multiple native threads within one process [11]. In other words,",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "executions of multiple native threads within one process [11]. In other words, Python is implemented in such a way that only one thread can be accessing the interpreter at a time. The exceptions are few: for example, while a thread is waiting for I/O, the interpreter is released so other threads can run [12]. In this literature, the GIL becomes a key limitation in multithreading with Python. As usual, multithreading actually performs worse than serial code [13, 14]. However, the GIL is necessary because CPython’s memory management is not thread-safe. A solution to this issue is to use multiple full processes instead of threads [15], where each process uses its own GIL. To overcome this limitation, we present a coprocess- based approach and bring parallel performance to Python",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "we present a coprocess- based approach and bring parallel performance to Python code. Sophisticated parallelism and wo rkflow management is hidden in QuantCloud system. D. High-Density Multicore Processors Technology has been shaping financial markets so much, that the traders are competing for the fastest equipment rather than the transaction itself. The heart of modern quantitative finance is to reduce the execution time of more complicated models by using more advanced machines. In this battlefield for speed, the processor plays a key role. Simply, a more powerful processor makes the analytics algorithm execute quicker so there is more room to crunch more data and harness more complex models while without sacrificing time. Faster computing means more doing. This is of practical interest to time-critical applications in the field. I",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "This is of practical interest to time-critical applications in the field. I n t o d a y ’ s p r o c e s s o r markets, Intel’s 2nd-generation Xeon Phi processor, codename as Knights Landing (KNL) [16, 17], is a novel high-density multicore processor and it has 64~72 cores per processor, optimized for a highly-parallel application. III. S YSTEM DESIGN Our system incorporates two parts: a big data infrastructure system and its integration interface with Python. The overview of this integrated system is shown in Figure 1. The design of this big data infrastructure system is extension of our previous work [18]. In this system, an approp riate embedded Python interface is built for effortless integration with this big data infrastructure. Data communication between",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "built for effortless integration with this big data infrastructure. Data communication between the main C++ program and embedded Python scripts is through a shared memory system. The Python script is used as a high-level language to program the sequential execution of an algorithm. The coprocesses that execute a code in Python seem to run sequentially instead of parallel and they are transparent at the user-application level. There have been no code changes in the Python scripts so this “as-is” embedding approach supplies a simple yet efficient method to use Python embedded in a C++ program for a complex big data application. Figure 1. Overview of the integrated Big Data and Parallel Python architecture in the QuantCloud suite A. Big Data Softwar e Infrastructure This Big Data infrastructure includes",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "suite A. Big Data Softwar e Infrastructure This Big Data infrastructure includes three components: User, Client and Server, in Figure 1. User part is an XML-script portal that is able to receive an application-user job and return CSV-format results to end users. Client part is a platform that executes the computing jobs. It parses a user job, queries the required data from the Server and conducts the job. The Server part is a platform that provides data-centric services. This is a distributed application architecture and adopts a sever-client model that partitions jobs and data between the Client and the 389 [Página 3] Server. In this system, Client and Server are the provider of computing and data services and User is a service requester. To communicate, Internet communication is",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "services and User is a service requester. To communicate, Internet communication is between user and client, and Intranet is between server and its clients. This system enables Cloud platforms as providers for finance big data analytics. Essence of cloud computing is Everything-as-a-Service. In this field, Server may reside on a Storage-as-a-Service provider operating on a cost-per- byte- stored and cost-per-byte-transferred basis. Client may use an Infrastructure-as-a-Service provider provisioning scalable computing resources and operating on a pay-per-use basis. Meanwhile, the finance big data analytics algorithms and models could be supplied as Software-as-a-Service in the Client and defined as pay-per-use software. Simply, User just runs a light-weight kernel thus it is able to be operated on ultra-portable devices. This design helps the big data analytics research products to quickly enter",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "This design helps the big data analytics research products to quickly enter the market with the advent of cloud computing technologies. The Server part manages the historical market information such as stock transactions. The market information is organized as multiple timeseries and indexed by its date and stock symbol. Here, data is first compressed then hashed before stored on the storage. Data compression is for saving space and hashing for security reason. In addition to data storage, Server responds to timeseries queries [18]. Before querying data, a Client needs to register a Server and establishes a link between data provider (Server) and data requester (Client). The Client part responds to users’ requesters and processes user jobs. Specifically, a user job describes: (1) the requested data information, such as",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "Specifically, a user job describes: (1) the requested data information, such as data duration, message type and stock symbols; (2) big data analytics models, such as moving-window timeseries analysis and autoregressive moving-average models; and (3) user-specific analytics codes in Python. Upon arrival of a user job, the Client parses the requested data information then queries timeseries data from its Server. Data analytics starts as soon as the queried data streams flow into this Client node. Only timeseries methods take into account possible internal structure on the market data streams. In Section 5, we would present two popular analytics methods: movi ng-window analysis method of financial timeseries data and autoregressive moving-average (ARMA) model. The user-specific analytics code is embedded to apply an analytics method on the managed timeseries. Here,",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "is embedded to apply an analytics method on the managed timeseries. Here, our focus is to enable embedded-Python API that allows finance engineers to: (1) easily implement a method in Python and (2) effortless integrate their method with this big data system for ultra-fast low-latency execution. The detail of integration with Python is presented in the following section. B. Automated Parallel Python Software System Transparency is of essence in the design of embedded parallelized Python APIs in this big data infrastructure system. So, the Python script that is embedded stays unchanged and is integrated “ as-is”. Figure 2 shows the integration flowchart of the codes in Python in the QuantCloud system. It adopts a coprocess -based mechanism. A parent process stands for a thread of main process and",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "mechanism. A parent process stands for a thread of main process and manages the timeseries data streams. At its inception, it spawns a child coprocess that is able to offload its workload. Communication between a parent and its child coprocess is using a parent-child shared memory. A shared memory is attached and its associated parent-child synchronization channel is established at the same time. To execute a code in Python, the child coprocess serializes the timeseries data in the C++ environment, transfers serialized data packets to the Python environm ent where serialized data is deserialized and re -formatted as data structures in Python. This completes data conversion from C++ to Python. The interpreter is called to execute the script as long as data is ready to use. The results",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "the script as long as data is ready to use. The results from the Python code are serialized then returned to the child coprocess. The child coprocess deserializes data packets then restores structures. The results are finally transmitted back to memory space of the parent process and then an acknowledge signal is sent to this parent process upon completion of this job. The code in Python is executed in a single-thread model. A child coprocess is operating in a multi -threaded asynchronous model. It has a built-in job queue able to buffer multiple jobs at the same time and executes the buffe red jobs as first-in first-out (FIFO). It operates three threads: thread 1 is for messaging with a parent process; thread 2 for ex ecuting the code",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "messaging with a parent process; thread 2 for ex ecuting the code in Python and thread 3 for deserializing results from the Python environment. This asynchronous execution mechanism, though complicates the implementation as requires more thread-safe codes, could effectively overlap the data serialization and the data analytics operations, thus it helps reduce the latency that is caused by the extra serialization operations. Optionally, an additional thread is configured to monitor the health state of the parent process periodically and provide fault tolerance. Particularly, it performs a safe shutdown when a failure is detected. Otherwise, an orphan process appears at occurrence of program faulty and error. Figure 2. Integration of automated parallel Python system in the big data infrastructure in the QuantCloud software suite IV. P ROTOTYPE IMPLEMENTATIONS We",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "data infrastructure in the QuantCloud software suite IV. P ROTOTYPE IMPLEMENTATIONS We build a prototype to study the performance characteristics of this proposed system. In this section, we present the software stack about the prototype implementation and the hardware that we use to benchmark this prototype. In next section, we present the finance big data analytics models that use this prototype for test and describe market data and performance measurements. A. Software Stack The prototype is coded in C++. The input script is in XML format and result is reported as in CSV file. The communication among User, Client and Server uses the TCP/IP protocol. The database on the Server and the query of timeseries data on the 390 [Página 4] Client follow our previous work [18]. The",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "on the 390 [Página 4] Client follow our previous work [18]. The automated parallel Python API is provided on the Client. Within one Client instance, the multithreaded programming is used for intra-node parallelism on shared memory. In this, the thread pool is used to manage the threads. The Python code is run in a coprocess that is referred to as child in Fig. 2. To interact with the main process, a memory segment is shared among the parent (the main program) and its child (the coprocess). This addresses the data transfer between the parent and its child. This child is responsible to interact with Python. The workflow of a child is as follows: the incomi ng timeseries is serialized and transferred to Python; in Python, serialized timeseries is",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "timeseries is serialized and transferred to Python; in Python, serialized timeseries is restored and reformatted as Python data types; then GIL is acquired to conduct the code in Python; last the produced result is reformatted, serialized and transmitted back to the child where the result is returned to its par ent’s memory. Upon task accomplishment, the child sends an acknowledgement signal to its parent. This completes the work cycle of a child coprocess. B. Hardware Platform System 1: Dell PowerEdge R720, installed with two Intel Xeon E5- 2603 processors at 1.8 GH z; a total of eight cores per server; 32 GB DDR3 RAM and 500 GB SATA hard drive. Max memory bandwidth is 34.1 GB/s. In this system, operating system is CentOS Linux 7.3 and compiler is",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "In this system, operating system is CentOS Linux 7.3 and compiler is GCC 4.8.5. This processor launched on Q1’12 and already discontinued at Q2’15 so it represents a legacy processor. System 2: QCT QuantaPlex S41T-2U4N system. Each node has one Intel Xeon Phi 7230F processor, codenamed Knights Landing (KNL). Each KNL has a total of 64 cores (1.3 GHz); 128 GB DDR4 RAM and 1 TB SATA SSD. Max memory bandwidth is 115.2 GB/s. In this system, operating system is Red Hat Enterprise Linux 7.2 and compiler is ICC 17.0.1. This processor launched on Q4’16 and it features a high -density compute optimized solution. V. E XPERMENTS We experiment the real-world financial analysis models and market tick datasets using this prototype. In this section, we first introduce the",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "tick datasets using this prototype. In this section, we first introduce the moving-window analysis and the autoregressive moving-average (ARMA) analysis of financial timeseries data. Then we describe the financial market tick datasets that we use to conduct the tests. Last, we describe the measurement in tests. A. Financial Analysis Models 1) Moving-Window Analysis A moving window method in financial timeseries is one of the most common approaches in many models [19]. We hereby simulate this approach by coding the user-specific data-process functions in Python. The input timeseries stream of tick data is managed by the big data infrastructure, including querying the timeseries, computing the logarithmic returns and formatting a fixed- length window. After these steps, the preprocessed data streams flow into downstream embedded-Python nodes where the Python-coded functions",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "preprocessed data streams flow into downstream embedded-Python nodes where the Python-coded functions are applied to the data. Final result is exported in CSV files. In this test, two data process functions: ‘abs’ and ‘ mean ’ are programmed in Python.The flowchart for these tests is shown in Figure 3. Among these, we test three scenarios: (a) “1 -node abs”: a single embedded-Python node is added to find absolute values of logarithmic returns; (b) “1-node abs + mean”: a single embedded-Python node is added to find the average of absolute logarithmic returns; (c) “2 -node abs + mean”: two embedded - Python nodes are added, where first node is added for finding absolute values of logarithmic returns and second node for finding the average of absolute values from first",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "and second node for finding the average of absolute values from first node. Actually, the result of case (c) is the same as that of case (b) and the difference is number of embedded-Python nodes. Figure 3. Flowchart for moving -window analysis: 1-node abs (left), 1-node abs+mean (middle) and 2-node abs+mean (right). 2) Autoregressive Moving Average (ARMA) Outlier detection and data cleaning is the first must-have step of most financial modeling pipelines [7, 20]. We hereby test the autoregressive moving average (ARMA) model on the financial timeseries data. The ARMA model is a tool to understand the values of timeseries. Same to previous study, the input dataset is the tick data for S&P500 trade. The output is the result of the ARMA model within certain fixed-length window of",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "is the result of the ARMA model within certain fixed-length window of financial timeseries. In this test, the ARMA model is coded in Python. In addition, the Hampel method [21, 22] is coded in Python and is used for truncating the outliers. Similarly, data flow is managed by this big data infrastructure. That is, the raw market tick data is preprocessed before entering the Hampel and AMRA nodes. The flowchart is in Figure 4. In this test, two Python nodes are inserted in this process pipeline. Figure 4:Flowchart for ARMA with Hampel method B. Performance Measurements Wallclock time is the amount of elapsed time from the start to the completion of a process pipeline, including the time that 391 [Página 5] queries data from database, prepares timeseries, computes",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "time that 391 [Página 5] queries data from database, prepares timeseries, computes logarithmic returns, executes Python scripts and writes results to disk. This timing result is called as overall wallclock time. Meanwhile, we measure the cumulative time that is spent in executing the codes in Python. In the practice, the time spent in executing a code in Python is the elapsed time from entering to leaving the Python code. This me asurement is done at the child coprocess side and includes the delay of C-Python API. This timing result is called as embedded Python wallclock time. The time measure is with a resolution of microseconds. Latency is reported in microseconds per tick and represents the amount of time elapsed for processing a single tick message on average. It",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "of time elapsed for processing a single tick message on average. It is computed as the overall wallclock time divided by the total number of tick messages. Speedup for the codes in Python is defined as a ratio of ܶ(1)over ܶ(݊) ,where ܶ(1)and ܶ(݊)are the elapsed times of 1 and ݊child coprocesses for the codes in Python: ܵ(݊)(ܶ= 1 ) / )݊(ܶ .Parallel efficiency is ܧ(݊)ܵ=(݊)݊/ .This speedup ratio ܵ(݊)and parallel efficiency ܧ(݊)is used to provide an estimate for how well this embedded- Python system speeds up. It is used to generate a plot of the elapsed time vs. the number of coprocesses and to understand the behavior of the parallelized Python scripts on multicore processor architectures. VI. R ESULTS AND DISCUSSION A. Performance on Intel Xeon E5-2603",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "architectures. VI. R ESULTS AND DISCUSSION A. Performance on Intel Xeon E5-2603 CPU 1) Moving-Window Analysis Figs. 5 and 6 present the overall wallclock time (in second) and the latency (in μs/tick), respectively, for processing S&P500 trade tick data for 7 days. These results show that: Our approach brings great parallel performance to the codes in Python. I n “1- node: abs” case, wallclock time is reduced from 177 to 56 sec and performance is improved by 68% (Fig. 5) and the latency is reduced to 0.82 μs/tick (Fig. 6). Calling more functions is more expensive. For example, the performance of the “1 -node: abs+m ean” case is ever worse than that of the “1 -node: abs” case (Fig. 5 ). The former calls two Python functions and",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "abs” case (Fig. 5 ). The former calls two Python functions and the latter calls only one once. Figure 5. Wallclock time in seconds for processing trade tick data of S&P500 stocks in 7 days (window period: 20)Figure 6. The latency in μs per tick for processing trade tick data of S&P500 stocks in 7 days (window period: 20) 2) ARMA model Figs. 7 and 8 present the overall and embedded -Python wallclock times (in seconds), respectively. In this test, we tested two cases: 16-stock and 8-stock. From results, this test affirmed the parallel performance our approach brings to the code in Python. As the number of coprocesses increases, the overall and the embedded-Python wallclock times are reduced in Figs. 7 and 8. The overall performance for 16-stock",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "are reduced in Figs. 7 and 8. The overall performance for 16-stock and 8-stock are improved by 51% and 43%, respectively (Fig. 7). The optimal number of coprocesses for the 16-stock case is 8 (Fig. 7). Figure 7. Overall wallclock time for the ARMA model. Figure 8. Embedded-Python wallclock time for ARMA.319 115288 88177 56 0100200300400 048 1 2 1 6Overall WallClock Time in Second Number of Coprocess20-point/S&P500/7-day Trade 2-node: abs + mean 1-node: abs + mean 1-node: abs1.69 1.29 0.82 012345 0 4 8 12 16Speed: microsecond per tick Number of Coprocess20-point/S&P500/7-day Trade 2-node: abs + mean 1-node: abs + mean 1-node: abs 2570 19131567 12501308 1282 1068 799732 05001,0001,5002,0002,5003,000 1248 1 6Overall WallClock Time in Second Number of CoprocessARMA/16 stocks/7-day trade ARMA/8 stocks/7-day trade 2552 1753",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "in Second Number of CoprocessARMA/16 stocks/7-day trade ARMA/8 stocks/7-day trade 2552 1753 1164 996 6841275 981 640 436 05001,0001,5002,0002,5003,000 1248 1 6Embedded-Python WallClock Time in Second Number of CoprocessARMA/16 stocks/7-day trade ARMA/8 stocks/7-day trade 392 [Página 6] B. Performance on Intel Xeon Phi 7230F 1) Moving-Window Analysis Fig. 9 presents the overall and embedded-Python wallclock times. This test reaffirmed the sc alability of our approach. With the increase of coprocesses, the performance is consistently improved and the speedup for Python codes is almost linear. 2) ARMA model We use 64 stocks and 7 -day trade in this test. Fig. 11 presents the overall wallclock time vs. the number of coprocesses. In this figure, both axes are in logarithmic scale to clarify the scalability. This test restated that our",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "in logarithmic scale to clarify the scalability. This test restated that our platform could bring significant performance improvement on this time-consuming ARMA model: the wallclock time is reduced from 6 days (21,008 seconds) to 20 minutes (1,183 seconds) in Fig. 11. The Python part is speeded up by a factor of 28.4. In most configurations, the parallel efficiency in the Python part is no less than 90%. Optimal number of coprocesses is 32 for this test. Figure 9. Overall and embedded-Python wallclock time for processing trade tick data of S&P500 stocks for 7 days. Figure 10. Overall wallclock time in second for the ARMA model on Intel Xeon Phi processor. C. Intel Xeon E5 vs. Intel Xeon Phi 2 We compare two Intel Xeon processor architectures at the",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "Xeon Phi 2 We compare two Intel Xeon processor architectures at the finance big data analytics level in this subsection. 1) Moving-Window Analysis In this test, we choose the “2 -node: abs+mean” case as it i s the most time-consuming case among three cases. Wallclock time is shown in Fig. 12. This comparison shows that Intel Xeon Phi processor consistently outperforms Intel Xeon E5 processor. The former is 2.67 times faster than the latter in this test. Our method improved the overall performance by 64% on Xeon E5 CPU and 84% on Xeon Phi CPU. The latency is reduced to 1.69 μs/tick on Xeon E5 and 0.63 μs/tick on Xeon Phi. The latter’s latency is only 1/3 of the former processor model. Figure 11. Overall wallclock time for",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "1/3 of the former processor model. Figure 11. Overall wallclock time for processing S&P500 trade data for 7 days, using 2-node: abs and mean. 2) ARMA model We compare the performance of ARMA model on Xeon E5 and Xeon Phi processors in two test sets as follows. Test 1: we test the same problem size: a total of 16 stocks and 7-day trade tick market data. The overall clock time vs. the number of coprocess is shown and compared in Fig. 12. This comparison shows when a small amount of coprocesses are used, Intel Xeon Phi processor actually performs worse than Intel Xeon E5. For example, when the Python code runs in a single process mode (i.e., number of coprocess is one), it takes Xeon Phi 5262 seconds,",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "(i.e., number of coprocess is one), it takes Xeon Phi 5262 seconds, while Xeon E5 needs only 2570 seconds that is ~2x faster than the Xeon Phi processor. However, as the number of coprocesses increases, Xeon Phi quickly outperforms Xeon E5 and shows its superiority with aid of massive parallel execution of Python codes. In this test, the Xeon Phi reduces the overall wallclock time to 418 seconds and is ~3x faster than the Xeon E5. Figure 12. Overall wallclock time of ARMA for 16 stocks and 7-day trade on Intel Xeon E5 and Xeon Phi CPUs.277 144 82 5043144173236 118 62 31168 4 0100200300 1 2 4 8 16 32 64WallClock Time in Second Number of Coprocess20-point/S&P500/7-day Trade Overall Embedded-Python 21,008 10,494 5,425 2,940 1,506 1,1831,384 1,0242,0484,0968,19216,38432,768",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "of Coprocess20-point/S&P500/7-day Trade Overall Embedded-Python 21,008 10,494 5,425 2,940 1,506 1,1831,384 1,0242,0484,0968,19216,38432,768 1 2 4 8 16 32 64Overall WallClock Time in Second Number of CoprocessARMA/64 stocks/7-day trade319 115277 43 0100200300400 1 2 4 8 16 32 64Overall Wallclock Time in Second Number of Coprocess20-point/S&P500/7-day Trade Intel Xeon E5-2603 processor Intel Xeon Phi 7230F processor 2,570 1,9131,567 1,250 1,3085,262 2,761 1,421 753 41801,0002,0003,0004,0005,0006,000 1248 16Overall WallClock Time in Second Number of CoprocessARMA/16 stocks/7-day trade Intel Xeon E5 Processor Intel Xeon Phi Processor 393 [Página 7] Thus, it is no coincidence that effective parallel execution of the Python code is essential at exploring the performance of the upcoming multicore platforms such as the Knights Landing and its processor family. In this work, we have already showed an effective coprocess-based",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "processor family. In this work, we have already showed an effective coprocess-based parallelism for the codes in Python and their seamless integration with a big data infrastructure. Test 2: we conduct a scalability test by using two problem sizes: 16 stocks for Xeon E5 an d 64 stocks for Xeon Phi. 7- day trade tick data is used for both. In results, the per-stock latency (i.e., second per stock) is presented in Fig. 13. These results show that: Xeon Phi processor consistently outperforms the Xeon E5, with the increase of coprocesses, in terms of the per- stock latency. The Xeon Phi processor excels at lowering the latency than the Xeon E5. The Xeon Phi is able to improve the latency by 95% but the Xeon E5 only by",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "to improve the latency by 95% but the Xeon E5 only by 52%. The Xeon Phi has a higher workload throughput than the Xeon E5. The Xeon Phi is able to process as many as 195 stocks in an hour while the Xeon E5 could just do a maximum of 46 stocks in an hour. In this measure, the Xeon Phi is 4.2x faster than the Xeon E5. Figure 13. Per-stock latency (second per stock) of ARMA for 16 stocks on Intel Xeon E5 and 64 stocks on Xeon Phi CPUs. Collectively, by observing these results in Tests 1 and 2, we can see that: (1) Intel Xeon E5 proce ssor is about 2x times faster than Intel Xeon Phi processor when the Python code is simply executed",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "than Intel Xeon Phi processor when the Python code is simply executed in a single process; (2) the power of a novel Intel Xeon Phi processor cannot be explored until the parallel execution of the Python code is employed. In other words, if the application cannot execute in parallel, it can hardly take advantage of novel modern processors’ capabilities a nd it may end up with an ev en worse performance. This appears true not only for these finance big data applications in this work but also for a wide range of modern applications [23-25]. D. Summary and Discussion In this section, we have tested the moving-window analysis and the ARMA model and compared the performance of Intel Xeon E5 and Xeon Phi processors. These results can demonstrate",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "of Intel Xeon E5 and Xeon Phi processors. These results can demonstrate that: our approach, a coprocess-based parallel execution of the code in Python could bring significant parallel performance to all cases. Specifically, the overall wallclock time and the time for the Python code is greatly reduced, as more coprocesses used. The speedup for the Python code is nearly linear and the parallel efficiency is around 90%. Our approach plays a key role at fully delivering the power of modern high density multicore processors. Clearly, the peak performance of Intel Xeon Phi pr ocessor is greatly larger than that of its previous Intel processors. However, fully exploring the power of such modern multico re processors requires highly parallel application programs. In the meanwhile, programming difficulty increases with increased",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "highly parallel application programs. In the meanwhile, programming difficulty increases with increased complexity of architectures. With the number of cores in mainstream processors predicted to scale to hundreds today, there is an urgent need to develop a highly- parallel application -specific platform for the real-world application users. To this aim, this work presents an approach that effectively brings coprocess-based parallel execution to the Python while integrates with a finance big data infrastructure. This yields an integrated big da ta and parallel python platform towards modern quantitative finance applications. This effort is of practical interest in the research and industry. VII. C ONCLUSION This work presents an integr ated big data and parallel Python platform for the quantitative fina nce applications. This platform incorporates a big data infrastructure system",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": "quantitative fina nce applications. This platform incorporates a big data infrastructure system to manage finance timeseries market data and a built-in embedded-Python API to execute the Python code on these managed timeseries streams. The code in Python is able to be executed in highly parallel. We prototype this proposed system and test our prototype with two popular applications and the NYSE tick data. Results show that our system could bring significant parallel performance to all of the cases; the execution time is greatly reduced as the number of processes increases; the spee dup for parallel Python is nearly linear and the parallel efficiency for most cases is as high as around 90%. These achievements demonstrated the efficacy of our proposed system model and the efficiency of our software",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "efficacy of our proposed system model and the efficiency of our software prototype at the real-world applications level. REFERENCES [1] J. J. Angel, L. E. Harris, and C. S. Spatt, \"Equity trading in the 21st century: An update,\" The Quarterly Journal of Finance, vol. 5, p. 1550002, 2015. [2] A. Carrion, \"Very fast money: High-frequency trading on the NASDAQ,\" Journal of Financial Markets, vol. 16, pp. 680-711, 2013. [3] I. Aldridge, High-frequency trading: a practical guide to algorithmic strategies and trading systems vol. 459: John Wiley and Sons, 2009. [4] M. A. Goldstein, P. Kumar, and F. C. Graves, \"Computerized and High ̺ Frequency Trading,\" Financial Review, vol. 49, pp. 177-202, 2014. [5] J.-P. Serbera and P. Paumard, \"The fall of high-frequency trading: A survey of competition and",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "P. Paumard, \"The fall of high-frequency trading: A survey of competition and profits,\" Research in International Busin ess and Finance, vol. 36, pp. 271-287, 2016. [6] G. Meyer and N. Bullock. (2017, March 30). Race for speed among algo traders hits peak. Available: https://www.ft.com/content/6961129e-14fa- 11e7-80f4-13e067d5072c [7] X. Shi, P. Zhang, and S. U. Khan, \"Quantitative Data Analysis in Finance,\" in Handbook of Big Data Technologies, A. Y. Zomaya and S. Sakr, Eds., ed Cham: Springer International Publishing, 2017, pp. 719 - 753.328 164 85 4624 18 22161 12098 78 82 050100150200250300350 1248 1 6 3 2 6 4Latency: second per stock Number of CoprocessARMA/7-day trade 64-stock on Xeon Phi 16-stock on Xeon E5 394 [Página 8] [8] B. Fang and P. Zhang, \"Big Data in Finance,\" in Big",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "[8] B. Fang and P. Zhang, \"Big Data in Finance,\" in Big Data Concepts, Theories, and Applications, S. Yu and S. Guo, Eds., ed Cham: Springer International Publishing, 2016, pp. 391-412. [9] M. Peat, \"Big data in finance,\" InFinance: The Magazine for Finsia Members, vol. 127, p. 34, 2013. [10] T. Seth and V. Chaudhary, \"Big Data in Finance,\" ed, 2015. [11] D. Beazley, \"Understanding the python gil,\" in PyCON Python Conference. Atlanta, Georgia, 2010. [12] K. Kinder, \"Event-driven programming with Twisted and Python,\" Linux journal, vol. 2005, p. 6, 2005. [13] (2015). SciPy Cookbook. url: http://scipy-cookbook.readthedocs.io/ [14] R. Odaira, J. G. Castanos, and H. Tomari, \"Eliminating global interpreter locks in ruby through hardware transactional memory,\" in ACM SIGPLAN Notices, 2014, pp. 131-142. [15] L. Dalcin, R. Paz,",
        "start_idx": 5684,
        "end_idx": 5812
      },
      {
        "text": "in ACM SIGPLAN Notices, 2014, pp. 131-142. [15] L. Dalcin, R. Paz, and M. A. Storti, \"MPI for Python,\" J. Parallel Distrib. Comput., vol. 65, pp. 1108-1115, 2005. [16] Y. Gao and W.-M. Chen, \"Family Relationship Inference Using Knights Landing Platform,\" in Cyber Security and Cloud Computing (CSCloud), 2017 IEEE 4th International Conference on, 2017, pp. 27-30. [17] C. Zhou, Y. Gao, and W. Howard, \"Evaluation of Combining Bootstrap with Multiple Imputation Using R on Knights Landing Platform,\" in Cyber Security and Cloud Computing (CSCloud), 2017 IEEE 4th International Conference on, 2017, pp. 14-17.[18] P. Zhang, K. Yu, J. Yu, and S. Khan, \"QuantCloud: Big Data Infrastructure for Quantitative Finance on the Cloud,\" IEEE Transactions on Big Data (in press) doi: 10.1109/TBDATA.2017.2649544, 2017. [19] N. R. Swanson, \"Money",
        "start_idx": 5800,
        "end_idx": 5928
      },
      {
        "text": "Big Data (in press) doi: 10.1109/TBDATA.2017.2649544, 2017. [19] N. R. Swanson, \"Money and output viewed through a rolling window,\" Journal of monetary Economics, vol. 41, pp. 455-474, 1998. [20] S. T. Rachev, S. V. Stoyanov, and F. J. Fabozzi, Risk and Uncertainty vol. 211: John Wiley & Sons, 2011. [21] R. K. Pearson, \"Outliers in process modeling and identification,\" IEEE Transactions on control systems technology, vol. 10, pp. 55-63, 2002. [22] H. Liu, S. Shah, and W. Jiang, \"On-line outlier detection and data cleaning,\" Computers & chemical engineering, vol. 28, pp. 1635-1647, 2004. [23] G. Lawson, M. Sosonkina, T. Ezer, and Y. Shen, \"Empirical Mode Decomposition for Modeling of Parallel Applications on Intel Xeon Phi Processors,\" in Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and",
        "start_idx": 5916,
        "end_idx": 6044
      },
      {
        "text": "in Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, 2017, pp. 1000-1008. [24] S. J. Pennycook, C. J. Hughes, M. Smelyanskiy, and S. A. Jarvis, \"Exploring simd for molecular dynamics, using intel® xeon® processors and intel® xeon phi coprocessors,\" in Parallel & Distributed Processing (IPDPS), 2013 IEEE 27th International Symposium on, 2013, pp. 1085- 1097. [25] J. Reinders, \"An overview of programming for Intel Xeon processors and Intel Xeon Phi coprocessors,\" Intel Corporation, Santa Clara, 2012. 395 [Página 9] [1] J. J. Angel, L. E. Harris, and C. S. Spatt, \"Equity trading in the 21st century: An update,\" The Quarterly Journal of Finance, vol. 5, p. 1550002, 2015. [2] A. Carrion, \"Very fast money: High-frequency trading on the NASDAQ,\" Journal of Financial Markets,",
        "start_idx": 6032,
        "end_idx": 6160
      },
      {
        "text": "\"Very fast money: High-frequency trading on the NASDAQ,\" Journal of Financial Markets, vol. 16, pp. 680-711, 2013. [3] I. Aldridge, High- frequency trading: a practical guide to algorithmic strategies and trading systems vol. 459: John Wiley and Sons, 2009. [4] M. A. Goldstein, P. Kumar, and F. C. Graves, \"Computerized and High‐Frequency Trading,\" Financial Review, vol. 49, pp. 177-202, 2014. [5] J.-P. Serbera and P. Paumard, \"The fall of high-frequency trading: A survey of competition and profits,\" Research in International Business and Finance, vol. 36, pp. 271-287, 2016. [6] G. Meyer and N. Bullock. (2017, March 30). Race for speed among algo traders hits peak . Available: https://www.ft.com/content/6961129e-14fa-11e7-80f4- 13e067d5072c [7] X. Shi, P. Zhang, and S. U. Khan, \"Quantitative Data Analysis in Finance,\" in Handbook of Big Data",
        "start_idx": 6148,
        "end_idx": 6276
      },
      {
        "text": "U. Khan, \"Quantitative Data Analysis in Finance,\" in Handbook of Big Data Technologies , A. Y. Zomaya and S. Sakr, Eds., ed Cham: Sp ringer International Publishing, 2017, pp. 719-753. [8] B. Fang and P. Zhang, \"Big Data in Finance,\" in Big Data Concepts, Theories, and Applications , S. Yu and S. Guo, Eds., ed Cham: Springer International Publishing, 2016, pp. 391-412. [9] M. Peat, \"Big data in finance,\" InFinance: The Magazine for Finsia Members, vol. 127, p. 34, 2013. [10] T. Seth and V. Chaudhary, \"Big Data in Finance,\" ed, 2015. [11] D. Beazley, \"Understanding the python gil,\" in PyCON Python Conference. Atlanta, Georgia , 2010. [12] K. Kinder, \"Event-driven programming with Twisted and Python,\" Linux journal, vol. 2005, p. 6, 2005. [13] (2015). SciPy Cookbook .",
        "start_idx": 6264,
        "end_idx": 6392
      },
      {
        "text": "Linux journal, vol. 2005, p. 6, 2005. [13] (2015). SciPy Cookbook . Available: http://scipy- cookbook.readthedocs.io/ [14] R. Odaira, J. G. Castanos, and H. Tomari, \"Eliminating global interpreter locks in ruby through hardware transactional memory,\" in ACM SIGPLAN Notices , 2014, pp. 131-142. [15] L. Dalcin, R. Paz, and M. A. Storti, \"MPI for Python,\" J. Parallel Distrib. Comput., vol. 65, pp. 1108-1115, 2005. [16] Y. Gao and W.- M. Chen, \"Family Relationship Inference Using Knights Landing Platform,\" in Cyber Security and Cloud Computing (CSCloud), 2017 IEEE 4th International Conference on, 2017, pp. 27-30. [17] C. Zhou, Y. Gao, and W. Howard, \"Evaluation of Combining Bootstrap with Multiple Imputation Using R on Knights Landing Platform,\" in Cyber Security and Cloud Computing (CSCloud), 2017 IEEE 4th International Conference on ,",
        "start_idx": 6380,
        "end_idx": 6508
      },
      {
        "text": "Security and Cloud Computing (CSCloud), 2017 IEEE 4th International Conference on , 2017, pp. 14-17. [18] P. Zhang, K. Yu, J. Yu, and S. Khan, \"QuantCloud: Big Data Infrastructure for Quantitative Finance on the Cloud,\" IEEE Transactions on Big Data (in press) doi: 10.1109/TBDATA.2017.2649544, 2017. [19] N. R. Swanson, \"Money and output viewed through a rolling window,\" Journal of monetary Economics, vol. 41, pp. 455-474, 1998. [20] S. T. Rachev, S. V. Stoyanov, and F. J. Fabozzi, Risk and Uncertainty vol. 211: John Wiley & Sons, 2011. [21] R. K. Pearson, \"Outliers in process modeling and identification,\" IEEE Transactions on control systems technology, vol. 10, pp. 55-63, 2002. [22] H. Liu, S. Shah, and W. Jiang, \"On-line outlier detection and data cleaning,\" Computers & chemical engineering, vol. 28,",
        "start_idx": 6496,
        "end_idx": 6624
      },
      {
        "text": "\"On-line outlier detection and data cleaning,\" Computers & chemical engineering, vol. 28, pp. 1635- 1647, 2004. [23] G. Lawson, M. Sosonkina, T. Ezer, and Y. Shen, \"Empirical Mode Decomposition for Modeling of Parallel Applications on Intel Xeon Phi Processors,\" in Proceedings of the 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing , 2017, pp. 1000-1008. [24] S. J. Pennycook, C. J. Hughes, M. Smelyanskiy, and S. A. Jarvis, \"Exploring simd for molecular dynamics, using intel® xeon® processors and intel® xeon phi coprocessors,\" in Parallel & Distributed Processing (IPDPS), 2013 IEEE 27th International Symposium on , 2013, pp. 1085-1097. [25] J. Reinders, \"An overview of programming for Intel Xeon processors and Intel Xeon Phi coprocessors,\" Intel Corporation, Santa Clara, 2012. 396",
        "start_idx": 6612,
        "end_idx": 6734
      },
      {
        "text": "Intel Corporation, Santa Clara, 2012. 396",
        "start_idx": 6728,
        "end_idx": 6734
      }
    ],
    "e0187208-dd3a-48a3-b50c-e5bb5e74fef4": [
      {
        "text": "[Página 1] SPECIAL SECTION ON ADVANCED OPTICAL IMAGING FOR EXTREME ENVIRONMENTS Received August 12, 2019, accepted August 20, 2019, date of publication September 4, 2019, date of current version October 1, 2019. Digital Object Identifier 10.1 109/ACCESS.2019.2939158 Optimizing the Electronic Health Records Through Big Data Analytics: A Knowledge-Based View CAIFENG ZHANG1, RUI MA2, SHIWEI SUN3,4, YUJIE LI 5, YICHUAN WANG 2, AND ZHIJUN YAN3,4 1Kaifeng Hospital of Traditional Chinese Medicine, Kaifeng 300193, China 2Shef eld University Management School, The University of Shef eld, Shef eld S10 2TN, U.K. 3Beijing Institute of Technology, Beijing 100811, China 4Sustainable Development Research Institute for Economy and Society, Beijing, China 5Yangzhou University, Yangzhou 225009, China Corresponding author: Shiwei Sun (shiweisun@bit.edu.cn) This work was supported in part by the Beijing Institute of Technology Research",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "work was supported in part by the Beijing Institute of Technology Research Fund Program for Young Scholars under Grant 3210012221902, and in part by the Taiyuan National Sustainable Development Agenda Major Project under Grant 3210041910009. ABSTRACT Many hospitals are suffering from ineffective use of big data analytics with electronic health records (EHRs) to generate high quality insights for their clinical practices. Organizational learning has been a key role in improving the use of big data analytics with EHRs. Drawing on the knowledge-based view and big data lifecycle, we investigate how the three modes of knowledge can achieve meaningful use of big data analytics with EHRs. To test the associations in the proposed research model, we surveyed 580 nurses of a large hospital in China in 2019. Structural equation",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "580 nurses of a large hospital in China in 2019. Structural equation modelling was used to examine relationships between knowledge mode of EHRs and meaningful use of EHRs. The results reveal that know-what about EHRs utilization, know-how EHRs storage and utilization, and know-why storage and utilization can improve nurses' meaningful use of big data analytics with EHRs. This study contributes to the existing digital health and big data literature by exploring the proper adaptation of analytical tools to EHRs from the different knowledge mode in order to shape meaningful use of big data analytics with EHRs. INDEX TERMS Big data analytics, electronic health records and impacts, knowledge-based view. I. INTRODUCTION With the aim of improving quality of care through the mean- ingful use of electronic health records (EHRs),",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "of care through the mean- ingful use of electronic health records (EHRs), the China government has promulgated the Electronic Health Record Architecture and Data Standard in 2009 as a guide for the hospitals. In this guide, EHRs are de ned as ``a complete collection of digital clinical information documenting the clinical care rendered to an individual in the Chinese EHR Standard'' [1]. Over two decades, EHRs has been suggested to enhance the healthcare service ef ciency and effectiveness, but it does not mean that simply adopting the EHRs system could lead to those bene ts. Healthcare providers need to make the EHR a routine in the daily work system in order to realize the payback. Thus, Health Information Technology for Economic and Clinical Health (HITECH) Act introduces the",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "Health Information Technology for Economic and Clinical Health (HITECH) Act introduces the ``meaningful use'' of EHR as the goal of adoption. The main objective of Act is to create meaningful and useful digital The associate editor coordinating the review of this manuscript and approving it for publication was Huimin Lu.medical records, including the entry and storage of EHRs, and optimize the utilization of EHRs. As of 2011, clinical data had reached 150 exabytes (1 EB D1018 bytes) worldwide, mainly in the form of EHRs [2]. Yet, considerable uncertainty still remains about the use of big data analytics within EHRs and its impact on clinical performance [3]. Such struggles are due to not only insuf cient fund and biased resource allocation at the national level but also lack of",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "and biased resource allocation at the national level but also lack of planning and governance for the use of big data analytics within EHRs at the hospital level [3], [4]. To address this challenge, although many hospitals in China have invested a great deal of cost, time and resources in learning the implementation and utilization of EHRs, they are still suffering from ineffective use of big data analytics within EHRs to generate high quality information for deci- sion making and reduce health disparities [3], [9]. One of the key reasons for this dif culty is the lack of full consideration of EHRs tness to the speci c situations of the particular organization [9]. It is important for healthcare practitioners to pay greater attention to understand how to absorb",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "for healthcare practitioners to pay greater attention to understand how to absorb the diverse VOLUME 7, 2019 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see http://creativecommons.org/licenses/by/4.0/ 136223 [Página 2] C. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics FIGURE 1. Optimizing the electronic health records through big data analytics. knowledge of EHRs. As such, little attention has been paid to understanding the role of knowledge mode in improving the use of big data analytics within EHRs. In this study, thus, we examine the relationship between the knowledge about big data analytics within EHRs and the outcome of EHRs adoption (i.e., meaningful use of EHRs). The remainder of this paper is structured as follows: the next section",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "The remainder of this paper is structured as follows: the next section serves as our theoretical background, which leads to the development of the research model and associ- ated hypothesis; followed by our research method, ndings and discussions, contributions to research, implications for practice and recommendations, then limitations and future research directions are discussed as our conclusion. II. OPTIMIZING THE ELECTRONIC HEALTH RECORDS THROUGH BIG DATA ANALYTICS The meaningful use of EHRs is crucial for improving clinical operations and healthcare service [5]. Big data analytics is a tool that enables healthcare organizations to reach this goal by optimizing EHRs through analytical algorithms. For exam- ple, Texas Health Harris Methodist Hospital Alliance uti- lizes medical sensor data to analyze patients' movements and monitor their actions throughout their hospital stay.",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "to analyze patients' movements and monitor their actions throughout their hospital stay. In this way they can provide healthcare services more ef ciently and accurately, optimize existing operations, and prevent some medical risks [6]. Indeed, the use of big data analtyics within EHRs is rooted in the concept of data life cycle framework that consists of three components: data collection, data stor- age, and data utilization, as shown in Figure 1. These logical components that perform speci c functions enable healthcare practitioner to understand how to transform the EHRs into meaningful clinical insights through big data analtyics. Data collection. This component contains all the data sources and content type of EHRs. In general, The EHRs aredivided into structured data (e.g., patient demographics, med- ication history, health status and",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "into structured data (e.g., patient demographics, med- ication history, health status and lab results) and unstructured data (e.g., diagnosis notes, clinical graphics, and medical images). These data are collected from various clinical units inside the hospital or from external units. Data storage. The EHRs are stored into appropriate databases depending on the source of data and content for- mat. This component aims to handle data from the vari- ous data sources by two steps: transformation and storage. The transformation engine is capable of moving, cleaning, splitting, translating, merging, sorting, and validating EHRs. For instance, structured EHRs data will be extracted from healthcare information systems and converted into a speci c standard data format, sorted by the criterion (e.g., patient identity, health status medication history), and then the record",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "criterion (e.g., patient identity, health status medication history), and then the record in the right place. In the next step, the EHRs are loaded into the target databases (e.g., Database Management System; DBMS, Hadoop distributed le systems; HDFS, or in a cloud) for further analysis. Data Utilization. This component is used to process all kinds of EHRs and report the summarized results for clinical decision making. The analysis of EHRs includes Map/Reduce, stream computing, and in-database analytics, depending on the type of data and the purpose of the analy- sis. Map/Reduce can provide the ability to process massive unstructured and structured EHRs in batch form in a mas- sively parallel processing environment. Stream computing can support near real time or real time analysis for EHRs. Though stream computing,",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "near real time or real time analysis for EHRs. Though stream computing, medical staffs can track EHRs in motion in order to respond to unexpected events and deter- mine next-best actions. In-database analytics is commonly used data mining approach that allows EHRs to be analyzed within database. It can provide high-speed parallel processing and offer a safe environment to process con dential patient information. This component also generates various visu- alization reporting and real-time and meaningful business insights derived from the analysis. The reporting system is a critical big data analtyics feature that allows EHRs to be visualized in a meaningful way to support medical staff day- to-day operations and clinical decisions. III. RESEARCH MODEL AND HYPOTHESIS DEVELOPMENT Prior research has acknowledged that organizational learn- ing has been",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "HYPOTHESIS DEVELOPMENT Prior research has acknowledged that organizational learn- ing has been an important enabler for improving the use of big data analytics within EHRs [7]\u0015[9]. From the aspect of information technology (IT) adoption, learning process plays a key role in the outcomes of the IT adoption. When the new IT is introduced to the organization, it implies that a large amount of knowledge is brought in [10], [11]. Organizations need to adopt a series of learning processes to merge the gap between what needs to be known and what is already known in order to understand how to use this knowledge effectively and ef ciently [10]. From the knowledge-based view (KBV), knowledge plays a pivotal role in increasing the organizations' competitive advantage and nancial perfor- mance [12],",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "role in increasing the organizations' competitive advantage and nancial perfor- mance [12], [13]. Effective knowledge activities in healthcare 136224 VOLUME 7, 2019 [Página 3] C. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics FIGURE 2. Proposed model of how three mode of knowledge about the use of big data analytics within EHRs for achieving meaningful use of EHRs. not only improve the existing operational capabilities of healthcare service but also reduce the care delivery costs and prevent potential medical errors [14], [15]. Drawing on the knowledge-based view (KBV), we develop our research model and associated hypotheses, as shown in Figure 2. KBV posits that organizational knowledge is viewed as a strategic resource of an organization. It also emphasizes that creating knowledge for the",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "resource of an organization. It also emphasizes that creating knowledge for the production of goods and services can acquire competitive advantage and organizational performance [12], [15]. In the context of EHRs implementation, an effective knowledge creation from EHRs is likely to be achieved by all medical staffs knowing how, why, what EHRs can be used properly. To understand the creation of knowledge, it is essential to explore the mode of knowledge. In general, the mode of knowledge activities can be classi ed into three categories according to the level of material involvement with the knowl- edge: knowing-what, knowing-how, and knowing-why [18]. Knowing-what refers to a declarative knowledge that con- tains information about activities and relationships [18]. This knowledge allows organizations to understand the digital health technologies in certain",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "This knowledge allows organizations to understand the digital health technologies in certain detail, such as the principle and characteristics of the technology, and to generate to a certain tangible products or outcomes. In the context of EHRs, hos- pitals need to understand what EHRs are, its features, and problems when it applies in practice. When they learn about EHRs, hospitals would perceive an attitude towards it and form the basic idea of how to adopt it effectively. Thus, we propose the following hypotheses. Hypothesis 1a (H1a): Knowing-what about the data collec- tion of EHRs will facilitate meaningful use of EHRs. Hypothesis 1b (H1b): Knowing-what about the data stor- age of EHRs will facilitate meaningful use of EHRs. Hypothesis 1c (H1c): Knowing-what about the data utiliza- tion of EHRs",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "EHRs. Hypothesis 1c (H1c): Knowing-what about the data utiliza- tion of EHRs will facilitate meaningful use of EHRs. Knowing-how is a procedural knowledge that includes the step-by-step procedures executable in a speci c system [16].Data analysts within healthcare organizations need to gain this type of knowledge in order to process EHRs effectively and meaningfully. For example, Tracking EHRs can generate real-time monitoring patient information such as alerts and proactive noti cations. Data analysts need to know what the most important outputs are and how to display them and send to interested users or made available in the form of dash- boards in real time. Knowing-how about processing EHRs can explore patterns of care and provide exceptional support for evidence based medical practices. Using knowing-how, healthcare organizations can also",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "support for evidence based medical practices. Using knowing-how, healthcare organizations can also address data quality issue through knowing well-de ned procedures and rules in an EHRs system. Thus, we propose the following hypotheses. Hypothesis 2a (H2a): Knowing-how about the data collec- tion of EHRs will facilitate meaningful use of EHRs. Hypothesis 2b (H2b): Knowing-how about the data storage of EHRs will facilitate meaningful use of EHRs. Hypothesis 2c (H2c): Knowing-how about the data utiliza- tion of EHRs will facilitate meaningful use of EHRs. Knowing-why is a contextual knowledge that enables users to solve the problems based on understanding contextual reasons and axiomatic principles [16], [17]. This knowledge provides explanations for rationalization about technology. In the context of EHRs, hospitals realize why EHRs should be used to generate better",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "of EHRs, hospitals realize why EHRs should be used to generate better clinical performance. This includes the examination of the speci c situation of their organizations and comparison of other alternative solutions. Also, organiza- tions should be aware of the impacts and consequences of uti- lizing EHRs. Besides the nancial and organizational impact of EHRs, hospitals also have to harness the possible chal- lenges when they use the EHRs system. In hospitals, a high level of knowing-why about EHRs can be accumulated by understanding of knowing-what and knowing-how involved in data collection, storage, and utilization of EHRs in the clinical system. Thus, we propose the following hypotheses. Hypothesis 3a (H3a): Knowing-why about the data collec- tion of EHRs will facilitate meaningful use of EHRs. Hypothesis 3b (H3b): Knowing-why",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "of EHRs will facilitate meaningful use of EHRs. Hypothesis 3b (H3b): Knowing-why about the data storage of EHRs will facilitate meaningful use of EHRs. Hypothesis 3c (H3c): Knowing-why about the data utiliza- tion of EHRs will facilitate meaningful use of EHRs. IV. METHODS A. SAMPLE AND DATA COLLECTION We investigate the relationship between knowledge mode of EHRs and meaningful use of EHRs among healthcare workers in China, primarily surveyed nurses after receiving ethics approval. An initial population set of 1,000 nurses was obtained from a large hospital in Henan province, China. The rst round of 1,000 questionnaires resulted in 351 invitations being rejected due to the availability. Of the 649 invitations that were seen by potential respondents, 580 responses were returned, completed and usable for the data analysis,",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "respondents, 580 responses were returned, completed and usable for the data analysis, showing a response rate of 89.37%. VOLUME 7, 2019 136225 [Página 4] C. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics TABLE 1. Demographic characteristics of the final sample with information of the participants ( nD580). Non-response bias was assessed by comparing the rst 25 percent with the last 25 percent of the responses for each variable using paired sample t-tests [18]. The results showed no statistically signi cant difference (p >0.05) between these two groups, indicating that non-response bias did not present a problem for this study. The demographic characteristics of the respondents are shown in Table 1. Among the 580 respondents, 86.20% were female. Most nurses (92.20%) were younger",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "Among the 580 respondents, 86.20% were female. Most nurses (92.20%) were younger than 40 years:23.30% were younger than 25 years, 40.30%were 25\u001530 years of age, 21.70% were 31\u001535 years of age, and 6.90% were 36\u001540 years of age. Most respondents had a bachelor's degree (91.40%). The respondent seniority (years of employ- ment) was evenly distributed, and the largest group had a seniority of 6\u001510 years (31.60%). A plurality of respondents (33.28%) worked in the internal medicine department. B. VARIABLES AND INSTRUMENTS The instrument used in this study was adapted from previ- ously validated instruments (presented in Appendix 5). All independent and dependent variables were collected using an online survey completed by each participant. The scale of knowing-what, knowing-how, and knowing-why about EHRs was adapted from Lee and Strong's",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "knowing-what, knowing-how, and knowing-why about EHRs was adapted from Lee and Strong's study [16] who proposed the three mode of knowledge underlying data collection, storage, and utilization and examined how knowledge held by different work roles affects data quality. This scale was used to rate the knowledge level of EHRs by which each participant acquires. A seven-point Likert-type scale was used to capture the responses, ranging from 1 Dvery small extent, through 4Daverage, to 7 Dvery large extent. The measurement of meaningful use of the EHR was developed from the regulation published by Department of Health and Human Services (DHHS) for the year 2011- 2012 [19]. Leading by Centers for Medicare and Medicaid Services, DHHS developed a list of criteria for meaningful use requirements on January 16, 2010",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "a list of criteria for meaningful use requirements on January 16, 2010 based on the call from Health Information Technology for Economic and Clinical Health (HITECH). Five items were developed according to those regulations to measure the performance of the adopted EHRs in hospital. A seven-point Likert-type scale was used to capture the responses, ranging from 1 Dstrongly disagree to 7Dstrongly agree. C. MEASUREMENT VALIDITY AND RELIABILITY The validity and reliability of measurements were assessed from the sample data set (n D580) collected for this study. As shown in Table 2, the loadings are all within accept- able ranges, and all but three items for knowing-what about EHRs storage, knowing-what about EHRs utilization, and knowing-how about EHRs utilization have loadings above the threshold of 0.5. All of the",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "EHRs utilization have loadings above the threshold of 0.5. All of the reliability coef cients (Cronbach's alphas) are above 0.80 (Table 2), con rming that the mea- surements are reliable. The correlations for each construct are presented in Table 3. Convergent validity was assessed by three criteria: (1) item loading; (2) composite reliability; and (3) average vari- ance extracted (A VE) [20]. The composite reliability scores range from 0.579 to 0.881. Each A VE is above 0.4, but KHEU (Table 2), which is acceptable. We assessed discrim- inant validity by checking whether each item loads more highly on its assigned construct than on other constructs, as suggested by Gefen, Straub and Boudreau [21]. Each item loading in the cross-loading table is markedly higher on its assigned construct than",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "in the cross-loading table is markedly higher on its assigned construct than on the other variables. Thus, 136226 VOLUME 7, 2019 [Página 5] C. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics TABLE 2. Reliability and validity measures of the research model. TABLE 3. Inter-construct correlations. our measurements demonstrate acceptable discriminant and convergent validities. In addition, we assessed the potential effect of common method bias statistically by conducting Harman's one-factor test [22] generated ten principal constructs; the unrotated factor solution shows that the rst construct explains only 11.11% of the variance, indicating that our data do not suf- fer from high common method bias. Consequently, this test suggest that common method bias is not a major concern for this study. V. RESULTS The",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "bias is not a major concern for this study. V. RESULTS The results from the regression analysis are shown in Table 4. The hypotheses were assessed by checking the direction and signi cance of path coef cients ( ) between dependent and independent variables. Our proposed research model is a good predictor of meaningful use of EHRs in the context of nursing department as the R2 accounts for 60.70% of the variance. According to the results, we found that different modes of knowledge can be used to improve nurses' effective use of EHRs. For example, our nding reveals that know- what, know-how and know-why about EHRs utilization can lead improved meaningful use of EHRs, thus H1c, H2c, and H3c are supported. This implies that EHRs utilization playsTABLE 4.",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "H2c, and H3c are supported. This implies that EHRs utilization playsTABLE 4. Standardized regression coefficients ) with p value ( 0.05). an important role in developing meaningful use of EHRs practice. In addition to the EHRs utilization, we also found that if nurses know how and why EHRs are stored, they are most likely to use EHRs effectively. Thus, H2b and H3b are supported. Surprisingly, knowing what, how, and why about how EHRs are collected does not improve meaningful use of EHRs, which H1a, H2a, and H3a are not supported. VI. THEORETICAL AND PRACTICAL CONTRIBUTIONS To strategically meaningful use of EHRs, prior work has developed many analytical approaches to effectively process EHRs. However, what kind of knowledge about the use of big data analtyics within EHRs should be",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "knowledge about the use of big data analtyics within EHRs should be created remains unknown. By addressing this research gap, the theoretical and practical contributions of this study are three-fold. Firstly, our ndings have partially con rmed knowledge about the use of big data analytics within EHRs matters for meaningful use of EHRs. This is among the rst study to investigate the use of big data analytics within EHRs from a knowledge- based view. Three mode of knowledge about the use of big data analytics for EHRs are identi ed and tested their impact on improving meaningful use of EHRs practices. Based on our ndings, healthcare organizations can make a strategic decision as to which type of knowledge and big data analytics components need to be enhanced to",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "of knowledge and big data analytics components need to be enhanced to improve meaningful use of EHRs. For example, improving meaningful use of EHRs does not require nurses to understand how, why, and what EHRs are collected within a hospital. Secondly, we found meaningful use of EHRs is highly in uenced by knowing-what, knowing-how and knowing- why about data utilization of EHRs as generally re ected in common sense. It is particularly important to gain knowl- edge regarding why various analtyics such as descriptive analtyics and predictive analytics can be used for EHRs. This result is consistent with Lee and Strong's [16] nding who recognizes the critical role that knowing-why plays in producing high data quality. Indeed, constant increasing large volume of EHRs is challenging healthcare organization's data",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "Indeed, constant increasing large volume of EHRs is challenging healthcare organization's data management capabilities [23]\u0015[26]. Needs for knowing-why about data utilization of EHRs is not unique for healthcare VOLUME 7, 2019 136227 [Página 6] C. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics TABLE 5. The items in the questionnaire and the results of EFA. 136228 VOLUME 7, 2019 [Página 7] C. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics TABLE 5. (Continued.) The items in the questionnaire and the results of EFA. but more important because the results extracted from the analysis of EHRs concerns patients' quality of care and well- being. A poor data utilization of EHRs may lead to issues such as billing errors, intentional",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "utilization of EHRs may lead to issues such as billing errors, intentional frauds, or medical mistakes. Thirdly, our ndings show that knowledge about data col- lection of EHRs does not matter for improving meaningful use of EHRs. A potential explanation is that in practice nurses are data collectors that know more about collecting accurate and complete healthcare records. Thus, knowledge about how to collect EHRs would not play an important role in improv- ing meaningful use of EHRs. Instead, they are interested in knowing more about making data relevant to their daily clinical tasks. VII. CONCLUSION This study has some limitations that may create interest- ing opportunities for future research. First, this study only collects data from a large hospital as the research sample. Although suf cient number",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "from a large hospital as the research sample. Although suf cient number of data points and high response rate may represent a large portion of population in a region of China, there is still a need to collect the data from the different hospitals to better generalize our research ndings. Future research may assess potential difference among age groups, among working experience groups, and among different clin- ical department groups, with a more representative sample. Second, future research could consider applying qualitative methods to complement the general lack of adequate survey methods. Third, examining the knowledge mode of EHRs with linear methods does not support the comprehensive view required to capture the non-linear interaction among these knowledge modes [6]. Future research could consider using fuzzy-set Qualitative Comparative Analysis",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "knowledge modes [6]. Future research could consider using fuzzy-set Qualitative Comparative Analysis as a data analysis approach to better explain how different knowledge mode ofEHRs simultaneously combine to achieve meaningful use of EHRs. Our study contributes to the existing digital health, big date literature and nursing literature in three ways. First, this research explores the proper adaptation of analytical tools to EHRs from the different knowledge mode in order to improve meaningful use of big data analytics within EHRs [29], [30]. Second, we identi ed the important the knowledge modes of EHRs (e.g., know-how, know-what, and know-why about EHRs utilization) that provides evidence regarding the ways in which how training programs/course of EHRs can be designed [29]. This also extends and deepens understanding of how meaningful use of",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "[29]. This also extends and deepens understanding of how meaningful use of EHRs practices can be improved [30]. It could be a useful guidance for hospital practitioners, outlining a variety of knowledge mode of EHRs that they can focus [23], [31], [32]. Third, this research proposes a conceptual model with a knowledge-based view to explicate the different knowledge mode of EHRs in the meaningful use of EHRs practice for nursing professionals. To the best of our knowledge, as yet, no previous studies have considered the knowledge mode of EHRs driving meaning use of EHRs in the nursing context. APPENDIX See Table 5. REFERENCES [1] W. Xu, Z. Guan, H. Cao, M. Lu, T. Li, and H. Zhang, ``Analysis and evaluation of the electronic health record standard in China:",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "Zhang, ``Analysis and evaluation of the electronic health record standard in China: A comparison with the American national standard ASTM E 1384,'' Int. J. Med. Inform. , vol. 80, no. 8, pp. 555\u0015561, Aug. 2011. [2] A. L. Kellermann and S. S. Jones, ``What it will take to achieve the as- yet-unful lled promises of health information technology,'' Health Affairs , vol. 32, no. 1, pp. 63\u001568, Jan. 2013. VOLUME 7, 2019 136229 [Página 8] C. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics [3] H. Lu, M. Kondo, Y. Li, J. Tan, H. Kim, S. Murakami, T. Aoki, and S. Kido, ``Extraction of GGO candidate regions on thoracic CT images using SuperVoxel-based graph cuts for healthcare systems,'' Mobile Netw. Appl. , vol.",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "using SuperVoxel-based graph cuts for healthcare systems,'' Mobile Netw. Appl. , vol. 23, no. 6, pp. 1669\u00151679, Dec. 2018. [4] Y. He and C. Johnson, ``Challenges of information security incident learn- ing: An industrial case study in a Chinese healthcare organization,'' Inform. Health Soc. Care , vol. 42, no. 4, pp. 393\u0015408, Oct. 2017. [5] C. Vuppalapati, A. Ilapakurti, and S. Kedari, ``The role of big data in cre- ating sense EHR, an integrated approach to create next generation mobile sensor and wearable data driven electronic health record (EHR),'' in Proc. IEEE 2nd Int. Conf. Big Data Comput. Service Appl. (BigDataService) , Oxford, U.K., Mar./Apr. 2016, pp. 293\u0015296. [6] Y. Wang, L. Kung, and T. A. Byrd, ``Big data analytics: Understanding its capabilities and potential bene ts for",
        "start_idx": 4176,
        "end_idx": 4304
      },
      {
        "text": "Byrd, ``Big data analytics: Understanding its capabilities and potential bene ts for healthcare organizations,'' Technol. Forecasting Social Change , vol. 126, no. 1, pp. 3\u001513, Jan. 2018. [7] J. L. Reardon and E. Davidson, ``An organizational learning perspective on the assimilation of electronic medical records among small physician practices,'' Eur. J. Inf. Syst. , vol. 16, no. 6, pp. 681\u0015694, Dec. 2007. [8] V. Venkatesh, X. Zhang, and T. A. Sykes, ```Doctors do too little tech- nology': A longitudinal eld study of an electronic healthcare system implementation,'' Inf. Syst. Res. , vol. 22, no. 3, pp. 419\u0015684, Sep. 2011. [9] Y. Wang and T. A. Byrd, ``Business analytics-enabled decision-making effectiveness through knowledge absorptive capacity in health care,'' J. Knowl. Manage. , vol. 21, no. 3, pp. 517\u0015539, May",
        "start_idx": 4292,
        "end_idx": 4420
      },
      {
        "text": "care,'' J. Knowl. Manage. , vol. 21, no. 3, pp. 517\u0015539, May 2017. [10] W. Ke and K. K. Wei, ``Organizational learning process: Its antecedents and consequences in enterprise system implementation,'' JGIM , vol. 14, no. 1, pp. 1\u001522, Jan. 2006. [11] R. L. Purvis, V. Sambamurthy, and R. W. Zmud, ``The assimilation of knowledge platforms in organizations: An empirical investigation,'' Org. Sci., vol. 12, no. 2, pp. 117\u0015135, Apr. 2001. [12] R. M. Grant, ``Toward a knowledge-based theory of the rm,'' Strategic Manage. J. , vol. 17, no. 2, pp. 109\u0015122, Dec. 1996. [13] M. Zack, J. McKeen, and S. Singh, ``Knowledge management and orga- nizational performance: An exploratory analysis,'' J. Knowl. Manage. , vol. 13, no. 6, pp. 392\u0015409, Oct. 2009. [14] R. Agarwal, G. G.",
        "start_idx": 4408,
        "end_idx": 4536
      },
      {
        "text": "13, no. 6, pp. 392\u0015409, Oct. 2009. [14] R. Agarwal, G. G. Gao, C. DesRoches, and A. K. Jha, ``Research commentary\u0016The digital transformation of healthcare: Current status and the road ahead,'' Inf. Syst. Res. , vol. 21, no. 4, pp. 796\u0015809, Dec. 2010. [15] B. Kogut and U. Zander, ``Knowledge of the rm, combinative capabilities, and the replication of technology,'' Org. Sci. , vol. 3, no. 3, pp. 383\u0015397, Aug. 1992. [16] Y. W. Lee and D. M. Strong, ``Knowing-why about data processes and data quality,'' J. Manage. Inf. Syst. , vol. 20, no. 3, pp. 13\u001539, Dec. 2003. [17] E. B. Swanson and N. C. Ramiller, ``Innovating mindfully with information technology,'' MIS Quart. , vol. 28, no. 4, pp. 553\u0015583, Dec. 2004. [18] J. S. Armstrong and",
        "start_idx": 4524,
        "end_idx": 4652
      },
      {
        "text": "28, no. 4, pp. 553\u0015583, Dec. 2004. [18] J. S. Armstrong and T. S. Overton, ``Estimating nonresponse bias in mail surveys,'' J. Marking Res. , vol. 14, no. 3, pp. 396\u0015402, Aug. 1977. [19] D. Blumenthal and M. Tavenner, ``The `Meaningful Use' regulation for electronic health records,'' New England J. Med. , vol. 363, no. 6, pp. 501\u0015504, Aug. 2010. [20] C. Fornell and D. F. Larcker, ``Evaluating structural equation models with unobservable variables and measurement error,'' J. Marketing Res. , vol. 18, no. 1, pp. 39\u001550, 1981. [21] D. Gefen, D. Straub, and M. C. Boudreau, ``Structural equation modeling and regression: Guidelines for research practice,'' Commun. Assoc. Inf. Syst., vol. 4, no. 1, p. 7, 2000. [22] P. M. Podsakoff, S. B. MacKenzie, J. Y. Lee, and",
        "start_idx": 4640,
        "end_idx": 4768
      },
      {
        "text": "2000. [22] P. M. Podsakoff, S. B. MacKenzie, J. Y. Lee, and N. P. Podsakoff, ``Common method biases in behavioral research: A critical review of the literature and recommended remedies,'' J. Appl. Psychol. , vol. 88, no. 5, pp. 879\u0015903, 2003. [23] J. M. Ferranti, M. K. Langman, D. Tanaka, J. McCall, and A. Ahmad, ``Bridging the gap: Leveraging business intelligence tools in support of patient safety and nancial effectiveness,'' J. Amer. Med. Inform. Assoc. , vol. 17, no. 2, pp. 136\u0015143, Mar. 2010. [24] W. Raghupathi and V. Raghupathi, ``Big data analytics in healthcare: Promise and potential,'' Health Inf. Sci. Syst. , vol. 2, no. 1, p. 3, 2014. [25] H. Monem, M. Afrasiabi, P. Rezvan, and S. AbediDehkordi, ``The impact of user quality and information quality",
        "start_idx": 4756,
        "end_idx": 4884
      },
      {
        "text": "Rezvan, and S. AbediDehkordi, ``The impact of user quality and information quality on the IS success in health- care context,'' J. Basic Appl. Sci. Res. , vol. 3, no. 10, pp. 40\u001551, Aug. 2013.[26] M. J. Ward, K. A. Marsolo, and C. M. Froehle, ``Applications of business analytics in healthcare,'' Bus. Horizons , vol. 57, no. 5, pp. 571\u0015582, Sep./Oct. 2014. [27] Y. Wang, L. Kung, S. Gupta, and S. Ozdemir, ``Leveraging big data analytics to improve quality of care in healthcare organizations: A con- gurational perspective,'' Brit. J. Manage. , vol. 30, no. 2, pp. 362\u0015388, Apr. 2019. [28] X. Li, R. S. Sedeh, L. Wang, and Y. Yang, ``Patient-record level integration of de-identi ed healthcare big databases,'' in Proc. IEEE Int. Conf. Big Data (Big Data)",
        "start_idx": 4872,
        "end_idx": 5000
      },
      {
        "text": "healthcare big databases,'' in Proc. IEEE Int. Conf. Big Data (Big Data) , Washington, DC, USA, Dec. 2016, pp. 1784\u00151786. [29] L. Baillie, S. Chadwick, R. Mann, and M. Brooke-Read, ``A survey of student nurses' and midwives' experiences of learning to use electronic health record systems in practice,'' Nurse Edu. Pract. , vol. 13, no. 5, pp. 437\u0015441, Sep. 2013. [30] Y. Ming and T. Zhang, ``Ef cient privacy-preserving access control scheme in electronic health records system,'' Sensors , vol. 18, no. 10, p. 3520, Oct. 2018. [31] S. M. R. Islam, D. Kwak, M. H. Kabir, M. Hossain, and K.-S. Kwak, ``The Internet of Things for health care: A comprehensive survey,'' IEEE Access , vol. 3, pp. 678\u0015708, 2015. [32] Y. Hao, L. Peng, H. Lu, M.",
        "start_idx": 4988,
        "end_idx": 5116
      },
      {
        "text": "3, pp. 678\u0015708, 2015. [32] Y. Hao, L. Peng, H. Lu, M. M. Hassan, and A. Alamri, ``Energy harvesting based body area networks for smart health,'' Sensors , vol. 17, no. 7, p. 1602, 2017. CAIFENG ZHANG was born in Henan, China, in 1962. She received the master's degree in regional economics from Henan University, China. She serves as the Associate Dean of the Kaifeng Hospi- tal of Traditional Chinese Medicine and also with the Hospital of Henan University of Traditional Chinese Medicine. Her research interests include big data analytics in healthcare, health communi- cation, medical tourism, and nursing management. She has published 16 articles in her research eld. RUI MA is currently a Doctoral Researcher with the Shef eld University Management School, The University of Shef eld,",
        "start_idx": 5104,
        "end_idx": 5232
      },
      {
        "text": "with the Shef eld University Management School, The University of Shef eld, U.K. Her current research interests include big data analytics in healthcare, medical tourism, sustainable development. She received the Excellent Presentation Award at the 10th International Conference on Systematic Inno- vation (ICSI). SHIWEI SUN received the Ph.D. degree in MIS from the Harbert College of Business, Auburn University, USA, in 2017. He is currently an Assistant Professor and an Associate Research Fellow with the School of Economics and Man- agement, Beijing Institute of Technology, China. His research interests include information tech- nology diffusion, innovation management, social media, and social networks. His research has appeared in several journals such as the Journal of Computer Information Systems ,Expert Systems with Applications , the International Journal of Information Management ,",
        "start_idx": 5220,
        "end_idx": 5348
      },
      {
        "text": ",Expert Systems with Applications , the International Journal of Information Management , and some other leading conference proceedings AMCIS, PACIS, and DSI. 136230 VOLUME 7, 2019 [Página 9] C. Zhang et al. : Optimizing the Electronic Health Records Through Big Data Analytics YUJIE LI received the B.S. degree in computer science and technology from Yangzhou Univer- sity, in 2009, the M.S. degree in electrical engi- neering from the Kyushu Institute of Technology and Yangzhou University, in 2012, respectively, and the Ph.D. degree from the Kyushu Institute of Technology, in 2015. From 2016 to 2017, she was a Lecturer with Yangzhou University. She is currently an Assistant Professor with Fukuoka University, Japan, and also a JSPS Research Fel- low with the Kyushu Institute of Technology, Japan. Her research interests",
        "start_idx": 5336,
        "end_idx": 5464
      },
      {
        "text": "Fel- low with the Kyushu Institute of Technology, Japan. Her research interests include computer vision, sensors, and image segmentation. YICHUAN WANG received the Ph.D. degree in business and information systems from the Raymond J. Harbert College of Business, Auburn University, USA. He is currently an Associate Professor of digital marketing with The Univer- sity of Shef eld. His research interests include examining the impact of digital technologies and information systems (e.g., big data analytics, AI, and social media) in in uencing practices in marketing, healthcare management, and tourism management. He has authored or coauthored more than 50 publications, including 5 book chapters and 30 refereed journal articles\u0016attracting in excess of 1300 citations. His research has appeared in journals, including the IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT , the British",
        "start_idx": 5452,
        "end_idx": 5580
      },
      {
        "text": "in journals, including the IEEE TRANSACTIONS ON ENGINEERING MANAGEMENT , the British Journal of Man- agement ,Information and Management ,Annals of Tourism Research , the Journal of Travel Research , the Journal of Business Research ,Industrial Marketing Management , theInternational Journal of Production Economics , Technological Forecasting and Social Change , and Computers in Human Behavior , among others. He received the Best Paper Award at the Global Marketing Conference and was listed as a nalist for the Best Paper Award at the 20th Americas Con- ference on Information Systems (AMCIS 2014) and the 13th International Conference on Operations and Supply Chain Management (ICOSCM 2019). He was also a recipient of the Research Excellence Award from Newcastle University, in 2018. ZHIJUN YAN received the Ph.D. degree from the",
        "start_idx": 5568,
        "end_idx": 5696
      },
      {
        "text": "Newcastle University, in 2018. ZHIJUN YAN received the Ph.D. degree from the Beijing Institute of Technology. His research inter- ests include health data analytics, health care man- agement, digital health, and electronic commerce. He is currently a Professor with the Department of Management Engineering and the Associate Dean of the School of Management and Economics, Beijing Institute of Technology. He has published in the Journal of Management Information Sys- tems,Information and Management ,Information Technology and People , the Journal of Electronic Commerce Research , and many Chinese journals. VOLUME 7, 2019 136231",
        "start_idx": 5684,
        "end_idx": 5776
      }
    ],
    "94ed8e1d-b515-406b-96c7-a65b89c140da": [
      {
        "text": "[Página 1] A Reliability Benchmark for Big Data Systems on JointCloud Yingying Zheng1,2, Lijie Xu1, Wei Wang1,2, Wei Zhou3, Ying Ding4* 1Institute of Software, Chinese Academy of Sciences 2University of Chinese Academy of Sciences, 3KSYUN 4Institute of space optoelectronic technology, Changchun Univers ity of Science and Technology Abstract —\u0003\u0003JointCloud provides a large-scale, flexible, and elas- tic computing resource platform. Big data systems such as MapReduce and Spark are widely deployed on this platform for big data processing. How to choose a cloud platform in accordance with the need of customers is a problem. Current performance benchmarking suites can choose suitable cloud platforms for customers. However, they do not consider the reliability of applications running atop big data systems. Thes e systems have high scalability, but the applications running",
        "start_idx": 0,
        "end_idx": 128
      },
      {
        "text": "data systems. Thes e systems have high scalability, but the applications running atop them often generate runtime errors, such as out of memory errors, I/O exceptions, and task timeouts. For users, they want to know whether the developed applications have potential application faults. For system designers and manag- ers, they want to know whether the deployed/updated systems have potential system faults. In addition, current benchmarks for big data system are also only designed for performance testing. To fill this gap, we propose a reliability benchmark, which contains representative applications, an abnormal data generator, and a configuration combination generator. Differ- ent from performance benchmarks, this benchmark (1) gener- ates abnormal test data according to the application character- istics, and (2) reduces the configuration combination space based on configuration features.",
        "start_idx": 116,
        "end_idx": 244
      },
      {
        "text": "istics, and (2) reduces the configuration combination space based on configuration features. Currently, we implemented this benchmark on Spark system. In our preliminary test, we found three types of errors (i.e., out of memory errors, timeou t and wrong results) in five SQL, Machine Learning, and Graph applications. Keywords-reliability; benchmark; big data system; Spark; cloud computing; I. INTRODUCTION JointCloud, as a new generation of cloud computing plat- form, provides joint cloud services and complex calculations. Its pay-as-you-go computing model provides a scalable plat- form for big data systems. Due to high scalability, big data systems such as MapReduce [ 1], Spark [ 2], and Flink [ 3] are now widely deployed on cloud platforms for big data pro- cessing. Because of the large number of cloud providers, there",
        "start_idx": 232,
        "end_idx": 360
      },
      {
        "text": "data pro- cessing. Because of the large number of cloud providers, there is a problem that how to choose a cloud platform in accordance with the need of customers. Previous studies [ 4, 5] provide performance benchmarking suites to choose a suitable cloud for customer. There is not a benchmark for choosing cloud for reliability of applications. However, the applications deployed on big data systems often suffer from runtime errors, such as out of memory errors [ 6], I/O excep- tions [ 7], and task timeouts [ 8]. These errors can directly lead to application failures and cannot be tolerated by current fault-tolerant mechanisms. \u0003 * Corresponding author A big data application can be denoted as ( input data , con- figurations , user code ). The input",
        "start_idx": 348,
        "end_idx": 476
      },
      {
        "text": "( input data , con- figurations , user code ). The input data is usually stored as data blocks on distributed file system. Configurations include system-specific configurations (e.g., input block size , parti- tion number ) and application-specific configurations (e.g., K- means application’s cluster k). User code refers to the user- defined functions, such as map() , reduce() , and join() , which process the input or intermediate data. Previous empirical studies [ 6, 7, 9, 10 ] have summarized the root causes of applications’ runtime errors: (1) applica- tion faults , including improper configurations, abnormal data, and user code defects. Improper configurations refer to large input data block size , small partition number , unbal- anced partition function , etc. Abnormal data refers to ex- ceptional input/intermediate/output",
        "start_idx": 464,
        "end_idx": 592
      },
      {
        "text": "anced partition function , etc. Abnormal data refers to ex- ceptional input/intermediate/output data, such as skewed data and high dimension data. User code defects contain memory leak, high time/space complexity, etc. (2) system faults , including hardware faults and software faults. Hardware faults refer to CPU, memory, network and disk failures. Software faults include logic-specific bugs, data races, etc. For users, they want to know whether the developed applications have potential application faults. For system designers and managers, they want to know whether the deployed/updated systems have potential system faults. Testing is a promising approach, but current benchmarks [ 11, 12, 13, 17 ] for big data systems are designed for perfor- mance testing. Since these benchmarks use normal input data and fixed configurations, they cannot be directly",
        "start_idx": 580,
        "end_idx": 708
      },
      {
        "text": "benchmarks use normal input data and fixed configurations, they cannot be directly used to detect potential faults. In this paper, we propose a reliability benchmark for big data systems. To detect the potential application/system faults, this benchmark g enerates abnormal input data, and combines both system-specific and application-specific configurations to test the applications. Different from per- formance benchmarks, this benchmark (1) generates ab- normal input data according to the application characteris- tics, and (2) reduces the configuration combination space based on configuration features. We implemented this benchmark on Spark system. This benchmark currently contains 10 representative appli- cations, an abnormal data generator, a configuration com- bination generator, and a test report generator. In our pre- liminary test, we found five errors: (1) out of memory error in a",
        "start_idx": 696,
        "end_idx": 824
      },
      {
        "text": "test, we found five errors: (1) out of memory error in a SQL operation where a small table inner joins a large table with skewed data; (2) wrong results in a SQL operation where a table participates in multiple join operations but not renamed; (3) out of memory error in RandomForest with high dimension data; (4) out of memory and timeout errors in LogisticRegression with high dimension, and abnormal 2017 IEEE 37th International Conference on Distributed Computing Systems Workshops 1545-0678/17 $31.00 © 2017 IEEE DOI 10.1109/ICDCSW.2017.18306 2017 IEEE 37th International Conference on Distributed Computing Systems Workshops 2332-5666/17 $31.00 © 2017 IEEE DOI 10.1109/ICDCSW.2017.18306 [Página 2] distribution data; (5) out of memory error in PageRank ap- plication with large and sparse data. In summary, our main contributions are as",
        "start_idx": 812,
        "end_idx": 940
      },
      {
        "text": "with large and sparse data. In summary, our main contributions are as follows: x A reliability benchmark is designed for big data sys- tems, which generates abnormal test data according to the application’s characteristics. x A greedy configuration combination method is de- signed to reduce the configuration combination space through analyzing the configuration independ- ence and correlation. x We found three types of errors (i.e., out of memory error, timeout, and wrong results) in five applica- tions. II. BENCHMARK DESIGN AND IMPLEMENTATION The reliability benchmark mainly contains four parts, as shown in Fig. 1: 1) Representative applications selection: It selects widely-used SQL, Machine Learing, and Graph applications. 2) Abnormal input data generation : It summarizes appli- cations’ computational characteristics and generate abnormal input data according to the characteristics.",
        "start_idx": 928,
        "end_idx": 1056
      },
      {
        "text": "cations’ computational characteristics and generate abnormal input data according to the characteristics. 3) Configuration combination test : It combines system- /application-specific configurations and reduce the combina- tion space to test the applications. 4) Test report generation : It analyzes testing results and reports the errors/faults. Fig. 1. The modules of the reliability benchmark A. Representative app lications selection We select the representative applications based on the following criteria: (1) the application represents a basic data operation or a widely-used algorithm; (2) the application has a standard or well-tested implementation. At present, we selected 10 applications from previous benchmark [ 11, 12, 13, 17 ] or Spark’s libraries such as Spark SQL, MLlib and GraphX library in Spark. Table I illustrates the type and computational characteristics of each selected",
        "start_idx": 1044,
        "end_idx": 1172
      },
      {
        "text": "Spark. Table I illustrates the type and computational characteristics of each selected applications. 1) Scan: A data filter query like SELECT * FROM TableA WHERE columnValue > x . 2) Aggregate: A data aggregation query like SELECT columnA , sum ( columnB ) AS total FROM TableA GROUP BY columnA ORDER BY total. 3) Join: A data join query like SELECT * FROM TableA INNER JOIN TableB ON A.columnA = B.columnA . 4) Mix: A query contains data f ilter, aggreg ation and join. 5) Logistic Regression: An iterative classification al- gorithm used to predict continuous or classified data. This algorithm uses a stochastic gradient-descent or L- BFGS algorithm to train the classification model. The mod- el parameters are calculated, updated, and propagated in each iteration. 6) K-means:",
        "start_idx": 1160,
        "end_idx": 1288
      },
      {
        "text": "el parameters are calculated, updated, and propagated in each iteration. 6) K-means: an unsupervised clustering algorithm which iteratively computes the K centers. 7) Decision Tree: a supervised classification algorithm which builds a tree to classify the data. 8) Random Forest: Different from Decision Tree, it builds multiple trees and combines them to classify the data. It uses random sampling to train each tree. 9) PageRank: An algorithm used by Google Search to rank websites in their search engine results. 10) Triangle C ount: It counts the number of different triangles in a directed or undirected graph. TABLE I. REPRESENTATIVE APPLICATIONS Type Application Computational Chara c- teristics Abnormal Data Cha r- acteristics SQL query Scan Filter operation LD, SKD Aggregate Aggregated operation Join Associated operation Mix Filter,Aggregated and Associated",
        "start_idx": 1276,
        "end_idx": 1404
      },
      {
        "text": "LD, SKD Aggregate Aggregated operation Join Associated operation Mix Filter,Aggregated and Associated operation Machine Learning Logistic Regression classification algorithm, Iterative computation LD, SD, HD, AD K-means Clustering Algorithm, Iterative calculation Decision Tree Classification/Regressio n, Breadth-first tree LD , HD, AD Random Forest Classification/Regressio n, Breadth-first tree Graph PageRank Iterative calculation LD, SD, AD TriangleCount Iterative calculation B. Abnormal input data generation Based on the summarized computational characteristics, this benchmark generates abnormal data for each application as shown in Table I. Abnormal data characteristics include: large data volume ( L D ) , skewed data (SKD), sparse data (SD), high dimension data (HD), and abnormal distribution data (AD). How to select the abno rmal data characteristics for different types of applications is summarized below. 1) SQL Query: The Scan",
        "start_idx": 1392,
        "end_idx": 1520
      },
      {
        "text": "different types of applications is summarized below. 1) SQL Query: The Scan , Aggregate , Join applications deal with key/value pairs. The computation complexity of the filter, aggreated, and associated operations are related to the key distribution, so this benchmark selects skewed data (i.e., generating uneven key distribution) as the abnormal input data. Join application is also related to the order of the operations. 2) Machine Learning: Machine Learning applications such as Logistic Regression a n d K-means take matrix-like features as input data, so the related data characteristics are : (1) matrix total size, (2) matrix dimension, (3) distribution of each matrix column, and (4) matrix sparsity. Other tree- based applications such as Decision Tree a n d Random Forest hold breadth-first trees in memory and use",
        "start_idx": 1508,
        "end_idx": 1636
      },
      {
        "text": "a n d Random Forest hold breadth-first trees in memory and use random sampling to train the trees. When the data dimension is high, the resource utilization will be high too. In addition, the 307 307 [Página 3] random sampling method will affect the stability of the computing results. 3) Graph: PageRank a n d Triangle Count applications use vertex-centric partition. In each interation, each vertex needs to send its computation resu lts to its adjacent vertices. So, the computation complexity of these applications are related to the edge distribution (i.e., the degree distribution of vertices). As a result, this benchmark generates skewed graph (i.e., some vertex has too many adjacent vertices) as the input data. Fig. 2 illustrates the process of generating abnormal in- put data for",
        "start_idx": 1624,
        "end_idx": 1752
      },
      {
        "text": "Fig. 2 illustrates the process of generating abnormal in- put data for the Random Forest application. The computa- tional characteristics of Random Forest application are breadth-first tree and random sampling. Then, we select the corresponding data features, namely large-scale, high- dimensional, and abnormal distribution, to generate the ab- normal input data. Fig. 2. Generate abnormal data for the Random Forest applicatio n C. Configuration combination test After generating the input data, the next task is to com- bine system-specific and app lication-specific configurations (e.g., as shown in Table II) to test the applications. TABLE II. CONFIGURATIONS OF THE RANDOM FOREST APPLICATION Type Configuration Description System- specific co n- figurations Input split number Data parallelism Partition number Task parallelism Application- specific co n- figurations maxBins Maximum number of bins",
        "start_idx": 1740,
        "end_idx": 1868
      },
      {
        "text": "Task parallelism Application- specific co n- figurations maxBins Maximum number of bins used for splitting features numClasses Number of classes for classification numTrees Number of trees in the random forest maxDepth Maximum depth of the tree The main problem is that the configuration combination space is too large. Suppose that an application has n config- urations, where the i-th configuration has mi optional values. So, the combination space is O(m1*m 2*···*m n). However, if the configurations satisfy the following two assumptions, the combination space can be reduced to O(n). 1. The configurations are independent of each other. 2. The mi values of configuration i are positively or nega- tively correlated with the applications’ performance (e.g., execution time or resource usage). Based on the above two assumptions, the application’s",
        "start_idx": 1856,
        "end_idx": 1984
      },
      {
        "text": "time or resource usage). Based on the above two assumptions, the application’s performance will become worst (may trigger runtime errors) when the configurations take boundary values. Accordingly, we designed a greedy algorithm (Algorithm 1) to combine the configurations. For example in Fig. 3 a), the application has three con- figurations. At first, this algorithm chooses the low boundary value of each configuration (i.e., 2-1-1). Then it changes the first configuration combination to be the high boundary val- ue 100. Now the configuration is 100-1-2. If the applica- tion’s resource utilization of (2-1-1) is less than that of (10 0- 1-2), the algorithm will fix 100 as the first configuration. Next, the algorithm repeats this selection on the other two configurations as shown in Fig. 3 b), c) and",
        "start_idx": 1972,
        "end_idx": 2100
      },
      {
        "text": "the other two configurations as shown in Fig. 3 b), c) and d). Finally, the application may generate runtime errors under the worst con- figuration combination (i.e., 100-10-2) in Fig. 3 d). However, if the configurations do not satisfy the given two assumptions, this benchmark uses binary search to select the worst value in each configuration. The average compu- ting complexity is O(logm1*logm2*…* logmn). Algorithm 1: Greedy configuration combination test 1. Give the range of each configuration. 2. Select a combination of each threshold value of each configura tion, then test, and record the resource occupancy. 3. Change the val ue of a configuration to another threshold, then test, and record the resource occupancy. 4. Compare the resource usage in the last two combinations of conf ig- urations,",
        "start_idx": 2088,
        "end_idx": 2216
      },
      {
        "text": "the resource usage in the last two combinations of conf ig- urations, and preserve the critical value of poor performance. 5. Return to step 2, and repeat until the exception or ends of test. If an exception was found, the configuration was found which can caus e failures. If no exception was found, the configuration with wor st resource usage or worst performance was found. Fig. 3. The process of reducing configuration combination space D. Test report generation After generating the abnormal data and running configu- ration combination test, a task report generator is performed to analyze the application’s runtime information and gener- ate test reports. The test reports mainly include: 1) what the runtime error is, 2) what abnormal data causes the runtime error, 3) what the",
        "start_idx": 2204,
        "end_idx": 2332
      },
      {
        "text": "is, 2) what abnormal data causes the runtime error, 3) what the worst configurations are. 308 308 [Página 4] TABLE III. PRELIMINARY TEST RESULTS Application Input Data Configurations Errors Join 10GB, skewed data Small table inner join big table OOM Mix 10GB, skewed data A table participates in multiple join opera- tions but not rename it Wrong results Random Forest 1 million instances, 1000-dimensional, Gamma-Poisson distribution numTrees = 100, maxDepth = 30, dimensions = 1000 OOM Logistic Regression 1.05GB sparse data with 1000 di mensions 4 executor (2 cores, 8G), split=134.13MB, partition number = 8 OOM, Timeout PageRank 10G data, 1 million vertices, 20 million edge 4 executor (2 cores, 8G), convergence accuracy = 0.001 OOM III. PRELIMINARY RESULTS A. Experimental setup We performed this reliability benchmark on",
        "start_idx": 2320,
        "end_idx": 2448
      },
      {
        "text": "III. PRELIMINARY RESULTS A. Experimental setup We performed this reliability benchmark on a 10-node cluster (including 1 master node and 9 slave nodes) using Spark-2.0 on Ubuntu-11.04 Operation System. Each node has 4 CPU, 16GB RAM and 2*1TB Disks. We tested each application 5 times and use the mean value. For SQL applications, the input table schemas (shown in Table IV) are as same as that used in Pavlo et al. [15]. How- ever, the input data of all the applications are generated by abnormal input data generator. TABLE IV. TABLE SCHEMAS Table name Column name Data type Rankings pageURL VARCHAR pageRank INT avgDuration INT UserVisits sourceIP VARCHAR destURL VARCHAR visitDate DATE adRevenue FLOAT userAgent VARCHAR countryCode VARCHAR languageCode VARCHAR searchWord VARCHAR duration INT B. Results The preliminary",
        "start_idx": 2436,
        "end_idx": 2564
      },
      {
        "text": "countryCode VARCHAR languageCode VARCHAR searchWord VARCHAR duration INT B. Results The preliminary results are shown in Table III. We found three types of errors (i.e., out of memory (OOM), timeout and wrong results) in five applications. C. Case studies 1) SQL Join When testing the Join query in Spark SQL, this bench- mark generates both normal data and abnormal data (skewed data) for each table shown in Table IV. Since the Join opera- tion is a binary operation, the Join order can be changed. So, the Join query has two sub-queries as shown in Table V. BigSmallJoin denotes Uservisits (large table) inner join Rankings (small table), while SmallBigJoin denotes Rank- ings (small table) inner join Uservisits (large table). Table IV shows the results of the two Join operations.",
        "start_idx": 2552,
        "end_idx": 2680
      },
      {
        "text": "(large table). Table IV shows the results of the two Join operations. Out of memory error occurs in the second SmallBigJoin , where a small table inner joins a large table with skewed data. The execution time of the parallel tasks in BigSmallJoin and SmallBigJoin applications are shown in Fig. 4. When a given data set is skewed, the number of pro- cessed records on a certain task increases significantly. The reason is that when the same key has too many values, these values will be pushed to the same task in shuffle phase. In this situation, the execution time of this task is far longer than that of the other tasks. By analyzing the inner join im- plementation in Spark, we found that: when two tables inner",
        "start_idx": 2668,
        "end_idx": 2796
      },
      {
        "text": "join im- plementation in Spark, we found that: when two tables inner join each other, the first table is considered as a driven tabl e, and the second table is considered as a buffer table. It will traverse each record in the drive table, look for the corre- sponding matching records in the buffer table, and put rec- ords into the matching table. So when we consider a large table as a buffer table, the matching records will be huge. If there is a large table with a seriously skewed data, the match- ing table will occupy much more memory, and out of memory error will occur when we query the relevant key. TABLE V. TEST RESULTS OF JOIN QUERY SQL Type Data type Execution Time BigSmallJoin Normal",
        "start_idx": 2784,
        "end_idx": 2912
      },
      {
        "text": "RESULTS OF JOIN QUERY SQL Type Data type Execution Time BigSmallJoin Normal data ˄large table ˅ 51s Skewed data ˄small table ˅ 59s SmallBigJoin Normal data ˄small table ˅ 56s Skewed data ˄large table ˅ Failed Fig. 4. Comparison of normal and skew data 2) Random Forest Application The configurations of the Random Forest application are shown in Table II. The generated abnormal data is 23.7GB with 104 dimensions. The data distribution is Gaussian dis- tribution. The test results are shown in Table VI. Configura- tions in group A are the initial values. Group B changes the configuration numTrees f rom 2 to 1 00 . A f ter th at , th e c on - figuration combination test found that the time and GC time increased",
        "start_idx": 2900,
        "end_idx": 3028
      },
      {
        "text": "- figuration combination test found that the time and GC time increased significantly. Therefore, the configuration combi- nation algorithm keeps the configuration numTrees to be 100 in group C. The next test is to change maxDepth to be 100. Out of memory error occurs in group C. If we continue test- ing using the configurations in group D, out of memory error will also occur. So, the worst configuration combination is 309 309 [Página 5] 100-5-32-10. However, for the configurations in group C and D, the out of memory errors will disappear if the data distri- bution is changed to the uniform distribution. It indicates tha t the application has potential faults while processing the data with Gaussian distribution. TABLE VI. TEST RESULTS OF RANDOM FOREST Configurations A",
        "start_idx": 3016,
        "end_idx": 3144
      },
      {
        "text": "with Gaussian distribution. TABLE VI. TEST RESULTS OF RANDOM FOREST Configurations A B C D numTrees 2 100 100 100 maxDepth 5 5 100 5 maxBins 5 5 5 32 Partition num 10 10 10 10 Running time 6.4min 41min OOM OOM IV. RELATED WORK The reliability of big data applications/systems has emerged as a critical problem for both academia and indus- try. Many researchers have performed empirical studies on big data application/system failures. However, the current benchmarks are not designed for reliability testing. Failure study on big data applications/systems: Li et al. [9] studied 250 failures in SCOPE jobs in Microsoft big data platform, and found 84.5% failures are caused by de- fects in data processing. They also found 3 OOM errors that are caused by accumulating",
        "start_idx": 3132,
        "end_idx": 3260
      },
      {
        "text": "processing. They also found 3 OOM errors that are caused by accumulating large data (e.g, all input rows) in memory. Xu et al . [6] s t u d i e d 1 2 3 O O M e r r o r s i n r e a l - world Hadoop/Spark applications and found three causes of out of memory errors: improper configurations, abnormal dataflow and memory-consuming user code. Kavulya et al. [7] analyzed 4100 failed Hadoop jobs, and found 36% fail- ures are array indexing errors and 23% failures are IOEx- ceptions. Zhou et al . [16] studied the quality issues of big data platform in Microsoft. They found 36% issues are caused by system side defects and 2 issues (1%) are m e m",
        "start_idx": 3248,
        "end_idx": 3376
      },
      {
        "text": "by system side defects and 2 issues (1%) are m e m o r y i s s u e s . G u n a w i et al . [10] studied 3655 develop- ment and deployment issues in cloud systems such as Ha- doop MapReduce, HDFS, and HBase. They found 87% is- sues are software faults, while 13% issues are hardware faults. They also reported 1 OOM error in HBase (users submit queries on large data sets) and 1 OOM error in Ha- doop File System (users create thousands of small files in parallel). These studies help us design the abnormal data generator and configuration generator. Big data benchmarks: Pavlo [ 15] designed a big SQL benchmark to compare the performance between MapRe- duce and relational",
        "start_idx": 3364,
        "end_idx": 3492
      },
      {
        "text": "big SQL benchmark to compare the performance between MapRe- duce and relational databases. Berkeley AMPLab developed a SQL benchmark [ 12] to compare the performance among Spark, Hive, Impala, etc. HiBench [ 13] is designed to test the performance of Hadoop and Spark. BigDataBench [ 17] in- cludes 14 real-world data sets, and 34 big data workloads. These benchmarks use normal data and fixed configurations to test the performance of big data systems. V. CONCLUSION AND FUTURE WORK Big data applications deployed on the cloud platform frequently suffer from runtime errors. However, current benchmarks are designed for performance testing and can- not be directly used for detecting potential faults. In this paper, we design a reliability benchmark for big data sys-tems and implement it on Spark. This benchmark",
        "start_idx": 3480,
        "end_idx": 3608
      },
      {
        "text": "benchmark for big data sys-tems and implement it on Spark. This benchmark first gen- erates abnormal input data according to the application characteristics, and then uses g reedy algorithm to combine system-/application-specific conf igurations for testing. Pre- liminary results show that this benchmark can detect appli- cation faults. In the future, we will build more applications into the benchmark and implement this benchmark on more systems such as Flink. ACKNOWLEDGMENT This work was supported by the National Key Research and Development Program of China (2016YFB1000103) and Youth Innovation Promotion Association, CAS (No. 2015088). REFERENCES [1] J. Dean and S. Ghemawat, “Mapreduce: Simplified data processing on large clusters,” in 6th Symposium on Operating System Design and Implementation (OSDI) , 2004, pp. 137–150. [2] M. Zaharia, M. Chowdhury, T. Das,",
        "start_idx": 3596,
        "end_idx": 3724
      },
      {
        "text": "(OSDI) , 2004, pp. 137–150. [2] M. Zaharia, M. Chowdhury, T. Das, A. Dave, J. Ma, M. McCauly, M . J. Franklin, S. Shenker, and I. Stoica, “Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing,” in NSDI , 2012, pp. 15–28. [3] “Apache Flink.” [Online]. Available: https://flink.apache.org/. [4] A. Li, et al. \"CloudCmp: comparing public cloud providers,\" in Proceedings of the 10th ACM SIGCOMM conference on Internet measurement (SIGCOMM). ,2010. [5] Lenk, Alexander, et al. \"What are you paying for? performance benchmarking for infrastructure-as-a-service offerings.\" Cloud Computing (CLOUD), 2011 IEEE International Conference on. IEEE, 2011. [6] L. Xu, W. Dou, F. Zhu, C. Gao, J. Liu, H. Zhong, and J. Wei, “Experi- ence report: A characteristic study on out of memory errors in distributed data-parallel applications,”",
        "start_idx": 3712,
        "end_idx": 3840
      },
      {
        "text": "A characteristic study on out of memory errors in distributed data-parallel applications,” in 26th IEEE International Symposium on Software Reliability Engineering (ISSRE) , 2015, pp. 518–529. [7] S. Kavulya, J. Tan, R. Gandhi, and P. Narasimhan, “An analysis of traces from a production mapreduce cluster,” in 10th IEEE/ACM International Conference on Cluster, Cloud and Grid Computing (CCGrid) , 2010. [8] “Spark reduce operation taking too long .” [On- line]. Available: http://stackoverflow.com/questions/33558593/ spark-reduce-operation-taking-too-long. [9] S. Li, H. Zhou, H. Lin, T. Xiao, H. Lin, W. Lin, and T. Xie, “A characteristic study on failures of production distributed data - parallel programs,” in 35th International Conference on Software Engineering (ICSE) , 2013, pp. 963–972. [10] H. S. Gunawi, M. Hao, T. Leesatapornwongsa, T. Patana-anake, T. Do, J. Adityatama, K. J.",
        "start_idx": 3828,
        "end_idx": 3956
      },
      {
        "text": "M. Hao, T. Leesatapornwongsa, T. Patana-anake, T. Do, J. Adityatama, K. J. Eliazar, A. Laksono, J. F. Lukman, V. Martin, and A. D. Satria, “What bugs live in the cloud? A study of 3000 + issues in cloud systems,” in Proceedings of the ACM Symposium on Cloud Computing (SoCC) , 2014, pp. 7:1– 7:14. [11] “Spark Performance Tests.” [Online]. Available: https://github.com/ databricks/spark-perf. [12] “ S p a r k S Q L B e n c h m a r k . ” [ O n l i n e ] . A v a i l a b l e : https://amplab.cs. berkeley.edu/benchmark/. [13] “HiBench: the bigdata micro benchmark suite.” [Online]. Availab le: https://github.com/intel-hadoop/HiBench. [14] M. Armbrust, et al. “Spark SQL: relational data processing in spark,” in",
        "start_idx": 3944,
        "end_idx": 4072
      },
      {
        "text": "M. Armbrust, et al. “Spark SQL: relational data processing in spark,” in Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data (SIGMOD) , 2015. [15] Pavlo, Andrew, et al. \"A comparison of approaches to large-scal e data analysis.\" in Proceedings of the 2009 ACM SIGMOD International Conference on Management of Data (SIGMOD) , 2009. [16] H . Z h o u , J . - G . L o u , H . Z h a n g , H . L i n , H . L i n , a n d T . Q i n , “ A n empirical study on quality issues of production big data platfo rm,” in ICSE , 2015. [17] L. Wang, et al, “Bigdatabench: A",
        "start_idx": 4060,
        "end_idx": 4188
      },
      {
        "text": "rm,” in ICSE , 2015. [17] L. Wang, et al, “Bigdatabench: A big data benchmark suite from internet services,” in HPCA , 2014. 310 310",
        "start_idx": 4176,
        "end_idx": 4201
      }
    ],
    "00253a0a-b357-4fd1-808b-956aaa892e73": [
      {
        "text": "This is a test document about data quality testing.",
        "start_idx": 0,
        "end_idx": 9
      }
    ],
    "cc081639-d2f6-4f15-8b61-4806dffc3e0b": [
      {
        "text": "Data quality is essential for accurate analytics and reporting.",
        "start_idx": 0,
        "end_idx": 9
      }
    ],
    "6da978c2-403a-4cce-a412-6a45c6091397": [
      {
        "text": "Data quality testing ensures that data meets the required standards. It includes validation of accuracy, completeness, consistency, and reliability. Testing should be performed regularly to maintain data integrity.",
        "start_idx": 0,
        "end_idx": 28
      }
    ],
    "b3ba56fe-df43-4648-abda-15f1454eafd2": [
      {
        "text": "Data quality testing ensures that data meets the required standards. It includes validation of accuracy, completeness, consistency, and reliability. Testing should be performed regularly to maintain data integrity.",
        "start_idx": 0,
        "end_idx": 28
      }
    ],
    "2c83d205-e2c9-416a-8a1d-9b0a4528e979": [
      {
        "text": "Data quality testing ensures that data meets the required standards. It includes validation of accuracy, completeness, consistency, and reliability. Testing should be performed regularly to maintain data integrity.",
        "start_idx": 0,
        "end_idx": 28
      }
    ],
    "d35b90d1-1cf6-45c6-969f-2854cbf20029": [
      {
        "text": "Data quality testing ensures that data meets the required standards. It includes validation of accuracy, completeness, consistency, and reliability. Testing should be performed regularly to maintain data integrity.",
        "start_idx": 0,
        "end_idx": 28
      }
    ],
    "81c441fb-2d00-42d2-b3b0-c40f3c09703e": [
      {
        "text": "Test content about data validation",
        "start_idx": 0,
        "end_idx": 5
      }
    ],
    "0e80936e-137c-477a-a883-8955c12fa14b": [
      {
        "text": "Persistence test content",
        "start_idx": 0,
        "end_idx": 3
      }
    ],
    "3416ba8e-6c9a-414a-b381-b15d61798675": [
      {
        "text": "This is a test document about data quality testing.",
        "start_idx": 0,
        "end_idx": 9
      }
    ],
    "97214baf-18f4-4c83-8008-3664a9548301": [
      {
        "text": "Data quality is essential for accurate analytics and reporting.",
        "start_idx": 0,
        "end_idx": 9
      }
    ],
    "ff649f97-f509-44be-9238-2207a3f99849": [
      {
        "text": "Data quality testing ensures that data meets the required standards. It includes validation of accuracy, completeness, consistency, and reliability. Testing should be performed regularly to maintain data integrity.",
        "start_idx": 0,
        "end_idx": 28
      }
    ],
    "2f8ae8f7-230f-4cfa-8f01-5410ece5b9f6": [
      {
        "text": "Data quality testing ensures that data meets the required standards. It includes validation of accuracy, completeness, consistency, and reliability. Testing should be performed regularly to maintain data integrity.",
        "start_idx": 0,
        "end_idx": 28
      }
    ],
    "8d168bd2-4708-41c7-ada3-056769ffd527": [
      {
        "text": "Data quality testing ensures that data meets the required standards. It includes validation of accuracy, completeness, consistency, and reliability. Testing should be performed regularly to maintain data integrity.",
        "start_idx": 0,
        "end_idx": 28
      }
    ],
    "9e63d88e-d5dd-4429-9826-e4f2f210d968": [
      {
        "text": "Data quality testing ensures that data meets the required standards. It includes validation of accuracy, completeness, consistency, and reliability. Testing should be performed regularly to maintain data integrity.",
        "start_idx": 0,
        "end_idx": 28
      }
    ],
    "f903b1e9-e47e-4d17-844f-f656effb52b0": [
      {
        "text": "Test content about data validation",
        "start_idx": 0,
        "end_idx": 5
      }
    ],
    "b9ea5edc-6448-4976-acb5-b547128a6674": [
      {
        "text": "Persistence test content",
        "start_idx": 0,
        "end_idx": 3
      }
    ]
  }
}