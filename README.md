# Data Quality Chatbot

DataForgeTest is an innovative tool designed to automate data quality testing for Big Data systems. By combining Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), this repository provides a cutting-edge solution for generating customizable test cases, creating comprehensive data quality rules, and executing them on scalable Spark clusters.
<img width="1024" height="1024" alt="image" src="https://github.com/user-attachments/assets/f17fd7ad-e9e9-464a-a55b-5d80af8ec578" />


# How to Run

## Windows - Execu√ß√£o Automatizada

### Primeira Execu√ß√£o ou Ap√≥s Atualizar Depend√™ncias
Execute `setup_start.bat` para:
1. Verificar e instalar todas as depend√™ncias
2. Configurar o ambiente completo
3. Iniciar os servi√ßos
4. Abrir o navegador automaticamente

### Desenvolvimento Di√°rio
Execute `dev_start.bat` para:
1. Inicializa√ß√£o r√°pida dos servi√ßos
2. Sem verifica√ß√£o de depend√™ncias
3. Ideal para uso durante desenvolvimento

## M√©todo Manual
Se preferir iniciar manualmente, abra dois terminais:

1. Terminal 1 (Backend):
```bash
# Ativar ambiente virtual
.venv\Scripts\activate

# Rodar API
cd src
python api.py
```

2. Terminal 2 (Frontend):
```bash
# Navegar at√© o diret√≥rio do frontend
cd frontend/frontend

# Iniciar aplica√ß√£o React
npm start
```

A aplica√ß√£o estar√° dispon√≠vel em:
- Frontend: http://localhost:3000
- Backend: http://localhost:5000

## RAG Support System

### Chat com Documenta√ß√£o (Support Page)

O sistema inclui um chat inteligente na p√°gina de Support que permite fazer perguntas sobre a documenta√ß√£o do projeto.

**Status Atual:** O sistema usa Simple RAG (busca por palavras-chave) por padr√£o.

**Para habilitar IA Claude Sonnet:**
1. Configure a chave API no arquivo `.env` (copie de `.env.example`)
2. Instale depend√™ncias adicionais: `pip install llama-index-llms-anthropic`
3. Veja o guia completo em: [`docs/RAG_TROUBLESHOOTING.md`](docs/RAG_TROUBLESHOOTING.md)

**Testes e Diagn√≥sticos:**
```bash
# Executar testes do sistema RAG
python tests/test_rag_integration.py
python tests/test_rag_api.py

# Diagn√≥stico completo do sistema
python tests/test_rag_diagnostics.py
```

üìö **Documenta√ß√£o RAG:**
- [`docs/RAG_QUICK_REFERENCE.md`](docs/RAG_QUICK_REFERENCE.md) - Refer√™ncia r√°pida
- [`docs/RAG_TROUBLESHOOTING.md`](docs/RAG_TROUBLESHOOTING.md) - Guia de solu√ß√£o de problemas
- [`docs/RAG_ANALYSIS_SUMMARY.md`](docs/RAG_ANALYSIS_SUMMARY.md) - An√°lise completa
- [`docs/RAG_TEST_RESULTS.md`](docs/RAG_TEST_RESULTS.md) - Resultados dos testes
- [`tests/README_TESTS.md`](tests/README_TESTS.md) - Documenta√ß√£o dos testes

## Vis√£o Geral

Este projeto oferece uma solu√ß√£o para automatizar a cria√ß√£o de regras de qualidade de dados e a gera√ß√£o de c√≥digo PySpark para aplic√°-las. Atrav√©s de uma interface de linha de comando, o usu√°rio pode responder a perguntas sobre seus dados e as regras desejadas. O chatbot ent√£o traduz essas respostas em uma DSL (Domain Specific Language) estruturada e, posteriormente, em um script PySpark funcional.

<img width="1918" height="896" alt="image" src="https://github.com/user-attachments/assets/a5c59bb3-049d-4417-8036-71b9f10e0b25" />

## Funcionalidades

*   **Coleta de Requisitos**: Interage com o usu√°rio via CLI para coletar informa√ß√µes sobre o dataset e as regras de qualidade de dados desejadas.
*   **Gera√ß√£o de DSL**: Converte as respostas do usu√°rio em um formato DSL (JSON) que representa as regras de qualidade de dados de forma estruturada e leg√≠vel por m√°quina.
*   **Gera√ß√£o de C√≥digo PySpark**: A partir da DSL gerada, o chatbot cria scripts PySpark prontos para serem executados em ambientes Spark, aplicando as regras de qualidade de dados definidas.
*   **Suporte a Diversas Regras**:
    *   **Valida√ß√£o de Esquema**: Verifica nomes e tipos de colunas, e se o dataset possui cabe√ßalho.
    *   **Integridade de Dados**:
        *   `not_null`: Garante que colunas n√£o contenham valores ausentes.
        *   `uniqueness`: Assegura que colunas ou combina√ß√µes de colunas tenham valores √∫nicos.
    *   **Restri√ß√µes de Valor**:
        *   `format`: Valida o formato de valores em colunas (e.g., datas, e-mails).
        *   `range`: Verifica se valores num√©ricos est√£o dentro de um intervalo especificado.
        *   `in_set`: Confere se valores pertencem a um conjunto predefinido de op√ß√µes.
        *   `regex`: Valida valores contra um padr√£o de express√£o regular.
        *   `value_distribution`: Permite verificar a frequ√™ncia de ocorr√™ncia de valores espec√≠ficos.

## Estrutura do Projeto

```
data-quality-chatbot/
‚îú‚îÄ‚îÄ data/                     # Diret√≥rio para dados de exemplo ou datasets de entrada.
‚îú‚îÄ‚îÄ dsl/                      # Armazena os arquivos DSL (JSON) gerados pelo chatbot.
‚îú‚îÄ‚îÄ output/                   # Armazena os scripts PySpark (.py) gerados.
‚îú‚îÄ‚îÄ notebooks/                # Notebooks Jupyter para experimenta√ß√£o ou demonstra√ß√£o.
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ chatbot/              # L√≥gica principal do chatbot e intera√ß√£o com o usu√°rio.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py           # Ponto de entrada do chatbot.
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ questions.py      # Defini√ß√£o das perguntas feitas ao usu√°rio.
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ answers.py        # Gerenciamento das respostas do usu√°rio.
‚îÇ   ‚îú‚îÄ‚îÄ code_generator/       # M√≥dulos para gerar c√≥digo PySpark a partir da DSL.
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pyspark_generator.py # L√≥gica de gera√ß√£o de c√≥digo PySpark.
‚îÇ   ‚îú‚îÄ‚îÄ dsl_parser/           # M√≥dulos para parsing e gera√ß√£o da DSL.
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ generator.py      # L√≥gica para converter respostas em DSL.
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ tests/                    # Testes unit√°rios para as diferentes partes do projeto.
‚îÇ   ‚îú‚îÄ‚îÄ test_generator.py
‚îÇ   ‚îú‚îÄ‚îÄ test_pyspark_generator.py
‚îÇ   ‚îî‚îÄ‚îÄ sample_answers.py
‚îî‚îÄ‚îÄ .gitignore                # Arquivo para ignorar arquivos e diret√≥rios no controle de vers√£o.
‚îî‚îÄ‚îÄ README.md                 # Este arquivo.
```

## Como Usar

### Pr√©-requisitos

*   Python 3.x
*   pip (gerenciador de pacotes do Python)

### Instala√ß√£o

1.  Clone o reposit√≥rio:
    ```bash
    git clone https://github.com/seu-usuario/data-quality-chatbot.git
    cd data-quality-chatbot
    ```
    (Nota: Substitua `https://github.com/seu-usuario/data-quality-chatbot.git` pelo link real do seu reposit√≥rio, se aplic√°vel.)

2.  Crie e ative um ambiente virtual (recomendado):
    ```bash
    python -m venv .venv
    # No Windows:
    .venv\\Scripts\\activate
    # No macOS/Linux:
    source .venv/bin/activate
    ```

3.  Instale as depend√™ncias:
    ```bash
    pip install -r requirements.txt
    ```
    (Nota: Se o arquivo `requirements.txt` n√£o existir, voc√™ precisar√° cri√°-lo com as depend√™ncias do projeto, como `pyspark` e outras que possam ser necess√°rias.)

### Executando o Chatbot

Para iniciar o chatbot, execute o seguinte comando no terminal:

```bash
python -m src.chatbot.main
```

O chatbot far√° uma s√©rie de perguntas sobre o seu dataset e as regras de qualidade de dados que voc√™ deseja aplicar. Ao final, ele exibir√° a DSL gerada e o c√≥digo PySpark correspondente, com a op√ß√£o de salv√°-los em arquivos.

### Exemplo de Intera√ß√£o

```
Welcome to the Data Quality Chatbot!

--- Dataset Information ---
What is the name of the dataset you want to validate? my_sales_data
What is the source of the data (e.g., a file path, a database table)? /data/sales.csv
What is the format of the data (e.g., CSV, JSON, Parquet)? CSV

--- Schema Validation ---
Does the data have a header? yes
What are the expected column names, in order? product_id, sale_date, amount, region
What is the expected data type for each column (e.g., string, integer, float, date)? string, date, float, string

--- Data Integrity ---
Which columns should not contain any missing values (i.e., are mandatory)? product_id, sale_date
Which columns should contain unique values (i.e., are primary keys)? product_id

--- Value Constraints ---
Are there any columns that should have a specific format (e.g., a date format like YYYY-MM-DD)? sale_date:YYYY-MM-DD
Are there any columns that should have a minimum or maximum value? amount:0:1000
Are there any columns that should only contain values from a specific set (e.g., a list of categories)? region:[North,South,East,West]
Are there any columns that should match a specific regular expression pattern? product_id:^[A-Z]{3}\\d{4}$
Are there any columns for which you want to check the value distribution (e.g., to ensure certain values appear with a specific frequency)? region:North:0.2:0.4

--- Generated DSL ---
{
    "dataset": {
        "name": "my_sales_data",
        "source": "/data/sales.csv",
        "format": "CSV",
        "has_header": true,
        "schema": {
            "product_id": "string",
            "sale_date": "date",
            "amount": "float",
            "region": "string"
        }
    },
    "rules": [
        {
            "type": "not_null",
            "column": "product_id"
        },
        {
            "type": "not_null",
            "column": "sale_date"
        },
        {
            "type": "uniqueness",
            "columns": [
                "product_id"
            ]
        },
        {
            "type": "format",
            "column": "sale_date",
            "format": "YYYY-MM-DD"
        },
        {
            "type": "range",
            "column": "amount",
            "min": 0.0,
            "max": 1000.0
        },
        {
            "type": "in_set",
            "column": "region",
            "values": [
                "North",
                "South",
                "East",
                "West"
            ]
        },
        {
            "type": "regex",
            "column": "product_id",
            "pattern": "^[A-Z]{3}\\d{4}$"
        },
        {
            "type": "value_distribution",
            "column": "region",
            "value": "North",
            "min_freq": 0.2,
            "max_freq": 0.4
        }
    ]
}

Would you like to save the generated DSL to a file? (yes/no): yes
Please enter a filename for the DSL (e.g., my_rules.json): sales_rules.json
DSL saved successfully to dsl\\sales_rules.json

--- Generating PySpark Code ---
# PySpark code generated by Data Quality Chatbot
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, isnull, countDistinct, regexp_extract, lit
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DateType

spark = SparkSession.builder.appName("DataQualityChecks").getOrCreate()

# Define schema
schema = StructType([
    StructField("product_id", StringType(), True),
    StructField("sale_date", DateType(), True),
    StructField("amount", FloatType(), True),
    StructField("region", StringType(), True)
])

# Load data
df = spark.read.csv("/data/sales.csv", header=True, schema=schema)

print("Running Data Quality Checks...")

# Rule: not_null for product_id
null_product_id_count = df.filter(isnull(col("product_id"))).count()
if null_product_id_count > 0:
    print(f"FAIL: Column 'product_id' has {null_product_id_count} null values.")
else:
    print("PASS: Column 'product_id' has no null values.")

# Rule: not_null for sale_date
null_sale_date_count = df.filter(isnull(col("sale_date"))).count()
if null_sale_date_count > 0:
    print(f"FAIL: Column 'sale_date' has {null_sale_date_count} null values.")
else:
    print("PASS: Column 'sale_date' has no null values.")

# Rule: uniqueness for product_id
total_rows = df.count()
distinct_product_id_count = df.select(countDistinct(col("product_id"))).collect()[0][0]
if total_rows != distinct_product_id_count:
    print(f"FAIL: Column 'product_id' has duplicate values. Total rows: {total_rows}, Distinct product_ids: {distinct_product_id_count}.")
else:
    print("PASS: Column 'product_id' has unique values.")

# Rule: format for sale_date (YYYY-MM-DD)
invalid_sale_date_format_count = df.filter(~col("sale_date").cast(StringType()).rlike("^\\d{4}-\\d{2}-\\d{2}$")).count()
if invalid_sale_date_format_count > 0:
    print(f"FAIL: Column 'sale_date' has {invalid_sale_date_format_count} values with invalid format (expected YYYY-MM-DD).")
else:
    print("PASS: Column 'sale_date' has valid format.")

# Rule: range for amount (min: 0.0, max: 1000.0)
out_of_range_amount_count = df.filter((col("amount") < 0.0) | (col("amount") > 1000.0)).count()
if out_of_range_amount_count > 0:
    print(f"FAIL: Column 'amount' has {out_of_range_amount_count} values out of expected range [0.0, 1000.0].")
else:
    print("PASS: Column 'amount' values are within range.")

# Rule: in_set for region (values: ['North', 'South', 'East', 'West'])
invalid_region_count = df.filter(~col("region").isin(['North', 'South', 'East', 'West'])).count()
if invalid_region_count > 0:
    print(f"FAIL: Column 'region' has {invalid_region_count} values not in the allowed set.")
else:
    print("PASS: Column 'region' values are in the allowed set.")

# Rule: regex for product_id (pattern: ^[A-Z]{3}\d{4}$)
invalid_product_id_regex_count = df.filter(~col("product_id").rlike("^[A-Z]{3}\\d{4}$")).count()
if invalid_product_id_regex_count > 0:
    print(f"FAIL: Column 'product_id' has {invalid_product_id_regex_count} values not matching the regex pattern '^[A-Z]{3}\\d{4}$'.")
else:
    print("PASS: Column 'product_id' values match the regex pattern.")

# Rule: value_distribution for region (value: North, min_freq: 0.2, max_freq: 0.4)
north_region_count = df.filter(col("region") == "North").count()
total_rows = df.count()
north_region_frequency = north_region_count / total_rows if total_rows > 0 else 0

if north_region_frequency >= 0.2 and north_region_frequency <= 0.4:
    print(f"PASS: Value 'North' in column 'region' has frequency {north_region_frequency:.2f}, which is within [0.2, 0.4].")
else:
    print(f"FAIL: Value 'North' in column 'region' has frequency {north_region_frequency:.2f}, which is outside [0.2, 0.4].")

spark.stop()

Would you like to save the generated PySpark code to a file? (yes/no): yes
Please enter a filename for the PySpark code (e.g., data_quality_script.py): sales_data_quality_checks.py
PySpark code saved successfully to output\\sales_data_quality_checks.py
