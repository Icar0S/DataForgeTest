# ğŸš€ ConfiguraÃ§Ã£o Gemini API para ProduÃ§Ã£o

## âœ… Sistema HÃ­brido Configurado

### ğŸ  Local (Desenvolvimento)
- **Provider:** Ollama
- **Modelo:** qwen2.5-coder:7b
- **Custo:** âœ… GrÃ¡tis
- **ConfiguraÃ§Ã£o:** AutomÃ¡tica (jÃ¡ funcionando)

### â˜ï¸ ProduÃ§Ã£o (Render)
- **Provider:** Gemini
- **Modelo:** gemini-1.5-flash (recomendado)
- **Custo:** ğŸ’° Pay-as-you-go (muito barato)
- **ConfiguraÃ§Ã£o:** Manual no Render

---

## ğŸ“‹ Passos para Configurar no Render

### 1ï¸âƒ£ Obter API Key do Gemini

1. Acesse: https://aistudio.google.com/app/apikey
2. FaÃ§a login com sua conta Google
3. Clique em **"Create API Key"**
4. Copie a chave gerada

### 2ï¸âƒ£ Configurar no Render

No painel do Render (https://dashboard.render.com):

1. VÃ¡ para seu serviÃ§o **dataforgetest-backend**
2. Clique em **"Environment"** (menu lateral)
3. Adicione as seguintes variÃ¡veis:

```bash
LLM_PROVIDER=gemini
GEMINI_API_KEY=sua-chave-aqui
GEMINI_MODEL=gemini-1.5-flash
```

4. Clique em **"Save Changes"**
5. O deploy serÃ¡ automaticamente reiniciado

### 3ï¸âƒ£ Verificar Funcionamento

ApÃ³s o deploy:

1. Acesse: https://dataforgetest-backend.onrender.com/rag/debug
2. Verifique se aparece:
   ```
   âœ… LLM initialized with provider: gemini, model: gemini-1.5-flash
   ```

3. Teste o chat: https://data-forge-test.vercel.app
4. FaÃ§a uma pergunta e veja a resposta inteligente!

---

## ğŸ§ª Testar Localmente com Gemini (Opcional)

Se quiser testar o Gemini localmente antes do deploy:

```bash
# Configure as variÃ¡veis
set GEMINI_API_KEY=sua-chave-aqui
set LLM_PROVIDER=gemini
set GEMINI_MODEL=gemini-1.5-flash

# Instale o pacote
pip install google-generativeai

# Execute o teste
python test_gemini.py
```

---

## ğŸ’° PreÃ§os do Gemini (Dezembro 2024)

### Gemini 1.5 Flash (Recomendado)
- **Input:** $0.075 / 1M tokens
- **Output:** $0.30 / 1M tokens
- **Contexto:** 1M tokens
- **Melhor para:** ProduÃ§Ã£o (rÃ¡pido e barato)

### Gemini 1.5 Pro
- **Input:** $1.25 / 1M tokens  
- **Output:** $5.00 / 1M tokens
- **Contexto:** 2M tokens
- **Melhor para:** Tarefas complexas

**Estimativa de custo:**
- 1000 perguntas/mÃªs â‰ˆ $0.50 - $2.00
- Muito mais barato que Claude! ğŸ‰

---

## ğŸ”„ AlternÃ¢ncia Entre Providers

### Desenvolvimento Local
```bash
# .env (local)
LLM_PROVIDER=ollama
LLM_MODEL=qwen2.5-coder:7b
```

### ProduÃ§Ã£o Render
```bash
# Render Environment Variables
LLM_PROVIDER=gemini
GEMINI_API_KEY=your-key
GEMINI_MODEL=gemini-1.5-flash
```

### Para Voltar ao Ollama
```bash
# Render Environment Variables
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
```
âš ï¸ Mas isso NÃƒO funcionarÃ¡ no Render (Ollama Ã© local)

---

## âœ… Checklist de Deploy

- [ ] Obter API key do Gemini
- [ ] Adicionar variÃ¡veis no Render:
  - [ ] `LLM_PROVIDER=gemini`
  - [ ] `GEMINI_API_KEY=sua-chave`
  - [ ] `GEMINI_MODEL=gemini-1.5-flash`
- [ ] Aguardar deploy automÃ¡tico
- [ ] Testar endpoint: `/rag/debug`
- [ ] Testar chat no frontend
- [ ] Verificar logs no Render

---

## ğŸ†˜ Troubleshooting

### Erro: "GEMINI_API_KEY is required"
âœ… Verifique se a variÃ¡vel foi adicionada no Render

### Erro: "google-generativeai package not installed"
âœ… O `requirements.txt` jÃ¡ tem o pacote, aguarde o deploy

### Chat retorna templates bÃ¡sicos
âœ… Verifique se `LLM_PROVIDER=gemini` estÃ¡ configurado

### Erro 429: "Quota exceeded"
âœ… VocÃª atingiu o limite gratuito, adicione billing no Google Cloud

---

## ğŸ“Š ComparaÃ§Ã£o de Providers

| Provider | Custo | Velocidade | Qualidade | Disponibilidade |
|----------|-------|------------|-----------|-----------------|
| **Ollama** | âœ… GrÃ¡tis | ğŸŸ¡ MÃ©dia | ğŸŸ¡ Boa | ğŸ”´ Local apenas |
| **Gemini** | âœ… Muito barato | âœ… RÃ¡pida | âœ… Excelente | âœ… Cloud |
| **Claude** | ğŸ”´ Caro | âœ… RÃ¡pida | âœ… Excelente | âœ… Cloud |

---

## ğŸ¯ ConclusÃ£o

Com essa configuraÃ§Ã£o:

âœ… **Local:** Ollama (grÃ¡tis, para desenvolvimento)  
âœ… **ProduÃ§Ã£o:** Gemini (barato, alta qualidade)  
âœ… **Fallback:** Templates simples (se LLM falhar)  

**Sistema 100% pronto para produÃ§Ã£o!** ğŸš€
