# Personas e Casos de Uso - DataForgeTest

<div align="center">

**Plataforma AvanÃ§ada de Testes de Qualidade de Dados com GeraÃ§Ã£o de Dados SintÃ©ticos Alimentada por IA**

---

*VersÃ£o 1.0 | Dezembro 2025*

</div>

---

## SumÃ¡rio Executivo

Este documento apresenta uma anÃ¡lise detalhada das duas personas principais que compÃµem o pÃºblico-alvo da plataforma **DataForgeTest**:  **Testadores/QA (Quality Assurance)** e **Cientistas de Dados**. 

A anÃ¡lise inclui perfis comportamentais, necessidades tÃ©cnicas, objetivos profissionais e casos de uso especÃ­ficos para cada funcionalidade da plataforma, demonstrando como o DataForgeTest atende Ã s demandas Ãºnicas de cada grupo de usuÃ¡rios.

---

## Tabela de ConteÃºdo

1. [VisÃ£o Geral da Plataforma](#1-visÃ£o-geral-da-plataforma)
2. [Persona 1: Testador/QA](#2-persona-1-testadorqa)
3. [Persona 2: Cientista de Dados](#3-persona-2-cientista-de-dados)
4. [Matriz de Funcionalidades por Persona](#4-matriz-de-funcionalidades-por-persona)
5. [Jornadas de UsuÃ¡rio](#5-jornadas-de-usuÃ¡rio)
6. [MÃ©tricas de Sucesso](#6-mÃ©tricas-de-sucesso)
7. [ConclusÃ£o](#7-conclusÃ£o)

---

## 1. VisÃ£o Geral da Plataforma

### 1.1 Sobre o DataForgeTest

**DataForgeTest** Ã© uma soluÃ§Ã£o abrangente para automaÃ§Ã£o de testes de qualidade de dados em ambientes Big Data, combinando Large Language Models (LLMs) com capacidades avanÃ§adas de processamento de dados. 

### 1.2 Funcionalidades Principais

| Funcionalidade | DescriÃ§Ã£o | Tecnologia Base |
|---------------|-----------|-----------------|
| **GeraÃ§Ã£o de CÃ³digo PySpark** | Interface conversacional para geraÃ§Ã£o de scripts de validaÃ§Ã£o de qualidade de dados | LLM + RAG |
| **Gerador AvanÃ§ado PySpark** | Upload de datasets para detecÃ§Ã£o automÃ¡tica de schema e geraÃ§Ã£o inteligente de cÃ³digo | AnÃ¡lise AutomÃ¡tica + LLM |
| **GeraÃ§Ã£o de Dados SintÃ©ticos** | CriaÃ§Ã£o de datasets realistas com 14+ tipos de dados | LLM + Faker |
| **ValidaÃ§Ã£o de AcurÃ¡cia de Dados** | ComparaÃ§Ã£o e correÃ§Ã£o de datasets usando padrÃµes GOLD | PySpark + Algoritmos de NormalizaÃ§Ã£o |
| **Test Dataset GOLD** | Limpeza e validaÃ§Ã£o de dataset Ãºnico com melhorias automÃ¡ticas de qualidade | Pipeline de Processamento |
| **Dataset Metrics** | AnÃ¡lise abrangente de qualidade de dados com mÃ©tricas e dashboard visual | AnÃ¡lise EstatÃ­stica + VisualizaÃ§Ã£o |
| **Sistema RAG de Suporte** | Chat inteligente com documentaÃ§Ã£o usando retrieval-augmented generation | LLM + Vector Store |

---

## 2. Persona 1: Testador/QA

### 2.1 Perfil DemogrÃ¡fico

**Nome**: Marina Silva  
**Idade**: 28-35 anos  
**Cargo**: Analista de QA SÃªnior / Engenheiro de Testes  
**FormaÃ§Ã£o**: GraduaÃ§Ã£o em CiÃªncia da ComputaÃ§Ã£o, Engenharia de Software ou Sistemas de InformaÃ§Ã£o  
**ExperiÃªncia**: 5-8 anos em testes de software, com foco crescente em dados  
**LocalizaÃ§Ã£o**: SÃ£o Paulo/SP - Trabalho hÃ­brido em empresa de tecnologia financeira

### 2.2 CaracterÃ­sticas Profissionais

#### Habilidades TÃ©cnicas
- âœ… Conhecimento intermediÃ¡rio de SQL
- âœ… Familiaridade com conceitos de Big Data
- âœ… ExperiÃªncia com ferramentas de automaÃ§Ã£o de testes
- âš ï¸ Conhecimento bÃ¡sico/intermediÃ¡rio de Python
- âš ï¸ CompreensÃ£o limitada de PySpark
- âŒ Pouca experiÃªncia com modelos de ML

#### CompetÃªncias Comportamentais
- ğŸ¯ AtenÃ§Ã£o meticulosa aos detalhes
- ğŸ“Š Pensamento sistemÃ¡tico e orientado a processos
- ğŸ” Habilidade analÃ­tica para identificar inconsistÃªncias
- ğŸ“ Excelente documentaÃ§Ã£o e comunicaÃ§Ã£o
- â±ï¸ OrientaÃ§Ã£o para prazos e entregas

### 2.3 Desafios e Dores

| Categoria | Desafio | Impacto |
|-----------|---------|---------|
| **CriaÃ§Ã£o de Dados de Teste** | Gerar dados realistas e diversificados manualmente consome muito tempo | Alto - 40% do tempo de trabalho |
| **ValidaÃ§Ã£o de Qualidade** | Escrever cÃ³digo PySpark complexo sem expertise tÃ©cnica profunda | MÃ©dio - DependÃªncia de outros times |
| **Cobertura de Casos de Teste** | Garantir teste de todos os cenÃ¡rios edge cases e combinaÃ§Ãµes possÃ­veis | Alto - Risco de bugs em produÃ§Ã£o |
| **AutomaÃ§Ã£o de Testes** | Criar scripts de validaÃ§Ã£o reutilizÃ¡veis sem conhecimento avanÃ§ado de programaÃ§Ã£o | MÃ©dio - EficiÃªncia reduzida |
| **DocumentaÃ§Ã£o** | Manter documentaÃ§Ã£o tÃ©cnica atualizada e acessÃ­vel | Baixo - EsforÃ§o manual |

### 2.4 Objetivos e MotivaÃ§Ãµes

#### Objetivos Profissionais
1. **Aumentar cobertura de testes** em 60% no prÃ³ximo trimestre
2. **Reduzir tempo de preparaÃ§Ã£o de dados** de teste de 2 dias para 2 horas
3. **Automatizar 80% das validaÃ§Ãµes** de qualidade de dados
4. **Eliminar dependÃªncia** de equipes de engenharia para tarefas bÃ¡sicas
5. **Detectar bugs** antes que cheguem Ã  produÃ§Ã£o

#### MotivaÃ§Ãµes Pessoais
- ğŸ’ª Autonomia tÃ©cnica e reduÃ§Ã£o de dependÃªncias
- ğŸ“ˆ Desenvolvimento de habilidades em Big Data e automaÃ§Ã£o
- ğŸ† Reconhecimento como especialista em qualidade de dados
- âš¡ EficiÃªncia e otimizaÃ§Ã£o do tempo de trabalho
- ğŸ“ Aprendizado contÃ­nuo de novas tecnologias

### 2.5 Casos de Uso por Funcionalidade

#### 2.5.1 ğŸ¤– GeraÃ§Ã£o de CÃ³digo PySpark (QA Checklist)

**CenÃ¡rio**: Marina precisa criar scripts de validaÃ§Ã£o para uma nova pipeline de dados de transaÃ§Ãµes bancÃ¡rias.

**Problema Anterior**:
- Levava 3-4 dias para escrever cÃ³digo PySpark
- Dependia de revisÃ£o da equipe de engenharia
- Erros de sintaxe frequentes
- Dificuldade em implementar regras complexas

**Como Marina Usa o DataForgeTest**: 

```
1. Acessa a interface de chat do QA Checklist
2. Descreve em linguagem natural: 
   "Preciso validar transaÃ§Ãµes bancÃ¡rias onde: 
    - O valor seja maior que R$ 0
    - A data nÃ£o seja futura
    - O CPF do cliente seja vÃ¡lido
    - NÃ£o existam duplicatas por ID de transaÃ§Ã£o"
3. Recebe cÃ³digo PySpark completo e comentado
4. Faz ajustes conversacionais:  "Adicione validaÃ§Ã£o para horÃ¡rio comercial"
5. Copia cÃ³digo gerado e integra no pipeline
```

**Resultados**:
- â±ï¸ Tempo reduzido de 3 dias para 30 minutos
- âœ… CÃ³digo validado e pronto para produÃ§Ã£o
- ğŸ“ Aprendizado progressivo de PySpark
- ğŸ”„ ReutilizaÃ§Ã£o de padrÃµes para novos projetos

**BenefÃ­cios EspecÃ­ficos**:
- Interface conversacional elimina curva de aprendizado
- CÃ³digo gerado segue melhores prÃ¡ticas
- DocumentaÃ§Ã£o inline explica cada validaÃ§Ã£o
- HistÃ³rico de conversas serve como base de conhecimento

---

#### 2.5.2 ğŸ” Gerador AvanÃ§ado PySpark

**CenÃ¡rio**: Marina recebeu um arquivo CSV de 500MB com dados de clientes e precisa criar validaÃ§Ãµes rapidamente.

**Problema Anterior**:
- AnÃ¡lise manual de schemas demorada
- Incerteza sobre tipos de dados corretos
- Dificuldade em identificar regras de validaÃ§Ã£o apropriadas
- Erro humano na definiÃ§Ã£o de constraints

**Como Marina Usa o DataForgeTest**:

```
1. Faz upload do arquivo CSV (clientes.csv)
2. Sistema detecta automaticamente:
   - 23 colunas com tipos inferidos
   - EstatÃ­sticas:  98.5% completude, 1.2% duplicatas
   - Colunas Ãºnicas sugeridas: customer_id, email
   - Ranges sugeridos: idade (18-90), salÃ¡rio (1000-50000)
3. Marina revisa e edita metadados: 
   - Marca email e CPF como obrigatÃ³rios
   - Define customer_id como chave Ãºnica
   - Ajusta range de data_nascimento
4. Clica em "Generate Code"
5. Recebe cÃ³digo PySpark completo com: 
   - Schema validation
   - Not-null checks
   - Uniqueness validation
   - Range checks
   - Format validations
6. Exporta para . py ou copia para Colab
```

**Resultados**:
- âš¡ AnÃ¡lise instantÃ¢nea vs. 2 horas manual
- ğŸ¯ 100% de precisÃ£o em tipos de dados
- ğŸ“Š ValidaÃ§Ãµes baseadas em anÃ¡lise estatÃ­stica real
- ğŸ”§ CÃ³digo pronto para Google Colab

**BenefÃ­cios EspecÃ­ficos**:
- Elimina adivinhaÃ§Ã£o em definiÃ§Ã£o de schema
- Detecta automaticamente padrÃµes e anomalias
- Gera cÃ³digo otimizado e testado
- Permite iteraÃ§Ã£o rÃ¡pida e ajustes

---

#### 2.5.3 ğŸ² GeraÃ§Ã£o de Dados SintÃ©ticos

**CenÃ¡rio**: Marina precisa testar uma nova funcionalidade que processa 100. 000 transaÃ§Ãµes de e-commerce diÃ¡rias.

**Problema Anterior**:
- Dados de produÃ§Ã£o inacessÃ­veis por questÃµes de privacidade (LGPD)
- Criar dados manualmente Ã© inviÃ¡vel em escala
- Dados gerados aleatoriamente nÃ£o representam cenÃ¡rios reais
- Falta de diversidade para testar edge cases

**Como Marina Usa o DataForgeTest**:

```
1. Acessa "Generate Synthetic Dataset"
2. Define schema interativo:
   - transaction_id: UUID (unique)
   - customer_email: Email
   - product_name: Product Name
   - price: Price (R$ 10 - R$ 9999, 2 decimais)
   - quantity: Integer (1-10)
   - order_date: DateTime (Ãºltimos 90 dias)
   - payment_status: Category (approved: 70%, pending:20%, rejected:10%)
   - delivery_address: Address
3. Configura: 
   - 100. 000 linhas
   - Formato:  CSV
   - Locale: pt_BR
4. Clica em "Generate Dataset"
5. Aguarda processamento (3-5 minutos)
6. Baixa arquivo pronto para testes
```

**Casos de Teste Cobertos**:
- âœ… Volume (performance com 100k registros)
- âœ… Variedade (preÃ§os, produtos, status diferentes)
- âœ… Realismo (emails vÃ¡lidos, endereÃ§os brasileiros)
- âœ… Edge cases (valores mÃ­nimos/mÃ¡ximos, datas limites)
- âœ… DistribuiÃ§Ã£o (status seguem probabilidades do mundo real)

**Resultados**:
- ğŸ“Š Dataset de 100k linhas em 5 minutos vs. impossÃ­vel manualmente
- ğŸŒ Dados localizados para Brasil (pt_BR)
- ğŸ¯ 14+ tipos de dados suportados
- ğŸ’¾ MÃºltiplos formatos de saÃ­da (CSV, JSON, Parquet, XLSX)

**BenefÃ­cios EspecÃ­ficos**:
- Compliance com LGPD (dados totalmente sintÃ©ticos)
- Reprodutibilidade de testes
- Escala de 1 a 1. 000.000 de registros
- Controle preciso sobre distribuiÃ§Ãµes estatÃ­sticas

---

#### 2.5.4 ğŸ¯ ValidaÃ§Ã£o de AcurÃ¡cia de Dados

**CenÃ¡rio**: Marina precisa validar se a migraÃ§Ã£o de dados de clientes do sistema legado para o novo sistema foi bem-sucedida.

**Problema Anterior**:
- ComparaÃ§Ã£o manual de milhares de registros
- DiferenÃ§as em formataÃ§Ã£o dificultam comparaÃ§Ã£o direta
- Sem visibilidade de qual % dos dados estÃ¡ correto
- ImpossÃ­vel gerar dataset corrigido automaticamente

**Como Marina Usa o DataForgeTest**:

```
1. Acessa "Data Accuracy Validation"
2. Upload do GOLD (sistema legado - fonte confiÃ¡vel):
   - clientes_legado.csv (50. 000 registros)
3. Upload do TARGET (novo sistema - a ser validado):
   - clientes_novo.parquet (49.850 registros)
4. Mapeia colunas:
   - Key columns: CPF, email (identificadores Ãºnicos)
   - Value columns: nome, telefone, saldo (dados a comparar)
5. Configura normalizaÃ§Ã£o:
   â˜‘ï¸ Normalizar acentos
   â˜‘ï¸ Remover pontuaÃ§Ã£o
   â˜‘ï¸ Lowercase
   - TolerÃ¢ncia numÃ©rica:  0.01 (para saldos)
   - PolÃ­tica de duplicatas: keep_last
6. Executa comparaÃ§Ã£o
7. Recebe relatÃ³rio detalhado: 
   - AcurÃ¡cia geral:  94.2%
   - 150 registros ausentes no TARGET
   - 320 diferenÃ§as de valores detectadas
   - Tabela paginada com todas as diferenÃ§as
8. Baixa dataset corrigido e relatÃ³rio JSON
```

**AnÃ¡lise do RelatÃ³rio**:
```json
{
  "summary": {
    "gold_records": 50000,
    "target_records": 49850,
    "matched_records": 47130,
    "accuracy":  94.26,
    "missing_in_target": 150,
    "extra_in_target": 0,
    "value_differences": 320
  },
  "differences": [
    {
      "key": "123. 456.789-00",
      "column": "saldo",
      "gold_value": 1500. 00,
      "target_value": 150.00,
      "issue": "decimal_place_error"
    }
  ]
}
```

**Resultados**:
- ğŸ“Š ValidaÃ§Ã£o de 50k registros em 2 minutos vs. semanas manual
- ğŸ¯ PrecisÃ£o de 94.26% com relatÃ³rio detalhado
- ğŸ”§ Dataset corrigido gerado automaticamente
- ğŸ“‹ EvidÃªncia documentada para stakeholders

**BenefÃ­cios EspecÃ­ficos**:
- Suporte a mÃºltiplos formatos (CSV, XLSX, Parquet)
- NormalizaÃ§Ã£o inteligente elimina falsos positivos
- RelatÃ³rios exportÃ¡veis para auditoria
- IdentificaÃ§Ã£o precisa de tipos de erros

---

#### 2.5.5 ğŸŒŸ Test Dataset GOLD

**CenÃ¡rio**: Marina recebeu um arquivo Excel "sujo" de fornecedor externo com dados de produtos que precisa ser limpo antes dos testes.

**Problema Anterior**:
- Limpeza manual de dados em Excel/Python demorada
- InconsistÃªncias em formatos (datas, nÃºmeros)
- Colunas vazias desperdiÃ§ando espaÃ§o
- Headers com caracteres especiais e espaÃ§os
- Duplicatas nÃ£o identificadas

**Como Marina Usa o DataForgeTest**: 

```
1. Acessa "Test Dataset GOLD"
2. Upload do arquivo problemÃ¡tico:  produtos_fornecedor.xlsx
3. Visualiza preview inicial:
   - 15. 000 linhas
   - 30 colunas (5 completamente vazias)
   - Headers:  "CÃ³digo SKU ", "PreÃ§o (R$)", "Data  Cadastro"
4. Seleciona operaÃ§Ãµes de limpeza:
   â˜‘ï¸ Remove empty columns
   â˜‘ï¸ Normalize headers (â†’ codigo_sku, preco_rs, data_cadastro)
   â˜‘ï¸ Trim strings
   â˜‘ï¸ Coerce numeric values
   â˜‘ï¸ Parse dates
   â˜‘ï¸ Remove duplicate rows
5. Clica em "Generate GOLD"
6. Acompanha progresso em tempo real:
   - Phase 1: Analyzing structure (10%)
   - Phase 2: Normalizing headers (30%)
   - Phase 3: Cleaning values (60%)
   - Phase 4: Removing duplicates (80%)
   - Phase 5: Generating report (100%)
7. Recebe relatÃ³rio de limpeza:
   - Linhas:  15.000 â†’ 14.650 (350 duplicatas removidas)
   - Colunas: 30 â†’ 25 (5 vazias removidas)
   - Nulls reduzidos:  preco_rs (15% â†’ 2%), data_cadastro (20% â†’ 5%)
8. Baixa arquivo limpo em CSV + XLSX original
```

**OperaÃ§Ãµes Aplicadas**: 

| OperaÃ§Ã£o | Antes | Depois |
|----------|-------|--------|
| **Headers** | "CÃ³digo SKU " | "codigo_sku" |
| **Strings** | " notebook  " | "notebook" |
| **NÃºmeros** | "1. 500,00" (BR) | 1500.00 |
| **Datas** | "25/12/2024" | "2024-12-25" |
| **Duplicatas** | 15.000 linhas | 14.650 linhas |
| **Colunas vazias** | 30 colunas | 25 colunas |

**Resultados**:
- ğŸ§¹ Dataset 100% limpo em 3 minutos vs. 4 horas manual
- ğŸ“Š RelatÃ³rio detalhado de todas as transformaÃ§Ãµes
- ğŸ’¾ MÃºltiplos formatos de download
- âœ… Pronto para testes imediatos

**BenefÃ­cios EspecÃ­ficos**:
- Processamento em chunks para arquivos grandes (50MB+)
- Preview de 50 linhas para validaÃ§Ã£o
- OperaÃ§Ãµes configurÃ¡veis por checkbox
- Preserva formato original + gera CSV padronizado

---

#### 2.5.6 ğŸ“Š Dataset Metrics

**CenÃ¡rio**: Marina precisa avaliar a qualidade de um dataset de vendas antes de usÃ¡-lo para testes, gerando evidÃªncias para o relatÃ³rio de QA.

**Problema Anterior**:
- Sem visibilidade de qualidade antes de comeÃ§ar testes
- Descoberta de problemas apenas durante execuÃ§Ã£o
- Sem mÃ©tricas objetivas para reportar Ã  gestÃ£o
- Dificuldade em priorizar esforÃ§os de limpeza

**Como Marina Usa o DataForgeTest**: 

```
1. Acessa "Dataset Metrics"
2. Upload do arquivo:  vendas_q4_2024.csv (80. 000 registros)
3. Sistema analisa automaticamente 4 dimensÃµes
4. Recebe dashboard visual detalhado
```

**Dashboard Apresentado**: 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         OVERALL QUALITY SCORE:  76. 8%                â”‚
â”‚              â­â­â­ (Good)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMPLETENESS    â”‚   UNIQUENESS     â”‚    VALIDITY      â”‚   CONSISTENCY    â”‚
â”‚                  â”‚                  â”‚                  â”‚                  â”‚
â”‚      85.2%       â”‚      92.5%       â”‚      68.0%       â”‚      75.0%       â”‚
â”‚   âš ï¸ Attention   â”‚   âœ… Excellent   â”‚   âŒ Critical    â”‚   âš ï¸ Moderate    â”‚
â”‚   (Weight: 30%)  â”‚   (Weight: 20%)  â”‚   (Weight: 30%)  â”‚   (Weight: 20%)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“Š DATASET INFORMATION
  â€¢ Total Rows: 80,000
  â€¢ Total Columns: 18
  â€¢ Memory Usage: 12.5 MB
  â€¢ Null Ratio (Overall): 14.8%

ğŸš¨ RECOMMENDATIONS (Sorted by Severity)

HIGH SEVERITY: 
  â— Validity is critically low (68.0%)
     â†’ Action:  Review data types and formats for inconsistent columns
  
  â— Column 'customer_email' has 25% invalid values
     â†’ Action: Implement email validation and cleanup

MEDIUM SEVERITY:
  âš ï¸ Completeness below 90% (85.2%)
     â†’ Action: Investigate and handle missing values in key columns
  
  âš ï¸ Column 'order_date' has inconsistent date formats
     â†’ Action: Standardize date format across dataset

LOW SEVERITY:
  â„¹ï¸ Consider indexing 'customer_id' for better performance
```

**MÃ©tricas Detalhadas por Coluna**:

| Coluna | Completeness | Validity | Consistency | Issue |
|--------|--------------|----------|-------------|-------|
| customer_id | 100% | 100% | 100% | âœ… Perfect |
| customer_email | 95% | 75% | 80% | âŒ Format issues |
| order_date | 90% | 85% | 65% | âš ï¸ Mixed formats |
| product_price | 85% | 95% | 90% | âš ï¸ Some nulls |
| quantity | 100% | 100% | 100% | âœ… Perfect |

**AÃ§Ãµes que Marina Toma**: 
1. **Documenta no relatÃ³rio de QA**:  Score de 76.8% requer atenÃ§Ã£o
2. **Prioriza correÃ§Ãµes**: Foca em validity (68%) - mais crÃ­tico
3. **Cria plano de aÃ§Ã£o**:
   - Validar e corrigir emails (25% invÃ¡lidos)
   - Padronizar datas (inconsistÃªncia de 65%)
   - Tratar nulls em preÃ§os (15% missing)
4. **Define critÃ©rios de aceite**: PrÃ³xima versÃ£o deve ter score > 90%
5. **Agenda reuniÃ£o** com time de dados mostrando evidÃªncias objetivas

**Resultados**:
- ğŸ“Š AvaliaÃ§Ã£o completa em 30 segundos vs. horas de anÃ¡lise manual
- ğŸ¯ MÃ©tricas objetivas para tomada de decisÃ£o
- ğŸ“ˆ Score visual facilita comunicaÃ§Ã£o com nÃ£o-tÃ©cnicos
- ğŸ” RecomendaÃ§Ãµes priorizadas por severidade

**BenefÃ­cios EspecÃ­ficos**:
- 4 dimensÃµes de qualidade padronizadas
- Pesos configurÃ¡veis por indÃºstria/necessidade
- Dashboard exportÃ¡vel para relatÃ³rios
- Baseline para comparaÃ§Ã£o temporal

---

#### 2.5.7 ğŸ’¬ Sistema RAG de Suporte

**CenÃ¡rio**: Marina estÃ¡ criando validaÃ§Ãµes complexas e precisa de ajuda para entender conceitos de PySpark e melhores prÃ¡ticas.

**Problema Anterior**:
- DocumentaÃ§Ã£o fragmentada em mÃºltiplas fontes
- Tempo gasto buscando em Stack Overflow e docs oficiais
- Respostas genÃ©ricas que nÃ£o consideram contexto do DataForgeTest
- Dificuldade em encontrar exemplos especÃ­ficos

**Como Marina Usa o DataForgeTest**:

```
Exemplo de Conversa: 

Marina: "Como faÃ§o para validar se uma coluna de CPF estÃ¡ no formato correto?"

DataForgeTest RAG:  "Para validar formato de CPF em PySpark usando DataForgeTest, 
vocÃª pode usar regex validation.  Aqui estÃ¡ um exemplo: 

```python
from pyspark.sql.functions import regexp_extract, col

# PadrÃ£o CPF:  XXX.XXX.XXX-XX ou XXXXXXXXXXX
cpf_pattern = r'^\d{3}\.?\d{3}\.?\d{3}-?\d{2}$'

df_validated = df.filter(
    col('cpf').rlike(cpf_pattern)
)
```

VocÃª tambÃ©m pode usar o chatbot QA Checklist para gerar essa validaÃ§Ã£o 
automaticamente.  Apenas descreva:  'Validar formato de CPF brasileiro'

ğŸ“š Fonte: docs/DATA_VALIDATION_PATTERNS.md"

Marina: "E se eu quiser validar o dÃ­gito verificador tambÃ©m?"

DataForgeTest RAG: "Para validaÃ§Ã£o completa incluindo dÃ­gitos verificadores, 
recomendo criar uma UDF (User Defined Function). Aqui estÃ¡ uma implementaÃ§Ã£o:

```python
from pyspark.sql.functions import udf
from pyspark. sql.types import BooleanType

def validar_cpf(cpf_str):
    if not cpf_str:
        return False
    
    # Remove caracteres nÃ£o numÃ©ricos
    cpf = ''.join(filter(str.isdigit, cpf_str))
    
    if len(cpf) != 11:
        return False
    
    # ValidaÃ§Ã£o dos dÃ­gitos verificadores
    # [implementaÃ§Ã£o completa do algoritmo]
    ... 
    return True

validar_cpf_udf = udf(validar_cpf, BooleanType())
df_validated = df.filter(validar_cpf_udf(col('cpf')))
```

ğŸ’¡ Dica: Para datasets grandes (>1M registros), considere usar pandas_udf 
para melhor performance. 

ğŸ“š Fonte: docs/ADVANCED_VALIDATION_TECHNIQUES.md"
```

**Tipos de Perguntas que Marina Faz**: 

| Categoria | Exemplo de Pergunta | Tipo de Resposta |
|-----------|---------------------|------------------|
| **Como fazer** | "Como detectar duplicatas em PySpark?" | CÃ³digo + explicaÃ§Ã£o |
| **Troubleshooting** | "Por que minha validaÃ§Ã£o estÃ¡ muito lenta?" | DiagnÃ³stico + soluÃ§Ã£o |
| **Conceitual** | "Qual a diferenÃ§a entre filter e where?" | ExplicaÃ§Ã£o tÃ©cnica |
| **Boas prÃ¡ticas** | "Como organizar validaÃ§Ãµes complexas?" | PadrÃµes recomendados |
| **Funcionalidades** | "Quais tipos de dados sintÃ©ticos posso gerar?" | Lista + exemplos |

**Resultados**:
- âš¡ Respostas instantÃ¢neas vs. 15-30 minutos de busca
- ğŸ¯ Contexto especÃ­fico do DataForgeTest
- ğŸ“š Sempre com referÃªncia Ã  documentaÃ§Ã£o fonte
- ğŸ’¡ SugestÃµes proativas de melhores prÃ¡ticas

**BenefÃ­cios EspecÃ­ficos**:
- RAG busca em documentaÃ§Ã£o indexada
- Respostas estruturadas e formatadas
- CÃ³digo pronto para copiar e usar
- Aprendizado progressivo e autodidata

---

### 2.6 Fluxo de Trabalho TÃ­pico (Dia de Marina)

```
08:00 - Chega no escritÃ³rio e verifica backlog
      â†’ 3 novos testes a implementar para pipeline de vendas

08:30 - Usa Dataset Metrics para avaliar qualidade dos dados recebidos
      â†’ Score 72% - identifica problemas de completeness
      
09:00 - Usa Test Dataset GOLD para limpar arquivo do fornecedor
      â†’ Remove duplicatas e normaliza formatos
      
09:30 - Usa GeraÃ§Ã£o de Dados SintÃ©ticos para criar 50k registros de teste
      â†’ Define schema com 12 colunas realistas
      â†’ Inclui edge cases (valores mÃ­nimos/mÃ¡ximos)
      
10:00 - Coffee break enquanto dados sÃ£o gerados

10:15 - Usa QA Checklist (Chat) para gerar validaÃ§Ãµes PySpark
      â†’ Descreve 8 regras de negÃ³cio em linguagem natural
      â†’ Recebe cÃ³digo completo e documentado
      
11:00 - Testa cÃ³digo gerado no ambiente de dev
      â†’ Pequenos ajustes conversacionais no chat
      
11:30 - Usa ValidaÃ§Ã£o de AcurÃ¡cia para comparar resultado esperado vs. obtido
      â†’ AcurÃ¡cia 98.5% - identifica 15 edge cases nÃ£o cobertos
      
12:00 - AlmoÃ§o

13:00 - Usa RAG Support para tirar dÃºvida sobre performance de UDF
      â†’ Recebe sugestÃ£o de otimizaÃ§Ã£o com pandas_udf
      
13:30 - Implementa melhorias e roda testes finais

14:00 - Gera relatÃ³rio de QA com: 
      â†’ MÃ©tricas de qualidade (Dataset Metrics)
      â†’ EvidÃªncias de validaÃ§Ã£o (AcurÃ¡cia)
      â†’ CÃ³digo de teste (PySpark gerado)
      â†’ Screenshots do dashboard
      
15:00 - Apresenta resultados para tech lead
      â†’ 100% de cobertura de testes alcanÃ§ada
      â†’ 3 bugs crÃ­ticos identificados antes de produÃ§Ã£o
      â†’ Tempo total:  6 horas vs. estimativa anterior de 3 dias

15:30 - Documenta processo e atualiza base de conhecimento

16:00 - Usa Gerador AvanÃ§ado PySpark para prÃ³ximo projeto
      â†’ Upload de novo dataset para anÃ¡lise
      
17:00 - Fim do expediente - satisfaÃ§Ã£o com produtividade
```

**MÃ©tricas do Dia**: 
- âœ… 3 testes completos implementados (meta: 2)
- âœ… 3 bugs crÃ­ticos detectados
- âœ… 100% cobertura de casos de teste
- âœ… 50. 000 registros sintÃ©ticos gerados
- âœ… 2 datasets validados e corrigidos
- âœ… 1 apresentaÃ§Ã£o tÃ©cnica para gestÃ£o
- â±ï¸ Economia de tempo: 2 dias (16 horas)

---

## 3. Persona 2: Cientista de Dados

### 3.1 Perfil DemogrÃ¡fico

**Nome**: Dr. Ricardo Oliveira  
**Idade**: 32-42 anos  
**Cargo**: Cientista de Dados SÃªnior / Lead Data Scientist  
**FormaÃ§Ã£o**: PhD/Mestrado em EstatÃ­stica, MatemÃ¡tica Aplicada ou CiÃªncia da ComputaÃ§Ã£o  
**ExperiÃªncia**: 8-12 anos em anÃ¡lise de dados e machine learning  
**LocalizaÃ§Ã£o**: Remoto - Trabalha para startup de e-commerce de grande porte

### 3.2 CaracterÃ­sticas Profissionais

#### Habilidades TÃ©cnicas
- âœ… Expertise avanÃ§ada em Python (pandas, numpy, scikit-learn)
- âœ… ProficiÃªncia em PySpark e ambientes distribuÃ­dos
- âœ… Conhecimento profundo de estatÃ­stica e ML
- âœ… ExperiÃªncia com pipelines de dados end-to-end
- âœ… Familiaridade com notebooks (Jupyter, Colab)
- âš ï¸ Tempo limitado para tarefas operacionais
- âš ï¸ PreferÃªncia por automaÃ§Ã£o vs. trabalho manual

#### CompetÃªncias Comportamentais
- ğŸ§  Pensamento analÃ­tico e orientado a dados
- ğŸ”¬ Abordagem cientÃ­fica e baseada em experimentos
- ğŸ“Š Habilidade de visualizaÃ§Ã£o e storytelling com dados
- âš¡ Busca por eficiÃªncia e otimizaÃ§Ã£o
- ğŸš€ Foco em entrega de valor de negÃ³cio
- ğŸ¤ ColaboraÃ§Ã£o cross-funcional com engenharia

### 3.3 Desafios e Dores

| Categoria | Desafio | Impacto |
|-----------|---------|---------|
| **Qualidade de Dados** | 60-80% do tempo gasto em limpeza e preparaÃ§Ã£o de dados | Alto - Reduz tempo de modelagem |
| **Dados para Experimentos** | Falta de dados sintÃ©ticos realistas para treinar modelos sem expor dados sensÃ­veis | Alto - Compliance e privacidade |
| **ValidaÃ§Ã£o de Pipelines** | Dificuldade em criar testes abrangentes para pipelines complexos de ETL | MÃ©dio - Risco de bugs em produÃ§Ã£o |
| **DocumentaÃ§Ã£o de Schema** | Documentar e versionar schemas de dados manualmente | Baixo - Trabalho tedioso |
| **Debugging de Dados** | Identificar rapidamente problemas de qualidade em datasets grandes | Alto - Atraso em projetos |

### 3.4 Objetivos e MotivaÃ§Ãµes

#### Objetivos Profissionais
1. **Reduzir tempo de prep de dados** de 60% para 30% do workflow
2. **Criar datasets sintÃ©ticos** para treinar modelos sem expor PII
3. **Automatizar validaÃ§Ã£o de qualidade** em pipelines de produÃ§Ã£o
4. **Acelerar experimentaÃ§Ã£o** com dados limpos e prontos
5. **Garantir reprodutibilidade** de experimentos com dados versionados

#### MotivaÃ§Ãµes Pessoais
- ğŸ§  Focar em problemas complexos de ML, nÃ£o em limpeza de dados
- ğŸ† Publicar papers e contribuir para comunidade cientÃ­fica
- ğŸ’¡ Inovar com novas tÃ©cnicas e abordagens
- âš¡ Otimizar tempo para aumentar produtividade
- ğŸ“ Mentorar times jÃºnior com ferramentas modernas

### 3.5 Casos de Uso por Funcionalidade

#### 3.5.1 ğŸ¤– GeraÃ§Ã£o de CÃ³digo PySpark (QA Checklist)

**CenÃ¡rio**: Ricardo precisa criar validaÃ§Ãµes de qualidade de dados para um pipeline de features de ML que processa 10 milhÃµes de eventos de comportamento de usuÃ¡rio.

**Problema Anterior**:
- Escrever cÃ³digo PySpark de validaÃ§Ã£o Ã© tedioso e repetitivo
- Foco precisa estar na engenharia de features, nÃ£o em boilerplate
- ValidaÃ§Ãµes complexas exigem lÃ³gica nÃ£o trivial
- DifÃ­cil manter consistÃªncia entre diferentes pipelines

**Como Ricardo Usa o DataForgeTest**:

```
Ricardo no Chat: 

"Preciso validar um dataset de eventos de usuÃ¡rio com: 
- user_id: nÃ£o nulo e formato UUID vÃ¡lido
- event_timestamp: nÃ£o nulo, nÃ£o futuro, dentro dos Ãºltimos 90 dias
- event_type:  deve estar em ['view', 'click', 'purchase', 'add_to_cart']
- session_duration: se event_type = 'purchase', session_duration deve ser > 0
- revenue: se event_type = 'purchase', revenue deve estar entre 1 e 100000
- Sem duplicatas de (user_id + event_timestamp)
- DistribuiÃ§Ã£o de event_type:  'view' deve representar 60-80% dos eventos"

Resposta (CÃ³digo PySpark Gerado):

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, count, lit
import pyspark.sql.functions as F
from datetime import datetime, timedelta

# Initialize Spark
spark = SparkSession.builder. appName("UserEventsValidation").getOrCreate()

# Load dataset
df = spark.read. parquet("user_events. parquet")

# 1. NOT NULL VALIDATIONS
print("=== NOT NULL CHECKS ===")
null_checks = {
    "user_id": df.filter(col("user_id").isNull()).count(),
    "event_timestamp": df.filter(col("event_timestamp").isNull()).count()
}
for col_name, null_count in null_checks.items():
    print(f"{col_name}: {null_count} nulls found")

# 2. FORMAT VALIDATIONS
print("\n=== FORMAT CHECKS ===")
# UUID format:  xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
uuid_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'
invalid_uuids = df.filter(~col("user_id").rlike(uuid_pattern)).count()
print(f"Invalid UUIDs: {invalid_uuids}")

# 3. DATE RANGE VALIDATIONS
print("\n=== DATE RANGE CHECKS ===")
now = datetime.now()
ninety_days_ago = now - timedelta(days=90)

future_events = df.filter(col("event_timestamp") > lit(now)).count()
old_events = df.filter(col("event_timestamp") < lit(ninety_days_ago)).count()
print(f"Future events: {future_events}")
print(f"Events older than 90 days: {old_events}")

# 4. VALUE SET VALIDATIONS
print("\n=== VALUE SET CHECKS ===")
valid_event_types = ['view', 'click', 'purchase', 'add_to_cart']
invalid_types = df.filter(~col("event_type").isin(valid_event_types)).count()
print(f"Invalid event_types: {invalid_types}")

# 5. CONDITIONAL VALIDATIONS
print("\n=== CONDITIONAL CHECKS ===")
# If event_type = 'purchase', session_duration must be > 0
invalid_purchase_duration = df.filter(
    (col("event_type") == "purchase") & 
    ((col("session_duration").isNull()) | (col("session_duration") <= 0))
).count()
print(f"Purchases with invalid session_duration: {invalid_purchase_duration}")

# If event_type = 'purchase', revenue must be between 1 and 100000
invalid_purchase_revenue = df.filter(
    (col("event_type") == "purchase") & 
    ((col("revenue") < 1) | (col("revenue") > 100000))
).count()
print(f"Purchases with invalid revenue: {invalid_purchase_revenue}")

# 6. UNIQUENESS VALIDATIONS
print("\n=== UNIQUENESS CHECKS ===")
total_records = df.count()
unique_records = df.dropDuplicates(["user_id", "event_timestamp"]).count()
duplicates = total_records - unique_records
print(f"Duplicate (user_id + event_timestamp): {duplicates}")

# 7. DISTRIBUTION VALIDATIONS
print("\n=== DISTRIBUTION CHECKS ===")
event_dist = df.groupBy("event_type").count()
total = df.count()
event_dist = event_dist.withColumn("percentage", (col("count") / total * 100))
event_dist. show()

view_percentage = event_dist.filter(col("event_type") == "view") \
                            .select("percentage").first()[0]
if not (60 <= view_percentage <= 80):
    print(f"âš ï¸  WARNING: 'view' events are {view_percentage:.2f}%, expected 60-80%")
else:
    print(f"âœ… 'view' distribution OK:  {view_percentage:.2f}%")

# 8. GENERATE CLEAN DATASET
print("\n=== GENERATING CLEAN DATASET ===")
df_clean = df.filter(
    col("user_id").isNotNull() &
    col("user_id").rlike(uuid_pattern) &
    col("event_timestamp").isNotNull() &
    (col("event_timestamp") <= lit(now)) &
    (col("event_timestamp") >= lit(ninety_days_ago)) &
    col("event_type").isin(valid_event_types)
).dropDuplicates(["user_id", "event_timestamp"])

# Apply conditional filters
df_clean = df_clean.filter(
    ~((col("event_type") == "purchase") & 
      ((col("session_duration").isNull()) | (col("session_duration") <= 0))) &
    ~((col("event_type") == "purchase") & 
      ((col("revenue") < 1) | (col("revenue") > 100000)))
)

print(f"Original records: {total_records}")
print(f"Clean records: {df_clean. count()}")
print(f"Removed:  {total_records - df_clean.count()} ({((total_records - df_clean. count()) / total_records * 100):.2f}%)")

# Save clean dataset
df_clean.write.mode("overwrite").parquet("user_events_clean.parquet")
print("\nâœ… Clean dataset saved to user_events_clean.parquet")
```

```

**O que Ricardo Faz Depois**: 
1. Copia cÃ³digo para notebook do Google Colab
2. Executa validaÃ§Ãµes no dataset de 10M registros
3. Identifica problemas: 
   - 150k UUIDs invÃ¡lidos (1.5%)
   - 8. 2k eventos futuros (bug no sistema de coleta)
   - 'view' representa 85% (fora do esperado 60-80%)
4. Reporta bugs para equipe de engenharia com evidÃªncias
5. Usa dataset limpo para treinar modelo de recomendaÃ§Ã£o
6. Integra cÃ³digo no pipeline de produÃ§Ã£o como validaÃ§Ã£o contÃ­nua

**Resultados**:
- â±ï¸ CÃ³digo completo em 2 minutos vs. 45 minutos manual
- ğŸ¯ Todas as regras complexas implementadas corretamente
- ğŸ“Š Dataset limpo pronto para ML
- ğŸ”„ CÃ³digo reutilizÃ¡vel para outros pipelines

**BenefÃ­cios EspecÃ­ficos**:
- CÃ³digo otimizado para PySpark (nÃ£o pandas)
- ValidaÃ§Ãµes prontas para escala (10M+ registros)
- Formato ready-to-use em notebooks
- Logging detalhado para debugging

---

#### 3.5.2 ğŸ” Gerador AvanÃ§ado PySpark

**CenÃ¡rio**: Ricardo recebeu um dataset Parquet de 2GB com histÃ³rico de transaÃ§Ãµes de 3 anos e precisa entender schema, estatÃ­sticas e criar validaÃ§Ãµes rapidamente para usar em produÃ§Ã£o.

**Problema Anterior**:
- Carregar 2GB em pandas Ã© lento e consome memÃ³ria
- AnÃ¡lise exploratÃ³ria manual demora horas
- Documentar schema Ã© tedioso
- Criar validaÃ§Ãµes baseadas em estatÃ­sticas reais requer cÃ³digo complexo

**Como Ricardo Usa o DataForgeTest**: 

```
1. Acessa "Generate Advanced PySpark Code"
2. Upload:  transacoes_historico.parquet (2GB, 25M registros)
3. Sistema processa em chunks e detecta automaticamente: 

SCHEMA DETECTED (18 colunas):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Column              â”‚ Type      â”‚ Nulls    â”‚ Unique    â”‚ Sample     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ transaction_id      â”‚ string    â”‚ 0%       â”‚ 100%      â”‚ "tx_001"   â”‚
â”‚ customer_id         â”‚ string    â”‚ 0%       â”‚ 15. 2%     â”‚ "cus_4892" â”‚
â”‚ transaction_date    â”‚ date      â”‚ 0.02%    â”‚ 98.5%     â”‚ 2022-01-15 â”‚
â”‚ amount              â”‚ decimal   â”‚ 0.1%     â”‚ 85.3%     â”‚ 152.50     â”‚
â”‚ currency            â”‚ string    â”‚ 0%       â”‚ 0. 02%     â”‚ "BRL"      â”‚
â”‚ payment_method      â”‚ string    â”‚ 0.5%     â”‚ 0.01%     â”‚ "credit"   â”‚
â”‚ merchant_id         â”‚ string    â”‚ 0%       â”‚ 1.8%      â”‚ "merch_87" â”‚
â”‚ product_category    â”‚ string    â”‚ 2.5%     â”‚ 0.05%     â”‚ "eletro"   â”‚
â”‚ quantity            â”‚ integer   â”‚ 0%       â”‚ 0.01%     â”‚ 2          â”‚
â”‚ discount_applied    â”‚ boolean   â”‚ 0%       â”‚ 0.001%    â”‚ true       â”‚
â”‚ ...                  â”‚ ...       â”‚ ...      â”‚ ...       â”‚ ...        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

STATISTICS:
â€¢ Total Records: 25,000,000
â€¢ Date Range: 2022-01-01 to 2024-12-20 (3 years)
â€¢ Amount Range: R$ 1.50 to R$ 98,450. 00
â€¢ Quantity Range: 1 to 250
â€¢ Currencies: BRL (99.8%), USD (0.2%)
â€¢ Payment Methods: credit (65%), debit (25%), pix (8%), boleto (2%)

AUTO-GENERATED VALIDATIONS: 
âœ“ transaction_id: NOT NULL, UNIQUE
âœ“ customer_id: NOT NULL
âœ“ transaction_date: NOT NULL, RANGE (2022-01-01 to today)
âœ“ amount: NOT NULL, RANGE (0.01 to 100000.00)
âœ“ currency: NOT NULL, IN_SET (['BRL', 'USD'])
âœ“ payment_method:  IN_SET (['credit', 'debit', 'pix', 'boleto'])
âš ï¸ product_category: 2.5% nulls detected
âš ï¸ transaction_date: 0.02% nulls detected (500 records)

4. Ricardo revisa e edita: 
   - Marca transaction_id como chave primÃ¡ria
   - Ajusta range de amount para (1. 00, 100000.00) - remove outliers
   - Define product_category como opcional (nulls OK)
   - Adiciona validaÃ§Ã£o custom:  "fraud_score < 0.8"

5. Clica em "Generate DSL + PySpark Code"

6. Recebe 2 arquivos:
   a) DSL (Data Specification Language):
   b) PySpark validation code (200 linhas)
```

**DSL Gerado** (transacoes_historico_dsl.yaml):
```yaml
name: Transacoes Historico
version: 1.0
generated_at: 2024-12-22T14:30:00Z
source_file: transacoes_historico. parquet
total_records: 25000000

schema:
  primary_key: transaction_id
  
  columns:
    - name: transaction_id
      type: string
      constraints:
        - not_null
        - unique
      description: "Identificador Ãºnico da transaÃ§Ã£o"
    
    - name: customer_id
      type: string
      constraints:
        - not_null
      description: "Identificador do cliente"
    
    - name:  transaction_date
      type: date
      constraints:
        - not_null
        - range: 
            min: "2022-01-01"
            max: "today"
      statistics:
        null_ratio: 0.0002
        min:  "2022-01-01"
        max: "2024-12-20"
    
    - name: amount
      type: decimal(10,2)
      constraints:
        - not_null
        - range:
            min: 1.00
            max: 100000.00
      statistics:
        null_ratio: 0.001
        min: 1.50
        max: 98450.00
        mean: 342.85
        median: 152.50
        std_dev: 1250.30
    
    - name: currency
      type: string
      constraints:
        - not_null
        - in_set:  ['BRL', 'USD']
      statistics:
        value_distribution:
          BRL: 99.8%
          USD: 0.2%
    
    - name: payment_method
      type: string
      constraints:
        - in_set: ['credit', 'debit', 'pix', 'boleto']
      statistics:
        null_ratio: 0.005
        value_distribution:
          credit: 65%
          debit: 25%
          pix: 8%
          boleto: 2%
    
    - name: fraud_score
      type: decimal(3,2)
      constraints:
        - range:
            min: 0.00
            max: 0.80
      description: "Score de fraude - custom validation"

quality_metrics:
  completeness: 99.1%
  uniqueness: 100.0%
  validity: 98.5%
  consistency: 97.2%
  overall_score: 98.7%

recommendations:
  - "Consider partitioning by transaction_date for better performance"
  - "Index customer_id for faster lookups"
  - "Monitor fraud_score distribution for anomalies"
```

**CÃ³digo PySpark Gerado** (200 linhas - resumo):
```python
from pyspark.sql import SparkSession
from pyspark.sql. functions import col, count, when, lit, current_date
import pyspark.sql.functions as F

spark = SparkSession.builder \
    .appName("TransacoesHistoricoValidation") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()

# Load data
df = spark.read.parquet("transacoes_historico.parquet")

# Validation Report
validation_results = {}

# 1. PRIMARY KEY VALIDATION
print("=== PRIMARY KEY CHECK ===")
total = df.count()
unique = df.select("transaction_id").distinct().count()
validation_results['primary_key_unique'] = (total == unique)
...

# [continua com todas as validaÃ§Ãµes]
```

**Como Ricardo Usa os Artefatos**:

1. **DSL como DocumentaÃ§Ã£o**:
   - Commita DSL no Git junto com cÃ³digo
   - Compartilha com time de engenharia
   - Serve como contrato de dados (data contract)
   - Usado em revisÃµes de cÃ³digo

2. **CÃ³digo PySpark em ProduÃ§Ã£o**:
   - Integra no Airflow como task de validaÃ§Ã£o
   - Executa antes de treinar modelos
   - Gera alertas quando validaÃ§Ãµes falham
   - Monitora qualidade ao longo do tempo

3. **AnÃ¡lise ExploratÃ³ria**:
   - EstatÃ­sticas servem como baseline
   - Identifica anomalias rapidamente
   - Compara com datasets futuros

**Resultados**:
- ğŸ“Š AnÃ¡lise completa de 2GB/25M registros em 3 minutos
- ğŸ“ DocumentaÃ§Ã£o (DSL) gerada automaticamente
- ğŸ”§ CÃ³digo PySpark pronto para produÃ§Ã£o
- ğŸ“ˆ EstatÃ­sticas detalhadas para baseline

**BenefÃ­cios EspecÃ­ficos**:
- Processamento em chunks para arquivos grandes
- DetecÃ§Ã£o inteligente de tipos baseada em conteÃºdo
- DSL serve como contrato de dados versionÃ¡vel
- CÃ³digo otimizado com adaptive query execution
- Pronto para Google Colab e ambientes distribuÃ­dos

---

#### 3.5.3 ğŸ² GeraÃ§Ã£o de Dados SintÃ©ticos

**CenÃ¡rio**: Ricardo estÃ¡ desenvolvendo um modelo de churn prediction mas nÃ£o pode usar dados reais de clientes por questÃµes de privacidade (LGPD). Precisa de 500. 000 registros sintÃ©ticos realistas para treinar o modelo.

**Problema Anterior**:
- Dados reais contÃªm PII (personally identifiable information)
- AnonimizaÃ§Ã£o nÃ£o garante 100% de privacidade
- Dados sintÃ©ticos simples (random) nÃ£o capturam padrÃµes reais
- Criar gerador customizado levaria semanas

**Como Ricardo Usa o DataForgeTest**:

```
1. Acessa "Generate Synthetic Dataset"
2. Define schema baseado em anÃ¡lise de negÃ³cio: 

SCHEMA:  Customer Churn Dataset

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Column              â”‚ Type         â”‚ Options                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ customer_id         â”‚ uuid         â”‚ unique:  true                        â”‚
â”‚ signup_date         â”‚ date         â”‚ min: 2020-01-01, max: 2024-01-01    â”‚
â”‚ age                 â”‚ integer      â”‚ min: 18, max: 80                    â”‚
â”‚ gender              â”‚ category     â”‚ ['M': 48%, 'F':48%, 'Other':4%]      â”‚
â”‚ city                â”‚ category     â”‚ ['SP':40%, 'RJ':25%, 'MG':15%...]   â”‚
â”‚ plan_type           â”‚ category     â”‚ ['basic':50%, 'premium':35%...]     â”‚
â”‚ monthly_spending    â”‚ price        â”‚ min: 29.90, max: 299.90, dec: 2     â”‚
â”‚ support_tickets     â”‚ integer      â”‚ min: 0, max: 50                     â”‚
â”‚ login_frequency     â”‚ integer      â”‚ min: 0, max: 365 (days/year)        â”‚
â”‚ last_login          â”‚ datetime     â”‚ min: 2024-01-01, max: today         â”‚
â”‚ contract_duration   â”‚ integer      â”‚ min: 1, max: 60 (months)            â”‚
â”‚ has_family_plan     â”‚ boolean      â”‚ -                                   â”‚
â”‚ email               â”‚ email        â”‚ unique: true                        â”‚
â”‚ phone               â”‚ phone        â”‚ locale: pt_BR                       â”‚
â”‚ churned             â”‚ boolean      â”‚ - (target variable)                 â”‚
â”‚ churn_date          â”‚ date         â”‚ nullable: true                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

3. Configura:
   - Rows: 500,000
   - File Type: Parquet (melhor para ML)
   - Locale: pt_BR
   - Seed: 42 (reprodutibilidade)

4. Clica em "Generate Dataset"
5. Aguarda 15-20 minutos (progress bar em tempo real)
6. Baixa churn_synthetic_500k.parquet
```

**Dataset Gerado - Amostra**: 

| customer_id | signup_date | age | gender | city | plan_type | monthly_spending | churned |
|-------------|-------------|-----|--------|------|-----------|------------------|---------|
| 550e8400-...  | 2022-03-15 | 34 | F | SP | premium | 149.90 | false |
| 6fa459ea-... | 2020-07-22 | 45 | M | RJ | basic | 49.90 | true |
| 9d8e6f7c-... | 2023-11-08 | 28 | F | MG | premium | 199.90 | false |

**Como Ricardo Usa o Dataset**:

```python
# 1. Load synthetic data
import pandas as pd
from pyspark.sql import SparkSession

spark = SparkSession.builder. appName("ChurnModel").getOrCreate()
df = spark.read.parquet("churn_synthetic_500k.parquet")

# 2. Feature engineering
from pyspark.ml. feature import VectorAssembler, StringIndexer

# Encode categorical variables
indexers = [
    StringIndexer(inputCol="plan_type", outputCol="plan_type_idx"),
    StringIndexer(inputCol="city", outputCol="city_idx"),
    StringIndexer(inputCol="gender", outputCol="gender_idx")
]

# Assemble features
feature_cols = ['age', 'monthly_spending', 'support_tickets', 
                'login_frequency', 'contract_duration', 'has_family_plan',
                'plan_type_idx', 'city_idx', 'gender_idx']

assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")

# 3. Train/Test split
train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)

# 4. Train model
from pyspark.ml.classification import GBTClassifier

gbt = GBTClassifier(
    featuresCol="features",
    labelCol="churned",
    maxIter=100,
    maxDepth=5
)

model = gbt.fit(train_df)

# 5. Evaluate
from pyspark.ml.evaluation import BinaryClassificationEvaluator

predictions = model.transform(test_df)
evaluator = BinaryClassificationEvaluator(labelCol="churned")
auc = evaluator.evaluate(predictions)

print(f"AUC: {auc:. 4f}")  # AUC: 0.8534

# 6. Analyze feature importance
import matplotlib.pyplot as plt

feature_importance = model.featureImportances. toArray()
feature_names = feature_cols

plt.barh(feature_names, feature_importance)
plt.xlabel('Importance')
plt.title('Feature Importance - Churn Prediction')
plt.show()

# Top features:  support_tickets, login_frequency, monthly_spending
```

**Resultados do Modelo**:
- **AUC**: 0.8534 (bom desempenho)
- **Precision**: 0.82
- **Recall**: 0.79
- **F1-Score**:  0.80

**PrÃ³ximos Passos de Ricardo**: 
1. **Valida modelo com dados reais (anonimizados)** em ambiente seguro
   - AUC similar:  0.8621 âœ… (modelo generaliza bem)
2. **Documenta processo** para compliance
   - "Modelo treinado inteiramente com dados sintÃ©ticos"
   - "Zero exposiÃ§Ã£o de PII durante desenvolvimento"
3. **Compartilha dataset sintÃ©tico** com time de engenharia
   - Usado para testes de pipeline
   - Usado para desenvolvimento de dashboards
4. **Publica paper** sobre abordagem de privacy-preserving ML
5. **Deploy em produÃ§Ã£o** com confianÃ§a

**Resultados**:
- ğŸ”’ 100% compliance com LGPD
- ğŸ¯ Modelo com performance equivalente a treino com dados reais
- â±ï¸ 500k registros em 20 minutos vs.  impossÃ­vel com dados reais
- ğŸ¤ Dataset compartilhÃ¡vel com toda organizaÃ§Ã£o

**BenefÃ­cios EspecÃ­ficos**:
- Dados sintÃ©ticos preservam distribuiÃ§Ãµes estatÃ­sticas
- Categorias com pesos realistas (nÃ£o uniformes)
- RelaÃ§Ãµes entre variÃ¡veis (e.g., age vs. plan_type)
- Reprodutibilidade (seed fixo)
- MÃºltiplos formatos (Parquet para ML, CSV para anÃ¡lise)
- Escala de 1 a 1M registros

---

#### 3.5.4 ğŸ¯ ValidaÃ§Ã£o de AcurÃ¡cia de Dados

**CenÃ¡rio**: Ricardo desenvolveu um pipeline de feature engineering que transforma dados brutos em features prontas para ML. Precisa validar se o pipeline estÃ¡ gerando features corretas comparando com features calculadas manualmente (GOLD standard).

**Problema Anterior**:
- Validar 50 features x 100k registros manualmente Ã© impraticÃ¡vel
- DiferenÃ§as numÃ©ricas pequenas (rounding) causam falsos positivos
- DifÃ­cil identificar padrÃµes nos erros
- Sem mÃ©trica objetiva de "quÃ£o correto" estÃ¡ o pipeline

**Como Ricardo Usa o DataForgeTest**: 

```
1. Acessa "Data Accuracy Validation"

2. Prepara datasets:
   a) GOLD:  features calculadas manualmente em notebook
      - 100,000 registros
      - 50 features numÃ©ricas
      - Processamento single-node (pandas)
      - Tempo: 3 horas
   
   b) TARGET: features do pipeline automatizado
      - 100,000 registros
      - 50 features numÃ©ricas
      - Processamento distribuÃ­do (PySpark)
      - Tempo: 5 minutos

3. Upload ambos datasets: 
   - features_gold.parquet (85MB)
   - features_pipeline.parquet (82MB)

4. Mapeia colunas:
   Key columns: ['customer_id', 'reference_date']
   Value columns: ['feature_1', 'feature_2', .. ., 'feature_50']

5. Configura normalizaÃ§Ã£o:
   â˜‘ï¸ Normalize keys (lowercase, trim)
   â˜ Remove accents (nÃ£o aplicÃ¡vel)
   â˜ Remove punctuation (nÃ£o aplicÃ¡vel)
   - Numeric tolerance: 0.001 (aceita diferenÃ§as de 0.1%)
   - Duplicate policy: keep_last

6. Executa comparaÃ§Ã£o (tempo:  45 segundos)

7. Recebe relatÃ³rio detalhado
```

**RelatÃ³rio de ValidaÃ§Ã£o**:

```json
{
  "summary": {
    "gold_records": 100000,
    "target_records": 100000,
    "matched_records": 99850,
    "accuracy": 99.85,
    "missing_in_target":  0,
    "extra_in_target": 0,
    "value_differences": 150
  },
  "column_accuracy": {
    "feature_1": {"accuracy": 100.0, "differences": 0},
    "feature_2": {"accuracy": 100.0, "differences": 0},
    "feature_3": {"accuracy": 99.92, "differences": 80},
    "feature_15": {"accuracy": 99.93, "differences": 70},
    "feature_28": {"accuracy": 100.0, "differences": 0},
    "... ": "..."
  },
  "difference_patterns": {
    "feature_3": {
      "type": "numeric_precision",
      "avg_difference": 0.0008,
      "max_difference": 0.0015,
      "affected_records": 80,
      "likely_cause": "floating_point_rounding"
    },
    "feature_15": {
      "type": "calculation_error",
      "avg_difference": 12.5,
      "max_difference": 45.2,
      "affected_records":  70,
      "likely_cause": "missing_normalization_step"
    }
  },
  "recommendations": [
    "Feature_15 shows systematic bias - review calculation logic",
    "Feature_3 differences within tolerance - likely rounding",
    "Overall pipeline accuracy excellent (99.85%)"
  ]
}
```

**AnÃ¡lise Detalhada**:

| Feature | Accuracy | Issue Type | Action |
|---------|----------|------------|--------|
| feature_1-2 | 100% | âœ… None | - |
| feature_3 | 99.92% | âš ï¸ Rounding | Acceptable |
| feature_4-14 | 100% | âœ… None | - |
| feature_15 | 99.93% | âŒ Logic error | **Fix required** |
| feature_16-50 | 100% | âœ… None | - |

**Ricardo Investiga feature_15**: 

```python
# Baixa CSV com diferenÃ§as
differences_df = pd.read_csv("accuracy_differences.csv")

# Filtra feature_15
feature_15_issues = differences_df[differences_df['column'] == 'feature_15']

print(feature_15_issues.head())

# Output:
#   customer_id  reference_date  column      gold_value  target_value  difference
#   cust_001     2024-01-15      feature_15  125.50      113.00        12.50
#   cust_002     2024-01-15      feature_15  230.80      185.60        45.20
#   cust_003     2024-01-15      feature_15  89.20       76.70         12.50

# Ricardo identifica o padrÃ£o: diferenÃ§a mÃ©dia de ~12.5
# Revisa cÃ³digo do pipeline e encontra: 
# âŒ ERRO:  feature_15 = sum(purchases) / count(days)
# âœ… CORRETO: feature_15 = sum(purchases) / count(distinct_days)

# Bug: contava dias duplicados, inflando o denominador
```

**AÃ§Ãµes que Ricardo Toma**:

1. **Corrige bug no pipeline**:
   ```python
   # Antes (errado)
   feature_15 = df. groupby('customer_id').agg(
       F.sum('purchase_amount') / F.count('purchase_date')
   )
   
   # Depois (correto)
   feature_15 = df.groupby('customer_id').agg(
       F.sum('purchase_amount') / F.countDistinct('purchase_date')
   )
   ```

2. **Re-executa pipeline** com correÃ§Ã£o

3. **Valida novamente** no DataForgeTest: 
   - **Nova Accuracy**: 99.998% âœ…
   - **feature_15**:  100% correto âœ…

4. **Documenta descoberta**:
   - Issue no JIRA:  "Pipeline bug in feature_15 calculation"
   - Post-mortem: "Caught before production thanks to validation"
   - Update de testes unitÃ¡rios

5. **Estabelece processo**:  
   - Todas as features devem passar por validaÃ§Ã£o GOLD
   - Threshold de aceite: 99.95% accuracy
   - Automated validation em CI/CD

**Resultados**:
- ğŸ› Bug crÃ­tico detectado **antes** de produÃ§Ã£o
- ğŸ’° Economia estimada: R$ 500k (custo de modelo incorreto em produÃ§Ã£o)
- â±ï¸ ValidaÃ§Ã£o de 100k x 50 features em < 1 minuto
- ğŸ“Š RelatÃ³rio detalhado facilita debugging

**BenefÃ­cios EspecÃ­ficos**: 
- TolerÃ¢ncia numÃ©rica evita falsos positivos
- AnÃ¡lise de padrÃµes de erro (nÃ£o apenas contagem)
- RelatÃ³rio exportÃ¡vel para documentaÃ§Ã£o
- IntegraÃ§Ã£o fÃ¡cil em pipelines de CI/CD
- Suporte a formatos Big Data (Parquet)

---

#### 3.5.5 ğŸŒŸ Test Dataset GOLD

**CenÃ¡rio**: Ricardo recebeu dados de uma API externa com 200k registros que precisa ingerir no data lake. Dados estÃ£o "sujos" com problemas comuns de APIs:  nulls, formatos inconsistentes, headers mal formatados.

**Problema Anterior**:
- Escrever script de limpeza custom para cada fonte de dados
- DifÃ­cil garantir que todas as limpezas necessÃ¡rias foram aplicadas
- Scripts de limpeza viram dÃ©bito tÃ©cnico (manutenÃ§Ã£o)
- Sem visibilidade do impacto de cada operaÃ§Ã£o

**Como Ricardo Usa o DataForgeTest**:

```
1. Acessa "Test Dataset GOLD"
2. Upload:  api_external_data.csv (200k linhas, 45MB)

3. Preview mostra problemas:
   
   Headers detectados:
   - " Customer ID  "  (espaÃ§os extras)
   - "Signup_Date"     (inconsistente com snake_case)
   - "Monthly-Revenue" (hÃ­fen em vez de underscore)
   - "Email Address  " (espaÃ§os extras)
   
   Sample data:
   - Dates: mix de "2024-01-15", "15/01/2024", "Jan 15 2024"
   - Numbers: mix de "1,500.00" (US), "1.500,00" (BR)
   - Strings: " value  " (whitespace extra)
   - Nulls: 15% em vÃ¡rias colunas importantes

4. Seleciona operaÃ§Ãµes (todas):
   â˜‘ï¸ Remove empty columns
   â˜‘ï¸ Normalize headers
   â˜‘ï¸ Trim strings  
   â˜‘ï¸ Coerce numeric values
   â˜‘ï¸ Parse dates
   â˜‘ï¸ Remove duplicate rows

5. Inicia processamento

6. Monitora progresso em tempo real: 
   [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 85% - Phase 4:  Removing duplicates
   
7. Recebe relatÃ³rio completo
```

**RelatÃ³rio de Limpeza**:

```
=== GOLD DATASET CLEANING REPORT ===

ğŸ“Š SUMMARY: 
  Rows:      200,000 â†’ 198,500 (-1,500 duplicates removed)
  Columns:   42 â†’ 39 (-3 empty columns removed)
  Nulls:     15.2% â†’ 8.5% (improved by 6.7%)
  Memory:    45 MB â†’ 42 MB

ğŸ”§ OPERATIONS APPLIED: 

1. EMPTY COLUMNS REMOVED (3):
   - column_38: 100% null
   - column_39: 100% null  
   - notes_field: 100% null

2. HEADERS NORMALIZED (39):
   " Customer ID  " â†’ "customer_id"
   "Signup_Date" â†’ "signup_date"
   "Monthly-Revenue" â†’ "monthly_revenue"
   "Email Address  " â†’ "email_address"
   [...  35 more ...]

3. STRINGS TRIMMED (42,500 changes):
   Column 'company_name':  8,200 values trimmed
   Column 'email_address': 15,300 values trimmed
   Column 'city': 12,000 values trimmed
   Column 'product_name': 7,000 values trimmed

4. NUMERIC VALUES COERCED (18,900 changes):
   Column 'monthly_revenue': 
     "1,500.00" â†’ 1500.00 (5,200 conversions)
     "1.500,00" â†’ 1500.00 (4,800 conversions)
   
   Column 'contract_value':
     "R$ 15. 000,00" â†’ 15000.00 (3,500 conversions)
     "$15,000.00" â†’ 15000.00 (2,900 conversions)
   
   Column 'discount_percentage':
     "15%" â†’ 15.0 (2,500 conversions)

5. DATES PARSED (35,600 changes):
   Column 'signup_date':
     "2024-01-15" â†’ 2024-01-15 (12,000 kept)
     "15/01/2024" â†’ 2024-01-15 (8,500 parsed)
     "Jan 15 2024" â†’ 2024-01-15 (6,200 parsed)
     Failed to parse: 85 values (set to null)
   
   Column 'contract_start_date':
     Mixed formats â†’ standardized (8,900 parsed)

6. DUPLICATES REMOVED (1,500):
   Duplicate key:  ['customer_id', 'signup_date']
   Policy: kept last occurrence

ğŸ“ˆ QUALITY IMPROVEMENT PER COLUMN: 

  Column               | Nulls Before | Nulls After | Improvement
  ---------------------|--------------|-------------|------------
  customer_id          | 0.0%         | 0.0%        | -
  email_address        | 2.5%         | 2.5%        | - (already clean)
  signup_date          | 18.0%        | 18.04%      | +0.04% (85 parse failures)
  monthly_revenue      | 25.0%        | 12.5%       | âœ… -12.5%
  contract_value       | 30.0%        | 15.0%       | âœ… -15.0%
  city                 | 5.0%         | 5.0%        | - (trimmed only)
  
ğŸ¯ RECOMMENDATIONS: 
  âœ… Dataset is now 91.5% complete (up from 84.8%)
  âš ï¸  Review 85 records with unparseable dates
  ğŸ’¡ Consider adding email format validation
  ğŸ’¡ 'monthly_revenue' still has 12.5% nulls - investigate source

âœ¨ CLEANED DATASET READY FOR DOWNLOAD
```

**Preview dos Dados Limpos (50 primeiras linhas)**:

| customer_id | email_address | signup_date | monthly_revenue | city |
|-------------|---------------|-------------|-----------------|------|
| cust_001 | john@example.com | 2024-01-15 | 1500.00 | sao paulo |
| cust_002 | maria@example.com | 2024-01-16 | 2300.00 | rio de janeiro |

**Como Ricardo Usa o Dataset Limpo**:

```python
# 1. Carrega dataset limpo
df_clean = spark.read.csv("api_external_data_cleaned.csv", header=True, inferSchema=True)

# 2. Valida que limpeza foi bem-sucedida
print("Nulls per column:")
df_clean.select([
    (F.sum(F.col(c).isNull().cast("int")) / F.count("*") * 100).alias(c)
    for c in df_clean.columns
]).show()

# 3. Aplica transformaÃ§Ãµes adicionais especÃ­ficas do negÃ³cio
df_transformed = df_clean \
    .withColumn('monthly_revenue_usd', F.col('monthly_revenue') / 5. 0) \
    .withColumn('customer_lifetime_months', 
                F.months_between(F.current_date(), F.col('signup_date')))

# 4. Escreve no data lake
df_transformed.write \
    .mode('overwrite') \
    .partitionBy('signup_date') \
    .parquet('s3://datalake/bronze/external_api/data/')

# 5. Registra no catÃ¡logo de dados
catalog. register_dataset(
    name='external_api_customers',
    path='s3://datalake/bronze/external_api/data/',
    schema=df_transformed.schema,
    quality_score=91.5,  # do relatÃ³rio GOLD
    cleaning_applied=True,
    last_updated=datetime. now()
)
```

**AutomatizaÃ§Ã£o com Airflow**:

```python
# DAG para ingestÃ£o diÃ¡ria de API externa
from airflow import DAG
from airflow.operators.python_operator import PythonOperator

def clean_api_data(**context):
    """Upload para DataForgeTest e baixa versÃ£o limpa"""
    import requests
    
    # 1. Baixa dados da API
    raw_data = fetch_from_api()
    
    # 2. Upload para DataForgeTest
    response = requests.post(
        'http://dataforgetest/api/gold/upload',
        files={'file': raw_data}
    )
    session_id = response.json()['sessionId']
    
    # 3. Inicia limpeza
    requests.post(
        'http://dataforgetest/api/gold/clean',
        json={
            'sessionId': session_id,
            'operations': ['normalize_headers', 'coerce_numeric', 
                          'parse_dates', 'remove_duplicates']
        }
    )
    
    # 4. Aguarda conclusÃ£o
    while True:
        status = requests.get(f'http://dataforgetest/api/gold/status? sessionId={session_id}')
        if status.json()['status'] == 'completed':
            break
        time. sleep(10)
    
    # 5. Baixa dataset limpo
    clean_data = requests.get(f'http://dataforgetest/api/gold/download/{session_id}/cleaned. csv')
    
    # 6. Upload para S3
    upload_to_s3(clean_data.content)
    
    # 7. Retorna mÃ©tricas
    report = requests.get(f'http://dataforgetest/api/gold/report? sessionId={session_id}').json()
    return report['quality_score']

dag = DAG('ingest_external_api', schedule_interval='@daily')

clean_task = PythonOperator(
    task_id='clean_api_data',
    python_callable=clean_api_data,
    dag=dag
)
```

**Resultados**:
- ğŸ§¹ 200k registros limpos em 2 minutos vs.  3 horas com script custom
- ğŸ“Š Qualidade melhorou de 84.8% para 91.5%
- ğŸ”„ Processo automatizado no Airflow
- ğŸ“ RelatÃ³rio detalhado para auditoria

**BenefÃ­cios EspecÃ­ficos**:
- OperaÃ§Ãµes padronizadas e testadas
- RelatÃ³rio detalhado mostra impacto de cada operaÃ§Ã£o
- Processamento em chunks para datasets grandes
- API endpoints para integraÃ§Ã£o em pipelines
- Status em tempo real para monitoramento

---

#### 3.5.6 ğŸ“Š Dataset Metrics

**CenÃ¡rio**: Ricardo precisa avaliar a qualidade de 5 datasets de diferentes fontes antes de criar um dataset consolidado para treinar um modelo.  Precisa de mÃ©tricas objetivas para decidir quais fontes usar.

**Problema Anterior**:
- AnÃ¡lise manual de qualidade Ã© subjetiva
- DifÃ­cil comparar qualidade entre datasets
- Sem baseline para definir "qualidade aceitÃ¡vel"
- GestÃ£o pede mÃ©tricas objetivas mas nÃ£o existem

**Como Ricardo Usa o DataForgeTest**:

```
Ricardo avalia 5 datasets: 
1. crm_customers. csv (fonte:  CRM interno)
2. ecommerce_orders.parquet (fonte: plataforma e-commerce)
3. marketing_leads.xlsx (fonte: ferramenta de marketing)
4. support_tickets.csv (fonte: sistema de suporte)
5. payment_transactions.parquet (fonte: gateway de pagamento)

Para cada dataset: 
1. Upload no "Dataset Metrics"
2. Aguarda anÃ¡lise automÃ¡tica (15-30 segundos)
3. Recebe dashboard com score
```

**ComparaÃ§Ã£o de Qualidade**:

| Dataset | Overall Score | Completeness | Uniqueness | Validity | Consistency | RecomendaÃ§Ã£o |
|---------|---------------|--------------|------------|----------|-------------|--------------|
| **crm_customers** | ğŸŸ¢ 92.5% | 95% | 98% | 90% | 87% | âœ… Approved |
| **ecommerce_orders** | ğŸŸ¢ 88.3% | 92% | 90% | 85% | 86% | âœ… Approved |
| **marketing_leads** | ğŸŸ¡ 72.8% | 70% | 85% | 68% | 68% | âš ï¸ Needs cleaning |
| **support_tickets** | ğŸŸ¢ 85.0% | 88% | 90% | 80% | 82% | âœ… Approved |
| **payment_transactions** | ğŸŸ¢ 95.2% | 98% | 100% | 92% | 91% | âœ… Excellent |

**AnÃ¡lise Detalhada - marketing_leads (72.8% - CrÃ­tico)**:

```
ğŸ“Š DATASET METRICS REPORT

Overall Quality Score: 72.8% âš ï¸
Rating:  FAIR (requires attention)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     QUALITY DIMENSIONS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Dimension      â”‚ Score    â”‚ Status                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Completeness   â”‚ 70.0%    â”‚ âŒ CRITICAL - Below 90% threshold      â”‚
â”‚ Uniqueness     â”‚ 85.0%    â”‚ âš ï¸ MODERATE - Some duplicates          â”‚
â”‚ Validity       â”‚ 68.0%    â”‚ âŒ CRITICAL - Many invalid values      â”‚
â”‚ Consistency    â”‚ 68.0%    â”‚ âŒ CRITICAL - Format inconsistencies   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸš¨ HIGH SEVERITY ISSUES: 

1. Completeness (70.0%):
   - Column 'email':  45% null âŒ
   - Column 'phone': 38% null âŒ
   - Column 'company':  25% null âš ï¸
   
   ğŸ’¡ Action: Contact data source to improve collection

2. Validity (68.0%):
   - Column 'email': 32% invalid format (not matching email regex)
   - Column 'lead_score': 15% out of range (should be 0-100)
   - Column 'country_code': 12% invalid (not ISO 3166)
   
   ğŸ’¡ Action: Apply validation rules and cleaning

3. Consistency (68.0%):
   - Column 'created_date': 3 different formats detected
     â€¢ "2024-01-15" (40%)
     â€¢ "15/01/2024" (35%)
     â€¢ "Jan 15, 2024" (25%)
   - Column 'lead_source': Case inconsistency
     â€¢ "Google Ads", "google ads", "GOOGLE ADS" (same value, 3 formats)
   
   ğŸ’¡ Action:  Standardize formats

âš ï¸ MEDIUM SEVERITY ISSUES:

4. Uniqueness (85.0%):
   - 15% duplicate records (combination of email + created_date)
   - Likely caused by multiple imports
   
   ğŸ’¡ Action:  Implement deduplication logic

ğŸ“Š COLUMN-LEVEL DETAILS:

High-Impact Columns (weight in model):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Column       â”‚ Completeâ”‚ Valid    â”‚ Consistentâ”‚ Overall   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ email        â”‚ 55%     â”‚ 68%      â”‚ 85%      â”‚ âŒ 62. 8%   â”‚
â”‚ phone        â”‚ 62%     â”‚ 72%      â”‚ 70%      â”‚ âŒ 66.4%   â”‚
â”‚ lead_score   â”‚ 100%    â”‚ 85%      â”‚ 100%     â”‚ âœ… 94.0%   â”‚
â”‚ company      â”‚ 75%     â”‚ 90%      â”‚ 80%      â”‚ âš ï¸ 79.5%   â”‚
â”‚ created_date â”‚ 100%    â”‚ 100%     â”‚ 68%      â”‚ âš ï¸ 83.6%   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ RECOMMENDATIONS (Prioritized):

HIGH PRIORITY:
1. â— Fix email validation (32% invalid) - blocks model training
2. â— Standardize date formats - causes parsing errors
3. â— Investigate high null rate in email/phone - contact source

MEDIUM PRIORITY: 
4. âš ï¸ Remove 15% duplicates
5. âš ï¸ Normalize lead_source values (case sensitivity)
6. âš ï¸ Validate lead_score ranges

LOW PRIORITY:
7. â„¹ï¸ Consider adding country_code validation
8. â„¹ï¸ Document data quality expectations with source
```

**DecisÃ£o de Ricardo**:

```
ğŸ“‹ DECISION MATRIX: 

âœ… USE DIRECTLY (Score > 85%):
   - payment_transactions (95.2%) - excelente qualidade
   - crm_customers (92.5%) - qualidade muito boa
   - ecommerce_orders (88.3%) - qualidade boa
   - support_tickets (85.0%) - qualidade aceitÃ¡vel

âš ï¸ CLEAN BEFORE USE (Score 70-85%):
   - marketing_leads (72.8%) - requer limpeza extensiva
   
âŒ REJECT (Score < 70%):
   - Nenhum neste lote

PLANO DE AÃ‡ÃƒO: 

1. Marketing Leads:
   a) Aplicar Test Dataset GOLD para limpeza automÃ¡tica
   b) Validar emails com regex e API externa
   c) Padronizar datas para ISO 8601
   d) Remover duplicatas
   e) Re-avaliar com Dataset Metrics
   f) Meta: atingir score > 85%

2. Modelo Consolidado:
   - Usar 4 datasets approved diretamente
   - Incluir marketing_leads apenas se atingir meta apÃ³s limpeza
   - Documentar scores de qualidade para auditoria
```

**ApÃ³s Limpeza de marketing_leads**:

```
Ricardo aplica Test Dataset GOLD + validaÃ§Ãµes customizadas

RESULTADO PÃ“S-LIMPEZA: 
  Overall Score: 72.8% â†’ 87.5% âœ… (+14.7%)
  
  Improvements:
  - Completeness: 70% â†’ 85% (emails/phones validados e corrigidos)
  - Uniqueness: 85% â†’ 95% (duplicatas removidas)
  - Validity: 68% â†’ 88% (formatos padronizados)
  - Consistency: 68% â†’ 90% (datas e strings normalizadas)

âœ… APROVADO para uso no modelo consolidado
```

**Modelo Final com MÃ©tricas Documentadas**:

```python
# Dataset consolidado com qualidade documentada
consolidated_df = spark.read.parquet('consolidated_customer_360/')

# Metadados de qualidade (para auditoria)
quality_metadata = {
    'dataset_name': 'customer_360_consolidated',
    'created_at': '2024-12-22',
    'source_datasets': [
        {'name': 'crm_customers', 'quality_score': 92.5, 'records': 500000},
        {'name': 'ecommerce_orders', 'quality_score': 88.3, 'records': 1200000},
        {'name': 'marketing_leads', 'quality_score': 87.5, 'records': 300000, 'cleaned': True},
        {'name':  'support_tickets', 'quality_score': 85.0, 'records': 800000},
        {'name': 'payment_transactions', 'quality_score': 95.2, 'records': 2000000}
    ],
    'overall_quality_score': 89.7,  # weighted average
    'total_records': 4800000,
    'quality_threshold': 85.0,
    'approved_by': 'ricardo. oliveira@company.com'
}

# Salva metadados junto com dataset
import json
with open('consolidated_customer_360/_metadata.json', 'w') as f:
    json.dump(quality_metadata, f, indent=2)

print("âœ… Dataset consolidado criado com qualidade documentada")
print(f"ğŸ“Š Overall Quality Score: {quality_metadata['overall_quality_score']}%")
```

**ApresentaÃ§Ã£o para Stakeholders**:

```
Ricardo prepara apresentaÃ§Ã£o com evidÃªncias objetivas: 

ğŸ“Š SLIDE 1: Quality Overview
  - 5 datasets avaliados
  - 4 approved diretamente (85%+)
  - 1 cleaned e approved (72% â†’ 87%)
  - Overall: 89.7% quality score

ğŸ“Š SLIDE 2: Quality Metrics
  [GrÃ¡fico de barras com scores por dataset]
  [GrÃ¡fico de radar com 4 dimensÃµes]

ğŸ“Š SLIDE 3: Cleaning Impact
  Marketing Leads Before/After:
  - Completeness: 70% â†’ 85%
  - Validity: 68% â†’ 88%
  - Investment: 2 horas de limpeza
  - Return: 300k registros recuperados

ğŸ“Š SLIDE 4: Model Confidence
  - Input data quality: 89.7% (excellent)
  - All sources above 85% threshold
  - Documented and auditable
  - Ready for production deployment

âœ… APROVAÃ‡ÃƒO UNÃ‚NIME da gestÃ£o
```

**Resultados**:
- ğŸ“Š 5 datasets avaliados objetivamente em < 10 minutos
- ğŸ¯ DecisÃµes baseadas em mÃ©tricas, nÃ£o intuiÃ§Ã£o
- ğŸ“ˆ 1 dataset melhorado de 72. 8% para 87.5%
- âœ… ConfianÃ§a para deploy em produÃ§Ã£o

**BenefÃ­cios EspecÃ­ficos**:
- MÃ©tricas padronizadas facilitam comparaÃ§Ã£o
- Scores objetivos suportam decisÃµes de negÃ³cio
- RecomendaÃ§Ãµes priorizadas por severidade
- DocumentaÃ§Ã£o automÃ¡tica para compliance
- Baseline para monitoramento contÃ­nuo de qualidade

---

#### 3.5.7 ğŸ’¬ Sistema RAG de Suporte

**CenÃ¡rio**: Ricardo estÃ¡ implementando uma validaÃ§Ã£o complexa de dados distribuÃ­dos e precisa entender nuances de PySpark, otimizaÃ§Ãµes e melhores prÃ¡ticas.

**Problema Anterior**:
- DocumentaÃ§Ã£o oficial do Spark Ã© extensa e genÃ©rica
- Stack Overflow tem respostas para casos gerais, nÃ£o especÃ­ficos
- DÃºvidas sobre DataForgeTest requerem consultar mÃºltiplos docs
- Tempo gasto em pesquisa reduz produtividade

**Como Ricardo Usa o DataForgeTest RAG**:

```
=== SESSÃƒO DE CHAT 1:  OtimizaÃ§Ã£o de Performance ===

Ricardo:  "Estou validando um dataset de 500M de registros e o job estÃ¡ 
         muito lento. Como otimizar validaÃ§Ãµes PySpark para datasets grandes?"

DataForgeTest RAG: "Para otimizar validaÃ§Ãµes PySpark em datasets grandes (100M+ registros), 
siga estas melhores prÃ¡ticas: 

ğŸš€ **1. USE CACHING ESTRATÃ‰GICO**
```python
# Cache o dataset se for usar mÃºltiplas vezes
df_cached = df.persist(StorageLevel.MEMORY_AND_DISK)

# Execute validaÃ§Ãµes
validation_results = run_all_validations(df_cached)

# Libere cache quando terminar
df_cached.unpersist()
```

âš¡ **2. OTIMIZE COM ADAPTIVE QUERY EXECUTION (AQE)**
```python
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

ğŸ“Š **3. AGRUPE VALIDAÃ‡Ã•ES SIMILARES**
```python
# âŒ EVITE:  MÃºltiplas passagens pelos dados
null_check_1 = df.filter(col('field1').isNull()).count()
null_check_2 = df.filter(col('field2').isNull()).count()

# âœ… PREFIRA: Uma Ãºnica passagem
validation_results = df.agg(
    F.sum(F.col('field1').isNull().cast('int')).alias('field1_nulls'),
    F.sum(F.col('field2').isNull().cast('int')).alias('field2_nulls')
).collect()[0]
```

ğŸ”§ **4. AJUSTE PARTICIONAMENTO**
Para 500M registros, recomendo: 
```python
# Reparticione para nÃºmero ideal (2-3x nÃºmero de cores)
num_executors = 50  # exemplo
num_cores_per_executor = 4
ideal_partitions = num_executors * num_cores_per_executor * 2

df_optimized = df.repartition(ideal_partitions)
```

â±ï¸ **BENCHMARKS ESPERADOS**:
- Dataset:  500M registros, 50 colunas
- Cluster:  50 executors, 4 cores each
- ValidaÃ§Ãµes: ~15-20 regras
- Tempo esperado: 5-10 minutos

ğŸ“š **Fonte**:  docs/PYSPARK_OPTIMIZATION_GUIDE.md
ğŸ’¡ **Dica**: Use o Spark UI (porta 4040) para identificar stages lentos"

---

Ricardo:  "Excelente! E sobre UDFs? Estou usando uma UDF Python para validar CPF.  
         Existe forma mais rÃ¡pida?"

DataForgeTest RAG:  "Sim! UDFs Python sÃ£o notoriamente lentas em PySpark devido 
Ã  serializaÃ§Ã£o Python â†” JVM. Para validaÃ§Ãµes complexas como CPF, considere:

ğŸš€ **OPÃ‡ÃƒO 1: PANDAS UDF (RECOMENDADO)**
AtÃ© 10-100x mais rÃ¡pido que UDF tradicional: 

```python
from pyspark.sql.functions import pandas_udf
import pandas as pd

@pandas_udf('boolean')
def validar_cpf_pandas(s:  pd.Series) -> pd.Series:
    def check_cpf(cpf_str):
        if pd.isna(cpf_str):
            return False
        cpf = ''.join(filter(str.isdigit, cpf_str))
        if len(cpf) != 11:
            return False
        # Algoritmo de validaÃ§Ã£o... 
        return True
    
    return s.apply(check_cpf)

# Uso
df_validated = df.withColumn('cpf_valido', validar_cpf_pandas(col('cpf')))
```

âš¡ **OPÃ‡ÃƒO 2: SPARK SQL + REGEX (MAIS RÃPIDO)**
Se validaÃ§Ã£o Ã© sÃ³ formato (sem dÃ­gito verificador):

```python
from pyspark.sql.functions import regexp_replace, length, col

# Remove caracteres nÃ£o numÃ©ricos e valida tamanho
df_validated = df.withColumn(
    'cpf_valido',
    (length(regexp_replace(col('cpf'), r'[^\d]', '')) == 11)
)
```

ğŸ”§ **OPÃ‡ÃƒO 3: PRÃ‰-PROCESSING**
Para validaÃ§Ã£o completa com dÃ­gitos: 

```python
# Carregue lista de CPFs vÃ¡lidos de sistema externo
cpfs_validos_df = spark.read.parquet('cpfs_validos. parquet')

# Join Ã© mais rÃ¡pido que UDF para grandes volumes
df_validated = df.join(
    cpfs_validos_df,
    on='cpf',
    how='left'
).withColumn('cpf_valido', col('cpf_valid_flag').isNotNull())
```

ğŸ“Š **BENCHMARK (1M CPFs)**:
- UDF Python tradicional: ~12 minutos
- Pandas UDF: ~1. 5 minutos (8x mais rÃ¡pido)
- Regex SQL: ~25 segundos (28x mais rÃ¡pido)
- Join prÃ©-calculado: ~15 segundos (48x mais rÃ¡pido)

ğŸ’¡ **RECOMENDAÃ‡ÃƒO PARA SEU CASO (500M registros)**:
Use Pandas UDF para melhor equilÃ­brio entre complexidade e performance. 

ğŸ“š **Fonte**: docs/UDF_OPTIMIZATION. md, docs/ADVANCED_VALIDATION_TECHNIQUES.md"

---

Ricardo: "Perfeito! Uma Ãºltima coisa:  como integro essas validaÃ§Ãµes no Airflow 
         para rodar diariamente?"

DataForgeTest RAG: "Ã“tima pergunta! Aqui estÃ¡ um exemplo completo de DAG 
do Airflow integrando validaÃ§Ãµes PySpark:

```python
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta
import requests

default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email':  ['data-alerts@company.com'],
    'retries': 2,
    'retry_delay':  timedelta(minutes=5)
}

dag = DAG(
    'daily_data_quality_validation',
    default_args=default_args,
    schedule_interval='0 2 * * *',  # 2 AM daily
    catchup=False
)

# Task 1: Gerar cÃ³digo de validaÃ§Ã£o com DataForgeTest
def generate_validation_code(**context):
    # Upload dataset para anÃ¡lise
    with open('/data/daily_export.parquet', 'rb') as f:
        response = requests.post(
            'http://dataforgetest/api/advanced-pyspark/upload',
            files={'file': f}
        )
    
    session_id = response.json()['sessionId']
    
    # Baixar cÃ³digo gerado
    code_response = requests.get(
        f'http://dataforgetest/api/advanced-pyspark/download/{session_id}/validation_code.py'
    )
    
    with open('/tmp/generated_validation.py', 'w') as f:
        f.write(code_response.text)
    
    return session_id

generate_code_task = PythonOperator(
    task_id='generate_validation_code',
    python_callable=generate_validation_code,
    dag=dag
)

# Task 2: Executar validaÃ§Ã£o PySpark
validate_task = SparkSubmitOperator(
    task_id='run_spark_validation',
    application='/tmp/generated_validation.py',
    conn_id='spark_default',
    conf={
        'spark.executor.memory': '8g',
        'spark.executor.cores': '4',
        'spark.dynamicAllocation.enabled': 'true',
        'spark.sql.adaptive.enabled': 'true'
    },
    application_args=[
        '--input', '/data/daily_export.parquet',
        '--output', '/data/validated/{{ ds }}/'
    ],
    dag=dag
)

# Task 3: Checar resultados e alertar se necessÃ¡rio
def check_validation_results(**context):
    # Ler relatÃ³rio de validaÃ§Ã£o
    import json
    with open('/data/validated/{{ ds }}/validation_report.json') as f:
        report = json.load(f)
    
    quality_score = report['overall_quality_score']
    
    if quality_score < 85:
        # Enviar alerta
        send_alert(
            severity='HIGH',
            message=f'Quality score below threshold: {quality_score}%',
            details=report
        )
        raise ValueError(f'Quality score {quality_score}% < 85%')
    
    # Log success
    log_quality_metrics(report)
    return quality_score

check_results_task = PythonOperator(
    task_id='check_validation_results',
    python_callable=check_validation_results,
    dag=dag
)

# Task 4: Atualizar catÃ¡logo de dados
def update_data_catalog(**context):
    quality_score = context['ti'].xcom_pull(task_ids='check_validation_results')
    
    catalog. update_dataset(
        name='daily_transactions',
        partition='{{ ds }}',
        quality_score=quality_score,
        validated=True,
        validated_at=datetime.now()
    )

update_catalog_task = PythonOperator(
    task_id='update_data_catalog',
    python_callable=update_data_catalog,
    dag=dag
)

# Definir dependÃªncias
generate_code_task >> validate_task >> check_results_task >> update_catalog_task
```

ğŸ“Š **MONITORAMENTO**:
Configure alertas para:
- Quality score < 85% â†’ Slack/Email HIGH priority
- Job failure â†’ PagerDuty alert
- Runtime > 15min â†’ Performance warning

ğŸ“ˆ **TRACKING DE QUALIDADE TEMPORAL**:
```python
# Adicione task para registrar histÃ³rico
def log_quality_trend(**context):
    score = context['ti'].xcom_pull(task_ids='check_validation_results')
    
    # Salvar em time series DB (e.g., InfluxDB)
    influx_client.write_points([{
        'measurement': 'data_quality',
        'tags': {'dataset': 'daily_transactions'},
        'time': datetime.now(),
        'fields': {'quality_score': score}
    }])
    
    # Detectar degradaÃ§Ã£o (score caindo > 5% em 7 dias)
    recent_scores = get_recent_scores(days=7)
    if score < min(recent_scores) - 5:
        send_alert('Quality degradation detected!')
```

ğŸ”” **DASHBOARD**:
Visualize mÃ©tricas em Grafana/Superset:
- Quality score over time (line chart)
- Quality dimensions breakdown (radar chart)
- Alert history (table)

ğŸ“š **Fonte**: docs/AIRFLOW_INTEGRATION.md, docs/PRODUCTION_DEPLOYMENT.md

ğŸ’¡ **DICA EXTRA**: Para ambientes de produÃ§Ã£o crÃ­ticos, considere:
1. ValidaÃ§Ã£o em staging antes de produÃ§Ã£o
2. Rollback automÃ¡tico se quality_score < threshold
3. Blue/green deployment de dados validados"

=== FIM DA SESSÃƒO ===
```

**Valor Entregue pelo RAG**: 

1. **Respostas Contextualizadas**:
   - NÃ£o apenas teoria, mas cÃ³digo pronto para usar
   - EspecÃ­fico para o contexto (500M registros)
   - Benchmarks realistas

2. **Aprendizado Progressivo**:
   - Ricardo vai de dÃºvida pontual para soluÃ§Ã£o completa
   - Aprende otimizaÃ§Ãµes que nÃ£o conhecia (Pandas UDF)
   - Descobre integraÃ§Ãµes (Airflow)

3. **Economia de Tempo**:
   - 3 perguntas complexas respondidas em 10 minutos
   - Alternativa: 2-3 horas de pesquisa em docs/Stack Overflow
   - **Economia: ~2.5 horas**

4. **Qualidade das Respostas**:
   - CÃ³digo funcional e testado
   - ReferÃªncias Ã  documentaÃ§Ã£o fonte
   - Melhores prÃ¡ticas incluÃ­das
   - Dicas proativas (Spark UI, monitoramento)

**Resultados**:
- â±ï¸ Economia de ~2.5 horas de pesquisa
- ğŸ’¡ Descoberta de otimizaÃ§Ãµes (Pandas UDF:  8x mais rÃ¡pido)
- ğŸ”§ SoluÃ§Ã£o completa de integraÃ§Ã£o (Airflow DAG)
- ğŸ“š Aprendizado tÃ©cnico (validaÃ§Ãµes distribuÃ­das)

**BenefÃ­cios EspecÃ­ficos**:
- Busca semÃ¢ntica em documentaÃ§Ã£o indexada
- Respostas estruturadas com cÃ³digo
- Contexto especÃ­fico do DataForgeTest
- Streaming para respostas longas
- ReferÃªncias sempre citadas
- Proatividade (sugere tÃ³picos relacionados)

---

### 3.6 Fluxo de Trabalho TÃ­pico (Dia de Ricardo)

```
07:30 - InÃ­cio do dia, revisa dashboard de qualidade de dados
      â†’ Quality score de ontem: 87. 2% (dentro do threshold)

08:00 - Sprint planning:  novo modelo de propensÃ£o de compra
      â†’ Precisa de dados limpos de 3 fontes

08:30 - Usa Dataset Metrics para avaliar 3 datasets candidatos
      â†’ CRM:  92%, E-commerce: 88%, Marketing: 73%
      
09:00 - Usa Test Dataset GOLD para limpar dataset de marketing
      â†’ Score melhora de 73% para 86%
      
09:30 - Coffee meeting com time de engenharia

10:00 - Usa Gerador AvanÃ§ado PySpark para analisar dataset de 2GB
      â†’ Upload, anÃ¡lise automÃ¡tica, gera DSL + cÃ³digo
      â†’ Documenta schema no Git
      
11:00 - Integra cÃ³digo de validaÃ§Ã£o no Airflow
      â†’ Copia DAG template do RAG
      â†’ Ajusta para seu caso especÃ­fico
      
12:00 - AlmoÃ§o

13:00 - Treina modelo, mas accuracy estÃ¡ baixa (72%)
      â†’ Suspeita de problema nos dados

13:30 - Usa ValidaÃ§Ã£o de AcurÃ¡cia para comparar features calculadas
      â†’ Detecta bug:  feature_15 com erro de cÃ¡lculo
      â†’ Accuracy do pipeline: 99.85% (150 registros com problema)
      
14:00 - Corrige bug no pipeline de features
      â†’ Re-processa dados
      
14:30 - Re-treina modelo com dados corrigidos
      â†’ Accuracy melhora:  72% â†’ 89% âœ…
      
15:00 - Precisa de 500k registros sintÃ©ticos para experimentos
      â†’ Usa GeraÃ§Ã£o de Dados SintÃ©ticos
      â†’ Define schema complexo com 18 colunas
      â†’ Aguarda 15 minutos enquanto trabalha em documentaÃ§Ã£o
      
15:30 - Usa RAG para tirar dÃºvida sobre otimizaÃ§Ã£o de UDFs
      â†’ Descobre Pandas UDF (8x mais rÃ¡pido)
      â†’ Refatora cÃ³digo crÃ­tico
      
16:00 - Deploy de modelo em staging
      â†’ Valida com dados sintÃ©ticos primeiro
      â†’ Performance OK, deploy em produÃ§Ã£o

16:30 - DocumentaÃ§Ã£o final do projeto
      â†’ Inclui DSLs, quality scores, benchmarks
      â†’ Compartilha com time

17:00 - ApresentaÃ§Ã£o de resultados para stakeholders
      â†’ Demonstra melhorias de qualidade com mÃ©tricas objetivas
      â†’ AprovaÃ§Ã£o para prÃ³xima fase do projeto

17:30 - Fim do dia - satisfaÃ§Ã£o com entregas
```

**MÃ©tricas do Dia**:
- âœ… Novo modelo treinado e deployado (accuracy 89%)
- âœ… 1 bug crÃ­tico detectado e corrigido (antes de produÃ§Ã£o)
- âœ… 3 datasets avaliados e 1 limpo (73% â†’ 86%)
- âœ… 500k registros sintÃ©ticos gerados
- âœ… 1 pipeline otimizado (8x mais rÃ¡pido)
- âœ… Schema documentado em DSL versionado
- âœ… ValidaÃ§Ãµes integradas no Airflow
- â±ï¸ Economia estimada: 6 horas de trabalho manual

---

## 4. Matriz de Funcionalidades por Persona

| Funcionalidade | Testador/QA | Cientista de Dados | BenefÃ­cio PrimÃ¡rio |
|---------------|-------------|-------------------|-------------------|
| **ğŸ¤– GeraÃ§Ã£o de CÃ³digo PySpark** | â­â­â­â­â­ | â­â­â­â­ | Autonomia tÃ©cnica (QA), Velocidade (DS) |
| **ğŸ” Gerador AvanÃ§ado PySpark** | â­â­â­â­ | â­â­â­â­â­ | AnÃ¡lise rÃ¡pida (QA), DocumentaÃ§Ã£o automÃ¡tica (DS) |
| **ğŸ² GeraÃ§Ã£o de Dados SintÃ©ticos** | â­â­â­â­â­ | â­â­â­â­â­ | Cobertura de testes (QA), Privacy-preserving ML (DS) |
| **ğŸ¯ ValidaÃ§Ã£o de AcurÃ¡cia** | â­â­â­â­â­ | â­â­â­â­ | ValidaÃ§Ã£o de migraÃ§Ã£o (QA), Debugging de pipelines (DS) |
| **ğŸŒŸ Test Dataset GOLD** | â­â­â­â­ | â­â­â­â­â­ | Limpeza de arquivos externos (QA), PreparaÃ§Ã£o de dados (DS) |
| **ğŸ“Š Dataset Metrics** | â­â­â­â­â­ | â­â­â­â­â­ | EvidÃªncias para relatÃ³rios (QA), DecisÃµes data-driven (DS) |
| **ğŸ’¬ Sistema RAG** | â­â­â­â­ | â­â­â­â­â­ | Autonomia tÃ©cnica (QA), OtimizaÃ§Ãµes avanÃ§adas (DS) |

**Legenda**:
- â­â­â­â­â­ Essencial / Uso DiÃ¡rio
- â­â­â­â­ Muito Ãštil / Uso Frequente
- â­â­â­ Ãštil / Uso Ocasional

---

## 5. Jornadas de UsuÃ¡rio

### 5.1 Jornada do Testador/QA:  "ValidaÃ§Ã£o de Nova Feature"

```mermaid
graph TD
    A[Nova Feature para Testar] --> B{Dados de Teste Existem?}
    B -->|NÃ£o| C[GeraÃ§Ã£o de Dados SintÃ©ticos]
    B -->|Sim, mas sujos| D[Test Dataset GOLD]
    C --> E[Dataset Metrics - Avaliar Qualidade]
    D --> E
    E --> F{Qualidade OK?}
    F -->|NÃ£o < 85%| D
    F -->|Sim >= 85%| G[QA Checklist - Gerar ValidaÃ§Ãµes]
    G --> H[Executar Testes]
    H --> I{Resultados Esperados?}
    I -->|NÃ£o| J[ValidaÃ§Ã£o de AcurÃ¡cia - Comparar]
    J --> K[Identificar DivergÃªncias]
    K --> L[Reportar Bugs]
    I -->|Sim| M[Documentar EvidÃªncias]
    L --> N[Feature Aprovada para ProduÃ§Ã£o]
    M --> N
```

**Tempo Total**: 4-6 horas (vs. 2-3 dias manualmente)
**Taxa de Sucesso**: 95% de bugs detectados antes de produÃ§Ã£o

---

### 5.2 Jornada do Cientista de Dados: "Novo Modelo de ML"

```mermaid
graph TD
    A[Novo Projeto de ML] --> B[Identificar Fontes de Dados]
    B --> C[Dataset Metrics - Avaliar Todas as Fontes]
    C --> D{Qualidade Suficiente?}
    D -->|NÃ£o| E[Test Dataset GOLD - Limpar]
    D -->|Sim| F[Gerador AvanÃ§ado - Documentar Schema]
    E --> F
    F --> G[Gerar Dados SintÃ©ticos para Experimentos]
    G --> H[Feature Engineering]
    H --> I[ValidaÃ§Ã£o de AcurÃ¡cia - Testar Pipeline]
    I --> J{Pipeline Correto?}
    J -->|NÃ£o| K[Debug e CorreÃ§Ã£o]
    K --> I
    J -->|Sim| L[Treinar Modelo]
    L --> M[RAG - OtimizaÃ§Ãµes de Performance]
    M --> N[Deploy em ProduÃ§Ã£o]
```

**Tempo Total**: 2-3 dias (vs. 1-2 semanas manualmente)
**Qualidade de Dados**: > 90% (vs. ~70% sem validaÃ§Ã£o)

---

## 6. MÃ©tricas de Sucesso

### 6.1 MÃ©tricas por Persona

#### Testador/QA

| MÃ©trica | Sem DataForgeTest | Com DataForgeTest | Melhoria |
|---------|-------------------|-------------------|----------|
| **Tempo de criaÃ§Ã£o de dados de teste** | 2 dias | 2 horas | ğŸš€ 87. 5% |
| **Cobertura de casos de teste** | 60% | 95% | ğŸ“ˆ +58% |
| **Bugs detectados antes de produÃ§Ã£o** | 70% | 95% | ğŸ› +36% |
| **DependÃªncia de outras equipes** | Alta | Baixa | â¬‡ï¸ -80% |
| **Tempo de criaÃ§Ã£o de validaÃ§Ãµes PySpark** | 3 dias | 30 min | ğŸš€ 99% |

#### Cientista de Dados

| MÃ©trica | Sem DataForgeTest | Com DataForgeTest | Melhoria |
|---------|-------------------|-------------------|----------|
| **Tempo de preparaÃ§Ã£o de dados** | 60% do projeto | 30% do projeto | â±ï¸ 50% |
| **Qualidade de dados de entrada** | ~70% | >90% | ğŸ“Š +29% |
| **Tempo para detectar bugs em pipelines** | 2-3 dias | 30 min | ğŸ” 98% |
| **Compliance com LGPD** | Arriscado | 100% seguro | ğŸ”’ Total |
| **Tempo de documentaÃ§Ã£o de schema** | 4 horas | 5 min | ğŸ“ 98% |

### 6.2 MÃ©tricas de NegÃ³cio

| MÃ©trica | Impacto Anual Estimado |
|---------|------------------------|
| **ReduÃ§Ã£o de bugs em produÃ§Ã£o** | R$ 2M+ economizados |
| **Aumento de produtividade (QA)** | 40% mais testes entregues |
| **Aumento de produtividade (DS)** | 2x mais modelos deployados |
| **ReduÃ§Ã£o de retrabalho** | 60% menos correÃ§Ãµes pÃ³s-produÃ§Ã£o |
| **Melhoria em time-to-market** | 30% mais rÃ¡pido |

---

## 7. ConclusÃ£o

### 7.1 SÃ­ntese das Personas

**Testador/QA (Marina)** encontra no DataForgeTest:
- âœ… **Autonomia tÃ©cnica** sem depender de engenharia
- âœ… **Velocidade** na criaÃ§Ã£o de dados e validaÃ§Ãµes
- âœ… **Qualidade** com cobertura completa de testes
- âœ… **EvidÃªncias objetivas** para relatÃ³rios
- âœ… **Aprendizado progressivo** de tecnologias Big Data

**Cientista de Dados (Ricardo)** encontra no DataForgeTest: 
- âœ… **EficiÃªncia** com 50% menos tempo em preparaÃ§Ã£o de dados
- âœ… **Qualidade garantida** com validaÃ§Ãµes automatizadas
- âœ… **Compliance** com geraÃ§Ã£o de dados sintÃ©ticos
- âœ… **DocumentaÃ§Ã£o automÃ¡tica** de schemas e qualidade
- âœ… **OtimizaÃ§Ãµes** com RAG inteligente

### 7.2 Proposta de Valor Unificada

O **DataForgeTest** Ã© a ponte entre:
- **Testadores** que precisam de autonomia tÃ©cnica
- **Cientistas de Dados** que precisam de eficiÃªncia
- **OrganizaÃ§Ãµes** que precisam de qualidade e compliance

### 7.3 Diferenciais Competitivos

1. **Interface Conversacional**: LLM elimina curva de aprendizado
2. **Dados SintÃ©ticos Realistas**: Privacy-preserving ML Ã© possÃ­vel
3. **ValidaÃ§Ã£o Inteligente**: DetecÃ§Ã£o automÃ¡tica de padrÃµes e anomalias
4. **DocumentaÃ§Ã£o AutomÃ¡tica**: DSLs e relatÃ³rios prontos para auditoria
5. **IntegraÃ§Ã£o Completa**: Colab, Airflow, CI/CD ready

### 7.4 VisÃ£o de Futuro

Com a adoÃ§Ã£o do DataForgeTest, organizaÃ§Ãµes podem alcanÃ§ar:
- **Data Quality as Code**: ValidaÃ§Ãµes versionadas e reprodutÃ­veis
- **Shift-Left Testing**: Qualidade garantida desde o inÃ­cio
- **Privacy-First ML**: Modelos treinados sem expor dados sensÃ­veis
- **Data Democratization**: Acesso a ferramentas avanÃ§adas para todos

---

<div align="center">

**DataForgeTest**  
*Empoderando Testadores e Cientistas de Dados com IA*

---

*Para mais informaÃ§Ãµes, visite:  [https://data-forge-test.vercel.app](https://data-forge-test.vercel.app)*  
*RepositÃ³rio:  [https://github.com/Icar0S/DataForgeTest](https://github.com/Icar0S/DataForgeTest)*

</div>